# 深度學習算法1-5_XAI

[toc]
<!-- toc --> 

# XAI

## News

- [深度學習模型為何建議你這樣做？要靠解釋性AI來告訴你（下） | iThome](https://www.ithome.com.tw/news/126281)

    > 例如華盛頓大學3位研究人員就提出了一套LIME（Local Interpretable Model-agnostic Explanations）框架，在訓練出DNN模型後，再針對一筆訓練資料（一張圖或一句話）中的局部資料進行干擾，來觀察是否會影響預測結果，而來判斷訓練資料中的那個部分對預測結果，具有更關鍵的作用，例如就可以判斷出，一張狗彈吉他照片，狗臉是辨識出動物物種的關鍵區域。這個作法也引起不少業界關注，例如Teradata技術長也特別看重ＬＩＭＥ的應用。
    > 
    > 不同於LIME是從訓練資料來尋找解釋力的作法，神經元刪除法則是從訓練後的神經網路模式來尋找解釋力。由於在深度神經網絡模型中，利用大量神經元來訓練模型，DeepMind團隊透過刪除獨立的特定神經元，或在隱藏層中的神經元，來觀察對模型預測力的影響。不過，DeepMind發現，那些可解釋的獨立神經元，對模型的效力不如那些「不可解釋」的神經元，其次，越有能力分辨未知圖片的神經網絡模型，越不受刪除神經元的影響。
    > 
    > 例如康乃爾大學兩位研究者就曾在2013年發表了一篇卷積神經網路視覺化的論文《Visualizing and Understanding Convolutional Networks》，這是CNN模型視覺化的開始，試圖透過反向推算，來找出每一層如何所辨識的圖片特徵，來說明CNN模型的辨識原因。
    > 
    > 而在長期作法上，沈向洋認為，還是得回到因果關係的處理。例如微軟正從張量空間著手（Tensor Space），試圖要建立一個數學模型，來串連符號AI和神經網絡兩種AI技術，希望能做到兼顧DNN的優勢，又能提供可解釋的能力。
    > 


## 可解釋性

- [深度学习的可解释性研究（一）—— 让模型具备说人话的能力](https://zhuanlan.zhihu.com/p/37223341)

- [AI.Why: A new architecture for creating AI models with interpretability from scratch (Part 1) | LinkedIn](https://www.linkedin.com/pulse/aiwhy-new-architecture-creating-ai-models-from-part-1-figurelli/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BgSQgo6WFQ%2Fy6K28sPHjiiA%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_flagship3_pulse_read-related)


### [1802.00614] Visual Interpretability for Deep Learning: a Survey

- [[1802.00614] Visual Interpretability for Deep Learning: a Survey](https://arxiv.org/abs/1802.00614)

    - [(1 封私信 / 83 条消息)如何理解Visual Interpretability这一系列paper的原理以及insight？ - 知乎](https://www.zhihu.com/question/266875930)

    > 谢邀。作为这两篇文章的作者，我尽力回答。关于interpretability of neural networks的研究近几年才刚刚起步，我和UCLA的团队成员近年来在该方向总结了七个工作（参见Visual Interpretability for Deep Learning: a Survey）。我们的目的依然是尽可能的定义清楚network interpretability所涉及的概念以及研究范围。
    > 
    > 在此之前，network interpretability的研究主要集中在两个方向上：1）network visualization and diagnosis，2）evaluation of network interpretability。在第一个方向上，人们提出了很多神经网络可视化的算法，去显示网络中某个filter所建模的视觉特征。另一些算法定位出与某个neural activation或某个网络输出密切相关的图像区域。一些学者通过挖掘神经网络的对抗性样本（adversarial samples）去分析网络的表达缺陷。第二个研究方向是提出一个对pre-trained network的可解释性的第三方的评测标准。如何定义一个网络内部表达的可解释性和如何评测可解释性依然是个相对开放的问题。在这个方向上，@周博磊CVPR17的文章[1]比较有代表性。
    > 
    > 相比之下，我们团队希望探索更深层次的网络可解释性，即 _**建立一个从神经网络内部表达到语义图模型的映射关系**_ ，以及进一步探讨基于可解释性在具体应用中的实际意义（比如与网络中层语义的交互）。
    > 
    > 比如，一个重要的思路就是，我们希望用一个explanatory model去归纳总结一个pre-trained CNN各个层上的主要特征表达。我们用这个explanatory model去解释CNN内部特征的潜在语义，使得CNN内部表达不再是个黑箱，而不是用这个explanatory model去做具体的分类分割等任务。在[2,3]中，我们希望把一个pre-trained CNN的conv-layers上的主要特征表达，拆分成一个可解释的图模型，即explanatory graph。这个explanatory graph作为这个CNN的特征翻译器，去解释CNN中每个filter的每个神经触发背后潜在的语义。一个CNN中的每个conv-layer的每个filter往往潜在地建模了多种不同的object parts，即这个filter可能被不同的parts所触发。我们的工作就是无监督地把这些filters的混乱的feature maps拆分成不同的part组份，从而增加网络的可解释性。具体的介绍请参考<https://zhuanlan.zhihu.com/p/31365150>。进一步在[4]中，我们用一个decision tree，作为一个解释性模型，去总结一个CNN在fully-connected layers上的不同decision modes。当一个CNN为某个图像做出一个prediction的时候，decision tree可以给出具体某个filter对这个prediction定量的贡献值，进一步给出某个object part对prediction的定量的贡献。
    > 
    > 我们的第二个思路是端对端的学习一个神经网络，使得其内部表达本身就是充分disentangled，充分可理解的。比如在[5]的interpretable CNN的研究中，我们在不标注object parts或texture的情况下，让算法自动的为CNN的每个高层filter去表示某个特定的object part语义，从而增加网络的可解释性。具体的介绍请参 <https://zhuanlan.zhihu.com/p/30074544>。
    > 
    > 归根结底，以上的研究通过不同的方法建立了从神经网络内部表达到语义图模型的映射关系。有了这样的映射关系以后，我们希望对神经网络的训练不仅仅是end-to-end的，而且可以middle-to-end和end-to-middle的学习。即，我们可以与网络的中层语义表达进行交互。当训练一个新的模型，我们可以根据网络的中层语义像搭积木一样去搭建一个新模型，而不是端对端的从底层学习特征表达。这种通过网络可解释性的迁移学习的思路，可以大大降低训练网络的标注成本，做到few-shot learning。比如在[6]和[7]中，我们利用对CNN内部特征的图模型表达，提出了通过人机问答交互方式去高效的学习新的模型。
    > 
    > [1] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In CVPR, 2017.
    > 
    > [2] Q. Zhang, R. Cao, F. Shi, Y. N. Wu, and S.-C. Zhu. Interpreting cnn knowledge via an explanatory graph. In AAAI, 2018.
    > 
    > [3] Q. Zhang, R. Cao, Y. N. Wu, and S.-C. Zhu. Growing interpretable part graphs on convnets via multi-shot learning. In AAAI, 2016.
    > 
    > [4] Quanshi Zhang, Yu Yang, Ying Nian Wu, and Song-Chun Zhu. Interpreting cnns via decision trees. arXiv:1802.00121, 2018.
    > 
    > [5] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolutional neural network. In arXiv:1710.00935, 2017.
    > 
    > [6] Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu. Mining object parts from cnns via active question-answering. In CVPR, 2017.
    > 
    > [7] Quanshi Zhang, Ruiming Cao, Shengming Zhang, Mark Edmonds, Ying Nian Wu, and Song-Chun Zhu. Interactively transferring cnn patterns for part localization. In arXiv:1708.01783, 2017.

### XNN：打開了自己黑箱的神經網絡

- [[1806.01933] Explainable Neural Networks based on Additive Index Models](https://arxiv.org/abs/1806.01933)

- [The Explainable Neural Network – Shagun Maheshwari – Medium](https://medium.com/@shagunm1210/the-explainable-neural-network-8f95256dcddb)

- [XNN：打開了自己黑箱的神經網絡 - 幫趣](http://bangqu.com/ZpsRD7.html)

    > 將人工神經網絡應用到特定系統的最大障礙之一是它的「黑箱」屬性。XNN（可解釋的神經網絡）是一種旨在「打開並解釋」神經網絡黑箱的新模型。
    > 
    > 使用人工神經網絡和機器學習算法訓練機器像人腦一樣學習信息已經越來越流行。這讓機器能夠準確地輸出給定任務的預測結果。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD71539086147203A65a5.png)
    > 
    > *在給定任務下訓練的 ANN 能進行貓和狗的分類*
    > 
    > 再舉一個例子，假定你想要訓練一臺機器，使其能根據客戶憑證確定銀行的哪些客戶離開銀行的概率最高。
    > 
    > 神經網絡將在包含每個客戶憑證（例如信用評分）的大數據集上進行訓練。它通過變量選擇識別數據中的哪些特徵對客戶的去留影響最大。神經網絡將學習這些特徵，並通過學習特徵，根據其憑證（特徵）自行準確預測哪些客戶離開銀行的概率最高。（參見：https://www.linkedin.com/pulse/creating-deep-neural-net-accurately-predict-churn-rate-maheshwari/）
    > 
    > 這些模型對大型數據集非常有效，因爲在大型數據集中很難進行手動的變量選擇和特徵工程。另外，與傳統統計方法相比，它們具有更好、更準確的預測性能。
    > 
    > **黑箱**
    > 
    > 然而 ANN 的一個問題在於，它們是黑箱。這意味着數據分析師或建模人員很難解釋輸入特徵與其響應（輸出）之間的關係。神經網絡越複雜意味着添加的神經元和層數越多，這使解釋和確定 ANN 中的哪個函數導致輸出變得愈發困難。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD715390861476996pLYv.png)
    > 
    > **應用**
    > 
    > 理解黑箱並具備解釋模型行爲的能力非常重要，這是因爲機器學習模型和 ANN 的使用需要納入醫療保健和金融等領域。事實證明，機器學習模型有可能徹底改變這些行業，提高某些任務的效率。
    > 
    > 瘋狂的是，研究人員實際上能創建一種機器學習算法來識別乳腺癌患者身體組織圖像中的乳腺癌模式，而且效果比人類病理學家還要好！機器學習模型能夠更快地識別乳腺癌模式，準確率達 89%，高於訓練有素的人類病理學家的平均準確率 73％！這只是 ANN 和機器學習模型多種實現方式的一個示例，它們能提供比傳統人類方法更高效、準確的工具。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD715390861486098s6ff.png)
    > 
    > 然而，儘管這些模型的準確率和效率已經經過驗證，但人們仍在猶豫是否將它們納入醫療保健和銀行業等領域，因爲這些模型具備黑箱屬性。解釋模型並解釋其行爲的能力對於這些行業至關重要，因爲它們涉及處理高風險問題並且必須受到嚴格監管。
    > 
    > **可解釋神經網絡**
    > 
    > 可解釋神經網絡（XNN）是機器學習模型的一項新進展，旨在爲模型提供可解釋的洞察力，消除其黑箱屬性。這意味着它能夠提供關於特徵以及網絡完成輸出（預測）過程中所學得的非線性變換的直白解釋。通過該模型，研究者能清楚地解釋輸入特徵與複雜神經網絡輸出之間的關係，因爲 XNN 網絡結構包含解釋這種關係的機制，並能對可視化該網絡所學習的函數起到幫助作用。
    > 
    > XNN 基於加性索引模型的概念，如下所示：
    > 
    > f (x) = g1 β1T x+ g2 β2T x+ - - - + gK βKT x
    > 
    > 左側的函數可以表示爲 K 個平滑函數 gi(-) 的和。這些平滑函數（即嶺函數）都用於在網絡中訓練的輸入特徵的線性組合（βiT x）。這使得加性索引模型能夠提供靈活的框架，通過嶺函數逼近網絡內的任意複雜函數，從而提供關於特徵和網絡學得的非線性變換的解釋。
    > 
    > **可解釋神經網絡架構**
    > 
    > 可解釋神經網絡提供加性索引模型的替代公式作爲結構化神經網絡。XNN 內置瞭解釋機制，這有助於解釋和理解模型內部過程以及該模型學到的函數。
    > 
    > 替代公式如下：
    > 
    > f (x) = μ + γ1 h1 β1T x+ γ2 h2 β1T x+ - - - + γK hK βKT x
    > 
    > 位移參數 μ 和尺度參數 γk 被用於模型擬合：通過正則化選擇適當數量的嶺函數。
    > 
    > XNN 結構中三個重要的組成部分包括：
    > 
    > i) 投影層（第一個隱藏層）；
    > 
    > ii) 子網絡（下圖的中間部分）；
    > 
    > iii) 組合層（最後的隱藏層）。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD715390861502396379I.png)
    > 
    > *XNN 結構*
    > 
    > **投影層**
    > 
    > 輸入層包含將輸入神經網絡的所有信息。輸入層全連接到投影層，在投影層上傳遞特徵（信息）。投影層由 K 個節點組成（每個節點對應一個嶺函數）。第一個隱藏層中的節點 i 的權重對應相應嶺函數輸入的係數（βi）。嶺函數有助於逼近輸入特徵中的複雜函數。投影層使用線性激活函數，以確保該層中的每個節點都學習輸入特徵的線性組合。應用了嶺函數的投影層中每個節點的輸出恰好可以用作一個子網絡的輸入。
    > 
    > **子網絡**
    > 
    > 子網絡主要用於學習應用於輸入特徵的嶺函數。嶺函數對於子網絡而言非常重要，因爲它們逼近投影層的複雜函數。這使得子網絡更容易學習並提供所習得的嶺函數的可解釋性，從而使數據分析師有能力理解子網絡的運作、理解從輸入到輸出的過程。子網絡只需要有足夠的結構，使每一個子網絡都能學習大量單變量函數。在模擬中，研究者發現，使用由具有非線性激活函數的兩個隱藏層組成的子網絡，足以在擬合模型時學習足夠靈活的嶺函數。
    > 
    > **組合層**
    > 
    > 組合層是 XNN 的最後一個隱藏層，由單個節點組成。節點的輸入包括嶺函數的所有輸出以及在子網絡中學習和添加的權重。在該層上使用線性激活函數，因此整個網絡的輸出是所有嶺函數的加權和的線性組合。
    > 
    > **XNN 組件可視化**
    > 
    > 內置於 xNN 中的結構（如投影層和子網絡）提供了一種機制來解釋這種網絡所學習的函數。該內置機制用投影和單變量嶺函數這些相對簡單的術語描述模型學得的複雜函數，以確保模型可解釋。
    > 
    > 下圖展示了研究者如何解釋和可視化子網絡中的單變量變換和嶺函數。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD71539086151909i173u.png)
    > 
    > *嶺函數（左）和訓練的 XNN 的對應投影索引（右）。*
    > 
    > 第一列解釋了單變量函數，其中包含由子網絡學習到達其輸出的權重。第二列顯示 βi 的值，即投影係數。投影係數說明輸入特徵的哪個組合在通過子網絡之前用作了每個嶺函數的輸入。這非常有用，因爲上圖展示了網絡最相關的特徵：縮放嶺函數和投影係數。
    > 
    > 在上面的例子中，從 xNN 的結構我們可以看出 Subnetwork 1 已經學習了立方勒讓德函數 (f3(-))，Subnetwork 2 已經學習了二次函數 (f2(-))，並且只有 x2 的係數非零。
    > 
    > **XNN 作爲替代模型**
    > 
    > XNN 還可以用作機器學習模型的替代模型，例如隨機森林（RF）和前饋神經網絡（FNN）。
    > 
    > *![](http://i2.bangqu.com/j/news/20181009/ZpsRD7153908615363721hHp.gif)有點像這樣......但不完全如此。*
    > 
    > 在這種情況下，RF 和 FNN 被認爲是基礎模型。由於 XNN 被設計爲一個可解釋模型，因此我們可以使用輸入特徵和基礎模型預測的相應輸出值來訓練 XNN。然後，XNN 就可以解釋基礎模型所學到的關係！
    > 
    > 使用更容易解釋的替代模型來幫助解釋複雜的機器學習模型，極大地增加了將機器學習模型融入不同行業的能力。
    > 
    > 可解釋神經網絡（XNN）是一個關鍵的機器學習模型。與其他機器學習模型不同，它能「打開」神經網絡的黑箱。該模型的結構和設計方式使其可以解釋學習的特徵以及導致其輸出或預測值的函數。這些可解釋性特徵非常吸引人，它本質上是有可加性的，並且能通過納入神經網絡的機制（如子網絡）直接得到解釋。
    > 
    > 無論將 XNN 用作主要模型還是用於更復雜模型的替代模型，XNN 都可以直接解釋模型如何使用輸入特徵進行預測。這項技術爲將機器學習模型整合入衆多不同行業提供了巨大的優勢，因爲它能夠超越現有系統，並且能夠清晰解釋它如何獲得輸出。
    > 
    > -   相關論文：Explainable Neural Networks based on Additive Index Models
    > 
    > -   論文地址：https://arxiv.org/abs/1806.01933
    > 
    > *原文鏈接：https://medium.com/@shagunm1210/the-explainable-neural-network-8f95256dcddb*



## LIME 

- [[1602.04938] "Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)

- [Introduction to Local Interpretable Model-Agnostic Explanations (LIME) - O'Reilly Media](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)

- [凭什么相信你，我的CNN模型？（篇二：万金油LIME)](https://bindog.github.io/blog/2018/02/11/model-explanation-2/)

    > ## 0x01 LIME
    > 
    > [LIME](http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)是KDD 2016上一篇非常漂亮的论文，思路简洁明了，适用性广，理论上可以解释任何分类器给出的结果。其核心思想是：对一个复杂的分类模型(黑盒)，在**局部**拟合出一个简单的可解释模型，例如线性模型、决策树等等。这样说比较笼统，我们从论文中的一张示例图来解释：
    > 
    > ![LIME](http://lc-cf2bfs1v.cn-n1.lcfile.com/f4682022fc64aa470120.png)
    > 
    > 如图所示，红色和蓝色区域表示一个复杂的分类模型（黑盒），图中加粗的红色十字表示需要解释的样本，显然，我们很难从全局用一个可解释的模型（例如线性模型）去逼近拟合它。但是，当我们把关注点从全局放到局部时，可以看到在某些局部是可以用线性模型去拟合的。具体来说，我们从加粗的红色十字样本周围采样，所谓采样就是对原始样本的特征做一些扰动，将采样出的样本用分类模型分类并得到结果（红十字和蓝色点），同时根据采样样本与加粗红十字的距离赋予权重（权重以标志的大小表示）。虚线表示通过这些采样样本学到的局部可解释模型，在这个例子中就是一个简单的线性分类器。在此基础上，我们就可以依据这个局部的可解释模型对这个分类结果进行解释了。
    > 
    > 一个看似复杂的模型通过我们巧妙的转换，就能够从局部上得到一个让人类理解的解释模型，光这样说还是显得有些空洞，具体来看看LIME在图像识别上的应用。我们希望LIME最好能生成和Grad-CAM一样的热力图解释。但是由于LIME不介入模型的内部，需要不断的扰动样本特征，这里所谓的样本特征就是指图片中一个一个的像素了。仔细一想就知道存在一个问题，LIME采样的特征空间太大的话，效率会非常低，而一张普通图片的像素少说也有上万个。若直接把每个像素视为一个特征，采样的空间过于庞大，严重影响效率；如果少采样一些，最终效果又会比较差。
    > 
    > 所以针对图像任务使用LIME时还需要一些特别的技巧，也就是考虑图像的空间相关和连续的特性。不考虑一些极小特例的情况下，图片中的物体一般都是由一个或几个连续的像素块构成，所谓像素块是指具有相似纹理、颜色、亮度等特征的相邻像素构成的有一定视觉意义的不规则像素块，我们称之为**超像素**。相应的，将图片分割成一个个超像素的算法称为**超像素分割算法**，比较典型的有SLIC超像素分割算法还有quickshit等，这些算法在`scikit-image`库中都已经实现好了，quickshit分割后如图所示：
    > 
    > ![mm](http://lc-cf2bfs1v.cn-n1.lcfile.com/e343bec75b31b7bbcd33.png)
    > 
    > 从特征的角度考虑，实际上就不再以单个像素为特征，而是以超像素为特征，整个图片的特征空间就小了很多，采样的过程也变的简单了许多。更具体的说，图像上的采样过程就是随机保留一部分超像素，隐藏另一部分超像素，如下所示：
    > 
    > ![light](http://lc-cf2bfs1v.cn-n1.lcfile.com/85a16c0af003b1dfe4d8.png)
    > 
    > 从图中可以很直观的看出这么做的意义：找出对分类结果影响最大的几个超像素，也就是说模型仅通过这几个像素块就已经能够自信的做出预测。这里还涉及到一个特征选择的问题，毕竟我们不可能穷举特征空间所有可能的样本，所以需要在有限个样本中找出那些关键的超像素块。虽然这部分没有在论文中过多提及，但在LIME的[代码实现](https://github.com/marcotcr/lime)中是一个重要部分，实现了前向搜索（forward selection）、Lasso和岭回归（ridge regression）等特征选择方式，默认当特征数小于等于6时采用前向搜索，其他情况采用岭回归。
    > 
    > 整体流程如图所示：
    > 
    > ![flow](http://lc-cf2bfs1v.cn-n1.lcfile.com/7f5f62a0ab431169c75d.png)
    > 
    > 和Grad-CAM一样，LIME同样可以对其他可能的分类结果进行解释。
    > 
    > ![effect](http://lc-cf2bfs1v.cn-n1.lcfile.com/6bb64a832e2cce97dc39.png)
    > 
    > LIME除了能够对图像的分类结果进行解释外，还可以应用到自然语言处理的相关任务中，如主题分类、词性标注等。因为LIME本身的出发点就是模型无关的，具有广泛的适用性。
    > 
    > 虽然LIME方法虽然有着很强的通用性，效果也挺好，但是在速度上却远远不如Grad-CAM那些方法来的快。当然这也是可以理解的，毕竟LIME在采样完成后，每张采样出来的图片都要通过原模型预测一次结果。
    > 
    > 说来也巧，在写这篇文章的时候，AAAI 2018的论文放出来了，其中有LIME作者的最新研究成果[Anchors](http://sameersingh.org/files/papers/anchors-aaai18.pdf)，顺道去了解了一下。Anchors指的是复杂模型在局部所呈现出来的很强的规则性的规律，注意和LIME的区别，LIME是在局部建立一个可理解的线性可分模型，而Anchors的目的是建立一套更精细的规则系统。不过看过论文以后感觉更多是在和文本相关的任务上有不错的表现，在图像相关的任务上并没有什么特别另人耳目一新的东西，只是说明了在Anchor（图像中指若干个超像素）固定的情况下，其他像素无论替换为什么，现有的模型都会罔顾人类常识，自信的做出错误判断。这部分内容由于前几年看多了Adversarial Samples，已经见怪不怪了。
    > 
    > ## 0x02 小结
    > 
    > 实际上在模型可解释性这块还有其他很多相关研究，包括最近的AAAI 2018上也有几篇这方面的文章，如[Beyond Sparsity: Tree Regularization of Deep Models for Interpretability](https://arxiv.org/abs/1711.06178)，这都在一定程度上说明，业内还是重视这个方向的。尤其在涉及到医疗、自动驾驶等人命关天的应用场合，可解释性显得尤为重要，最后也希望更多感兴趣的同学加入到这个行列来~
    > 


## PLNN

- [裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡 - 幫趣](http://bangqu.com/HqmsS6.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > 這篇論文研究了以**分段線性函數**爲激活函數的**分段線性神經網絡（Piecewise Linear Neural Network， PLNN）**。分段線性函數在不同的定義域區間內對應不同的線性函數。經典的 MaxOut 、ReLU 以及 ReLU 的一些變體都是分段線性函數。從微分學的角度來看，只要分段數目足夠多，連續光滑的 sigmoid 、tanh 等激活函數也都可以用分段線性函數來無限逼近。
    > 
    > 與現有的規範做法一樣，**該論文通過求解一個分段線性神經網絡 *N* 的決策特徵來解釋 *N* 的決策行爲。但與現有方法大爲不同的是，論文對 *N* 的解釋具有如下兩個獨特的優點**：
    > 
    > 1\. **準確性（Exactness）**：該論文構造了一個具有簡潔**解析形式**的新模型 *M* ，並證明了 *M* 和 *N* 在數學上等價。因此，*M* 的決策特徵能夠**準確**描述 *N* 的真實決策行爲。
    > 
    > 2\. **一致性（Consistency）**：該論文證明了 *M* 是一個分段線性函數，並以**解析形式**給出了 *M* 在其定義域中的各個分段區間，以及 *M* 在每個區間上的線性決策函數。因爲在相同分段區間中的所有輸入實例共享同一個線性決策函數的決策特徵，所以由模型 *M* 對這些輸入實例所提供的解釋是**完全一致**的。
    > 
    > ### OpenBox - 通向準確性和一致性的金鑰匙
    > 
    > 該論文的作者們提出了全新的 OpenBox 方法對分段線性神經網絡（PLNN）的決策行爲提供準確、一致的解釋。「OpenBox」 這個名字也很貼切地描述了作者們使用簡潔的解析方法「打開」深度神經網絡這個「黑盒子」的過程。
    > 
    > OpenBox 方法適用於所有 PLNN。本文將用以 PReLU 爲激活函數的 PLNN 爲例子詳細介紹 OpenBox 方法的技術要點。
    > 
    > **1\. 對單個輸入實例的準確解釋方法**
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306184692f5.jpg)
    > 
    > ##### 圖 2：隱層神經元的激活狀態（status）
    > 
    > 如圖 2 所示，對於以 PReLU 爲激活函數的隱層神經元，其**激活狀態（status）**分爲兩種情況：（1）當 status = 0 時，*z* < 0，該神經元使用左半段的線性函數來建立輸入 *z* 和輸出 *a* 之間的映射關係；（2）當 status = 1 時，*z* >= 0，該神經元使用右半段的線性函數來建立 *z* 到 *a* 的映射。**值得注意的是，不論神經元處於何種激活狀態，*z* 和 *a* 之間的映射關係始終是線性的**。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b3062540b557.jpg)
    > 
    > ##### 圖 3：一個 PLNN 和其隱層神經元的激活狀態
    > 
    > 如圖 3 所示，給定一個輸入實例 *x* ，我們可以將所有隱層神經元的激活狀態按綠色虛線所示的順序排列成一個向量 Conf(*x*)。這個向量被稱作 PLNN 對輸入實例 *x* 的**配置（Configuration）**。
    > 
    > 由於 PLNN 的網絡結構和參數都是給定的，所有神經元的激活狀態都唯一依賴於輸入實例 *x*，因此 Conf(*x*) 由輸入實例 *x* 唯一決定。因爲 *x* 本身是一個給定的常量，所以 Conf(*x*) 也是一個常量。因此，圖 3 中 PLNN 的每個隱層神經元的運算實質上都是由常量 Conf(*x*) 所確定的線性運算。因爲一系列線性運算的嵌套依然是線性運算，**所以在 Conf(*x*) 爲常量的情況下，PLNN 中所有隱藏層的運算整體等價於一個簡單的線性運算 *Wx+b***。
    > 
    > **綜上所述，對於任意給定的輸入實例 *x*，整個 PLNN 嚴格等價於如公式 1 所示的線性分類器。其中，二元組 (*W*, *b*) 以解析形式準確地給出了該 PLNN 對於輸入實例 x 的決策平面。**（注：證明及求解過程請參見原文）
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306575eaffa.jpg)
    > 
    > ##### 公式 1
    > 
    > 顯然，解釋 PLNN 在單個輸入實例上的決策行爲並不能很好地解釋 PLNN 的總體行爲。下面我們將介紹 OpenBox 如何解釋 PLNN 的總體行爲。
    > 
    > **2\. 對一個分段線性神經網絡的準確、一致解釋方法**
    > 
    > 作者們發現，在 PLNN 的網絡結構和參數給定的情況下，公式 1 中的線性分類器 *F(x)* 由 Conf(*x*) 決定。**這意味着對於任意兩個不同的輸入實例 *x* 和 *x'* 而言，只要 Conf(*x*)＝Conf(*x'*)，*x* 和 *x'* 就共享同一個線性分類器，而且對 *x* 和 *x'* 的解釋也將完全一致**。
    > 
    > 那麼，輸入實例 *x* 和 *x'* 需要滿足什麼條件，才能使 Conf(*x*)＝Conf(*x'*) 呢？ 
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b3065f3c1251.jpg)
    > 
    > ##### 圖 4：在 Conf(*x*) 給定的情況下，每一個隱層神經元的輸入 *z* 所必須滿足的不等式約束
    > 
    > 通過進一步推導，作者們發現在 Conf(*x*) 給定的情況下，每一個隱層神經元的輸入 *z* 都必須滿足由該神經元激活狀態所決定的不等式約束。圖 4 給出了當 Conf(x) = [1, 0, 1, 0, 0, 1, 1] 時， PLNN 的所有隱層神經元的輸入 z 必須滿足的一組**線性不等式約束**。
    > 
    > 因爲每個隱層神經元的輸入 *z* 都是輸入實例 *x* 的線性函數，所以這組關於輸入 *z* 的線性不等式約束實質上是對輸入實例 *x* 的一組線性不等式約束。我們將這組線性不等式約束的集合定義爲 *P*。
    > 
    > **很顯然，所有滿足 *P* 中線性不等式約束的輸入實例 *x* 都具有相同的 Conf(*x*)，因此這些實例共享同一個線性分類器，並具有完全一致的解釋。**
    > 
    > 實質上，*P* 中的每一個不等式都定義了一個線性邊界，所有線性邊界一起組成了一個**凸多面體（Convex Polytope，CP）**。在凸多面體中的所有輸入實例都滿足 *P* 中的所有不等式，因此這些輸入實例 *x* 都具有相同的 Conf(*x*)，並且共享同一個線性分類器。我們把這個存在於局部區域的凸多面體和它所對應的線性分類器統稱爲**局部線性分類器（Local Linear Classifier，LLC）**。
    > 
    > **對於任意給定的 PLNN，不同的隱層神經元激活狀態對應着不同的 Conf(*x*)，而每一個 Conf(*x*) 都確定了一個局部線性分類器。因此，一個 PLNN 嚴格等價於一組局部線性分類器。我們把這組局部線性分類器的集合標記爲 *M*，並將其作爲 PLNN 的解釋模型。**
    > 
    > **因爲 *M* 和 PLNN 是等價的，而且同一個凸多面體中的所有實例都共享同樣的解釋，所以由 *M* 所得到的解釋是準確且一致的。**
    > 
    > 給定一個輸入實例 *x*，我們如何使用 *M* 來解釋 PLNN 對 *x* 的決策行爲呢？
    > 
    > 首先，我們從 *M* 中找到 *x* 所屬的局部線性分類器。然後，我們解析出該局部線性分類器的**決策特徵（Decision Feature）**以及其凸多面體的**邊界特徵（Polytope Boundary Feature，PBF）**。最後，我們使用決策特徵來解釋 PLNN 對 *x* 的決策行爲，並使用邊界特徵來解釋 *x* 被當前局部線性分類器包含的原因。
    > 
    > 論文還對計算 *M* 的時間複雜度進行了嚴格的理論分析和證明。**對於 *n* 個不同的輸入實例，若每個輸入實例的特徵維數爲 *d*，OpenBox 解釋所有輸入實例的時間複雜度僅爲 O(*nd*)。因爲特徵維數 *d* 通常被看作常量，所以 OpenBox 的時間複雜度是線性的。**
    > 
    > ### 實驗部分
    > 
    > 作者們把 OpenBox 和目前最頂級的解釋方法 LIME［Ribeiro *et al*. KDD 2016］做了實驗對比。實驗重點關注以下五個問題：
    > 
    > 1\. 局部線性分類器長什麼樣？
    > 
    > 2\. LIME 和 OpenBox 給出的解釋是否準確、一致？
    > 
    > 3\. 局部線性分類器的決策特徵易於理解嗎？如果附加非負、稀疏約束，能繼續提升這些決策特徵的語義特性嗎？
    > 
    > 4\. 如何解釋局部線性分類器的邊界特徵（PBF）？
    > 
    > 5\. 利用 OpenBox 提供的解釋，我們能否構造新樣本來欺騙 PLNN？能否查出 PLNN 在某些樣本上做出錯誤決策的原因？
    > 
    > **實驗一：合成數據集可視化局部線性分類器**
    > 
    > 如圖 5(a) 所示，作者們通過二維歐式空間中的均勻採樣生成了一個包含 20,000 個實例的合成數據集 SYN。其中，紅色和藍色樣本點分別代表正例和負例。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b307e27943ca.jpg)
    > 
    > 圖 5：OpenBox 在合成數據集 SYN 上的實驗結果
    > 
    > 由於實驗目標是可視化模型 *M* 中的所有局部線性分類器，整個實驗過程無需使用測試數據，因此作者們使用 SYN 中的所有樣本來訓練 PLNN。圖 5(b) 顯示了 PLNN 在 SYN 上的預測結果。
    > 
    > 圖 5(c) 可視化了模型 *M* 中每一個局部線性分類器對應的凸多面體。 作者們用相同的顏色標出了屬於同一個局部線性分類器的所有實例，發現屬於相同局部線性分類器的實例都包含於同一個凸多面體（在二維空間中表現爲凸多邊形）。顯然，這個結果完全符合論文的理論分析。
    > 
    > 圖 5(d) 展示了構成模型 *M* 的決策邊界的所有局部線性分類器。圖中的每一條實線都表示一個局部線性分類器的決策邊界，這些局部線性分類器共同構成了模型 *M* 的總體決策邊界。對比圖 5(b) 和 5(d) 可以發現模型 *M* 的總體決策邊界和 PLNN 的決策邊界完全一致。這個結果證實了模型 *M* 和 PLNN 之間的等價性。
    > 
    > **實驗二：FMNIST 數據集驗證解釋的準確性和一致性**
    > 
    > 該實驗在 FMNIST 數據集上對比了 LIME 和 OpenBox（模型M）所提供解釋的準確性和一致性。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b307e430525a.jpg)
    > 
    > ##### 圖 6: OpenBox 和 LIME 在 FMNIST-2 數據集上的準確性（Exactness）和一致性（Consistency）
    > 
    > 首先，作者們通過比較 LIME、OpenBox（模型 *M*）和 PLNN 對 FMNIST-2 數據集中 600 個測試樣本的決策輸出來衡量 LIME 和 OpenBox 各自解釋模型的準確性。如圖 6(a) 所示，LIME 的決策輸出和 PLNN 的決策輸出有着很大不同，這說明 LIME 的解釋模型和 PLNN 非常不同，因此它無法準確解釋 PLNN 的決策行爲。相比之下，OpenBox 計算出的模型 *M* 和 PLNN 對於所有測試樣本的決策輸出完全相同，這說明模型 M 等價於 PLNN，因此它能夠準確地解釋 PLNN 的決策行爲。
    > 
    > 隨後，作者們使用輸入實例 *x* 和其最近鄰實例 *x'* 的解釋結果的餘弦相似度（Cosine Similarity）來衡量 LIME 和 OpenBox 所提供解釋的一致性。餘弦相似度越高，解釋模型所提供解釋的一致性就越高。如圖 6(b) 所示，由於模型 *M* 對同一凸多面體內的實例提供完全相同的解釋，OpenBox 的餘弦相似度幾乎總保持爲 1。但是最近鄰實例 *x'* 與 輸入實例 *x*  並不總是屬於同一個凸多面體，因此 OpenBox 在某些實例上的餘弦相似度小於 1。相比之下，LIME 的餘弦相似度遠低於 OpenBox，這說明 OpenBox 所提供解釋的一致性遠高於 LIME。
    > 
    > **實驗三：OpenBox 提取的決策特徵具有人類可理解的強語義特點**
    > 
    > 除了準確性和一致性，一個好的解釋還必須具有人類可理解的強語義特點。在本實驗中，作者們將 OpenBox 在 FMNIST-1 數據集上提取的決策特徵可視化，發現這些特徵具有易於理解的強語義特點。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306b12f3205.jpg)
    > 
    > 圖 7: OpenBox 和邏輯迴歸（Logistic Regression，LR）在 FMNIST-1 數據集上的決策特徵（Decision Feature，DF）
    > 
    > 圖 7(a) 和 7(f) 給出了 FMNIST-1 中的兩類圖像的平均圖（Average Image）。其中，圖 7(a) 對應正例樣本**短靴（Ankle Boot）**，圖 7(f) 對應負例樣本**包包（Bag）**。
    > 
    > 作者們訓練了多個羅輯迴歸模型（Logistic Regression，LR）作爲基線（Baseline）。其中，LR 模型是以短靴爲正樣本訓練得到的，LR-F 模型是以包包爲正樣本訓練得到的，LR-NS 和 LR-NSF 分別是在 LR 和 LR-F 的基礎上附加稀疏、非負約束得到的。此外，作者們還訓練了兩個 PLNN 模型作爲 OpenBox 的解釋對象。其中，PLNN 是以短靴爲正樣本訓練得到的，PLNN-NS 是在 PLNN 的基礎上附加稀疏、非負約束得到的。
    > 
    > 圖 7 給出了上述所有模型的決策特徵，其中 PLNN 和 PLNN-NS 的決策特徵由 OpenBox 提供。很明顯，PLNN 的決策特徵與 LR 和LR-F 的決策特徵具有極爲相似語義。將這些決策特徵與圖 7(a) 和 7(f) 中的平均圖仔細對比可以發現，這些決策特徵準確地描述了短靴和包包之間的差別。更有趣的是，PLNN 的決策特徵比 LR 和 LR-F 的決策特徵包含了更多細節信息。這是因爲 PLNN 的每一個局部線性分類器僅需區分包含於凸多面體中的一小部分樣本，所以 PLNN 能夠使用大量的局部線性分類器捕捉更多細節特徵。然而，LR 和 LR-F 只能使用一個線性平面劃分所有正負例樣本，因此它們只能捕捉大量樣本的平均差異。因爲 PLNN 捕捉到了更多細節特徵，所以它取得了比 LR 和 LR-F 好得多的分類精度。
    > 
    > 通過對比 PLNN-NS，LR-NS 和 LR-NSF 的決策特徵，我們發現非負、稀疏約束對於增強 PLNN-NS 決策特徵的語義同樣有效。我們還觀察到 PLNN-NS 捕獲了比 LR-NS 和 LR-NSF 多得多的細節特徵，也因此取得了相對較高的分類精度。
    > 
    > **實驗四：OpenBox 提取的邊界特徵也具有很強的語義特性**
    > 
    > 關於 OpenBox 所提取的局部線性分類器，不僅其決策特徵具有很強的語義特點，其凸多面體的邊界特徵也具有很強的語義特性。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306b8ccf418.jpg)
    > 
    > ##### 圖 8: OpenBox 在 FMNIST-1 數據集上提取的邊界特徵（Polytope Boundary Feature，PBF）
    > 
    > 在本實驗中，作者們在 FMNIST-1 上訓練了一個 PLNN，並用 OpenBox 解析出該 PLNN 的三個局部線性分類器對應的凸多面體。圖 8(a)-(d) 給出了這些凸多面體的邊界特徵，它們分別對應了｛包包，短靴，包包，包包｝。圖 8(e) 給出了定義這些邊界特徵的線性不等式，以及其對應的凸多面體中所包含的各類別樣本數量。關於圖 8(e) 中的線性不等式，「／」代表該不等式定義的邊界爲無效邊界；「> 0」代表凸多面體內的樣本與該不等式的邊界特徵具有很強的相關性；「<= 0」 代表凸多面體內的樣本與該不等式的邊界特徵沒有強相關性。
    > 
    > 以圖 8(e) 中的第一個凸多面體爲例，由其線性不等式的狀態可知該凸多面體所包含的樣本與圖 8(b)-(c) 中短靴和包包的邊界特徵有強相關性。因此，第一個凸多面體中包含了大量的短靴和包包。類似的，對圖 8(e) 中的第二個凸多面體而言，其中的樣本僅與短靴的邊界特徵呈正相關，因此該凸多面體中的樣本僅有短靴而沒有包包。通過上述實驗結果不難看出，**OpenBox 提取的邊界特徵具有很強的語義特性**。
    > 
    > 除了上述精彩實驗，作者們還利用 OpenBox 提供的解釋來構造欺騙 PLNN 的新樣本，以及查找 PLNN 在某些樣本上做出錯誤決策的原因。在這些有趣的任務上，論文中的實驗也給出了明顯優於現有方法的結果。
    > 
    > ### 結論
    > 
    > 作者們通過證明分段線性神經網絡嚴格等價於一組局部線性分類器，以簡潔的解析形式給出了一種準確、一致且高效的神經網絡解釋方法------OpenBox。大量實驗結果表明，OpenBox 不僅可以準確、一致地描述分段線性神經網絡的總體行爲，還能夠對分段線性神經網絡進行有效的欺騙攻擊和錯誤查找。作者們談到，他們將繼續拓展這一方法，使其能夠有效地解釋使用連續、光滑激活函數（如：sigmoid、tanh）的深度神經網絡。
    > 
    > 詳細內容請參見原論文：<https://arxiv.org/abs/1802.06259>


