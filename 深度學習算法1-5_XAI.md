# 深度學習算法1-5_XAI

[toc]
<!-- toc --> 

# XAI

## News

- [深度學習模型為何建議你這樣做？要靠解釋性AI來告訴你（下） | iThome](https://www.ithome.com.tw/news/126281)

    > 例如華盛頓大學3位研究人員就提出了一套LIME（Local Interpretable Model-agnostic Explanations）框架，在訓練出DNN模型後，再針對一筆訓練資料（一張圖或一句話）中的局部資料進行干擾，來觀察是否會影響預測結果，而來判斷訓練資料中的那個部分對預測結果，具有更關鍵的作用，例如就可以判斷出，一張狗彈吉他照片，狗臉是辨識出動物物種的關鍵區域。這個作法也引起不少業界關注，例如Teradata技術長也特別看重ＬＩＭＥ的應用。
    > 
    > 不同於LIME是從訓練資料來尋找解釋力的作法，神經元刪除法則是從訓練後的神經網路模式來尋找解釋力。由於在深度神經網絡模型中，利用大量神經元來訓練模型，DeepMind團隊透過刪除獨立的特定神經元，或在隱藏層中的神經元，來觀察對模型預測力的影響。不過，DeepMind發現，那些可解釋的獨立神經元，對模型的效力不如那些「不可解釋」的神經元，其次，越有能力分辨未知圖片的神經網絡模型，越不受刪除神經元的影響。
    > 
    > 例如康乃爾大學兩位研究者就曾在2013年發表了一篇卷積神經網路視覺化的論文《Visualizing and Understanding Convolutional Networks》，這是CNN模型視覺化的開始，試圖透過反向推算，來找出每一層如何所辨識的圖片特徵，來說明CNN模型的辨識原因。
    > 
    > 而在長期作法上，沈向洋認為，還是得回到因果關係的處理。例如微軟正從張量空間著手（Tensor Space），試圖要建立一個數學模型，來串連符號AI和神經網絡兩種AI技術，希望能做到兼顧DNN的優勢，又能提供可解釋的能力。
    > 

### Yoshua Bengio首次中国演讲：深度学习通往人类水平AI的挑战

- [机器之心](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650751497&idx=1&sn=f396e6f56c25a1486d482f9ae264cfdf&chksm=871a8677b06d0f61473811f55b8270f85018d51dfd2a0a5810da427346ad9a9515e081730fc3&mpshare=1&scene=24&srcid=#rd)

    > 11 月 7 日，Yoshua Bengio 受邀来到北京参加第二十届「二十一世纪的计算」国际学术研讨会。会上以及随后受邀前往清华时，他给出了题为「深度学习通往人类水平 AI 的挑战」（Challenges for Deep Learning towards Human-Level AI）的演讲。机器之心在 Yoshua Bengio 的授权下介绍了整篇演讲，读者可点击「阅读原文」下载 PPT。
    > 
    > 演讲中，Bengio 以去年发布在 arXiv 的研究计划论文「有意识先验」（The consciousness prior）为主旨，重申了他与 Yann Lecun 十年前提出的解纠缠（disentangle）观念：我们应该以「关键要素需要彼此解纠缠」为约束，学习用于描述整个世界的高维表征（unconscious state）、用于推理的低维特征（conscious state），以及从高维到低维的注意力机制------这正是深度学习通往人类水平 AI 的挑战。
    > 
    > 虽然主题看起来比较广大，但实际上，Bengio 讨论了非常多的技术细节内容。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBCNiamBfEkAJo96sPWEQJzCaU6dJPcgXiaSMkxk7ndYcicrKmmAouN9g6Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > *图：Bengio在清华。*
    > 
    > Bengio 认为，直观上而言，目前的深度神经网络非常擅长于从文字图像等原始数据抽取高层语义信息，它们会直接在隐藏空间做预测，这就类似于在无意识空间做预测。但是实际上每一次预测所需要的具体信息都非常精简，因此实际上我们可以使用注意力机制挑选具体的信息，并在这种有意识空间进行预测，这种模型和建模方法才能真正理解最初的输入样本。
    > 
    > 
    > **演讲**
    > 
    > 今天我将介绍我与合作者共同探讨的一些问题，关于深度学习研究的下一步发展以及如何通向真正人工智能。
    > 
    > 在此之前，我想先纠正一个目前看来非常普遍的误解，即「深度学习没有理论依据，我们不知道深度学习是如何工作的。」
    > 
    > 我的很多工作都围绕深度学习理论展开。这也是为什么我在大约 12 年前开始研究深度学习的原因。虽然深度学习仍然有诸多未解之谜，但现在我们已经对它的很多重要方面有了更好的理解。
    > 
    > 我们更好地理解了为什么优化问题并不像人们想象中那样棘手，或者说局部极小值问题并不像 90 年代的研究者认为的那样是一个巨大障碍。我们更好地理解了为什么像随机梯度下降这样看起来非常「脑残」的方法实际上在优化和泛化方面都非常高效。
    > 
    > 这只是我们在过去十年中学到的一小部分，而它们有助于我们理解为什么深度学习真正好用。数学家和理论研究者仍然对此展现出了极大的兴趣，因为深度学习开始在诸多领域变得极为重要。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBiaGUFx0F8Cqkzan47Cav3EnibE8PMgiat6Dpkvib2wpnS8qaQMqQC3gD3g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > **从人类的两种认知类型解释经典 AI 与神经网络的失败**
    > 
    > 我今天演讲的主题是「通往人类水平的 AI」：我们试图让计算机能够进行人与动物所擅长的「决策」，为此，计算机需要掌握知识------这是几乎全体 AI 研究者都同意的观点。他们持有不同意见的部分是，我们应当如何把知识传授给计算机。
    > 
    > 经典 AI（符号主义）试图将我们能够用语言表达的那部分知识放入计算机中。但是除此之外，我们还有大量直观的（intuitive）、 无法用语言描述的、不能通过「意识」获得的知识，它们很难应用于计算机中，而这就是机器学习的用武之地------我们可以训练机器去获取那些我们无法以编程形式给予它们的知识。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBRSVfkQFKK3icldBdtolSicWuZF3TroiaUqygPcu9WXFKgWT5k7UjuWapQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 深度学习和 AI 领域有很大进步、大量行业应用。但是它们使用的都是监督学习，即计算机无需真正发掘底层概念、高级表征和数据中的因果关系。事实上，如果你用不同的方式攻击这些模型，就像很多对抗方法所做的那样，仅仅微调输入，也会使模型变得非常愚蠢。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBXucg8P0Af5kjR1RCDw0Klaf3XO2RCtXicO9Kyqx0cJFAqyBuicBqulicw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 举例来说，我们在一篇论文中改变图像的傅立叶频谱，变换后，图像的类别对于人类来说仍然很明显，但是在自然图像上训练的卷积网络的识别率则变得非常糟糕。
    > 
    > 对我来说，现在的系统的失败之处在于，它们无法捕捉我们真正想让机器捕捉到的高级抽象（high level abstraction）。事实上，这是我和合作者希望设计出能够发现高级表征的学习机器的原因：这样的表征可以捕捉构成数据的根本因素。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBzSQfxrKOMtdibsEzH4264LSBHxdIuqVtR3hnpKAF6FWmmWdK3TpKkgQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 我在大约十年前介绍过「解纠缠」（disentangle）这个概念，即在一个好的表征空间中，不同要素的变化应该可以彼此分离。（而在像素空间中，所有的变化都彼此纠缠着的。）十年之后，我们认为，除了解纠缠变量，我们还希望系统能解纠缠计算。解纠缠和因果的概念相关，而因果正是机器学习界需要重点关注的领域，我将在之后回到这个话题的讨论。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBXCXxclKCtheRrxtwukFYLAW0E45dibNf129XNY4Jq7wAAx6avQYUCDw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 五年前，我在一篇综述论文提出，为了还原那些可以解释数据的根本要素，我们需要引入知识。我们不能只是从零开始学习，还需要对世界作出一些可能比较温和的假设。这对于解纠缠变量会有帮助。空间、时间以及边际独立性可能是一些过于强的假设，但也值得考虑。
    > 
    > 一个先验是某些要素对应于世界的某些「可控层面」（controllable aspect）。例如我手上这个翻页器，它有一个三维坐标，而我可以通过移动它改变坐标。这种空间位置体系在我们的大脑中也明确存在，因为这是我们能控制的世界层面。
    > 
    > 因此在世界的意图、动作、策略和层面的表征之间有着很强的联系。与其用最底层的像素表征关于世界的信息，对于智能体而言，用更高级的、可交互的、与控制相关的要素来表征信息会方便的多。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBB2ZALb3KsxJTwRaCQgly4ZIGzKnvkcvQ0oTl9EiafE8htvD3gRKbLibsw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 在谈及具体的深度学习工作之前，让我先介绍一下心理学家是如何划分人类认知活动的，这有助于我们理解当前深度学习的优势以及我们应该如何走向人类水平的 AI。
    > 
    > 人类的认知任务可以分为系统 1 认知（System 1 cognition）和系统 2 认知（System 2 cognition）。系统 1 认知任务是那些你可以在不到 1 秒时间内无意识完成的任务。例如你可以很快认出手上拿着的物体是一个瓶子，但是无法向其他人解释如何完成这项任务。这也是当前深度学习擅长的事情，「感知」。系统 2 认知任务与系统 1 任务的方式完全相反，它们很「慢」。例如我要求你计算「23+56」，大多数人需要遵循一定的规则、按照步骤完成计算。这是有意识的行为，你可以向别人解释你的做法，而那个人可以重现你的做法------这就是算法。计算机科学正是关于这项任务的学科。
    > 
    > 而我对此的观点是，AI 系统需要同时完成这两类任务。经典 AI 试图用符号的方法完成系统 2 任务，其失败的原因很多，其中之一是我们拥有的很多知识并不在系统 2 层面，而是在系统 1 层面。所以当你只使用系统 2 知识，你的体系缺少了一部分重要的内容：那些自下而上的有根源知识（Grounded knowledge）。有根源自然语言学习（Ground language learning）是 NLP 的一个子领域，研究者试图用除了文本之外的其他形式，例如图像、视频，去将语言与感知层面的知识联系起来，构建一个世界模型。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBDydJrIGpDrGIgrkBicmm9hobiaf8CndkIyGGjKgFvaNzw5UVxNo8BBew/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > **意识先验**
    > 
    > 我接下来将介绍意识先验，意识领域的研究正逐渐变成主流。我在这里将聚焦于意识的最重要问题：当你注意某些东西，或者在你的意识中浮现了某些东西的时候，你意识到了它的某些现实层面情景。
    > 
    > 深度学习的表征学习关注信息如何被表征，以及如何管理信息。因此对于意识先验很基本的一个观察是，在特定时刻处于你意识中的想法（thought）是非常低维的。其信息量可能不超过一句话、一张图像，并且处于一个你可以进行推理的空间内。
    > 
    > 你可以将一个「想法」看做是经典 AI 中的一条「规则」。每个想法只涉及很少的概念，就像一句话中只有几个单词。从机器学习的角度来看，你可以利用很少的变量进行预测，准确度还很高。这种具有良好性质的低维表征空间是非常罕见的，例如，尝试通过给定的 3 到 4 个像素来预测 1 个像素是不可行的。但是人类可以通过自然语言做到这一点。例如，如果我说「下雨时，人们更可能会撑伞。」这里仅有两个二值随机变量，是否下雨和是否撑伞。并且这种语句具备很强的预测能力。即使它仅使用了很少的变量，也能给出很高概率的预测结果。也就是说，根据很少的信息来执行预测。
    > 
    > 因此，我将「意识」称作一个「先验」，是因为意识是一个约束条件、一个正则化项、一个假设：我们可以用非常少的变量进行大量的预测。
    > 
    > 满足这些条件意味着我们需要好的空间表征。好的表征的一个特性是当把数据映射到该空间时，变量之间的依赖关系只需要用很少的概念表达（例如规则），且涉及很少的维度。
    > 
    > 学习好的表征意味着可以将知识用两种方式表达：在编码器中，将原始数据映射到高级空间；通过规则将变量关联起来并执行预测。
    > 
    > 因此我们有两种形式的解纠缠。我以前的论文仅考虑了解纠缠变量，现在我们还考虑了解纠缠规则。如果我们将这些变量看成是代表因果变量的因子，这对应着一种因果机制。因果变量是指在因果陈述中使用的变量，例如「下雨导致人们撑伞」。这些变量需要处在一个好的表征空间来作出因果陈述。像素空间并非能够进行因果陈述的合适表征空间：我们无法说某些像素的改变导致了其它像素的改变，而在因果空间中推理是可行的。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBiaR5N7yibHkHaAJcjZicYAABMiae9vXLG6x2NUJbWVibx1ibVTCicLVdDyic7g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 那么要如何实现这种表征呢？对此，注意力机制是一种很重要的工具。注意力机制在过去几年获得了很大的成功，尤其是在机器翻译中，它可以按顺序选取重点关注的信息。
    > 
    > 更棒的是你可以使用软注意力来实现整个系统的端到端训练。我们不需要设计一个独立的系统来做这种选择。你可以将注意力机制作为在某些全局目标下端到端训练的更大系统的一部分。而这正是深度学习擅长的地方。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBOXfSG2J66ibbtfribqoMm5Stcic9CzeszfjsmyBNzuqxXQNGgHUQicnkpg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 在架构方面，意识先验在「原始输入」和「某些更高级的表征」之外，还引入了第三个层次：这也就是有意识状态（conscious state）。
    > 
    > 如上所示无意识状态通常是深度学习所考虑的表征，是模型将数据映射到的一些表示空间。这些隐藏表征通常有非常高的维度与稀疏性，因为任何时候都只有少数变量与输入相关。在此之外，我们还会使用注意力机制选择无意识状态（高维隐藏表征）的几个重要维度，并在有意识状态下表示它们。进入有意识状态的信息就像短期记忆，我们可以使用注意力机制选择一些重要的信息，并通过某种交互表示它们。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBykpQPTJg9OlEbVqngkOsDnLQfJnRzOgwdO92cnvhqLZrOD1wNJh3HQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 这个理论框架还有非常多的细节需要完善，去年我们主要关注其中的一个方面：目标函数。机器学习和深度学习中的标准训练目标函数都基于最大似然估计，而即使与最大似然无关的目标函数，例如 GAN 的一些目标函数，也是在像素级别进行构建的。然而，我们实际上想要在隐藏空间中表现出可预测性。
    > 
    > 这很难做到，但我们其实可以训练一些不需要返回到像素空间的机器学习算法，例如主成分分析（PCA）。我们可以像自编码器那样用最小化重构误差训练 PCA：这是在像素空间中构造目标函数，但同时我们也可以在隐藏空间中训练它，例如我们希望降维后的表征每一个都有非常大的方差，从而捕捉到足够多的输入信息。
    > 
    > 但我们不止想做 PCA，我们希望有更强大的模型。其中一个很好的扩展概念是互信息（mutual information），它允许我们在编码器输出的隐藏空间中定义目标函数。这个想法很早就已经提出来了，在联接主义的早期，Suzanna Becker 就认为我们应该「寻找数据变换的方法，使空间中的近邻特征拥有比较高的互信息水平」，以此进行无监督图像学习。我认为这是一个被遗忘的重要方向。
    > 
    > > 注：接下来 Bengio 沿着互信息这个方向介绍了很多研究论文，包括它们的基本过程、核心思想和技术等，这里只给出了研究论文列表，感兴趣的读者可以查看原论文。
    > >
    > >
    > >
    > > -   Learning Independent Features with Adversarial Nets for Non-linear ICA，ArXiv:1710.05050
    > >
    > >
    > > -   MINE: Mutual Information Neural Estimation，ArXiv:1801.04062
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBIAicibp6LMAchXGhibdo2o9LaMogz0bZELpjR5XLVrQmdd2z1fkTLZibwQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > **意识先验的现实意义：世界模型实现人类水平的语言模型**
    > 
    > 回到系统 1 和系统 2 认知任务，以及意识先验。这些概念的实际意义是什么？
    > 
    > 首先，为了真正理解语言，我们要构建同时具有系统 1 和系统 2 能力的系统。当下的 NLP 算法与 NLP 产品，无论是机器翻译、语音识别、问答系统，还是根本不能理解任何东西的阅读理解，所有这些系统都仅仅是在大型文本语料库和标签上做训练而已。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBU5BdRMGTkqkJVdKnmMTjOcpejia5L1gBzLP0WSRPOx84sfyH0Xb80Tw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 我认为这样是不够的，你可以从它们犯的错误中发现这一点。举个例子，你可以对系统做个测试，看他们能否消除这些 Winograd 模式歧义句：「The women stopped taking pills because they were pregnant（怀孕）.」这里的「they」指什么？是 women 还是 pills？「The women stopped taking pills because they were carcinogenic（致癌）」这句中的「they」又指代什么？事实证明，机器仅仅通过研究样本的使用模式是不足以回答这个问题的，机器需要真正理解「女性」和「药」是什么，因为如果我把「怀孕」换成「致癌」，答案就从「女性」变成了「药」。在人类看来这个问题非常简单，但是现有的机器系统回答起来比随机猜测好不了多少。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBRlfMkKCNoUQ1zsD3H9KG5MU8lXrksGfv7FhTRSs6nazUdPyFcu4KqQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 当我们想要构建能理解语言的系统时，我们必须问问自己，对于机器而言理解问题或文档意味着什么。如果它们需要相关知识，那么从哪里获取这些知识呢？我们又该如何训练那些具备特定知识的系统？
    > 
    > 有一个个思想实验可以帮助我们看清仅在文本上训练模型的局限。想象一下你乘坐宇宙飞船到达另一个星球。外星人说着你听不懂的语言，这时如果你能够捕捉到他们在交流中传达的信息，或许你可以训练语言模型以理解外星语言。而那个星球与地球有一个区别：那里的通信通道不带噪声（地球上的通信通道是有噪声的，因此，人类语音为了在噪声中保持鲁棒性，包含了大量信息冗余。）
    > 
    > 由于外星的通信通道没有噪声，因此传输信息的最佳方式是压缩信息。而信息被压缩后，看起来和噪声没什么区别：在你看来，它们交换的都是一些独立同分布的比特信息，语言建模和 NLP 工具也无法帮到你。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBB9FpBicXXHHS04iaubbO48eNF1lUCHYaTkueibAeoFD6CJlicC57Q7myS0Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 这个时候我们该怎么办呢？我们需要做更多工作。仅观察信息本身是不够的，你必须找出它们的意图，理解它们的语境和行为的原因。因此，在语言建模之外，你必须建模环境并理解原因，这意味着大量额外工作。AI 领域研究者「懒惰」又「贪婪」，他们不想进行额外工作，因此他们尝试仅通过观察文本来解决语言理解问题。然而很不幸，这并不会给出有效解决方案。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBB1LK0cZSayP3tZENMydf1bDEU4qruibcDFJvDAKicamsIu48zialGJr76w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 一种可行方法是先学习一个不错的世界模型，然后基于该模型解决语言问题，就像根据语言模型弄清楚某个单词的意义一样。我认为婴儿在一定程度上就是这么做的，因为婴儿并非一开始就使用语言进行学习，最初它们只是尝试理解环境。但是在某个时间点，将「学习语言模型」和「学习世界模型」两种学习模式结合起来是有益的。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBzAib3W4RqGsnf8TSCzr4bUqFcKscMhIzpOtxGgOL8KpwM2sEiceHcLmw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 语言可以提供良好表征。因为如果想弄懂这些语义变量，深度学习应该从感知器中提取出语义。比如你妈妈说「狗」，恰好这时你看到了一只狗，这就很有帮助，因为当你在不同语境中使用这个词时你的感官感知是不同的。这就是监督学习性能好的原因。
    > 
    > 事实上，以监督学习方式训练出的深层网络的表征比无监督模型好很多，最起码对于目前的无监督学习来说。我认为应该将二者结合起来，不过你必须理解世界的运行方式。世界运行方式的一个方面是因果关系，机器学习目前对此缺乏关注。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBk16U5uCrJiciaIv019Wh9aSYLq4VRxiamwKZ03ziblJSNHtPZPvFh3mh0g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 具体而言，我们的学习理论在这方面仍然很匮乏。目前的学习理论假设测试分布与训练分布相同，但是该假设并不成立。你在训练集上构建的系统在现实世界中可能效果并不好，因为测试分布与训练分布不同。
    > 
    > 因此我认为我们应该创建新的学习理论，它应该不会基于「测试分布与训练分布相同」这样生硬的假设。我们可以采用物理学家的方式，假设训练分布和测试分布的底层因果机制相同。这样即使动态系统的初始条件不同，底层物理机制仍然不会改变。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBicQFPErSzXTLbNN1oibzA3HqIR1C5aibZVDZHtE0n1UbnL8K5QzDd1sDg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 那么如何去做呢？事实上，构建好的世界模型令人望而生畏，我没有足够的计算能力对真实世界建模，因此我认为更合理的方法是利用机器学习，机器学习研究不是关于 AI 应该具备哪些知识的研究，而是提出优秀的学习算法的研究。优秀的机器学习算法理应在任何分布中都可以良好运行。
    > 
    > 近年来深度学习社区涌现了大量关于搭建虚拟环境的研究，如在深度强化学习体系下，人们构建虚拟环境并在其中测试不同的智能体学习步骤。深度强化学习最酷的一点是便于做科学实验，我们可以借助虚拟环境测试理论，更快速地获取反馈。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBq80Oiao2Af0CgIvac1PHURD9dFOrBYoibMx2vgOO2nhPBkGTU0YMgNdg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 在我实验室开始的一个项目，是 1971 年 Winograd 用 SHRDLU 系统进行 blocks world 实验的延伸。他们当初试图建立一个能够用自然语言执行任务的系统，比如「拿起一个红色的木块」，但他们试图用基于规则的经典 AI 来实现目标。这在某种程度上起作用了，但它和大多数规则系统一样非常脆弱。它无法扩展，因为你需要手动设计大量知识，像当前大多数脆弱且无法扩展的对话系统一样。我认为，除非我们真正做更多的基础研究，否则这种情况不会改善。
    > 
    > **BabyAI 平台：模拟世界模型**
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBB4RbLjjgrAQXAF9tMLoWxQxMuYWCqWdOjHBJfPQ8iaQNGtr4IgnC1ndg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 所以我们构建了一个叫做 BabyAI（或 BabyAI game）的平台，其中设置了有一个「学习者」和一个「人类」的游戏或场景。
    > 
    > 学习者就是「baby AI」，我们要为学习者设计学习算法，而其中的人类与学习者互动，并使用自然语言帮助它理解周围的环境。人类可以通过课程学习（curriculum learning）、为学习者设计正确的问题以及考虑学习者知道什么和不知道什么等等来帮助它。当然了，课程学习本身就是一个有趣的研究领域，因为如果我们能够构建出计算机与人类互动的更好系统，那也会非常有用。
    > 
    > 所以我们在 2D 网格世界中构建了一个非常简单的环境，并能在其中使用类似「把蓝色钥匙放在绿色的球旁边」这种简单的自然语言表述。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBDhy0w6aQMpa6hNxRDibOQllqKQeB6jomYeZ4wUq45Ex2S3m33604EwA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 在这个阶段，我们有 19 个学习者应该能够学习的难度级别和任务类型。我们还设计和训练了一个知道如何解决任务的启发式专家。当然，这个专家扮演的是人类的角色，因为在这个阶段，我们实际上还不想让人类参与进来。所以我们希望能够模拟人类，然后查看和测试不同的学习者表现如何。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBztaWib4qzcvrsrUIXYw7TNEaicWrh8SppNyiaZuSQvnpfLI06Gy2btic3A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 我们有更大的版本，不同级别有不同的房间数量和不同类别的任务。我们定义了一系列的概念，比如房间和迷宫，也定义了一系列动作，如去某个地方、打开、捡、放等等，以及使用这些概念的不同任务。当你进阶学习更加复杂的任务，需要的概念也越来越多。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8MqEamqaMnic6Wb2Aia6gcBBWfKPZBBPQCcNt6EJVnaDzfMrONmf0c84qLAgCK33QpnqLnZqalbXXg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
    > 
    > 但是，我们目前尝试过的机器学习方法还做不到这一点。如果我们有真正的人类来教 baby，他们就不需要给 baby 提供成百上千的轨迹示例。
    > 
    > 我们尝试了模仿学习和强化学习。在强化学习中，人类会提供奖励。在学习者收敛之前，他需要在数百万轨迹上提供数百万条奖励。但即使是效率更高的模仿学习（类似监督学习），如果要从模仿示例中学习，对于一个人来说，花时间训练这些系统还是远远超出了我们认为的合理范围。
    > 
    > 我们还发现当前的系统可以非常快速地学习来做这样的工作，但要达到 99% 的正确回答率还需要大量训练。因此我们认为可以用这些基准来研究简单效率数据、不同学习程序效率。



## 可解釋性

- [深度学习的可解释性研究（一）—— 让模型具备说人话的能力](https://zhuanlan.zhihu.com/p/37223341)

- [AI.Why: A new architecture for creating AI models with interpretability from scratch (Part 1) | LinkedIn](https://www.linkedin.com/pulse/aiwhy-new-architecture-creating-ai-models-from-part-1-figurelli/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BgSQgo6WFQ%2Fy6K28sPHjiiA%3D%3D&licu=urn%3Ali%3Acontrol%3Ad_flagship3_pulse_read-related)


### [1802.00614] Visual Interpretability for Deep Learning: a Survey

- [[1802.00614] Visual Interpretability for Deep Learning: a Survey](https://arxiv.org/abs/1802.00614)

    - [(1 封私信 / 83 条消息)如何理解Visual Interpretability这一系列paper的原理以及insight？ - 知乎](https://www.zhihu.com/question/266875930)

    > 谢邀。作为这两篇文章的作者，我尽力回答。关于interpretability of neural networks的研究近几年才刚刚起步，我和UCLA的团队成员近年来在该方向总结了七个工作（参见Visual Interpretability for Deep Learning: a Survey）。我们的目的依然是尽可能的定义清楚network interpretability所涉及的概念以及研究范围。
    > 
    > 在此之前，network interpretability的研究主要集中在两个方向上：1）network visualization and diagnosis，2）evaluation of network interpretability。在第一个方向上，人们提出了很多神经网络可视化的算法，去显示网络中某个filter所建模的视觉特征。另一些算法定位出与某个neural activation或某个网络输出密切相关的图像区域。一些学者通过挖掘神经网络的对抗性样本（adversarial samples）去分析网络的表达缺陷。第二个研究方向是提出一个对pre-trained network的可解释性的第三方的评测标准。如何定义一个网络内部表达的可解释性和如何评测可解释性依然是个相对开放的问题。在这个方向上，@周博磊CVPR17的文章[1]比较有代表性。
    > 
    > 相比之下，我们团队希望探索更深层次的网络可解释性，即 _**建立一个从神经网络内部表达到语义图模型的映射关系**_ ，以及进一步探讨基于可解释性在具体应用中的实际意义（比如与网络中层语义的交互）。
    > 
    > 比如，一个重要的思路就是，我们希望用一个explanatory model去归纳总结一个pre-trained CNN各个层上的主要特征表达。我们用这个explanatory model去解释CNN内部特征的潜在语义，使得CNN内部表达不再是个黑箱，而不是用这个explanatory model去做具体的分类分割等任务。在[2,3]中，我们希望把一个pre-trained CNN的conv-layers上的主要特征表达，拆分成一个可解释的图模型，即explanatory graph。这个explanatory graph作为这个CNN的特征翻译器，去解释CNN中每个filter的每个神经触发背后潜在的语义。一个CNN中的每个conv-layer的每个filter往往潜在地建模了多种不同的object parts，即这个filter可能被不同的parts所触发。我们的工作就是无监督地把这些filters的混乱的feature maps拆分成不同的part组份，从而增加网络的可解释性。具体的介绍请参考<https://zhuanlan.zhihu.com/p/31365150>。进一步在[4]中，我们用一个decision tree，作为一个解释性模型，去总结一个CNN在fully-connected layers上的不同decision modes。当一个CNN为某个图像做出一个prediction的时候，decision tree可以给出具体某个filter对这个prediction定量的贡献值，进一步给出某个object part对prediction的定量的贡献。
    > 
    > 我们的第二个思路是端对端的学习一个神经网络，使得其内部表达本身就是充分disentangled，充分可理解的。比如在[5]的interpretable CNN的研究中，我们在不标注object parts或texture的情况下，让算法自动的为CNN的每个高层filter去表示某个特定的object part语义，从而增加网络的可解释性。具体的介绍请参 <https://zhuanlan.zhihu.com/p/30074544>。
    > 
    > 归根结底，以上的研究通过不同的方法建立了从神经网络内部表达到语义图模型的映射关系。有了这样的映射关系以后，我们希望对神经网络的训练不仅仅是end-to-end的，而且可以middle-to-end和end-to-middle的学习。即，我们可以与网络的中层语义表达进行交互。当训练一个新的模型，我们可以根据网络的中层语义像搭积木一样去搭建一个新模型，而不是端对端的从底层学习特征表达。这种通过网络可解释性的迁移学习的思路，可以大大降低训练网络的标注成本，做到few-shot learning。比如在[6]和[7]中，我们利用对CNN内部特征的图模型表达，提出了通过人机问答交互方式去高效的学习新的模型。
    > 
    > [1] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In CVPR, 2017.
    > 
    > [2] Q. Zhang, R. Cao, F. Shi, Y. N. Wu, and S.-C. Zhu. Interpreting cnn knowledge via an explanatory graph. In AAAI, 2018.
    > 
    > [3] Q. Zhang, R. Cao, Y. N. Wu, and S.-C. Zhu. Growing interpretable part graphs on convnets via multi-shot learning. In AAAI, 2016.
    > 
    > [4] Quanshi Zhang, Yu Yang, Ying Nian Wu, and Song-Chun Zhu. Interpreting cnns via decision trees. arXiv:1802.00121, 2018.
    > 
    > [5] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolutional neural network. In arXiv:1710.00935, 2017.
    > 
    > [6] Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu. Mining object parts from cnns via active question-answering. In CVPR, 2017.
    > 
    > [7] Quanshi Zhang, Ruiming Cao, Shengming Zhang, Mark Edmonds, Ying Nian Wu, and Song-Chun Zhu. Interactively transferring cnn patterns for part localization. In arXiv:1708.01783, 2017.

### XNN：打開了自己黑箱的神經網絡

- [[1806.01933] Explainable Neural Networks based on Additive Index Models](https://arxiv.org/abs/1806.01933)

- [The Explainable Neural Network – Shagun Maheshwari – Medium](https://medium.com/@shagunm1210/the-explainable-neural-network-8f95256dcddb)

- [XNN：打開了自己黑箱的神經網絡 - 幫趣](http://bangqu.com/ZpsRD7.html)

    > 將人工神經網絡應用到特定系統的最大障礙之一是它的「黑箱」屬性。XNN（可解釋的神經網絡）是一種旨在「打開並解釋」神經網絡黑箱的新模型。
    > 
    > 使用人工神經網絡和機器學習算法訓練機器像人腦一樣學習信息已經越來越流行。這讓機器能夠準確地輸出給定任務的預測結果。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD71539086147203A65a5.png)
    > 
    > *在給定任務下訓練的 ANN 能進行貓和狗的分類*
    > 
    > 再舉一個例子，假定你想要訓練一臺機器，使其能根據客戶憑證確定銀行的哪些客戶離開銀行的概率最高。
    > 
    > 神經網絡將在包含每個客戶憑證（例如信用評分）的大數據集上進行訓練。它通過變量選擇識別數據中的哪些特徵對客戶的去留影響最大。神經網絡將學習這些特徵，並通過學習特徵，根據其憑證（特徵）自行準確預測哪些客戶離開銀行的概率最高。（參見：https://www.linkedin.com/pulse/creating-deep-neural-net-accurately-predict-churn-rate-maheshwari/）
    > 
    > 這些模型對大型數據集非常有效，因爲在大型數據集中很難進行手動的變量選擇和特徵工程。另外，與傳統統計方法相比，它們具有更好、更準確的預測性能。
    > 
    > **黑箱**
    > 
    > 然而 ANN 的一個問題在於，它們是黑箱。這意味着數據分析師或建模人員很難解釋輸入特徵與其響應（輸出）之間的關係。神經網絡越複雜意味着添加的神經元和層數越多，這使解釋和確定 ANN 中的哪個函數導致輸出變得愈發困難。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD715390861476996pLYv.png)
    > 
    > **應用**
    > 
    > 理解黑箱並具備解釋模型行爲的能力非常重要，這是因爲機器學習模型和 ANN 的使用需要納入醫療保健和金融等領域。事實證明，機器學習模型有可能徹底改變這些行業，提高某些任務的效率。
    > 
    > 瘋狂的是，研究人員實際上能創建一種機器學習算法來識別乳腺癌患者身體組織圖像中的乳腺癌模式，而且效果比人類病理學家還要好！機器學習模型能夠更快地識別乳腺癌模式，準確率達 89%，高於訓練有素的人類病理學家的平均準確率 73％！這只是 ANN 和機器學習模型多種實現方式的一個示例，它們能提供比傳統人類方法更高效、準確的工具。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD715390861486098s6ff.png)
    > 
    > 然而，儘管這些模型的準確率和效率已經經過驗證，但人們仍在猶豫是否將它們納入醫療保健和銀行業等領域，因爲這些模型具備黑箱屬性。解釋模型並解釋其行爲的能力對於這些行業至關重要，因爲它們涉及處理高風險問題並且必須受到嚴格監管。
    > 
    > **可解釋神經網絡**
    > 
    > 可解釋神經網絡（XNN）是機器學習模型的一項新進展，旨在爲模型提供可解釋的洞察力，消除其黑箱屬性。這意味着它能夠提供關於特徵以及網絡完成輸出（預測）過程中所學得的非線性變換的直白解釋。通過該模型，研究者能清楚地解釋輸入特徵與複雜神經網絡輸出之間的關係，因爲 XNN 網絡結構包含解釋這種關係的機制，並能對可視化該網絡所學習的函數起到幫助作用。
    > 
    > XNN 基於加性索引模型的概念，如下所示：
    > 
    > f (x) = g1 β1T x+ g2 β2T x+ - - - + gK βKT x
    > 
    > 左側的函數可以表示爲 K 個平滑函數 gi(-) 的和。這些平滑函數（即嶺函數）都用於在網絡中訓練的輸入特徵的線性組合（βiT x）。這使得加性索引模型能夠提供靈活的框架，通過嶺函數逼近網絡內的任意複雜函數，從而提供關於特徵和網絡學得的非線性變換的解釋。
    > 
    > **可解釋神經網絡架構**
    > 
    > 可解釋神經網絡提供加性索引模型的替代公式作爲結構化神經網絡。XNN 內置瞭解釋機制，這有助於解釋和理解模型內部過程以及該模型學到的函數。
    > 
    > 替代公式如下：
    > 
    > f (x) = μ + γ1 h1 β1T x+ γ2 h2 β1T x+ - - - + γK hK βKT x
    > 
    > 位移參數 μ 和尺度參數 γk 被用於模型擬合：通過正則化選擇適當數量的嶺函數。
    > 
    > XNN 結構中三個重要的組成部分包括：
    > 
    > i) 投影層（第一個隱藏層）；
    > 
    > ii) 子網絡（下圖的中間部分）；
    > 
    > iii) 組合層（最後的隱藏層）。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD715390861502396379I.png)
    > 
    > *XNN 結構*
    > 
    > **投影層**
    > 
    > 輸入層包含將輸入神經網絡的所有信息。輸入層全連接到投影層，在投影層上傳遞特徵（信息）。投影層由 K 個節點組成（每個節點對應一個嶺函數）。第一個隱藏層中的節點 i 的權重對應相應嶺函數輸入的係數（βi）。嶺函數有助於逼近輸入特徵中的複雜函數。投影層使用線性激活函數，以確保該層中的每個節點都學習輸入特徵的線性組合。應用了嶺函數的投影層中每個節點的輸出恰好可以用作一個子網絡的輸入。
    > 
    > **子網絡**
    > 
    > 子網絡主要用於學習應用於輸入特徵的嶺函數。嶺函數對於子網絡而言非常重要，因爲它們逼近投影層的複雜函數。這使得子網絡更容易學習並提供所習得的嶺函數的可解釋性，從而使數據分析師有能力理解子網絡的運作、理解從輸入到輸出的過程。子網絡只需要有足夠的結構，使每一個子網絡都能學習大量單變量函數。在模擬中，研究者發現，使用由具有非線性激活函數的兩個隱藏層組成的子網絡，足以在擬合模型時學習足夠靈活的嶺函數。
    > 
    > **組合層**
    > 
    > 組合層是 XNN 的最後一個隱藏層，由單個節點組成。節點的輸入包括嶺函數的所有輸出以及在子網絡中學習和添加的權重。在該層上使用線性激活函數，因此整個網絡的輸出是所有嶺函數的加權和的線性組合。
    > 
    > **XNN 組件可視化**
    > 
    > 內置於 xNN 中的結構（如投影層和子網絡）提供了一種機制來解釋這種網絡所學習的函數。該內置機制用投影和單變量嶺函數這些相對簡單的術語描述模型學得的複雜函數，以確保模型可解釋。
    > 
    > 下圖展示了研究者如何解釋和可視化子網絡中的單變量變換和嶺函數。
    > 
    > ![](http://i2.bangqu.com/j/news/20181009/ZpsRD71539086151909i173u.png)
    > 
    > *嶺函數（左）和訓練的 XNN 的對應投影索引（右）。*
    > 
    > 第一列解釋了單變量函數，其中包含由子網絡學習到達其輸出的權重。第二列顯示 βi 的值，即投影係數。投影係數說明輸入特徵的哪個組合在通過子網絡之前用作了每個嶺函數的輸入。這非常有用，因爲上圖展示了網絡最相關的特徵：縮放嶺函數和投影係數。
    > 
    > 在上面的例子中，從 xNN 的結構我們可以看出 Subnetwork 1 已經學習了立方勒讓德函數 (f3(-))，Subnetwork 2 已經學習了二次函數 (f2(-))，並且只有 x2 的係數非零。
    > 
    > **XNN 作爲替代模型**
    > 
    > XNN 還可以用作機器學習模型的替代模型，例如隨機森林（RF）和前饋神經網絡（FNN）。
    > 
    > *![](http://i2.bangqu.com/j/news/20181009/ZpsRD7153908615363721hHp.gif)有點像這樣......但不完全如此。*
    > 
    > 在這種情況下，RF 和 FNN 被認爲是基礎模型。由於 XNN 被設計爲一個可解釋模型，因此我們可以使用輸入特徵和基礎模型預測的相應輸出值來訓練 XNN。然後，XNN 就可以解釋基礎模型所學到的關係！
    > 
    > 使用更容易解釋的替代模型來幫助解釋複雜的機器學習模型，極大地增加了將機器學習模型融入不同行業的能力。
    > 
    > 可解釋神經網絡（XNN）是一個關鍵的機器學習模型。與其他機器學習模型不同，它能「打開」神經網絡的黑箱。該模型的結構和設計方式使其可以解釋學習的特徵以及導致其輸出或預測值的函數。這些可解釋性特徵非常吸引人，它本質上是有可加性的，並且能通過納入神經網絡的機制（如子網絡）直接得到解釋。
    > 
    > 無論將 XNN 用作主要模型還是用於更復雜模型的替代模型，XNN 都可以直接解釋模型如何使用輸入特徵進行預測。這項技術爲將機器學習模型整合入衆多不同行業提供了巨大的優勢，因爲它能夠超越現有系統，並且能夠清晰解釋它如何獲得輸出。
    > 
    > -   相關論文：Explainable Neural Networks based on Additive Index Models
    > 
    > -   論文地址：https://arxiv.org/abs/1806.01933
    > 
    > *原文鏈接：https://medium.com/@shagunm1210/the-explainable-neural-network-8f95256dcddb*

### [1810.10708] Learning with Interpretable Structure from RNN

- [[1810.10708] Learning with Interpretable Structure from RNN](https://arxiv.org/abs/1810.10708)

- [周志華等提出 RNN 可解釋性方法，看看 RNN 內部都幹了些什麼 - 幫趣](http://bangqu.com/2WM1X2.html)

    > 結構化學習（Structure learning）的主要任務是處理結構化的輸出，它不像分類問題那樣爲每個獨立的樣本預測一個值。這裏所說的結構可以是圖、序列、樹形結構和向量等。一般用於結構化輸出的機器學習算法有各種概率圖模型、感知機和 SVM 等。在過去的數十年裏，結構化學習已經廣泛應用於目標追蹤、目標定位和語義解析等任務，而多標籤學習和聚類等很多問題同樣與結構化學習有很強的關聯。
    > 
    > 一般來說，結構化學習會使用結構化標註作爲監督信息，並藉助相應的算法來預測這些結構化信息而實現優良的性能。然而，隨着機器學習算法變得越來越複雜，它們的可解釋性則變得越來越重要，這裏的可解釋性指的是如何理解學習過程的內在機制或內部流程。在這篇論文中，周志華等研究者重點關注深度學習模型，並探索如何學習這些模型的結構以提升模型可解釋性。
    > 
    > 探索深度學習模型的可解釋性通常都比較困難，然而對於 RNN 等特定類型的深度學習模型，我們還是有方法解決的。循環神經網絡（RNN）作爲深度神經網絡中的主要組成部分，它們在各種序列數據任務中有非常廣泛的應用，特別是那些帶有門控機制的變體，例如帶有一個門控的 MGU、帶有兩個門控的 GRU 和三個門控的 LSTM。
    > 
    > 除了我們熟悉的 RNN 以外，還有另一種工具也能捕捉序列數據，即有限狀態機（Finite State Automaton/FSA）。FSA 由有限狀態和狀態之間的轉換組成，它將從一個狀態轉換爲另一個狀態以響應外部序列輸入。FSA 的轉換過程有點類似於 RNN，因爲它們都是一個一個接收序列中的輸入元素，並在相應的狀態間傳遞。與 RNN 不同的是，FSA 的內部機制更容易被解釋，因爲我們更容易模擬它的過程。此外，FSA 在狀態間的轉換具有物理意義，而 RNN 只有數值計算的意義。
    > 
    > FSA 的這些特性令周志華團隊探索從 RNN 中學習一個 FSA 模型，並利用 FSA 的天然可解釋性能力來理解 RNN 的內部機制，因此周志華等研究者採用 FSA 作爲他們尋求的可解釋結構。此外，這一項研究與之前關於結構化學習的探索不同。之前的方法主要關注結構化的預測或分類結果，這一篇文章主要關注中間隱藏層的輸出結構，這樣才能更好地理解 RNN 的內在機制。
    > 
    > 爲了從 RNN 中學習 FSA，並使用 FSA 解釋 RNN 的內在機制，我們需要知道如何學習 FSA 以及具體解釋 RNN 中的什麼。對於如何學習 FSA，研究者發現非門控的經典 RNN 隱藏狀態傾向於構造一些集羣。但是仍然存在一些重要的未解決問題，其中之一是我們不知道構造集羣的傾向在門控 RNN 中是否也存在。我們同樣需要考慮效率問題，因爲門控 RNN 變體通常用於大型數據集中。對於具體解釋 RNN 中的什麼，研究者分析了門控機制在 LSTM、GRU 和 MGU 等模型中的作用，特別是不同門控 RNN 中門的數量及其影響。鑑於 FSA 中狀態之間的轉換是有物理意義的，因此我們可以從與 RNN 對應的 FSA 轉換推斷出語義意義。
    > 
    > 在這篇論文中，周志華等研究者嘗試從 RNN 學習 FSA，他們首先驗證了除不帶門控的經典 RNN 外，其它門控 RNN 變體的隱藏狀態同樣也具有天然的集羣屬性。然後他們提出了兩種方法，其一是高效的聚類方法 k-means++。另外一種方法根據若相同序列中隱藏狀態相近，在幾何空間內也相近的現象而提出，這一方法被命名爲 k-means-x。隨後研究者通過設計五個必要的元素來學習 FSA，即字母表、一組狀態、初始狀態、一組接受狀態和狀態轉換，他們最後將這些方法應用到了模擬數據和真實數據中。
    > 
    > 對於人工模擬數據，研究者首先表示我們可以理解在運行過程學習到的 FSA。然後他們展示了準確率和集羣數量之間的關係，並表示門控機制對於門控 RNN 是必要的，並且門越少越好。這在一定程度上解釋了爲什麼只有一個門控的 MGU 在某種程度上優於其它門控 RNN。
    > 
    > 對於情感分析這一真實數據，研究者發現在數值計算的背後，RNN 的隱藏狀態確實具有區分語義差異性的能力。因爲在對應的 FSA 中，導致正類 / 負類輸出的詞確實在做一些正面或負面的人類情感理解。
    > 
    > **論文：Learning with Interpretable Structure from RNN**
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X21541311204867JL852.png)
    > 
    > 論文地址：https://arxiv.org/pdf/1810.10708.pdf
    > 
    > 摘要：在結構化學習中，輸出通常是一個結構，可以作爲監督信息用於獲取良好的性能。考慮到深度學習可解釋性在近年來受到了越來越多的關注，如果我們能重深度學習模型中學到可解釋的結構，將是很有幫助的。在本文中，我們聚焦於循環神經網絡（RNN），它的內部機制目前仍然是沒有得到清晰的理解。我們發現處理序列數據的有限狀態機（FSA）有更加可解釋的內部機制，並且可以從 RNN 學習出來作爲可解釋結構。我們提出了兩種不同的聚類方法來從 RNN 學習 FSA。我們首先給出 FSA 的圖形，以展示它的可解釋性。從 FSA 的角度，我們分析了 RNN 的性能如何受到門控數量的影響，以及數值隱藏狀態轉換背後的語義含義。我們的結果表明有簡單門控結構的 RNN 例如最小門控單元（MGU）的表現更好，並且 FSA 中的轉換可以得到和對應單詞相關的特定分類結果，該過程對於人類而言是可理解的。
    > 
    > **本文的方法**
    > 
    > 在這一部分，我們介紹提出方法的直覺來源和方法框架。我們將 RNN 的隱藏狀態表示爲一個向量或一個點。因此當多個序列被輸入到 RNN 時，會積累大量的隱藏狀態點，並且它們傾向於構成集羣。爲了驗證該結論，我們展示了在 MGU、SRU、GRU 和 LSTM 上的隱藏狀態點的分佈，如圖 1（a）到（d）所示。
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X21541311206396395Z3.png)*圖 1：隱藏狀態點由 t-SNE 方法降維成 2 個維度，我們可以看到隱藏狀態點傾向於構成集羣。*
    > 
    > 圖 2 展示了整個框架。我們首先在訓練數據集上訓練 RNN，然後再對應驗證數據 V 的所有隱藏狀態 H 上執行聚類，最後學習一個關於 V 的 FSA。再得到 FSA 後，我們可以使用它來測試未標記數據或直接畫出圖示。再訓練 RNN 的第一步，我們利用了和 [ZWZZ16] 相同的策略，在這裏忽略了細節。之後，我們會詳細介紹隱藏狀態聚類和 FSA 學習步驟（參見原文）。
    > 
    > *![](http://i2.bangqu.com/j/news/20181104/2WM1X215413112075895xw47.png)*圖 2：本文提出算法的框架展示。黃色圓圈表示隱藏狀態，由 h_t 表示，這裏 t 是時間步。「A」是循環單元，接收輸入 x_t 和 h_t-1 並輸出 h_t。結構化 FSA 的雙圓圈是接受狀態。總體來說，該框架由三個步驟構成，即訓練 RN 你模型、聚類隱藏狀態和輸出結構化 FSA。**
    > 
    > 完整的從 RNN 學習 FSA 的過程如算法 1 所示。我們將該方法稱爲 LISOR，並展示了兩種不同的聚類算法。基於 k-means++ 的被稱爲「LISOR-k」，基於 k-means-x 的被稱爲「LISOR-x」。通過利用構成隱藏狀態點的聚類傾向，LISOR-k 和 LISOR-x 都可以從 RNN 學習到良好泛化的 FSA。
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X2154131120937623Msg.png)
    > 
    > **實驗結果**
    > 
    > 在這一部分，我們在人工和真實任務上進行了實驗，並可視化了從對應 RNN 模型學習到的 FSA。除此之外，在兩個任務中，我們討論了我們如何從 FSA 解釋 RNN 模型，以及展示使用學習到的 FSA 來做分類的準確率。
    > 
    > 第一個人工任務是在一組長度爲 4 的序列中（只包含 0 和 1）識別序列「0110」（任務「0110」）. 這是一個簡單的只包含 16 個不同案例的任務。我們在訓練集中包含了 1000 個實例，通過重複實例來提高準確率。我們使用包含所有可能值且沒有重複的長度爲 4 的 0-1 序列來學習 FSA，並隨機生成 100 個實例來做測試。
    > 
    > 第二個人工任務是確定一個序列是否包含三個連續的 0（任務「000」）。這裏對於序列的長度沒有限制，因此該任務有無限的實例空間，並且比任務「0110」更困難。我們隨機生成 3000 個 0-1 訓練實例，其長度是隨機確定的。我們還生成了 500 個驗證實例和 500 個測試實例。
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X2154131121204729DtT.png)*表 2：分別基於 LISOR-k 和 LISOR-x 方法，當從 4 個 RNN 中學習到的 FSA 在任務「0110」的準確率首次達到 1.0 時，集羣的數量（n_c）。注意這些值是越小越高效，並且可解釋性越好。不同試驗中訓練得到的 RNN 模型使用了不同的參數初始化。*
    > 
    > 如表 2 所示，我們可以看到在從 MGU 學習到的 FSA 的平均集羣數量總是能以最小的集羣數量達到準確率 1.0。集羣數量爲 65 意味着 FSA 的準確率在直到 n_c 爲 64 時都無法達到 1.0。每次試驗的最小集羣數量和平均最小集羣數量加粗表示。
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X21541311212598Y5c7J.png)*表 3：分別基於 LISOR-k 和 LISOR-x 方法，當從 4 個 RNNzho 中學習到的 FSA 在任務「000」的準確率首次達到 0.7 時，集羣的數量（n_c）。注意這些值是越小越高效，並且可解釋性越好。*
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X2154131121454364mc6.png)*圖 3：在任務「0110」訓練 4 個 RNN 時學習得到的 FSA 結構圖示。集羣數量 k 由 FSA 首次達到準確率 1.0 時的聚類數量決定。0110 的路由用紅色表示。注意在圖（d）中由 4 個獨立於主要部分的節點。這是因爲我們捨棄了當輸入一個符號來學習一個確定性 FSA 時更小頻率的轉換。*
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X215413112172784I5ir.png)*圖 4：在任務「000」訓練 4 個 RNN 時學習得到的 FSA 結構圖示。集羣數量 k 由 FSA 首次達到準確率 0.7 時的集羣數量決定。*
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X215413112192119i42M.png)*圖 7：在情感分析任務上訓練的 MGU 學習到的 FSA。這裏的 FSA 經過壓縮，並且相同方向上的相同兩個狀態之間的詞被分成同一個詞類。例如，「word class 0-1」中的詞全部表示從狀態 0 轉換爲狀態 1。*
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X2154131122036966271.png)*表 5：從狀態 0 轉換的詞稱爲可接受狀態（即包含積極電影評論的狀態 1），其中大多數詞都是積極的。這裏括號中的數字表示詞來源的 FSA 編號。*
    > 
    > ![](http://i2.bangqu.com/j/news/20181104/2WM1X21541311224544826S7.png)*表 4：當集羣數量爲 2 時情感分類任務的準確率。*
    > 
    > 文章來源：[機器之心](https://www.jiqizhixin.com/articles/110404)

## LIME 

- [[1602.04938] "Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)

- [Introduction to Local Interpretable Model-Agnostic Explanations (LIME) - O'Reilly Media](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)

- [凭什么相信你，我的CNN模型？（篇二：万金油LIME)](https://bindog.github.io/blog/2018/02/11/model-explanation-2/)

    > ## 0x01 LIME
    > 
    > [LIME](http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)是KDD 2016上一篇非常漂亮的论文，思路简洁明了，适用性广，理论上可以解释任何分类器给出的结果。其核心思想是：对一个复杂的分类模型(黑盒)，在**局部**拟合出一个简单的可解释模型，例如线性模型、决策树等等。这样说比较笼统，我们从论文中的一张示例图来解释：
    > 
    > ![LIME](https://i.imgur.com/emp2zi0.jpg)
    > 
    > 如图所示，红色和蓝色区域表示一个复杂的分类模型（黑盒），图中加粗的红色十字表示需要解释的样本，显然，我们很难从全局用一个可解释的模型（例如线性模型）去逼近拟合它。但是，当我们把关注点从全局放到局部时，可以看到在某些局部是可以用线性模型去拟合的。具体来说，我们从加粗的红色十字样本周围采样，所谓采样就是对原始样本的特征做一些扰动，将采样出的样本用分类模型分类并得到结果（红十字和蓝色点），同时根据采样样本与加粗红十字的距离赋予权重（权重以标志的大小表示）。虚线表示通过这些采样样本学到的局部可解释模型，在这个例子中就是一个简单的线性分类器。在此基础上，我们就可以依据这个局部的可解释模型对这个分类结果进行解释了。
    > 
    > 一个看似复杂的模型通过我们巧妙的转换，就能够从局部上得到一个让人类理解的解释模型，光这样说还是显得有些空洞，具体来看看LIME在图像识别上的应用。我们希望LIME最好能生成和Grad-CAM一样的热力图解释。但是由于LIME不介入模型的内部，需要不断的扰动样本特征，这里所谓的样本特征就是指图片中一个一个的像素了。仔细一想就知道存在一个问题，LIME采样的特征空间太大的话，效率会非常低，而一张普通图片的像素少说也有上万个。若直接把每个像素视为一个特征，采样的空间过于庞大，严重影响效率；如果少采样一些，最终效果又会比较差。
    > 
    > 所以针对图像任务使用LIME时还需要一些特别的技巧，也就是考虑图像的空间相关和连续的特性。不考虑一些极小特例的情况下，图片中的物体一般都是由一个或几个连续的像素块构成，所谓像素块是指具有相似纹理、颜色、亮度等特征的相邻像素构成的有一定视觉意义的不规则像素块，我们称之为**超像素**。相应的，将图片分割成一个个超像素的算法称为**超像素分割算法**，比较典型的有SLIC超像素分割算法还有quickshit等，这些算法在`scikit-image`库中都已经实现好了，quickshit分割后如图所示：
    > 
    > ![mm](https://i.imgur.com/4QYC5kJ.jpg)
    > 
    > 从特征的角度考虑，实际上就不再以单个像素为特征，而是以超像素为特征，整个图片的特征空间就小了很多，采样的过程也变的简单了许多。更具体的说，图像上的采样过程就是随机保留一部分超像素，隐藏另一部分超像素，如下所示：
    > 
    > ![light](https://i.imgur.com/Mb31DFf.jpg)
    > 
    > 从图中可以很直观的看出这么做的意义：找出对分类结果影响最大的几个超像素，也就是说模型仅通过这几个像素块就已经能够自信的做出预测。这里还涉及到一个特征选择的问题，毕竟我们不可能穷举特征空间所有可能的样本，所以需要在有限个样本中找出那些关键的超像素块。虽然这部分没有在论文中过多提及，但在LIME的[代码实现](https://github.com/marcotcr/lime)中是一个重要部分，实现了前向搜索（forward selection）、Lasso和岭回归（ridge regression）等特征选择方式，默认当特征数小于等于6时采用前向搜索，其他情况采用岭回归。
    > 
    > 整体流程如图所示：
    > 
    > ![flow](https://i.imgur.com/5Oo80Ph.jpg)
    > 
    > 和Grad-CAM一样，LIME同样可以对其他可能的分类结果进行解释。
    > 
    > ![effect](https://i.imgur.com/3I3pNrM.jpg)
    > 
    > LIME除了能够对图像的分类结果进行解释外，还可以应用到自然语言处理的相关任务中，如主题分类、词性标注等。因为LIME本身的出发点就是模型无关的，具有广泛的适用性。
    > 
    > 虽然LIME方法虽然有着很强的通用性，效果也挺好，但是在速度上却远远不如Grad-CAM那些方法来的快。当然这也是可以理解的，毕竟LIME在采样完成后，每张采样出来的图片都要通过原模型预测一次结果。
    > 
    > 说来也巧，在写这篇文章的时候，AAAI 2018的论文放出来了，其中有LIME作者的最新研究成果[Anchors](http://sameersingh.org/files/papers/anchors-aaai18.pdf)，顺道去了解了一下。Anchors指的是复杂模型在局部所呈现出来的很强的规则性的规律，注意和LIME的区别，LIME是在局部建立一个可理解的线性可分模型，而Anchors的目的是建立一套更精细的规则系统。不过看过论文以后感觉更多是在和文本相关的任务上有不错的表现，在图像相关的任务上并没有什么特别另人耳目一新的东西，只是说明了在Anchor（图像中指若干个超像素）固定的情况下，其他像素无论替换为什么，现有的模型都会罔顾人类常识，自信的做出错误判断。这部分内容由于前几年看多了Adversarial Samples，已经见怪不怪了。
    > 
    > ## 0x02 小结
    > 
    > 实际上在模型可解释性这块还有其他很多相关研究，包括最近的AAAI 2018上也有几篇这方面的文章，如[Beyond Sparsity: Tree Regularization of Deep Models for Interpretability](https://arxiv.org/abs/1711.06178)，这都在一定程度上说明，业内还是重视这个方向的。尤其在涉及到医疗、自动驾驶等人命关天的应用场合，可解释性显得尤为重要，最后也希望更多感兴趣的同学加入到这个行列来~
    > 


## PLNN

- [裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡 - 幫趣](http://bangqu.com/HqmsS6.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > 這篇論文研究了以**分段線性函數**爲激活函數的**分段線性神經網絡（Piecewise Linear Neural Network， PLNN）**。分段線性函數在不同的定義域區間內對應不同的線性函數。經典的 MaxOut 、ReLU 以及 ReLU 的一些變體都是分段線性函數。從微分學的角度來看，只要分段數目足夠多，連續光滑的 sigmoid 、tanh 等激活函數也都可以用分段線性函數來無限逼近。
    > 
    > 與現有的規範做法一樣，**該論文通過求解一個分段線性神經網絡 *N* 的決策特徵來解釋 *N* 的決策行爲。但與現有方法大爲不同的是，論文對 *N* 的解釋具有如下兩個獨特的優點**：
    > 
    > 1\. **準確性（Exactness）**：該論文構造了一個具有簡潔**解析形式**的新模型 *M* ，並證明了 *M* 和 *N* 在數學上等價。因此，*M* 的決策特徵能夠**準確**描述 *N* 的真實決策行爲。
    > 
    > 2\. **一致性（Consistency）**：該論文證明了 *M* 是一個分段線性函數，並以**解析形式**給出了 *M* 在其定義域中的各個分段區間，以及 *M* 在每個區間上的線性決策函數。因爲在相同分段區間中的所有輸入實例共享同一個線性決策函數的決策特徵，所以由模型 *M* 對這些輸入實例所提供的解釋是**完全一致**的。
    > 
    > ### OpenBox - 通向準確性和一致性的金鑰匙
    > 
    > 該論文的作者們提出了全新的 OpenBox 方法對分段線性神經網絡（PLNN）的決策行爲提供準確、一致的解釋。「OpenBox」 這個名字也很貼切地描述了作者們使用簡潔的解析方法「打開」深度神經網絡這個「黑盒子」的過程。
    > 
    > OpenBox 方法適用於所有 PLNN。本文將用以 PReLU 爲激活函數的 PLNN 爲例子詳細介紹 OpenBox 方法的技術要點。
    > 
    > **1\. 對單個輸入實例的準確解釋方法**
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306184692f5.jpg)
    > 
    > ##### 圖 2：隱層神經元的激活狀態（status）
    > 
    > 如圖 2 所示，對於以 PReLU 爲激活函數的隱層神經元，其**激活狀態（status）**分爲兩種情況：（1）當 status = 0 時，*z* < 0，該神經元使用左半段的線性函數來建立輸入 *z* 和輸出 *a* 之間的映射關係；（2）當 status = 1 時，*z* >= 0，該神經元使用右半段的線性函數來建立 *z* 到 *a* 的映射。**值得注意的是，不論神經元處於何種激活狀態，*z* 和 *a* 之間的映射關係始終是線性的**。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b3062540b557.jpg)
    > 
    > ##### 圖 3：一個 PLNN 和其隱層神經元的激活狀態
    > 
    > 如圖 3 所示，給定一個輸入實例 *x* ，我們可以將所有隱層神經元的激活狀態按綠色虛線所示的順序排列成一個向量 Conf(*x*)。這個向量被稱作 PLNN 對輸入實例 *x* 的**配置（Configuration）**。
    > 
    > 由於 PLNN 的網絡結構和參數都是給定的，所有神經元的激活狀態都唯一依賴於輸入實例 *x*，因此 Conf(*x*) 由輸入實例 *x* 唯一決定。因爲 *x* 本身是一個給定的常量，所以 Conf(*x*) 也是一個常量。因此，圖 3 中 PLNN 的每個隱層神經元的運算實質上都是由常量 Conf(*x*) 所確定的線性運算。因爲一系列線性運算的嵌套依然是線性運算，**所以在 Conf(*x*) 爲常量的情況下，PLNN 中所有隱藏層的運算整體等價於一個簡單的線性運算 *Wx+b***。
    > 
    > **綜上所述，對於任意給定的輸入實例 *x*，整個 PLNN 嚴格等價於如公式 1 所示的線性分類器。其中，二元組 (*W*, *b*) 以解析形式準確地給出了該 PLNN 對於輸入實例 x 的決策平面。**（注：證明及求解過程請參見原文）
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306575eaffa.jpg)
    > 
    > ##### 公式 1
    > 
    > 顯然，解釋 PLNN 在單個輸入實例上的決策行爲並不能很好地解釋 PLNN 的總體行爲。下面我們將介紹 OpenBox 如何解釋 PLNN 的總體行爲。
    > 
    > **2\. 對一個分段線性神經網絡的準確、一致解釋方法**
    > 
    > 作者們發現，在 PLNN 的網絡結構和參數給定的情況下，公式 1 中的線性分類器 *F(x)* 由 Conf(*x*) 決定。**這意味着對於任意兩個不同的輸入實例 *x* 和 *x'* 而言，只要 Conf(*x*)＝Conf(*x'*)，*x* 和 *x'* 就共享同一個線性分類器，而且對 *x* 和 *x'* 的解釋也將完全一致**。
    > 
    > 那麼，輸入實例 *x* 和 *x'* 需要滿足什麼條件，才能使 Conf(*x*)＝Conf(*x'*) 呢？ 
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b3065f3c1251.jpg)
    > 
    > ##### 圖 4：在 Conf(*x*) 給定的情況下，每一個隱層神經元的輸入 *z* 所必須滿足的不等式約束
    > 
    > 通過進一步推導，作者們發現在 Conf(*x*) 給定的情況下，每一個隱層神經元的輸入 *z* 都必須滿足由該神經元激活狀態所決定的不等式約束。圖 4 給出了當 Conf(x) = [1, 0, 1, 0, 0, 1, 1] 時， PLNN 的所有隱層神經元的輸入 z 必須滿足的一組**線性不等式約束**。
    > 
    > 因爲每個隱層神經元的輸入 *z* 都是輸入實例 *x* 的線性函數，所以這組關於輸入 *z* 的線性不等式約束實質上是對輸入實例 *x* 的一組線性不等式約束。我們將這組線性不等式約束的集合定義爲 *P*。
    > 
    > **很顯然，所有滿足 *P* 中線性不等式約束的輸入實例 *x* 都具有相同的 Conf(*x*)，因此這些實例共享同一個線性分類器，並具有完全一致的解釋。**
    > 
    > 實質上，*P* 中的每一個不等式都定義了一個線性邊界，所有線性邊界一起組成了一個**凸多面體（Convex Polytope，CP）**。在凸多面體中的所有輸入實例都滿足 *P* 中的所有不等式，因此這些輸入實例 *x* 都具有相同的 Conf(*x*)，並且共享同一個線性分類器。我們把這個存在於局部區域的凸多面體和它所對應的線性分類器統稱爲**局部線性分類器（Local Linear Classifier，LLC）**。
    > 
    > **對於任意給定的 PLNN，不同的隱層神經元激活狀態對應着不同的 Conf(*x*)，而每一個 Conf(*x*) 都確定了一個局部線性分類器。因此，一個 PLNN 嚴格等價於一組局部線性分類器。我們把這組局部線性分類器的集合標記爲 *M*，並將其作爲 PLNN 的解釋模型。**
    > 
    > **因爲 *M* 和 PLNN 是等價的，而且同一個凸多面體中的所有實例都共享同樣的解釋，所以由 *M* 所得到的解釋是準確且一致的。**
    > 
    > 給定一個輸入實例 *x*，我們如何使用 *M* 來解釋 PLNN 對 *x* 的決策行爲呢？
    > 
    > 首先，我們從 *M* 中找到 *x* 所屬的局部線性分類器。然後，我們解析出該局部線性分類器的**決策特徵（Decision Feature）**以及其凸多面體的**邊界特徵（Polytope Boundary Feature，PBF）**。最後，我們使用決策特徵來解釋 PLNN 對 *x* 的決策行爲，並使用邊界特徵來解釋 *x* 被當前局部線性分類器包含的原因。
    > 
    > 論文還對計算 *M* 的時間複雜度進行了嚴格的理論分析和證明。**對於 *n* 個不同的輸入實例，若每個輸入實例的特徵維數爲 *d*，OpenBox 解釋所有輸入實例的時間複雜度僅爲 O(*nd*)。因爲特徵維數 *d* 通常被看作常量，所以 OpenBox 的時間複雜度是線性的。**
    > 
    > ### 實驗部分
    > 
    > 作者們把 OpenBox 和目前最頂級的解釋方法 LIME［Ribeiro *et al*. KDD 2016］做了實驗對比。實驗重點關注以下五個問題：
    > 
    > 1\. 局部線性分類器長什麼樣？
    > 
    > 2\. LIME 和 OpenBox 給出的解釋是否準確、一致？
    > 
    > 3\. 局部線性分類器的決策特徵易於理解嗎？如果附加非負、稀疏約束，能繼續提升這些決策特徵的語義特性嗎？
    > 
    > 4\. 如何解釋局部線性分類器的邊界特徵（PBF）？
    > 
    > 5\. 利用 OpenBox 提供的解釋，我們能否構造新樣本來欺騙 PLNN？能否查出 PLNN 在某些樣本上做出錯誤決策的原因？
    > 
    > **實驗一：合成數據集可視化局部線性分類器**
    > 
    > 如圖 5(a) 所示，作者們通過二維歐式空間中的均勻採樣生成了一個包含 20,000 個實例的合成數據集 SYN。其中，紅色和藍色樣本點分別代表正例和負例。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b307e27943ca.jpg)
    > 
    > 圖 5：OpenBox 在合成數據集 SYN 上的實驗結果
    > 
    > 由於實驗目標是可視化模型 *M* 中的所有局部線性分類器，整個實驗過程無需使用測試數據，因此作者們使用 SYN 中的所有樣本來訓練 PLNN。圖 5(b) 顯示了 PLNN 在 SYN 上的預測結果。
    > 
    > 圖 5(c) 可視化了模型 *M* 中每一個局部線性分類器對應的凸多面體。 作者們用相同的顏色標出了屬於同一個局部線性分類器的所有實例，發現屬於相同局部線性分類器的實例都包含於同一個凸多面體（在二維空間中表現爲凸多邊形）。顯然，這個結果完全符合論文的理論分析。
    > 
    > 圖 5(d) 展示了構成模型 *M* 的決策邊界的所有局部線性分類器。圖中的每一條實線都表示一個局部線性分類器的決策邊界，這些局部線性分類器共同構成了模型 *M* 的總體決策邊界。對比圖 5(b) 和 5(d) 可以發現模型 *M* 的總體決策邊界和 PLNN 的決策邊界完全一致。這個結果證實了模型 *M* 和 PLNN 之間的等價性。
    > 
    > **實驗二：FMNIST 數據集驗證解釋的準確性和一致性**
    > 
    > 該實驗在 FMNIST 數據集上對比了 LIME 和 OpenBox（模型M）所提供解釋的準確性和一致性。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b307e430525a.jpg)
    > 
    > ##### 圖 6: OpenBox 和 LIME 在 FMNIST-2 數據集上的準確性（Exactness）和一致性（Consistency）
    > 
    > 首先，作者們通過比較 LIME、OpenBox（模型 *M*）和 PLNN 對 FMNIST-2 數據集中 600 個測試樣本的決策輸出來衡量 LIME 和 OpenBox 各自解釋模型的準確性。如圖 6(a) 所示，LIME 的決策輸出和 PLNN 的決策輸出有着很大不同，這說明 LIME 的解釋模型和 PLNN 非常不同，因此它無法準確解釋 PLNN 的決策行爲。相比之下，OpenBox 計算出的模型 *M* 和 PLNN 對於所有測試樣本的決策輸出完全相同，這說明模型 M 等價於 PLNN，因此它能夠準確地解釋 PLNN 的決策行爲。
    > 
    > 隨後，作者們使用輸入實例 *x* 和其最近鄰實例 *x'* 的解釋結果的餘弦相似度（Cosine Similarity）來衡量 LIME 和 OpenBox 所提供解釋的一致性。餘弦相似度越高，解釋模型所提供解釋的一致性就越高。如圖 6(b) 所示，由於模型 *M* 對同一凸多面體內的實例提供完全相同的解釋，OpenBox 的餘弦相似度幾乎總保持爲 1。但是最近鄰實例 *x'* 與 輸入實例 *x*  並不總是屬於同一個凸多面體，因此 OpenBox 在某些實例上的餘弦相似度小於 1。相比之下，LIME 的餘弦相似度遠低於 OpenBox，這說明 OpenBox 所提供解釋的一致性遠高於 LIME。
    > 
    > **實驗三：OpenBox 提取的決策特徵具有人類可理解的強語義特點**
    > 
    > 除了準確性和一致性，一個好的解釋還必須具有人類可理解的強語義特點。在本實驗中，作者們將 OpenBox 在 FMNIST-1 數據集上提取的決策特徵可視化，發現這些特徵具有易於理解的強語義特點。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306b12f3205.jpg)
    > 
    > 圖 7: OpenBox 和邏輯迴歸（Logistic Regression，LR）在 FMNIST-1 數據集上的決策特徵（Decision Feature，DF）
    > 
    > 圖 7(a) 和 7(f) 給出了 FMNIST-1 中的兩類圖像的平均圖（Average Image）。其中，圖 7(a) 對應正例樣本**短靴（Ankle Boot）**，圖 7(f) 對應負例樣本**包包（Bag）**。
    > 
    > 作者們訓練了多個羅輯迴歸模型（Logistic Regression，LR）作爲基線（Baseline）。其中，LR 模型是以短靴爲正樣本訓練得到的，LR-F 模型是以包包爲正樣本訓練得到的，LR-NS 和 LR-NSF 分別是在 LR 和 LR-F 的基礎上附加稀疏、非負約束得到的。此外，作者們還訓練了兩個 PLNN 模型作爲 OpenBox 的解釋對象。其中，PLNN 是以短靴爲正樣本訓練得到的，PLNN-NS 是在 PLNN 的基礎上附加稀疏、非負約束得到的。
    > 
    > 圖 7 給出了上述所有模型的決策特徵，其中 PLNN 和 PLNN-NS 的決策特徵由 OpenBox 提供。很明顯，PLNN 的決策特徵與 LR 和LR-F 的決策特徵具有極爲相似語義。將這些決策特徵與圖 7(a) 和 7(f) 中的平均圖仔細對比可以發現，這些決策特徵準確地描述了短靴和包包之間的差別。更有趣的是，PLNN 的決策特徵比 LR 和 LR-F 的決策特徵包含了更多細節信息。這是因爲 PLNN 的每一個局部線性分類器僅需區分包含於凸多面體中的一小部分樣本，所以 PLNN 能夠使用大量的局部線性分類器捕捉更多細節特徵。然而，LR 和 LR-F 只能使用一個線性平面劃分所有正負例樣本，因此它們只能捕捉大量樣本的平均差異。因爲 PLNN 捕捉到了更多細節特徵，所以它取得了比 LR 和 LR-F 好得多的分類精度。
    > 
    > 通過對比 PLNN-NS，LR-NS 和 LR-NSF 的決策特徵，我們發現非負、稀疏約束對於增強 PLNN-NS 決策特徵的語義同樣有效。我們還觀察到 PLNN-NS 捕獲了比 LR-NS 和 LR-NSF 多得多的細節特徵，也因此取得了相對較高的分類精度。
    > 
    > **實驗四：OpenBox 提取的邊界特徵也具有很強的語義特性**
    > 
    > 關於 OpenBox 所提取的局部線性分類器，不僅其決策特徵具有很強的語義特點，其凸多面體的邊界特徵也具有很強的語義特性。
    > 
    > ![裴健團隊KDD新作：革命性的新方法，準確、一致地解釋深度神經網絡](http://i2.bangqu.com/lf1/news/20180629/5b306b8ccf418.jpg)
    > 
    > ##### 圖 8: OpenBox 在 FMNIST-1 數據集上提取的邊界特徵（Polytope Boundary Feature，PBF）
    > 
    > 在本實驗中，作者們在 FMNIST-1 上訓練了一個 PLNN，並用 OpenBox 解析出該 PLNN 的三個局部線性分類器對應的凸多面體。圖 8(a)-(d) 給出了這些凸多面體的邊界特徵，它們分別對應了｛包包，短靴，包包，包包｝。圖 8(e) 給出了定義這些邊界特徵的線性不等式，以及其對應的凸多面體中所包含的各類別樣本數量。關於圖 8(e) 中的線性不等式，「／」代表該不等式定義的邊界爲無效邊界；「> 0」代表凸多面體內的樣本與該不等式的邊界特徵具有很強的相關性；「<= 0」 代表凸多面體內的樣本與該不等式的邊界特徵沒有強相關性。
    > 
    > 以圖 8(e) 中的第一個凸多面體爲例，由其線性不等式的狀態可知該凸多面體所包含的樣本與圖 8(b)-(c) 中短靴和包包的邊界特徵有強相關性。因此，第一個凸多面體中包含了大量的短靴和包包。類似的，對圖 8(e) 中的第二個凸多面體而言，其中的樣本僅與短靴的邊界特徵呈正相關，因此該凸多面體中的樣本僅有短靴而沒有包包。通過上述實驗結果不難看出，**OpenBox 提取的邊界特徵具有很強的語義特性**。
    > 
    > 除了上述精彩實驗，作者們還利用 OpenBox 提供的解釋來構造欺騙 PLNN 的新樣本，以及查找 PLNN 在某些樣本上做出錯誤決策的原因。在這些有趣的任務上，論文中的實驗也給出了明顯優於現有方法的結果。
    > 
    > ### 結論
    > 
    > 作者們通過證明分段線性神經網絡嚴格等價於一組局部線性分類器，以簡潔的解析形式給出了一種準確、一致且高效的神經網絡解釋方法------OpenBox。大量實驗結果表明，OpenBox 不僅可以準確、一致地描述分段線性神經網絡的總體行爲，還能夠對分段線性神經網絡進行有效的欺騙攻擊和錯誤查找。作者們談到，他們將繼續拓展這一方法，使其能夠有效地解釋使用連續、光滑激活函數（如：sigmoid、tanh）的深度神經網絡。
    > 
    > 詳細內容請參見原論文：<https://arxiv.org/abs/1802.06259>


