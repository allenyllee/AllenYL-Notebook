# 機器學習題庫2

[toc]
<!-- toc --> 

## [BAT机器学习面试1000题系列（126-130题）](https://zhuanlan.zhihu.com/p/31797529)

**126.衡量分类器的好坏。**

@我愛大泡泡，来源：[http://blog.csdn.net/woaidapaopao/article/details/77806273](https://link.zhihu.com/?target=http%3A//blog.csdn.net/woaidapaopao/article/details/77806273)\
　　这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。\
几种常用的指标：

精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）

召回率 recall = TP/(TP+FN) = TP/ P

F1值： 2/F1 = 1/recall + 1/precision

ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N

**127.机器学习和统计里面的auc的物理意义是什么？**

　　详情参见<https://www.zhihu.com/question/39840928>

**128.观察增益gain, alpha和gamma越大，增益越小？**

@AntZ：xgboost寻找分割点的标准是最大化gain. 考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中计算Gain按最大值找出最佳的分割点。它的计算公式分为四项, 可以由正则化项参数调整(lamda为叶子权重平方和的系数, gama为叶子数量):

![](https://pic3.zhimg.com/80/v2-0ab0e7d08ff2d04896fd9d07a4c37a8b_hd.jpg)

第一项是假设分割的左孩子的权重分数, 第二项为右孩子, 第三项为不分割总体分数, 最后一项为引入一个节点的复杂度损失 由公式可知, gama越大gain越小, lamda越大, gain可能小也可能大。 原问题是alpha而不是lambda, 这里paper上没有提到, xgboost实现上有这个参数. 上面是我从paper上理解的答案,下面是搜索到的: [https://zhidao.baidu.com/question/2121727290086699747.html?fr=iks&word=xgboost+lamda&ie=gbk](https://link.zhihu.com/?target=https%3A//zhidao.baidu.com/question/2121727290086699747.html%3Ffr%3Diks%26word%3Dxgboost%2Blamda%26ie%3Dgbk) lambda[默认1]权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。11、alpha[默认1]权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。

**129.什么造成梯度消失问题? 推导一下**

@许韩，来源：<https://www.zhihu.com/question/41233373/answer/145404190>

Yes you should understand backdrop－Andrej Karpathy

How does the ReLu solve the vanishing gradient problem?

神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。

梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。

![](https://pic2.zhimg.com/80/v2-50e9ca26d8a49059dc627641faa3cfc3_hd.jpg)

**130.什么是梯度消失和梯度爆炸？**

@寒小阳，反向传播中链式法则带来的连乘，如果有数很小趋于0，结果就会特别小（梯度消失）；如果数都比较大，可能结果会很大（梯度爆炸）。 @单车，下段来源：<https://zhuanlan.zhihu.com/p/25631496> 层数比较多的神经网络模型在训练时也是会出现一些问题的，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。

例如，对于下图所示的含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。

![](https://pic1.zhimg.com/80/v2-9b3c96b0aa5d03e3db497c1a208739b5_hd.jpg)

而这种问题为何会产生呢？以下图的反向传播为例（假设每一层只有一个神经元且对于每一层

![](https://pic4.zhimg.com/80/v2-3fbae495563ce11fe62d8b1af076d3c7_hd.jpg)

，其中

![](https://pic2.zhimg.com/80/v2-ad833f75b3b19092f9c0c05f5f31de1f_hd.jpg)

为sigmoid函数）

![](https://pic3.zhimg.com/80/v2-4835fec6d18aee10d0f01ad3cc238a11_hd.jpg)

可以推导出

![](https://pic4.zhimg.com/80/v2-cd11a3e8a54f3a545c76f5cb246db0fc_hd.jpg)

而sigmoid的导数

![](https://pic3.zhimg.com/80/v2-d4a786658d3e6e78b01958db95373033_hd.jpg)

如下图

![](https://pic2.zhimg.com/80/v2-7847e89877399dce2893539cbb039088_hd.jpg)

可见，

![](https://pic3.zhimg.com/80/v2-d4a786658d3e6e78b01958db95373033_hd.jpg)

的最大值为1/4，而我们初始化的网络权值|w|通常都小于1，因此

![](https://pic1.zhimg.com/80/v2-6033932d58bd007fe04de5ff6e57e3b3_hd.jpg)

，因此对于上面的链式求导，层数越多，求导结果

![](https://pic4.zhimg.com/80/v2-ea1c7b5f6fbfe496a36c197ff14a648e_hd.jpg)

越小，因而导致梯度消失的情况出现。

这样，梯度爆炸问题的出现原因就显而易见了，即

![](https://pic2.zhimg.com/80/v2-4eb62e303fe26a55ebcae72c32edff9d_hd.jpg)

，也就是w比较大的情况。但对于使用sigmoid激活函数来说，这种情况比较少。因为

![](https://pic2.zhimg.com/80/v2-c06f71ad2c7987e68d824aa3cf51453f_hd.jpg)

的大小也与w有关（z=wx+b），除非该层的输入值x在一直一个比较小的范围内。

其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。

## [BAT机器学习面试1000题系列（131-135题）](https://zhuanlan.zhihu.com/p/31830550)

**131.如何解决梯度消失和梯度膨胀?**

（1）梯度消失：\
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0\
可以采用ReLU激活函数有效的解决梯度消失的情况\
（2）梯度膨胀\
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大\
可以通过激活函数来解决

**132.推导下反向传播Backpropagation**

@我愛大泡泡，来源：[http://blog.csdn.net/woaidapaopao/article/details/77806273](https://link.zhihu.com/?target=http%3A//blog.csdn.net/woaidapaopao/article/details/77806273)

反向传播是在求解损失函数L对参数w求导时候用到的方法，目的是通过链式法则对参数进行一层一层的求导。这里重点强调：要将参数进行随机初始化而不是全部置0，否则所有隐层的数值都会与输入相关，这称为对称失效。

大致过程是:

1.首先前向传导计算出所有节点的激活值和输出值，

![](https://pic4.zhimg.com/80/v2-7ae887b057729abbb695085f04f61e24_hd.jpg)

2.计算整体损失函数：

![](https://pic1.zhimg.com/80/v2-12e3a9d17c5ca8255881bedf9d9d1a72_hd.jpg)

3.然后针对第L层的每个节点计算出残差（这里是因为UFLDL中说的是残差，本质就是整体损失函数对每一层激活值Z的导数），所以要对W求导只要再乘上激活函数对W的导数即可

![](https://pic1.zhimg.com/80/v2-b678de038ada3d27706924d3ca1cb26e_hd.jpg)

![](https://pic2.zhimg.com/80/v2-a410531e04deb37c78e86c1763ffdb1e_hd.jpg)

**133.SVD和PCA**

　　PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。

**134.数据不平衡问题**

这主要是由于数据分布不平衡造成的。解决方法如下：

-   采样，对小样本加噪声采样，对大样本进行下采样
-   进行特殊的加权，如在Adaboost中或者SVM中
-   采用对不平衡数据集不敏感的算法
-   改变评价标准：用AUC/ROC来进行评价
-   采用Bagging/Boosting/ensemble等方法
-   考虑数据的先验分布

**135.简述神经网络的发展**

MP模型+sgn---->单层感知机（只能线性）+sgn--- Minsky 低谷 --->多层感知机+BP+sigmoid---- (低谷) --->深度学习+pre-training+ReLU/sigmoid


## [BAT机器学习面试1000题系列（136-140题）](https://zhuanlan.zhihu.com/p/31853224)

**136.深度学习常用方法**

@SmallisBig，来源：[http://blog.csdn.net/u010496169/article/details/73550487](https://link.zhihu.com/?target=http%3A//blog.csdn.net/u010496169/article/details/73550487)\
全连接DNN（相邻层相互连接、层内无连接）：\
AutoEncoder(尽可能还原输入)、Sparse Coding（在AE上加入L1规范）、RBM（解决概率问题）----->特征探测器------>栈式叠加 贪心训练\
RBM---->DBN\
解决全连接DNN的全连接问题----->CNN\
解决全连接DNN的无法对时间序列上变化进行建模的问题----->RNN---解决时间轴上的梯度消失问题------->LSTM

**137.神经网络模型（Neural Network）因受人类大脑的启发而得名。**

![](https://pic3.zhimg.com/80/v2-1ad08b7b25b2117daeadedfa8c8263d8_hd.jpg)

神经网络由许多神经元（Neuron）组成，每个神经元接受一个输入，对输入进行处理后给出一个输出，如下图所示。请问下列关于神经元的描述中，哪一项是正确的？

![](https://pic2.zhimg.com/80/v2-aa8abfb37009fc1cdf96b1c6445478ab_hd.jpg)

A.每个神经元只有一个输入和一个输出

B.每个神经元有多个输入和一个输出

C.每个神经元有一个输入和多个输出

D.每个神经元有多个输入和多个输出

E.上述都正确

　　答案：（E）

　　每个神经元可以有一个或多个输入，和一个或多个输出。

**138.下图是一个神经元的数学表示**

![](https://pic2.zhimg.com/80/v2-e357dd7a90f7a248d2fe03b9cd8cd735_hd.jpg)

　　这些组成部分分别表示为：

　　- x1, x2,..., xN：表示神经元的输入。可以是输入层的实际观测值，也可以是某一个隐藏层（Hidden Layer）的中间值

　　- w1, w2,...,wN：表示每一个输入的权重

　　- bi：表示偏差单元/偏移量（bias unit）。作为常数项加到激活函数的输入当中，类似截距（Intercept）

　　- a：作为神经元的激励函数（Activation），可以表示为

![](https://pic1.zhimg.com/80/v2-a7bbda86842fa2e9f4c4453ba6b24a30_hd.jpg)

　　- y：神经元输出

　　考虑上述标注，线性等式（y = mx + c）可以被认为是属于神经元吗：

　　A． 是

　　B． 否

　　答案：（A）

　　一个不包含非线性的神经元可以看作是线性回归函数（Linear Regression Function）。

**139.在一个神经网络中，知道每一个神经元的权重和偏差是最重要的一步。如果知道了神经元准确的权重和偏差，便可以近似任何函数，但怎么获知每个神经的权重和偏移呢？** A 搜索每个可能的权重和偏差组合，直到得到最佳值\
B 赋予一个初始值，然后检查跟最佳值的差值，不断迭代调整权重\
C 随机赋值，听天由命\
D 以上都不正确的\
答案：（C）选项C是对梯度下降的描述。

**140.梯度下降算法的正确步骤是什么？**

　　1.计算预测值和真实值之间的误差

　　2.重复迭代，直至得到网络权重的最佳值

　　3.把输入传入网络，得到输出值

　　4.用随机值初始化权重和偏差

　　5.对每一个产生误差的神经元，调整相应的（权重）值以减小误差

答案：正确步骤排序是：4, 3, 1, 5, 2

## [BAT机器学习面试1000题系列（141-145题）](https://zhuanlan.zhihu.com/p/31871909)

**141.已知：**\
　　- 大脑是有很多个叫做神经元的东西构成，神经网络是对大脑的简单的数学表达。\
　　- 每一个神经元都有输入、处理函数和输出。\
　　- 神经元组合起来形成了网络，可以拟合任何函数。\
　　- 为了得到最佳的神经网络，我们用梯度下降方法不断更新模型\
　　给定上述关于神经网络的描述，什么情况下神经网络模型被称为深度学习模型？

A 加入更多层，使神经网络的深度增加\
B 有维度更高的数据\
C 当这是一个图形识别的问题时\
D 以上都不正确\
答案：（A）\
更多层意味着网络更深。没有严格的定义多少层的模型才叫深度模型，目前如果有超过2层的隐层，那么也可以及叫做深度模型。

**142.卷积神经网络可以对一个输入进行多种变换（旋转、平移、缩放），这个表述正确吗？**\
　　答案：错误

　　把数据传入神经网络之前需要做一系列数据预处理（也就是旋转、平移、缩放）工作，神经网络本身不能完成这些变换。

**143.下面哪项操作能实现跟神经网络中Dropout的类似效果？** A Boosting\
B Bagging\
C Stacking\
D Mapping

　　答案：B

Dropout可以认为是一种极端的Bagging，每一个模型都在单独的数据上训练，同时，通过和其他模型对应参数的共享，从而实现模型参数的高度正则化。

**144.下列哪一项在神经网络中引入了非线性？**

　　A 随机梯度下降

　　B 修正线性单元（ReLU）

　　C 卷积函数

　　D 以上都不正确

答案：（B）

修正线性单元是非线性的激活函数。

**145.在训练神经网络时，损失函数(loss)在最初的几个epochs时没有下降，可能的原因是？（）**

![](https://pic2.zhimg.com/80/v2-a4855aef2d5e3d46380fdb8876caf679_hd.jpg)

　　A 学习率(learning rate)太低\
　　B 正则参数太高\
　　C 陷入局部最小值

　　D 以上都有可能

答案：（A）


## [BAT机器学习面试1000题系列（146-150题）](https://zhuanlan.zhihu.com/p/31898959)

**146.下列哪项关于模型能力（model capacity）的描述是正确的？（指神经网络模型能拟合复杂函数的能力）**

A 隐藏层层数增加，模型能力增加

B Dropout的比例增加，模型能力增加

C 学习率增加，模型能力增加

D 都不正确

答案：（A）

**147.如果增加多层感知机（Multilayer Perceptron）的隐藏层层数，分类误差便会减小。这种陈述正确还是错误？**\
　　答案：错误

　　并不总是正确。过拟合可能会导致错误增加。

**148.构建一个神经网络，将前一层的输出和它自身作为输入。**

![](https://pic4.zhimg.com/80/v2-5f205f3757bf8c8b9746d182487529da_hd.jpg)

下列哪一种架构有反馈连接？

A 循环神经网络

　　B 卷积神经网络

　　C 限制玻尔兹曼机

　　D 都不是

　　答案：（A）

**149.下列哪一项在神经网络中引入了非线性？在感知机中（Perceptron）的任务顺序是什么？** 1.随机初始化感知机的权重\
2.去到数据集的下一批（batch）\
3.如果预测值和输出不一致，则调整权重\
4.对一个输入样本，计算输出值\
　　答案：1 - 4 - 3 - 2

**150.假设你需要调整参数来最小化代价函数（cost function），可以使用下列哪项技术？**

　　A． 穷举搜索

　　B． 随机搜索

　　C． Bayesian优化

　　D． 以上任意一种

　　答案：（D）
  
  
  
## [BAT机器学习面试1000题系列（151-155题）](https://zhuanlan.zhihu.com/p/31945131)

**151.在下面哪种情况下，一阶梯度下降不一定正确工作（可能会卡住）？**

![](https://pic4.zhimg.com/80/v2-ee663892ab2f899264b9f2108552d5ab_hd.jpg)

　　答案：（B）

这是鞍点（Saddle Point）的梯度下降的经典例子。另，本题来源于：[https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/)。

**152.下图显示了训练过的3层卷积神经网络准确度，与参数数量(特征核的数量)的关系。**

![](https://pic1.zhimg.com/80/v2-b389482fee69928b38b9cffb921c7723_hd.jpg)

　　从图中趋势可见，如果增加神经网络的宽度，精确度会增加到一个特定阈值后，便开始降低。造成这一现象的可能原因是什么？

　　A 即使增加卷积核的数量，只有少部分的核会被用作预测

　　B 当卷积核数量增加时，神经网络的预测能力（Power）会降低

　　C 当卷积核数量增加时，它们之间的相关性增加(correlate)，导致过拟合

　　D 以上都不正确

　　答案：（C）

　　如C选项指出的那样，可能的原因是核之间的相关性。

**153.假设我们有一个如下图所示的隐藏层。隐藏层在这个网络中起到了一定的降纬作用。假如现在我们用另一种维度下降的方法，比如说主成分分析法(PCA)来替代这个隐藏层。**

![](https://pic4.zhimg.com/80/v2-f26af0ed5229c4d55f2f392d41d79fdc_hd.jpg)

　　那么，这两者的输出效果是一样的吗？

　　答案：不同，因为PCA用于相关特征而隐层用于有预测能力的特征

**154.神经网络能组成函数(y=1/x)吗？**

　　答案：可以，因为激活函数可以是互反函数

**155.下列哪个神经网络结构会发生权重共享？**

　　A.卷积神经网络

　　B.循环神经网络

　　C.全连接神经网络

　　D.选项A和B

　　答案：（D）
  
  
## [BAT机器学习面试1000题系列（156-160题）](https://zhuanlan.zhihu.com/p/31978113)

**156.批规范化(Batch Normalization)的好处都有啥？**

　　A.在将所有的输入传递到下一层之前对其进行归一化（更改）

　　B.它将权重的归一化平均值和标准差

　　C.它是一种非常有效的反向传播(BP)方法

　　D.这些均不是

　　答案：（A）

**157.在一个神经网络中，下面哪种方法可以用来处理过拟合？**

A Dropout

B 分批归一化(Batch Normalization)

C 正则化(regularization)

D 都可以

　　答案：（D）

**158.如果我们用了一个过大的学习速率会发生什么？**

A 神经网络会收敛

B 不好说

C 都不对

D 神经网络不会收敛

　　答案：（D）

**159.下图所示的网络用于训练识别字符H和T，如下所示：**

**

![](https://pic1.zhimg.com/80/v2-fd0e9dc5e24e29270d1c6ffa04ea7909_hd.jpg)

**

网络的输出是什么？

![](https://pic4.zhimg.com/80/v2-38e844eebf6745dba142f5df0f03393a_hd.jpg)

D.可能是A或B，取决于神经网络的权重设置\
答案：（D）\
不知道神经网络的权重和偏差是什么，则无法判定它将会给出什么样的输出。

**160.假设我们已经在ImageNet数据集(物体识别)上训练好了一个卷积神经网络。然后给这张卷积神经网络输入一张全白的图片。对于这个输入的输出结果为任何种类的物体的可能性都是一样的，对吗？**

A 对的

B 不知道

　　C 看情况

　　D 不对

　　答案：（D）

各个神经元的反应是不一样的


## [BAT机器学习面试1000题系列（161-165题）](https://zhuanlan.zhihu.com/p/32004423)

**161.当在卷积神经网络中加入池化层(pooling layer)时，变换的不变性会被保留，是吗？**

A 不知道

B 看情况

C 是

D 否

　　答案：（C）

使用池化时会导致出现不变性。

**162.**当数据过大以至于无法在RAM中同时处理时，哪种梯度下降方法更加有效？

A 随机梯度下降法(Stochastic Gradient Descent)

B 不知道

C 整批梯度下降法(Full Batch Gradient Descent)

D 都不是

　　答案：（A）

**163.下图是一个利用sigmoid函数作为激活函数的含四个隐藏层的神经网络训练的梯度下降图。这个神经网络遇到了梯度消失的问题。下面哪个叙述是正确的？**

**

![](https://pic2.zhimg.com/80/v2-584948662e91cd078a988c3c0603978c_hd.jpg)

**

　　A 第一隐藏层对应D，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应A\
　　B 第一隐藏层对应A，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应D\
　　C 第一隐藏层对应A，第二隐藏层对应B，第三隐藏层对应C，第四隐藏层对应D\
　　D 第一隐藏层对应B，第二隐藏层对应D，第三隐藏层对应C，第四隐藏层对应A

　　答案：（A）

由于反向传播算法进入起始层，学习能力降低，这就是梯度消失。

**164.对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，二是都设成0，下面哪个叙述是正确的？**

A 其他选项都不对

B 没啥问题，神经网络会正常开始训练

C 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西

D 神经网络不会开始训练，因为没有梯度改变

答案：（C）

**165.下图显示，当开始训练时，误差一直很高，这是因为神经网络在往全局最小值前进之前一直被卡在局部最小值里。为了避免这种情况，我们可以采取下面哪种策略？**

**

![](https://pic4.zhimg.com/80/v2-f3a656c63b3b9ebd94987e1781a3cff0_hd.jpg)

**

A 改变学习速率，比如一开始的几个训练周期不断更改学习速率\
B 一开始将学习速率减小10倍，然后用动量项(momentum)\
C 增加参数数目，这样神经网络就不会卡在局部最优处\
D 其他都不对

　　答案：（A）

　　选项A可以将陷于局部最小值的神经网络提取出来。
  
  
## [BAT机器学习面试1000题系列（166-170题）](https://zhuanlan.zhihu.com/p/32119999)

**166.**对于一个图像识别问题(在一张照片里找出一只猫)，下面哪种神经网络可以更好地解决这个问题？

A 循环神经网络

B 感知机

C 多层感知机

D 卷积神经网络

卷积神经网络将更好地适用于图像相关问题，因为考虑到图像附近位置变化的固有性质。

　　答案：（D）

**167.假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低**

**

![](https://pic2.zhimg.com/80/v2-9f8abdb843dc7c678f4bbf37e3b1aa4d_hd.jpg)

**

**你认为数据有问题，于是你画出了数据并且发现也许是数据的偏度过大造成了这个问题。**

**

![](https://pic2.zhimg.com/80/v2-f91119bbcbcc2f1444b37a655af0653e_hd.jpg)

**

**你打算怎么做来处理这个问题？**

A 对数据作归一化

B 对数据取对数变化 C 都不对 D 对数据作主成分分析(PCA)和归一化 答案：（D） 首先将相关的数据去掉，然后将其置零。

**168.下面那个决策边界是神经网络生成的？**

A. A\
B. D\
C. C\
D. B\
E. 以上都有

　　答案：（E）

**169.在下图中，我们可以观察到误差出现了许多小的"涨落"。 这种情况我们应该担心吗？**

**

![](https://pic4.zhimg.com/80/v2-339cdae5e096153939abf41d3a215ecc_hd.jpg)

**

A 需要，这也许意味着神经网络的学习速率存在问题\
B 不需要，只要在训练集和交叉验证集上有累积的下降就可以了\
C 不知道\
D 不好说\
答案：（B）\
选项B是正确的，为了减少这些"起伏"，可以尝试增加批尺寸(batch size)。

**170.在选择神经网络的深度时，下面那些参数需要考虑？**

1 神经网络的类型(如MLP,CNN) 2 输入数据 3 计算能力(硬件和软件能力决定) 4 学习速率 5 映射的输出函数

A 1,2,4,5

B 2,3,4,5

C 都需要考虑

D 1,3,4,5

　　答案：（C）

所有上述因素对于选择神经网络模型的深度都是重要的。


## [BAT机器学习面试1000题系列（171-175题）](https://zhuanlan.zhihu.com/p/32157649)

**171.考虑某个具体问题时，你可能只有少量数据来解决这个问题。不过幸运的是你有一个类似问题已经预先训练好的神经网络。可以用下面哪种方法来利用这个预先训练好的网络？**

　　A 把除了最后一层外所有的层都冻住，重新训练最后一层

　　B 对新数据重新训练整个模型

　　C 只对最后几层进行调参(fine tune)

　　D 对每一层模型进行评估，选择其中的少数来用

　　答案：（C）

**172.增加卷积核的大小对于改进卷积神经网络的效果是必要的吗？**

　　答案：不是，增加核函数的大小不一定会提高性能。这个问题在很大程度上取决于数据集。

**173.请简述神经网络的发展史**

@SIY.Z。本题解析来源：<https://zhuanlan.zhihu.com/p/29435406>

sigmoid会饱和，造成梯度消失。于是有了ReLU。

ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。

强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。

太深了，梯度传不下去，于是有了highway。

干脆连highway的参数都不要，直接变残差，于是有了ResNet。

强行稳定参数的均值和方差，于是有了BatchNorm。

在梯度流中增加噪声，于是有了 Dropout。

RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。

LSTM简化一下，有了GRU。

GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。

WGAN对梯度的clip有问题，于是有了WGAN-GP。

**174.说说spark的性能调优**

[https://tech.meituan.com/spark-tuning-basic.html](https://link.zhihu.com/?target=https%3A//tech.meituan.com/spark-tuning-basic.html)

[https://tech.meituan.com/spark-tuning-pro.html](https://link.zhihu.com/?target=https%3A//tech.meituan.com/spark-tuning-pro.html)

**175.如何理解LSTM网络？**

@Not_GOD，本题解析来源：[http://www.jianshu.com/p/9dc9f41f0b29/](https://link.zhihu.com/?target=http%3A//www.jianshu.com/p/9dc9f41f0b29/)

**Recurrent Neural Networks**


人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。\
传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题------使用电影中先前的事件推断后续的事件。\
RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。

![](https://pic2.zhimg.com/80/v2-374c766cc17dd7c2765e71fb0d40bfb1_hd.jpg)

RNN 包含循环

在上面的示例图中，神经网络的模块，A，正在读取某个输入 x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。\
这些循环使得 RNN 看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。RNN 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：

![](https://pic4.zhimg.com/80/v2-7e378380593e2add7caa5112991c8c2d_hd.jpg)

展开的 RNN

链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。\
并且 RNN 也已经被人们应用了！在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。我建议大家参考 Andrej Karpathy 的博客文章------The Unreasonable Effectiveness of Recurrent Neural Networks （[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](https://link.zhihu.com/?target=http%3A//karpathy.github.io/2015/05/21/rnn-effectiveness/)）来看看更丰富有趣的 RNN 的成功应用。\
而这些成功应用的关键之处就是 LSTM 的使用，这是一种特别的 RNN，比标准的 RNN 在很多的任务上都表现得更好。几乎所有的令人振奋的关于 RNN 的结果都是通过 LSTM 达到的。这篇博文也会就 LSTM 进行展开。

**长期依赖（Long-Term Dependencies）问题**


RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。\
有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 "the clouds are in the sky" 最后的词，我们并不需要任何其他的上下文 ------ 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。

![](https://pic3.zhimg.com/80/v2-0858b504dee028653b4027aa5b140eec_hd.jpg)

不太长的相关信息和位置间隔

但是同样会有一些更加复杂的场景。假设我们试着去预测"I grew up in France... I speak fluent French"最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。\
不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。

![](https://pic2.zhimg.com/80/v2-c41c40b11ccb39e075de41cdc406ceae_hd.jpg)

相当长的相关信息和位置间隔

在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。\
然而，幸运的是，LSTM 并没有这个问题！

**LSTM 网络**


**Long Short Term 网络------ 一般就叫做 LSTM ------是一种 RNN 特殊的类型，可以学习长期依赖信息。如@寒小阳所说：LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。LSTM的"记忆"我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的输入为前状态ht-1和当前输入xt。这些"细胞"会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。**


LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。\
LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！\
所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。

![](https://pic4.zhimg.com/80/v2-17e66042027a8d29cf6129f96d1f0bd5_hd.jpg)

标准 RNN 中的重复模块包含单一的层

LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。

![](https://pic4.zhimg.com/80/v2-8352a4e333b4a5db36cb69c316f4f083_hd.jpg)

LSTM 中的重复模块包含四个交互的层

不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。

![](https://pic2.zhimg.com/80/v2-6703da534a9199690409e207382c7883_hd.jpg)

LSTM 中的图标

在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。

**LSTM 的核心思想**


LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。\
细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。

![](https://pic1.zhimg.com/80/v2-07145e8a69a1978ef38f1a8188634df3_hd.jpg)

LSTM 有通过精心设计的称作为"门"的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。

![](https://pic2.zhimg.com/80/v2-c68f6761ae29a9c0c51b196d05200f75_hd.jpg)

Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表"不许任何量通过"，1 就指"允许任意量通过"！

LSTM 拥有三个门，来保护和控制细胞状态。

**逐步理解 LSTM**


在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取 h_{t-1} 和x_t，输出一个在 0 到 1 之间的数值给每个在细胞状态 C_{t-1} 中的数字。1 表示"完全保留"，0 表示"完全舍弃"。\
让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。

![](https://pic3.zhimg.com/80/v2-53316f70820439ceffa9b515c2d22704_hd.jpg)

决定丢弃信息

下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 "输入门层" 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，\tilde{C}_t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。\
在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。

![](https://pic2.zhimg.com/80/v2-57f481f3d6c6ab7f31eb4a1ffe6f7a35_hd.jpg)

确定更新的信息

现在是更新旧细胞状态的时间了，C_{t-1} 更新为 C_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。\
我们把旧状态与 f_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i_t * \tilde{C}_t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。\
在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。

![](https://pic3.zhimg.com/80/v2-bdb4079e0f05cff60fa9587e7e23f787_hd.jpg)

更新细胞状态

最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。\
在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。

![](https://pic4.zhimg.com/80/v2-6c871a8ce5bd9992679e3ed6df761d99_hd.jpg)

输出信息

**LSTM 的变体**


我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。\
其中一个流形的 LSTM 变体，就是由 Gers & Schmidhuber (2000) 提出的，增加了 "peephole connection"。是说，我们让 门层 也会接受细胞状态的输入。

![](https://pic2.zhimg.com/80/v2-8cb45dbd1f68922d27b8aa2e88492f0b_hd.jpg)

peephole 连接

上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。

另一个变体是通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。

![](https://pic4.zhimg.com/80/v2-cb2dd927616679b63ebd3f871940d389_hd.jpg)

coupled 忘记门和输入门

另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。

![](https://pic2.zhimg.com/80/v2-843ecec460397f5811d1a7566f2f453a_hd.jpg)

GRU

这里只是部分流行的 LSTM 变体。当然还有很多其他的，如Yao, et al. (2015) 提出的 Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014) 提出的 Clockwork RNN。\
要问哪个变体是最好的？其中的差异性真的重要吗？Greff, et al. (2015) 给出了流行变体的比较，结论是他们基本上是一样的。Jozefowicz, et al. (2015) 则在超过 1 万种 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。

![](https://pic1.zhimg.com/80/v2-cbbf923c67e41f8ede183da5465975af_hd.jpg)

Jozefowicz等人论文截图

**结论**

刚开始，我提到通过 RNN 得到重要的结果。本质上所有这些都可以使用 LSTM 完成。对于大多数任务确实展示了更好的性能！\
由于 LSTM 一般是通过一系列的方程表示的，使得 LSTM 有一点令人费解。然而本文中一步一步地解释让这种困惑消除了不少。\
LSTM 是我们在 RNN 中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是："Yes! 下一步已经有了------那就是注意力！" 这个想法是让 RNN 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，Xu, et al.(2015)已经这么做了------如果你希望深入探索注意力可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索......\
注意力也不是 RNN 研究领域中唯一的发展方向。例如，Kalchbrenner, et al. (2015) 提出的 Grid LSTM 看起来也是很有前途。使用生成模型的 RNN，诸如Gregor, et al. (2015) Chung, et al. (2015) 和 Bayer & Osendorfer (2015) 提出的模型同样很有趣。在过去几年中，RNN 的研究已经相当的燃，而研究成果当然也会更加丰富！\
再次说明下，本题解析基本取自Not_GOD翻译Christopher Olah 博文的《理解LSTM网络》，致谢。


## [BAT机器学习面试1000题系列（176-180题）](https://zhuanlan.zhihu.com/p/32183429)

**176.常见的分类算法有哪些？** SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

**177.常见的监督学习算法有哪些？** 感知机、svm、人工神经网络、决策树、逻辑回归

**178.在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）** A. 增加训练集量\
B. 减少神经网络隐藏层节点数\
C. 删除稀疏的特征\
D. SVM算法中使用高斯核/RBF核代替线性核\
正确答案：D\
@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)\
一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。\
B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现"过拟合"的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合\
D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分------当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数之一。

**179.下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测？** A.AR模型\
B.MA模型\
C.ARMA模型\
D.GARCH模型\
正确答案：D\
@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)\
AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。\
MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。\
ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。\
GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。

**180.以下哪个属于线性分类器最佳准则?** A.感知准则函数\
B.贝叶斯分类\
C.支持向量机\
D.Fisher准则

正确答案：ACD

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。

支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）

Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化"广义瑞利商"。

根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。

## [BAT机器学习面试1000题系列（181-185题）](https://zhuanlan.zhihu.com/p/32226365)

**181.基于二次准则函数的H-K算法较之于感知器算法的优点是（）?**

A.计算量小

B.可以判别问题是否线性可分

C.其解完全适用于非线性可分的情况

D.其解的适应性更好

正确答案：BD

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

HK算法思想很朴实,就是在最小均方误差准则下求得权矢量.

他相对于感知器算法的优点在于,他适用于线性可分和非线性可分得情况,对于线性可分的情况,给出最优权矢量,对于非线性可分得情况,能够判别出来,以退出迭代过程.

**182.以下说法中正确的是（）？**

A.SVM对噪声(如来自其他分布的噪声样本)鲁棒

B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同

C.Boosting和Bagging都是组合多个分类器投票的方法,二者都是根据单个分类器的正确率决定其权重

D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少

正确答案：BD

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

A、SVM对噪声（如来自其他分布的噪声样本）鲁棒

SVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。

B、在AdaBoost算法中所有被分错的样本的权重更新比例相同

AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被凸显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。以此类推，将所有的弱分类器重叠加起来，得到强分类器。

C、Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重。

**Bagging与Boosting的区别：**

取样方式不同。

Bagging采用均匀取样，而Boosting根据错误率取样。

Bagging的各个预测函数没有权重，而Boosting是有权重的。

Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。

**183.输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为（）：**

A. 95

B. 96

C. 97

D. 98

正确答案：C

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

首先我们应该知道卷积或者池化后大小的计算公式：

out_height=（(input_height - filter_height + padding_top+padding_bottom)/stride_height ）+1

out_width=（(input_width - filter_width + padding_left+padding_right)/stride_width ）+1

其中，padding指的是向外扩展的边缘大小，而stride则是步长，即每次移动的长度。

这样一来就容易多了，首先长宽一般大，所以我们只需要计算一个维度即可，这样，经过第一次卷积后的大小为: （200-5+2）/2+1，取99；经过第一次池化后的大小为：（99-3）/1+1 为97；经过第二次卷积后的大小为： （97-3+2）/1+1 为97

**184.在spss的基础分析模块中，作用是"以行列表的形式揭示数据之间的关系"的是（ ）**

A. 数据描述

B. 相关

C. 交叉表

D. 多重相应

正确答案：C

**185.一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：（）。**

A. 二分类问题

B. 多分类问题

C. 层次聚类问题

D. k-中心点聚类问题

E. 回归问题

F. 结构分析问题

正确答案：B

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。

层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。

K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。

回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。

结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。

多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。

## [BAT机器学习面试1000题系列（186-190题）](https://zhuanlan.zhihu.com/p/32255483)

**186.关于 logit 回归和 SVM 不正确的是（）**

A.Logit回归目标函数是最小化后验概率

B. Logit回归可以用于预测事件发生概率的大小

C. SVM目标是结构风险最小化

D.SVM可以有效避免模型过拟合

正确答案： A

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误

B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确

C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。

D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。

**187.有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是()**

A. 2x+y=4

B. x+2y=5

C. x+2y=3

D. 2x-y=0

正确答案：C

解析：这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。

**188.下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？**

A.准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率

B.召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率

C.正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高

D.为了解决准确率和召回率冲突问题，引入了F1分数

正确答案：C

解析：对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：

TP------将正类预测为正类数

FN------将正类预测为负类数

FP------将负类预测为正类数

TN------将负类预测为负类数

由此：

精准率定义为：P = TP / (TP + FP)

召回率定义为：R = TP / (TP + FN)

F1值定义为： F1 = 2 P R / (P + R)

精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。

**189.以下几种模型方法属于判别式模型(Discriminative Model)的有()**

1)混合高斯模型 2)条件随机场模型 3)区分度训练 4)隐马尔科夫模型

A.2,3

B.3,4

C.1,4

D.1,2

正确答案：A

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

常见的判别式模型有：Logistic regression（logistical 回归）

Linear discriminant analysis（线性判别分析）

Supportvector machines（支持向量机）

Boosting（集成学习）

Conditional random fields（条件随机场）

Linear regression（线性回归）

Neural networks（神经网络）

常见的生成式模型有:Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）

Hidden Markov model（隐马尔可夫）

NaiveBayes（朴素贝叶斯）

AODE（平均单依赖估计）

Latent Dirichlet allocation（LDA主题模型）

Restricted Boltzmann Machine（限制波兹曼机）

生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。

**190.SPSS中，数据整理的功能主要集中在（ ）等菜单中**

A.数据

B.直销

C.分析

D.转换

正确答案：AD

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

解析：对数据的整理主要在数据和转换功能菜单中。


## [BAT机器学习面试1000题系列（191-195题）](https://zhuanlan.zhihu.com/p/32322322)

**191.深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为m∗n，n∗p，p∗q，且m<n<p<q，以下计算顺序效率最高的是（）**

A.(AB)C

B.AC(B)

C.A(BC)

D.所以效率都相同

正确答案：A

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

首先，根据简单的矩阵知识，因为 A*B ， A 的列数必须和 B 的行数相等。因此，可以排除 B 选项，

然后，再看 A 、 C 选项。在 A 选项中，m∗n 的矩阵 A 和n∗p的矩阵 B 的乘积，得到 m∗p的矩阵 A*B ，而 A∗B的每个元素需要 n 次乘法和 n-1 次加法，忽略加法，共需要 m∗n∗p次乘法运算。同样情况分析 A*B 之后再乘以 C 时的情况，共需要 m∗p∗q次乘法运算。因此， A 选项 (AB)C 需要的乘法次数是 m∗n∗p+m∗p∗q 。同理分析， C 选项 A (BC) 需要的乘法次数是 n∗p∗q+m∗n∗q。

由于m∗n∗p<m∗n∗q，m∗p∗q<n∗p∗q，显然 A 运算次数更少，故选 A 。

**192.Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:()**

A.各类别的先验概率P(C)是相等的

B.以0为均值，sqr(2)/2为标准差的正态分布

C.特征变量X的各个维度是类别条件独立随机变量

D.P(X|C)是高斯分布

正确答案：C

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

朴素贝叶斯的条件就是每个变量相互独立。

**193.关于支持向量机SVM,下列说法错误的是（）**

A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力

B.Hinge 损失函数，作用是最小化经验分类错误

C.分类间隔为1/||w||，||w||代表向量的模

D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习

正确答案：C

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

A正确。考虑加入正则化项的原因：想象一个完美的数据集，y>1是正类，y<-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变"歪"很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么"歪"了，使得分类间隔变大，提高了泛化能力。

B正确。

C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。

D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和ai∗yi∗xi，a变小使得w变小，因此间隔2/||w||变大

**194.在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()**

A.EM算法

B.维特比算法

C.前向后向算法

D.极大似然估计

正确答案：D

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

前向后向算法：用来算概率

极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。

**195.假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是：**

A.这个被重复的特征在模型中的决定作用会被加强

B.模型效果相比无重复特征的情况下精确度会降低

C.如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。

D.当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题

E.NB可以用来做最小二乘回归

F.以上说法都不正确

正确答案：BD


## [BAT机器学习面试1000题系列（196-200题）](https://zhuanlan.zhihu.com/p/32362052)

**196.L1与L2范数**

在Logistic Regression 中,如果同时加入L1和L2范数,会产生什么效果()

A.可以做特征选择,并在一定程度上防止过拟合

B.能解决维度灾难问题

C.能加快计算速度

D.可以获得更准确的结果

正确答案:A

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

Ｌ１范数具有系数解的特性，但是要注意的是，Ｌ１没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。

在代价函数后面加上正则项，Ｌ１即是Ｌｏｓｓｏ回归，Ｌ２是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。因此选择A。

对于机器学习中的范数规则化，也就是L0,L1,L2范数的详细解答，请参阅《范数规则化》([http://blog.csdn.net/zouxy09/article/details/24971995/](https://link.zhihu.com/?target=http%3A//blog.csdn.net/zouxy09/article/details/24971995/))

**197.正则化**

机器学习中L1正则化和L2正则化的区别是？

A.使用L1可以得到稀疏的权值

B.使用L1可以得到平滑的权值

C.使用L2可以得到稀疏的权值

D.使用L2可以得到平滑的权值

正确答案:AD

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0.

L2主要功能是为了防止过拟合，当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。

L1正则化/Lasso

L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

L2正则化/Ridge regression

L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。

可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。

因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

具体的，可以参阅《机器学习之特征选择》与《机器学习范数正则化》。

**198.势函数法**

位势函数法的积累势函数K(x)的作用相当于Bayes判决中的()

A.后验概率

B.先验概率

C.类概率密度

D.类概率密度与先验概率的乘积

正确答案:AD

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

事实上，AD说的是一回事。 具体的，势函数详解请看------《势函数法》。

**199.隐马尔可夫**

隐马尔可夫模型三个基本问题以及相应的算法说法正确的是（ ）

A.评估---前向后向算法

B.解码---维特比算法

C.学习---Baum-Welch算法

D.学习---前向后向算法

正确答案:ABC

解析：评估问题，可以使用前向算法、后向算法、前向后向算法。

**200.特征比数据量还大时，选择什么样的分类器？**

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。


## [BAT机器学习面试1000题系列（201-205题）](https://zhuanlan.zhihu.com/p/32396385)

**201.下列属于无监督学习的是：**

A.k-means

B.SVM

C.最大熵

D.CRF

正确答案：A

解析： A是聚类，BC是分类，D是序列化标注，也是有监督学习。

**202.下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）**

A.特征灵活

B.速度快

C.可容纳较多上下文信息

D.全局最优

正确答案：B

解析： CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢

CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ------------与HMM比较

同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­------------与MEMM比较

CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。------------与ME比较

**203.数据清理中，处理缺失值的方法是?**

A.估算

B.整例删除

C.变量删除

D.成对删除

正确答案：ABCD

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。

估算(estimation)。最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。

整例删除(casewise deletion)是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。

变量删除(variable deletion)。如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。

成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。

采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。

**204.关于线性回归的描述,以下正确的有:**

A.基本假设包括随机干扰项是均值为0,方差为1的标准正态分布

B.基本假设包括随机干扰下是均值为0的同方差正态分布

C.在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量

D.在违背基本假设时,模型不再可以估计

E.可以用DW检验残差是否存在序列相关性

F.多重共线性会使得参数估计值方差减小

正确答案：ACEF

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

1、AB一元线性回归的基本假设有：

（1）随机误差项是一个期望值或平均值为0的随机变量；

（2）对于解释变量的所有观测值，随机误差项有相同的方差；

（3）随机误差项彼此不相关；

（4）解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立；

（5）解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵；

（6）随机误差项服从正态分布

2、CD 违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。

当存在异方差时，普通最小二乘法估计存在以下问题： 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。

3、E杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶 自相关 最常用的方法。

4、F所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响

（1）完全共线性下参数估计量不存在

（2）近似共线性下OLS估计量非有效

多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF)

（3）参数估计量经济含义不合理

（4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外

（5）模型的预测功能失效。变大的方差容易使区间预测的"区间"变大，使预测失去意义。

对于线性回归模型,当响应变量服从正态分布,误差项满足高斯--马尔科夫条件（零均值、等方差、不相关）时,回归参数的最小二乘估计是一致最小方差无偏估计。

当然，该条件只是理想化的假定，为的是数学上有相应的较为成熟的结论。其实大多数实际问题都不完全满足这些理想化的假定。

线性回归模型理论的发展正是在不断克服理想化条件不被满足时得到许多新方法。如加权LSE、岭估计、压缩估计、BOX_COX变换等一系列段。做实际工作时一定是要超越书本上的理想化条件的。

**205.影响聚类算法效果的主要原因有：**

A.特征选取

B.模式相似性测度

C.分类准则

D.已知类别的样本质量

正确答案：ABC

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)

解析：这道题应该是很简单的，D之所以不正确，是因为聚类是对无类别的数据进行聚类，不使用已经标记好的数据。

前面的ABC选项，可以参考：《聚类分析》与《各类算法的比较》。


## [BAT机器学习面试1000题系列（206-210）](https://zhuanlan.zhihu.com/p/32426932)

**206.以下哪个是常见的时间序列算法模型** A.RSI\
B.MACD\
C.ARMA\
D.KDJ

正确答案：C\
解析： 自回归滑动平均模型(ARMA) ，其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。\
其他三项都不是一个层次的。\
A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .\
B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .\
D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .

**207.下列不是SVM核函数的是：** A.多项式核函数\
B.logistic核函数\
C.径向基核函数\
D.Sigmoid核函数

正确答案：B\
@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)\
SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数.\
核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：\
(1)线性核函数 ：K ( x , x i ) = x ⋅ x i\
(2)多项式核 ：K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d\
(3)径向基核（RBF）：K ( x , x i ) = exp ( - ∥ x - x i ∥ 2 σ 2 )\
Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。\
(4)傅里叶核 ：K ( x , x i ) = 1 - q 2 2 ( 1 - 2 q cos ( x - x i ) + q 2 )\
(5)样条核 ：K ( x , x i ) = B 2 n + 1 ( x - x i )\
(6)Sigmoid核函数 ：K ( x , x i ) = tanh ( κ ( x , x i ) - δ )\
采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。\
在选取核函数解决实际问题时，通常采用的方法有：\
一是利用专家的先验知识预先选定核函数；\
二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．\
三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．

**208.已知一组数据的协方差矩阵P,下面关于主分量说法错误的是()** A.主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小\
B.在经主分量分解后,协方差矩阵成为对角矩阵\
C.主分量分析就是K-L变换\
D.主分量是通过求协方差矩阵的特征值得到

正确答案：C\
解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。

**209.在分类问题中,我们经常会遇到正负样本数据量不等的情况,比如正样本为10w条数据,负样本只有1w条数据,以下最合适的处理方法是()** A.将负样本重复10次,生成10w样本量,打乱顺序参与分类\
B.直接进行分类,可以最大限度利用数据\
C.从10w正样本中随机抽取1w参与分类\
D.将负样本每个权重设置为10,正样本权重为1,参与训练过程

正确答案:ACD\
解析：1. 重采样。 A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。\
2\. 欠采样。 C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。\
如果1：10算是均匀的话，可以将多数类分割成为1000份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这1000个分类器用assemble的方法组合位一个分类器。A选项可以看作此方式，因而相对比较合理。\
另：如果目标是 预测的分布 跟训练的分布一致，那就加大对分布不一致的惩罚系数。\
3\. 权值调整。 D方案也是其中一种方式。\
当然，这只是在数据集上进行相应的处理，在算法上也有相应的处理方法。

**210.在统计模式识分类问题中，当先验概率未知时，可以使用()?**

A.最小损失准则

B.N-P判决

C.最小最大损失准则

D.最小误判概率准则

正确答案:BC

@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)\
选项 A ,最小损失准则中需要用到先验概率

选项B ,在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。\
1\. p(y)已知，直接使用贝叶斯公式求后验概率即可；\
2\. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。\
聂曼-皮尔逊决策（N-P判决）可以归结为找阈值a，即：\
如果p（x|w1）/p（x|w2）>a，则 x属于w1；\
如果p（x|w1）/p（x|w2）<a，则 x属于w 2；

选项C ,最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。

## [BAT机器学习面试1000题系列（211-215）](https://zhuanlan.zhihu.com/p/32461681)

**211.解决隐马模型中预测问题的算法是?**

A.前向算法\
B.后向算法\
C.Baum-Welch算法\
D.维特比算法

正确答案：D\
@刘炫320，本题题目及解析来源：[http://blog.csdn.net/column/details/16442.html](https://link.zhihu.com/?target=http%3A//blog.csdn.net/column/details/16442.html)\
A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。\
C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；\
D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。

**212.一般，k-NN最近邻方法在( )的情况下效果较好**

A.样本较多但典型性不好\
B.样本较少但典型性好\
C.样本呈团状分布\
D.样本呈链状分布

正确答案：B\
解析：K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B\
样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。

**213.下列方法中，可以用于特征降维的方法包括（）**

A.主成分分析PCA\
B.线性判别分析LDA\
C.深度学习SparseAutoEncoder\
D.矩阵奇异值分解SVD\
E.最小二乘法LeastSquares

正确答案：ABCD\
解析：降维的3种常见方法ABD，都是线性的。深度学习是降维的方法这个就比较新鲜了，事实上，细细想来，也是降维的一种方法，因为如果隐藏层中的神经元数目要小于输入层，那就达到了降维，但如果隐藏层中的神经元如果多余输入层，那就不是降维了。\
最小二乘法是线性回归的一种解决方法，其实也是投影，但是并没有进行降维。

**214.下面哪些是基于核的机器学习算法?()**

A.Expectation Maximization（EM）（最大期望算法）\
B.Radial Basis Function（RBF）（径向基核函数）\
C.Linear Discrimimate Analysis（LDA）（主成分分析法）\
D.Support Vector Machine（SVM）（支持向量机）

正确答案：BCD\
解析：径向基核函数是非常常用的核函数，而主成分分析法的常规方法是线性的，但是当遇到非线性的时候，同样可以使用核方法使得非线性问题转化为线性问题。支持向量机处理非线性的问题的时候，核函数也是非常重要的。

**215.试推导样本空间中任意点x到超平面（w,b）的距离公式。**

![](https://pic2.zhimg.com/80/v2-9562bab105bfa7780ae53a765f285272_hd.jpg)


## [BAT机器学习面试1000题系列（216-220）](https://zhuanlan.zhihu.com/p/32545713)

**216.从网上下载或自己编程实现一个卷积神经网络，并在手写字符识别数据MNIST上进行试验测试。** 解析详见：[http://blog.csdn.net/snoopy_yuan/article/details/71703019](https://link.zhihu.com/?target=http%3A//blog.csdn.net/snoopy_yuan/article/details/71703019)

**217.神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属是好的属性但不必要的？** @Hengkai Guo，本题解析来源：<https://www.zhihu.com/question/67366051>\
说说我对一个好的激活函数的理解吧，有些地方可能不太严谨，欢迎讨论。（部分参考了Activation function。）\
1\. 非线性：即导数不是常数。这个条件前面很多答主都提到了，是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。\
2\. 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。\
3\. 计算简单：正如题主所说，非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。\
4\. 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x>0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x<0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。\
5\. 单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。\
6\. 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。\
7\. 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。\
8\. 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。\
9\. 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。

参考文献：\
[1] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.\
[2] Lin M, Chen Q, Yan S. Network in network[J]. arXiv preprint arXiv:1312.4400, 2013.\
[3] Maas A L, Hannun A Y, Ng A Y. Rectifier nonlinearities improve neural network acoustic models[C]//Proc. ICML. 2013, 30(1).\
[4] He K, Zhang X, Ren S, et al. Delving\
deep into rectifiers: Surpassing human-level performance on imagenet\
classification[C]//Proceedings of the IEEE international conference on\
computer vision. 2015: 1026-1034.\
[5] Glorot X, Bengio Y. Understanding the\
difficulty of training deep feedforward neural networks[C]//Proceedings\
of the Thirteenth International Conference on Artificial Intelligence\
and Statistics. 2010: 249-256.\
[6] He K, Zhang X, Ren S, et al. Deep\
residual learning for image recognition[C]//Proceedings of the IEEE\
conference on computer vision and pattern recognition. 2016: 770-778.\
[7] Goodfellow I J, Warde-Farley D, Mirza M, et al. Maxout networks[J]. arXiv preprint arXiv:1302.4389, 2013.\
[8] Klambauer G, Unterthiner T, Mayr A, et al. Self-Normalizing Neural Networks[J]. arXiv preprint arXiv:1706.02515, 2017.\
[9] Ioffe S, Szegedy C. Batch\
normalization: Accelerating deep network training by reducing internal\
covariate shift[C]//International Conference on Machine Learning. 2015:\
448-456.

**218.梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？**


**@李振华，<https://www.zhihu.com/question/68109802/answer/262143638>**


深度神经网络"容易收敛到局部最优"，很可能是一种想象，实际情况是，我们可能从来没有找到过"局部最优"，更别说全局最优了。

很多人都有一种看法，就是"局部最优是神经网络优化的主要难点"。这来源于一维优化问题的直观想象。在单变量的情形下，优化问题最直观的困难就是有很多局部极值，如

![](https://pic3.zhimg.com/80/v2-6c5fd3c8fd60dfa68f0ae0a95c2cb100_hd.jpg)

人们直观的想象，高维的时候这样的局部极值会更多，指数级的增加，于是优化到全局最优就更难了。然而单变量到多变量一个重要差异是，单变量的时候，Hessian矩阵只有一个特征值，于是无论这个特征值的符号正负，一个临界点都是局部极值。但是在多变量的时候，Hessian有多个不同的特征值，这时候各个特征值就可能会有更复杂的分布，如有正有负的不定型和有多个退化特征值（零特征值）的半定型

![](https://pic2.zhimg.com/80/v2-43393e60c96ca91f31727172b24a2335_hd.jpg)

在后两种情况下，是很难找到局部极值的，更别说全局最优了。

现在看来，神经网络的训练的困难主要是鞍点的问题。在实际中，我们很可能也从来没有真的遇到过局部极值。Bengio组这篇文章Eigenvalues of the Hessian in Deep Learning（[https://arxiv.org/abs/1611.07476](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.07476)）里面的实验研究给出以下的结论：

> - Training stops at a point that has a small gradient. The norm of the gradient is not zero, therefore it does not, technically speaking, converge to a critical point.\
> - There are still negative eigenvalues even when they are small in magnitude.

另一方面，一个好消息是，即使有局部极值，具有较差的loss的局部极值的吸引域也是很小的Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes。（[https://arxiv.org/abs/1706.10239](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.10239)）

> For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima.

所以，很可能我们实际上是在"什么也没找到"的情况下就停止了训练，然后拿到测试集上试试，"咦，效果还不错"。

补充说明，这些都是实验研究结果。理论方面，各种假设下，深度神经网络的Landscape 的鞍点数目指数增加，而具有较差loss的局部极值非常少。

**219.EM算法、HMM、CRF**

这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。\
（1）EM算法\
EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。\
注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。\
（2）HMM算法\
隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。\
马尔科夫三个基本问题：

-   概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。--》前向后向算法
-   学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。--》Baum-Welch(也就是EM算法)和极大似然估计。
-   预测问题：已知模型和观测序列，求解对应的状态序列。--》近似算法（贪心算法）和维比特算法（动态规划求最优路径）

（3）条件随机场CRF\
　　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。\
　　之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。

（4）HMM和CRF对比\
　　其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。

**220.CNN常用的几个模型**

**名称**

**特点**

LeNet5

没啥特点-不过是第一个CNN应该要知道

AlexNet

引入了ReLU和dropout，引入数据增强、池化相互之间有覆盖，三个卷积一个最大池化+三个全连接层

VGGNet

采用1*1和3*3的卷积核以及2*2的最大池化使得层数变得更深。常用VGGNet-16和VGGNet19

Google Inception Net

这个在控制了计算量和参数量的同时，获得了比较好的分类性能，和上面相比有几个大的改进： 1、去除了最后的全连接层，而是用一个全局的平均池化来取代它； 2、引入Inception Module，这是一个4个分支结合的结构。所有的分支都用到了1*1的卷积，这是因为1*1性价比很高，可以用很少的参数达到非线性和特征变换。 3、Inception V2第二版将所有的5*5变成2个3*3，而且提出来著名的Batch Normalization； 4、Inception V3第三版就更变态了，把较大的二维卷积拆成了两个较小的一维卷积，加速运算、减少过拟合，同时还更改了Inception Module的结构。

微软ResNet残差神经网络(Residual Neural Network)

1、引入高速公路结构，可以让神经网络变得非常深 2、ResNet第二个版本将ReLU激活函数变成y=x的线性函数


## [BAT机器学习面试1000题系列（221-225）](https://zhuanlan.zhihu.com/p/32622819)

**221.带核的SVM为什么能分类非线性问题？**\
　　核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积

**222.常用核函数及核函数的条件：**

　　核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。

-   RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。
-   线性核：主要用于线性可分的情况
-   多项式核

**223.Boosting和Bagging**


（1）随机森林\
　　随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：

1）Boostrap从袋内有放回的抽取样本值

2）每次随机抽取一定数量的特征（通常为sqr(n)）。\
　　分类问题：采用Bagging投票的方式选择类别频次最高的\
　　回归问题：直接取每颗树结果的平均值。

常见参数误差分析优点缺点1、树最大深度

2、树的个数

3、节点上的最小样本数

4、特征数(sqr(n))oob(out-of-bag)

将各个树的未采样样本作为预测样本统计误差作为误分率可以并行计算

不需要特征选择

可以总结出特征重要性

可以处理缺失数据

不需要额外设计测试集在回归上不能输出连续结果

（2）Boosting之AdaBoost\
　　Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。\
（3）Boosting之GBDT\
　　将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。\
　　注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。

（4）Xgboost\
这个工具主要有以下几个特点：

-   支持线性分类器
-   可以自定义损失函数，并且可以用二阶偏导
-   加入了正则化项：叶节点数、每个叶节点输出score的L2-norm
-   支持特征抽样
-   在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。

**224.逻辑回归相关问题**


（1）公式推导一定要会

（2）逻辑回归的基本概念\
　　这个最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。

（3）L1-norm和L2-norm\
　　其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。\
　　但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。

（4）LR和SVM对比\
　　首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss。

![](https://pic3.zhimg.com/80/v2-92d397e2d90030689b7521ddafe1ef13_hd.jpg)\
　　其次，两者都是线性模型。\
　　最后，SVM只考虑支持向量（也就是和分类相关的少数点）\
（5）LR和随机森林区别\
　　随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。\
（6）常用的优化方法\
　　逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。\
　　一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。\
　　二阶方法：牛顿法、拟牛顿法：\
　　这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。\
拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

**225.用贝叶斯机率说明Dropout的原理**


@许韩，来源：<https://zhuanlan.zhihu.com/p/25005808>

参见论文：Dropout as a Bayesian Approximation: Insights and Applications

（[http://mlg.eng.cam.ac.uk/yarin/PDFs/Dropout_as_a_Bayesian_approximation.pdf](https://link.zhihu.com/?target=http%3A//mlg.eng.cam.ac.uk/yarin/PDFs/Dropout_as_a_Bayesian_approximation.pdf)）

## [BAT机器学习面试1000题系列（226-230）](https://zhuanlan.zhihu.com/p/32656102)

**226.为什么很多做人脸的Paper会最后加入一个Local Connected Conv？**

@许韩，来源：<https://zhuanlan.zhihu.com/p/25005808>

以FaceBook DeepFace 为例：

**

![](https://pic3.zhimg.com/80/v2-0b125d6d3f3bcd1430008911f4369f0a_hd.jpg)

**

DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。

**227.什么是共线性, 跟过拟合有什么关联?**

　　@抽象猴，来源：<https://www.zhihu.com/question/41233373/answer/145404190>

　　共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。

　　共线性会造成冗余，导致过拟合。

　　解决方法：排除变量的相关性／加入权重正则。

**228.为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optima？**

参见：The Loss Surfaces of Multilayer Networks（[https://arxiv.org/pdf/1412.0233.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1412.0233.pdf)）

**229.机器学习中的正负样本**

在分类问题中，这个问题相对好理解一点，比如人脸识别中的例子，正样本很好理解，就是人脸的图片，负样本的选取就与问题场景相关，具体而言，如果你要进行教室中学生的人脸识别，那么负样本就是教室的窗子、墙等等，也就是说，不能是与你要研究的问题毫不相关的乱七八糟的场景图片，这样的负样本并没有意义。负样本可以根据背景生成，有时候不需要寻找额外的负样本。一般3000-10000的正样本需要5，000,000-100,000,000的负样本来学习，在互金领域一般在入模前将正负比例通过采样的方法调整到3:1-5:1。

**230.机器学习中，有哪些特征选择的工程方法？**

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已

1\. 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；

2\. 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；

3.通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；

4\. 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；

5.通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。

6.通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。


## [BAT机器学习面试1000题系列（231-235）](https://zhuanlan.zhihu.com/p/32738678)


**231.在一个n维的空间中， 最好的检测outlier(离群点)的方法是：**

A. 作正态分布概率图\
B. 作盒形图\
C. 马氏距离\
D. 作散点图

答案：C\
马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。

更多请详见：[http://eurekastatistics.com/using-mahalanobis-distance-to-find-outliers/](https://link.zhihu.com/?target=http%3A//eurekastatistics.com/using-mahalanobis-distance-to-find-outliers/)和[http://blog.csdn.net/v_july_v/article/details/8203674](https://link.zhihu.com/?target=http%3A//blog.csdn.net/v_july_v/article/details/8203674)

**232.对数几率回归（logistics regression）和一般回归分析有什么区别？**

A. 对数几率回归是设计用来预测事件可能性的\
B. 对数几率回归可以用来度量模型拟合程度\
C. 对数几率回归可以用来估计回归系数\
D. 以上所有

答案：D\
A: 对数几率回归其实是设计用来解决分类问题的\
B: 对数几率回归可以用来检验模型对数据的拟合度\
C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。

**233.bootstrap数据是什么意思？（提示：考"bootstrap"和"boosting"区别）**

A. 有放回地从总共M个特征中抽样m个特征\
B. 无放回地从总共M个特征中抽样m个特征\
C. 有放回地从总共N个样本中抽样n个样本\
D. 无放回地从总共N个样本中抽样n个样本\
答案：C

**234."过拟合"只在监督学习中出现，在非监督学习中，没有"过拟合"，这是：** A. 对的\
B. 错的\
答案：B\
我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）

**235.对于k折交叉验证, 以下对k的说法正确的是 :**

A. k越大, 不一定越好, 选择大的k会加大评估时间\
B. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)\
C. 在选择k时, 要最小化数据集之间的方差\
D. 以上所有

答案：D\
k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.\
如果不明白bias和variance的概念, 参考:\
Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning\
Understanding the Bias-Variance Tradeoff



## [BAT机器学习面试1000题系列（236-240）](https://zhuanlan.zhihu.com/p/32777073)

**236.回归模型中存在多重共线性, 你如何解决这个问题？**

1\. 去除这两个共线性变量\
2\. 我们可以先去除一个共线性变量\
3\. 计算VIF(方差膨胀因子), 采取相应措施\
4\. 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归.\
以下哪些是对的:

A. 1\
B. 2\
C. 2和3\
D. 2, 3和4

答案: D\
解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值<=4说明相关性不是很高, VIF值>=10说明相关性较高.\
我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。

**237.模型的高bias是什么意思, 我们如何降低它 ?**

A. 在特征空间中减少特征\
B. 在特征空间中增加特征\
C. 增加数据点\
D. B和C\
E. 以上所有

答案: B\
bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !

**238.训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个:**

![](https://pic2.zhimg.com/80/v2-a82191db53707492cbee584707a9bcc2_hd.jpg)

A. Outlook\
B. Humidity\
C. Windy\
D. Temperature

答案: A信息增益, 增加平均子集纯度, 详细研究, 请戳下面链接:\
A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)\
Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

**239.对于信息增益, 决策树分裂节点, 下面说法正确的是:**

1\. 纯度高的节点需要更多的信息去区分\
2\. 信息增益可以用"1比特-熵"获得\
3\. 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的

A. 1\
B. 2\
C.2和3\
D. 所有以上\
答案: C\
详细研究, 请戳下面链接:\
A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)\
Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio

**240\. 如果SVM模型欠拟合, 以下方法哪些可以改进模型 :**

A. 增大惩罚参数C的值\
B. 减小惩罚参数C的值\
C. 减小核系数(gamma参数)

答案: A

如果SVM模型欠拟合, 我们可以调高参数C的值, 使得模型复杂度上升.LibSVM中，SVM的目标函数是：

![](https://pic4.zhimg.com/80/v2-4414be4d7cef5c3ff723075079885591_hd.jpg)

而, gamma参数是你选择径向基函数作为kernel后,该函数自带的一个参数.隐含地决定了数据映射到新的特征空间后的分布.\
gamma参数与C参数无关. gamma参数越高, 模型越复杂.


## [BAT机器学习面试1000题系列（241-245）](https://zhuanlan.zhihu.com/p/32808814)

**241.下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是 :**

![](https://pic1.zhimg.com/80/v2-fa5bef959226cb26e96b07af768cff66_hd.jpg)

A. g1 > g2 > g3

B. g1 = g2 = g3

C. g1 < g2 < g3

D. g1 >= g2 >= g3

E. g1 <= g2 <= g3

答案: C

**242.假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 :**

1\. 模型分类的召回率会降低或不变

2\. 模型分类的召回率会升高

3\. 模型分类准确率会升高或不变

4\. 模型分类准确率会降低

A. 1

B. 2

C.1和3

D. 2和4

E. 以上都不是

答案: C

这篇文章讲述了阈值对准确率和召回率影响 :

Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest Classifiers

**243."点击率问题"是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是 :**

A. 模型预测准确率已经很高了, 我们不需要做什么了

B. 模型预测准确率不高, 我们需要做点什么改进模型

C. 无法下结论

D. 以上都不对

答案: B

99%的预测准确率可能说明, 你预测的没有点进去的人很准确 (因为有99%的人是不会点进去的, 这很好预测). 不能说明你的模型对点进去的人预测准确, 所以, 对于这样的非平衡数据集, 我们要把注意力放在小部分的数据上, 即那些点击进去的人.

详细参考: [https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/](https://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/)

**244.使用k=1的knn算法, 下图二类分类问题, "+" 和 "o" 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少 :**

![](https://pic4.zhimg.com/80/v2-bd1e42639b98445ca3ccf96f77ef5e6f_hd.jpg)

A. 0%

B. 100%

C. 0% 到 100%

D. 以上都不是

答案: B

knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%

**245.我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以 :**

A. 增加树的深度

B. 增加学习率 (learning rate)

C. 减少树的深度

D. 减少树的数量

答案: C

A.增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.

B.决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)

D.决策树只有一棵树, 不是随机森林.

## [BAT机器学习面试1000题系列（246-250）](https://zhuanlan.zhihu.com/p/32836526)

**246.对于神经网络的说法, 下面正确的是 :**

1\. 增加神经网络层数, 可能会增加测试数据集的分类错误率\
2\. 减少神经网络层数, 总是能减小测试数据集的分类错误率\
3\. 增加神经网络层数, 总是能减小训练数据集的分类错误率

A. 1

B. 1 和 3

C. 1 和 2

D. 2

答案: A

深度神经网络的成功, 已经证明, 增加神经网络层数, 可以增加模型范化能力, 即, 训练数据集和测试数据集都表现得更好. 但更多的层数, 也不一定能保证有更好的表现（[https://arxiv.org/pdf/1512.03385v1.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385v1.pdf)）. 所以, 不能绝对地说层数多的好坏, 只能选A

**247.假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？**

A. 设C=1

B. 设C=0

C. 设C=无穷大

D. 以上都不对

答案: C

C无穷大保证了所有的线性不可分都是可以忍受的.

**248.训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类:**

A. 正确

B. 错误

答案: A

SVM模型中, 真正影响决策边界的是支持向量

**249.以下哪些算法, 可以用神经网络去构造:**

1\. KNN\
2\. 线性回归\
3\. 对数几率回归

A. 1和 2

B. 2 和 3

C. 1, 2 和 3

D. 以上都不是

答案: B

1\. KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙\
2\. 最简单的神经网络, 感知器, 其实就是线性回归的训练\
3\. 我们可以用一层的神经网络构造对数几率回归

**250.请选择下面可以应用隐马尔科夫(HMM)模型的选项:**

A. 基因序列数据集

B. 电影浏览数据集

C. 股票市场数据集

D. 所有以上

答案: D

只要是和时间序列问题有关的 , 都可以试试HMM



## [BAT机器学习面试1000题系列（251-255）](https://zhuanlan.zhihu.com/p/32870201)

**251.我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 :**

A. 我们随机抽取一些样本, 在这些少量样本之上训练

B. 我们可以试用在线机器学习算法

C. 我们应用PCA算法降维, 减少特征数

D. B 和 C

E. A 和 B

F. 以上所有

答案: F

**252.我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 :**

1\. 使用前向特征选择方法\
2\. 使用后向特征排除方法\
3\. 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征.\
4\. 查看相关性表, 去除相关性最高的一些特征

A. 1 和 2

B. 2, 3和4

C. 1, 2和4

D. All

答案: D

1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法

2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法.

3.用相关性的度量去删除多余特征, 也是一个好方法

所有D是正确的

**253.对于随机森林和GradientBoosting Trees, 下面说法正确的是:**

1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的.

2.这两个模型都使用随机特征子集, 来生成许多单个的树.

3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好

A. 2

B. 1 and 2

C. 1, 3 and 4

D. 2 and 4

答案: A

1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所有说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系.

2.这两个模型都使用随机特征子集, 来生成许多单个的树.

所有A是正确的

**254.对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的"不依赖假设"总是成立, 因为所有主要成分是正交的, 这个说法是 :**

A. 正确的

B. 错误的

答案: B.

这个说法是错误的, 首先, "不依赖"和"不相关"是两回事, 其次, 转化过的特征, 也可能是相关的

**255.对于PCA说法正确的是 :**

1\. 我们必须在使用PCA前规范化数据\
2\. 我们应该选择使得模型有最大variance的主成分\
3\. 我们应该选择使得模型有最小variance的主成分\
4\. 我们可以使用PCA在低维度上做数据可视化

A. 1, 2 and 4

B. 2 and 4

C. 3 and 4

D. 1 and 3

E. 1, 3 and 4

答案: A

1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).

2）我们总是应该选择使得模型有最大variance的主成分

3）有时在低维度上左图是需要PCA的降维帮助的

## [BAT机器学习面试1000题系列（256-260）](https://zhuanlan.zhihu.com/p/32953175)

**256.对于下图, 最好的主成分选择是多少 ?**

![](https://pic3.zhimg.com/80/v2-7bd76801a434f4a3994907e92d5387d7_hd.jpg)

A. 7

B. 30

C. 35

D. 不确定

答案: B

主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。

**257.数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是 :**

A. 单个模型之间有高相关性

B. 单个模型之间有低相关性

C. 在集成学习中使用"平均权重"而不是"投票"会比较好

D. 单个模型都是用的一个算法

答案: B

详细请参考下面文章:

Basics of Ensemble Learning Explained in Simple English

Kaggle Ensemble Guide

5 Easy questions on Ensemble Modeling everyone should know

**258.在有监督学习中， 我们如何使用聚类方法？** :

A. 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习

B. 我们可以使用聚类"类别id"作为一个新的特征项， 然后再用监督学习分别进行学习

C. 在进行监督学习之前， 我们不能新建聚类类别

D. 我们不可以使用聚类"类别id"作为一个新的特征项， 然后再用监督学习分别进行学习

A. 2 和 4

B. 1 和 2

C. 3 和 4

D. 1 和 3

答案: B

我们可以为每个聚类构建不同的模型， 提高预测准确率。

"类别id"作为一个特征项去训练， 可以有效地总结了数据特征。

所以B是正确的

**259.以下说法正确的是 :**

A. 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的

B. 如果增加模型复杂度， 那么模型的测试错误率总是会降低

C. 如果增加模型复杂度， 那么模型的训练错误率总是会降低

D. 我们不可以使用聚类"类别id"作为一个新的特征项， 然后再用监督学习分别进行学习

答案: C

考的是过拟合和欠拟合的问题。

**260.对应GradientBoosting tree算法， 以下说法正确的是 :**

A. 当增加最小样本分裂个数，我们可以抵制过拟合

B. 当增加最小样本分裂个数，会导致过拟合

C. 当我们减少训练单个学习器的样本个数，我们可以降低variance

D. 当我们减少训练单个学习器的样本个数，我们可以降低bias

A. 2 和 4

B. 2 和 3

C. 1 和 3

D. 1 和 4

答案: C

最小样本分裂个数是用来控制"过拟合"参数。太高的值会导致"欠拟合"，这个参数应该用交叉验证来调节。

第二点是靠bias和variance概念的。

## [BAT机器学习面试1000题系列（261-265）](https://zhuanlan.zhihu.com/p/32980788)

**261.以下哪个图是KNN算法的训练边界 :**

![](https://pic4.zhimg.com/80/v2-883758433cae01593bd9b101f289e652_hd.jpg)

A) B

B) A

C) D

D) C

E) 都不是

答案: B

KNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。

**262.如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？**

A. 是的，这说明这个模型的范化能力已经足以支持新的数据集合了

B. 不对，依然后其他因素模型没有考虑到，比如噪音数据

答案: B\
没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。

**263.下面的交叉验证方法 :**

i. 有放回的Bootstrap方法

ii. 留一个测试样本的交叉验证

iii. 5折交叉验证

iv. 重复两次的5折教程验证

当样本是1000时，下面执行时间的顺序，正确的是：

A. i > ii > iii > iv

B. ii > iv > iii > i

C. iv > i > ii > iii

D. ii > iii > iv > i

答案: B

所有B是正确的

-   Boostrap方法是传统地随机抽样，验证一次的验证方法，只需要训练1次模型，所以时间最少。
-   留一个测试样本的交叉验证，需要n次训练过程（n是样本个数），这里，要训练1000个模型。
-   5折交叉验证需要训练5个模型。
-   重复2次的5折交叉验证，需要训练10个模型。

**264.变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？**

1\. 多个变量其实有相同的用处\
2\. 变量对于模型的解释有多大作用\
3\. 特征携带的信息\
4\. 交叉验证

A. 1 和 4

B. 1, 2 和 3

C. 1,3 和 4

D. 以上所有

答案: C

注意， 这题的题眼是考虑模型效率，所以不要考虑选项2.

**265.对于线性回归模型，包括附加变量在内，以下的可能正确的是 :**

1\. R-Squared 和 Adjusted R-squared都是递增的\
2\. R-Squared 是常量的，Adjusted R-squared是递增的\
3\. R-Squared 是递减的， Adjusted R-squared 也是递减的\
4\. R-Squared 是递减的， Adjusted R-squared是递增的

A. 1 和 2

B. 1 和 3

C. 2 和 4

D. 以上都不是

答案: D

R-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。

每次你为模型加入预测器，R-squared递增或不变.

详细请看这个链接：[https://discuss.analyticsvidhya.com/t/difference-between-r-square-and-adjusted-r-square/264/3](https://link.zhihu.com/?target=https%3A//discuss.analyticsvidhya.com/t/difference-between-r-square-and-adjusted-r-square/264/3)[](https://link.zhihu.com/?target=https%3A//discuss.analyticsvidhya.com/t/difference-between-r-square-and-adjusted-r-square/264/3)

## [BAT机器学习面试1000题系列（266-270）](https://zhuanlan.zhihu.com/p/33016414)

**266.对于下面三个模型的训练情况， 下面说法正确的是 :**

![](https://pic4.zhimg.com/80/v2-2939f3ab9934a257d10b1f1dd3d96e88_hd.jpg)

1\. 第一张图的训练错误与其余两张图相比，是最大的\
2\. 最后一张图的训练效果最好，因为训练错误最小\
3\. 第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型\
4\. 第三张图相对前两张图过拟合了\
5\. 三个图表现一样，因为我们还没有测试数据集

A. 1 和 3

B. 1 和 3

C. 1, 3 和 4

D. 5

答案: C

**267.对于线性回归，我们应该有以下哪些假设？ :**

1\. 找到利群点很重要, 因为线性回归对利群点很敏感\
2\. 线性回归要求所有变量必须符合正态分布\
3\. 线性回归假设数据没有多重线性相关性

A. 1 和 2

B. 2 和 3

C. 1,2 和 3

D. 以上都不是

答案: D

利群点要着重考虑, 第一点是对的

不是必须的, 当然, 如果是正态分布, 训练效果会更好

有少量的多重线性相关性是可以的, 但是我们要尽量避免

**268.当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论:**

1\. Var1和Var2是非常相关的\
2\. 因为Var和Var2是非常相关的, 我们可以去除其中一个\
3\. Var3和Var1的1.23相关系数是不可能的

A. 1 and 3

B. 1 and 2

C. 1,2 and 3

D. 1

答案: C

Var1和Var2相关系数是负的, 所以这是多重线性相关, 我们可以考虑去除其中一个.

一般地, 如果相关系数大于0.7或者小于-0.7, 是高相关的

相关性系数范围应该是 [-1,1]

**269.如果在一个高度非线性并且复杂的一些变量中"一个树模型可能比一般的回归模型效果更好"是**

A. 对的

B. 错的

答案: A

**270.对于维度极低的特征，选择线性还是非线性分类器？** 非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。\
1\. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM\
2\. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel\
3\. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。

## [BAT机器学习面试1000题系列（271-275）](https://zhuanlan.zhihu.com/p/33057428)

**271.SVM、LR、决策树的对比。**

模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝

损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失

数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感\
数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

**272.什么是ill-condition病态问题？**

训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。

**273.简述KNN最近邻分类算法的过程？**\
1\. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；\
2\. 对上面所有的距离值进行排序；\
3\. 选前k个最小距离的样本；\
4\. 根据这k个样本的标签进行投票，得到最后的分类类别；

**274.常用的聚类划分方式有哪些？列举代表算法。**\
1\. 基于划分的聚类:K-means，k-medoids，CLARANS。\
2\. 基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。\
3\. 基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。\
4\. 基于网格的方法：STING，WaveCluster。\
5\. 基于模型的聚类：EM,SOM，COBWEB。

**275.下面对集成学习模型中的弱学习者描述错误的是？**\
A.他们经常不会过拟合\
B.他们通常带有高偏差，所以其并不能解决复杂学习问题\
C.他们通常会过拟合\
答案：C，弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。


## [BAT机器学习面试1000题系列（276-280）](https://zhuanlan.zhihu.com/p/33145504)

**276.下面哪个/些选项对 K 折交叉验证的描述是正确的？**

1.增大 K 将导致交叉验证结果时需要更多的时间

2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心

3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量

A. 1 和 2

B. 2 和 3

C. 1 和 3

D. 1、2 和 3

答案（D)：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。

**277.最出名的降维算法是 PAC 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？**\
A. X_projected_PCA 在最近邻空间能得到解释\
B. X_projected_tSNE 在最近邻空间能得到解释\
C. 两个都在最近邻空间能得到解释\
D. 两个都不能在最近邻空间得到解释\
答案（B）：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。

**278.给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？**\
A. D1= C1, D2 < C2, D3 > C3\
B. D1 = C1, D2 > C2, D3 > C3\
C. D1 = C1, D2 > C2, D3 < C3\
D. D1 = C1, D2 < C2, D3 < C3\
E. D1 = C1, D2 = C2, D3 = C3\
答案（E）：特征之间的相关性系数不会因为特征加或减去一个数而改变。

**279.为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？**\
A. 将数据转换成零均值\
B. 将数据转换成零中位数\
C. 无法做到\
答案（A）：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。

**280.假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。**

注意：所有其他超参数是相同的，所有其他因子不受影响。

1.深度为 4 时将有高偏差和低方差

2.深度为 4 时将有低偏差和低方差

A. 只有 1

B. 只有 2

C. 1 和 2

D. 没有一个

答案（A)：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。


## [BAT机器学习面试1000题系列（281-285）](https://zhuanlan.zhihu.com/p/33177339)

**281.在 k-均值算法中，以下哪个选项可用于获得全局最小？**

A. 尝试为不同的质心（centroid）初始化运行算法\
B. 调整迭代的次数\
C. 找到集群的最佳数量\
D. 以上所有\
答案（D）：所有都可以用来调试以找到全局最小。

**282.你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？**

A. 第一个 w2 成了 0，接着 w1 也成了 0\
B. 第一个 w1 成了 0，接着 w2 也成了 0\
C. w1 和 w2 同时成了 0\
D. 即使在 C 成为大值之后，w1 和 w2 都不能成 0\
答案（B）：通过观察图像我们发现，即使只使用 x2，我们也能高效执行分类。因此一开始 w1 将成 0；当正则化参数不断增加时，w2 也会越来越接近 0。

**283.假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。**\
A.如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它。\
B.对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大。\
C.log-loss 越低，模型越好

D.以上都是\
答案为（D）

**284.下面哪个选项中哪一项属于确定性算法？**\
A.PCA\
B.K-Means\
C. 以上都不是\
答案为（A）：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。

**285.特征向量的归一化方法有哪些？**

-   线性函数转换，表达式如下：

y=(x-MinValue)/(MaxValue-MinValue)

-   对数函数转换，表达式如下：

y=log10 (x)

-   反余切函数转换 ，表达式如下：

y=arctan(x)*2/PI

-   减去均值，除以方差：

y=(x-means)/ variance


## [BAT机器学习面试1000题系列（286-290）](https://zhuanlan.zhihu.com/p/33206641)

**286.优化算法及其优缺点？**

温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。\
1）随机梯度下降\
优点：可以一定程度上解决局部最优解的问题\
缺点：收敛速度较慢\
2）梯度下降\
优点：容易陷入局部最优解\
缺点：收敛速度较快\
3）mini_batch梯度下降\
综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。\
4）牛顿法\
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。\
5）拟牛顿法\
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

**287.RF与GBDT之间的区别与联系？**\
1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。\
2）不同点：

a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成\
b 组成随机森林的树可以并行生成，而GBDT是串行生成\
c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和\
d 随机森林对异常值不敏感，而GBDT对异常值比较敏感\
e 随机森林是减少模型的方差，而GBDT是减少模型的偏差\
f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化

**288.两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。**\
A 正确

B 错误\
答案为（A）：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。

**289.下面哪个/些超参数的增加可能会造成随机森林数据过拟合？**\
A 树的数量

B 树的深度

C 学习速率\
答案为（B）：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。

**290.目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是多少？**\
A. -(5/8 log(5/8) + 3/8 log(3/8))\
B. 5/8 log(5/8) + 3/8 log(3/8)\
C. 3/8 log(5/8) + 5/8 log(3/8)\
D. 5/8 log(3/8) -- 3/8 log(5/8)\
答案为（A）



## [BAT机器学习面试1000题系列（291-295）](https://zhuanlan.zhihu.com/p/33259582)

**291.下面有关序列模式挖掘算法的描述，错误的是？（C）**\
A AprioriAll算法和GSP算法都属于Apriori类算法，都要产生大量的候选序列\
B FreeSpan算法和PrefixSpan算法不生成大量的候选序列以及不需要反复扫描原数据库\
C 在时空的执行效率上，FreeSpan比PrefixSpan更优\
D 和AprioriAll相比，GSP的执行效率比较高

@CS青雀，本题解析来源：[http://blog.csdn.net/ztf312/article/details/50889238](https://link.zhihu.com/?target=http%3A//blog.csdn.net/ztf312/article/details/50889238)\
1\. Apriori算法 ：关联分析原始算法，用于从候选项集中发现频繁项集。两个步骤：进行自连接、进行剪枝。缺点：无时序先后性。\
AprioriAll算法：AprioriAll算法与Apriori算法的执行过程是一样的，不同点在于候选集的产生，需要区分最后两个元素的前后。\
AprioriSome算法：可以看做是AprioriAll算法的改进\
AprioriAll算法和AprioriSome算法的比较：\
（1）AprioriAll用 去计算出所有的候选Ck，而AprioriSome会直接用 去计算所有的候选 ，因为 包含 ，所以AprioriSome会产生比较多的候选。\
（2）虽然AprioriSome跳跃式计算候选，但因为它所产生的候选比较多，可能在回溯阶段前就占满内存。\
（3）如果内存占满了，AprioriSome就会被迫去计算最后一组的候选。\
（4）对于较低的支持度，有较长的大序列，AprioriSome算法要好些。\
2\. GPS算法：类Apriori算法。用于从候选项集中发现具有时序先后性的频繁项集。两个步骤：进行自连接、进行剪枝。缺点：每次计算支持度，都需要扫描全部数据集；对序列模式很长的情况，由于其对应的短的序列模式规模太大，算法很难处理。\
3\. SPADE算法：改进的GPS算法，规避多次对数据集D进行全表扫描的问题。与GSP算法大体相同，多了一个ID_LIST记录，使得每一次的ID_LIST根据上一次的ID_LIST得到（从而得到支持度）。而ID_LIST的规模是随着剪枝的不断进行而缩小的。所以也就解决了GSP算法多次扫描数据集D问题。\
4\. FreeSpan算法：即频繁模式投影的序列模式挖掘。核心思想是分治算法。基本思想为：利用频繁项递归地将序列数据库投影到更小的投影数据库集中，在每个投影数据库中生成子序列片断。这一过程对数据和待检验的频繁模式集进行了分割，并且将每一次检验限制在与其相符合的更小的投影数据库中。\
优点：减少产生候选序列所需的开销。缺点：可能会产生许多投影数据库，开销很大，会产生很多的\
5\. PrefixSpan 算法：从FreeSpan中推导演化而来的。收缩速度比FreeSpan还要更快些。

**292.下列哪个不属于常用的文本分类的特征选择算法？**\
A 卡方检验值\
B 互信息\
C 信息增益\
D 主成分分析

常采用特征选择方法。常见的六种特征选择方法：\
1）DF(Document Frequency) 文档频率\
DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性\
2）MI(Mutual Information) 互信息法\
互信息法用于衡量特征词与文档类别直接的信息量。\
如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向"低频"的特征词。\
相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。\
3）(Information Gain) 信息增益法\
通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。\
4）CHI(Chi-square) 卡方检验法\
利用了统计学中的"假设检验"的基本思想：首先假设特征词与类别直接是不相关的\
如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。\
5）WLLR(Weighted Log Likelihood Ration)加权对数似然\
6）WFO（Weighted Frequency and Odds）加权频率和可能性\
[http://blog.csdn.net/ztf312/article/details/50890099](https://link.zhihu.com/?target=http%3A//blog.csdn.net/ztf312/article/details/50890099)

**293.类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？(D)**\
A 伪逆法-径向基（ＲＢＦ）神经网络的训练算法，就是解决线性不可分的情况\
B 基于二次准则的H-K算法：最小均方差准则下求得权矢量，二次准则解决非线性问题\
C 势函数法－非线性\
D 感知器算法－线性分类算法

**294.机器学习中做特征选择时，可能用到的方法有？ （E）**\
A、卡方\
B、信息增益\
C、平均互信息\
D、期望交叉熵\
E 以上都有

**295.下列方法中，不可以用于特征降维的方法包括（E）**\
A 主成分分析PCA\
B 线性判别分析LDA\
C 深度学习SparseAutoEncoder\
D 矩阵奇异值分解SVD\
E 最小二乘法LeastSquares

A.特征降维方法主要有：PCA，LLE，Isomap

B.LDA:线性判别分析，可用于降维\
C.稀疏自编码就是用少于输入层神经元数量的隐含层神经元去学习表征输入层的特征，相当于把输入层的特征压缩了，所以是特征降维。

D.SVD和PCA类似，也可以看成一种降维方法


## [BAT机器学习面试1000题系列（296-300）](https://zhuanlan.zhihu.com/p/33287191)

**296.一般，k-NN最近邻方法在（ A）的情况下效果较好。**

A．样本较多但典型性不好

B．样本呈团状分布

C．样本较少但典型性好\
D．样本呈链状分布

**297.下列哪些方法可以用来对高维数据进行降维（A B C D E F）**\
A LASSO\
B 主成分分析法\
C 聚类分析\
D 小波分析法\
E 线性判别法\
F 拉普拉斯特征映射

解析：lasso通过参数缩减达到降维的目的；\
pca就不用说了\
线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；\
小波分析有一些变换的操作降低其他干扰可以看做是降维\
拉普拉斯请看这个[http://f.dataguru.cn/thread-287243-1-1.html](https://link.zhihu.com/?target=http%3A//f.dataguru.cn/thread-287243-1-1.html)

**298.以下描述正确的是（D）**\
A SVM是这样一个分类器，它寻找具有最小边缘的超平面，因此它也经常被称为最小边缘分类器\
B 在聚类分析当中，簇内的相似性越大，簇间的差别越大，聚类的效果就越差\
C 在决策树中，随着树中结点输变得太大，即使模型的训练误差还在继续降低，但是检验误差开始增大，这是出现了模型拟合不足的原因\
D 聚类分析可以看作是一种非监督的分类

**299.以下说法中正确的是（C）**\
A SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性\
B 在adaboost算法中，所有被分错样本的权重更新比例相同\
C boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重\
D 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少

**300.关于正态分布,下列说法错误的是（C）**\
A.正态分布具有集中性和对称性\
B.正态分布的均值和方差能够决定正态分布的位置和形态\
C.正态分布的偏度为0，峰度为1\
D.标准正态分布的均值为0，方差为1


## [BAT机器学习面试1000题系列（301-305）](https://zhuanlan.zhihu.com/p/33309834)

**301.在以下不同的场景中,使用的分析方法不正确的有 （B）**\
A.根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级\
B.根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式\
C.用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫\
D.根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女

**302.什么是梯度爆炸？**

【解析】误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。\
在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。\
网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。

**303.梯度爆炸会引发什么问题？**

【解析】在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。\
梯度爆炸导致学习过程不稳定。---《深度学习》，2016.\
在循环神经网络中，梯度爆炸会导致网络不稳定，无法利用训练数据学习，最好的结果是网络无法学习长的输入序列数据。

**304.如何确定是否出现梯度爆炸？**

【解析】训练过程中出现梯度爆炸会伴随一些细微的信号，如：\
模型无法从训练数据中获得更新（如低损失）。\
模型不稳定，导致更新过程中的损失出现显著变化。\
训练过程中，模型损失变成 NaN。\
如果你发现这些问题，那么你需要仔细查看是否出现梯度爆炸问题。\
以下是一些稍微明显一点的信号，有助于确认是否出现梯度爆炸问题。\
训练过程中模型梯度快速变大。\
训练过程中模型权重变成 NaN 值。\
训练过程中，每个节点和层的误差梯度值持续超过 1.0。

**305.如何修复梯度爆炸问题？**\
【解析】有很多方法可以解决梯度爆炸问题，本节列举了一些最佳实验方法。\
1\. 重新设计网络模型\
在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。\
使用更小的批尺寸对网络训练也有好处。\
在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。\
2\. 使用 ReLU 激活函数\
在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。\
使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。\
3\. 使用长短期记忆网络\
在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。\
使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。\
采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。\
4\. 使用梯度截断（Gradient Clipping）\
在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。\
处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。\
------《Neural Network Methods in Natural Language Processing》，2017.\
具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。\
梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。\
------《深度学习》，2016.\
在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。\
默认值为 clipnorm=1.0 、clipvalue=0.5。详见：[https://keras.io/optimizers/](https://link.zhihu.com/?target=https%3A//keras.io/optimizers/)。\
5\. 使用权重正则化（Weight Regularization）\
如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。\
对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。\
------On the difficulty of training recurrent neural networks，2013.\
在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。


## [BAT机器学习面试1000题系列（306-310）](https://zhuanlan.zhihu.com/p/33343760)

**306\. LSTM神经网络输入输出究竟是怎样的？**\
@YJango，本题解析来源：<https://www.zhihu.com/question/41949741>\
Recurrent Layers------介绍（<https://zhuanlan.zhihu.com/p/24720659?refer=YJango>）

-   第一要明确的是神经网络所处理的单位全部都是：向量

下面就解释为什么你会看到训练数据会是矩阵和张量

-   常规feedforward 输入和输出：矩阵

输入矩阵形状：(n_samples, dim_input)\
输出矩阵形状：(n_samples, dim_output)\
注：真正测试/训练的时候，网络的输入和输出就是向量而已。加入n_samples这个维度是为了可以实现一次训练多个样本，求出平均梯度来更新权重，这个叫做Mini-batch gradient descent。 如果n_samples等于1，那么这种更新方式叫做Stochastic Gradient Descent (SGD)。\
**Feedforward 的输入输出的本质都是单个向量。**

-   常规Recurrent (RNN/LSTM/GRU) 输入和输出：张量

输入张量形状：(time_steps, n_samples, dim_input)\
输出张量形状：(time_steps, n_samples, dim_output)\
注：同样是保留了Mini-batch gradient descent的训练方式，但不同之处在于多了time step这个维度。\
**Recurrent 的任意时刻的输入的本质还是单个向量，只不过是将不同时刻的向量按顺序输入网络。所以你可能更愿意理解为一串向量 a sequence of vectors，或者是矩阵。**

python代码表示预测的话：

```
import numpy as np
#当前所累积的hidden_state,若是最初的vector，则hidden_state全为0
hidden_state=np.zeros((n_samples, dim_input))
#print(inputs.shape)：（time_steps, n_samples, dim_input)
outputs = np.zeros((time_steps, n_samples, dim_output))

for i in range(time_steps):
    #输出当前时刻的output，同时更新当前已累积的hidden_state outputs[i],
    hidden_state = RNN.predict(inputs[i],hidden_state)
#print(outputs.shape)：(time_steps, n_samples, dim_output)

```

但需要注意的是，Recurrent nets的输出也可以是矩阵，而非三维张量，取决于你如何设计。

1.  若想用一串序列去预测另一串序列，那么输入输出都是张量 (例如语音识别或机器翻译 一个中文句子翻译成英文句子（一个单词算作一个向量），机器翻译还是个特例，因为两个序列的长短可能不同，要用到seq2seq；
2.  若想用一串序列去预测一个值，那么输入是张量，输出是矩阵 （例如，情感分析就是用一串单词组成的句子去预测说话人的心情）

**Feedforward 能做的是向量对向量的one-to-one mapping，\
Recurrent 将其扩展到了序列对序列 sequence-to-sequence mapping.**\
但单个向量也可以视为长度为1的序列。所以有下图几种类型：

![](https://pic3.zhimg.com/80/v2-d4c592a9a6f8f7f977462a85baee112a_hd.jpg)

除了最左侧的one to one是feedforward 能做的，右侧都是Recurrent所扩展的

若还想知道更多

-   可以将Recurrent的横向操作视为累积已发生的事情，并且LSTM的memory cell机制会选择记忆或者忘记所累积的信息来预测某个时刻的输出。
-   以概率的视角理解的话：就是不断的conditioning on已发生的事情，以此不断缩小sample space
-   RNN的思想是: current output不仅仅取决于current input，还取决于previous state；可以理解成current output是由current input和previous hidden state两个输入计算而出的。并且每次计算后都会有信息残留于previous hidden state中供下一次计算

**307.以下关于PMF(概率质量函数),PDF(概率密度函数),CDF(累积分布函数)描述错误的是？**\
A.PDF描述的是连续型随机变量在特定取值区间的概率\
B.CDF是PDF在特定区间上的积分\
C.PMF描述的是离散型随机变量在特定取值点的概率\
D.有一个分布的CDF函数H(x),则H(a)等于P(X<=a)

正确答案：A\
解析：\
概率质量函数 (probability mass function，PMF)是离散随机变量在各特定取值上的概率。\
概率密度函数（p robability density function，PDF ）是对 连续随机变量 定义的，本身不是概率，只有对连续随机变量的取值进行积分后才是概率。\
累积分布函数（cumulative distribution function，CDF） 能完整描述一个实数随机变量X的概率分布，是概率密度函数的积分。对于所有实数x 与pdf相对。

**308.线性回归的基本假设有哪些？(ABDE)** A.随机误差项是一个期望值为0的随机变量；\
B.对于解释变量的所有观测值，随机误差项有相同的方差；\
C.随机误差项彼此相关；\
D.解释变量是确定性变量不是随机变量，与随机误差项之间相互独立；\
E.随机误差项服从正态分布

**309.处理类别型特征时，事先不知道分类变量在测试集中的分布。要将 one-hot encoding（独热码）应用到类别型特征中。那么在训练集中将独热码应用到分类变量可能要面临的困难是什么？**\
A. 分类变量所有的类别没有全部出现在测试集中\
B. 类别的频率分布在训练集和测试集是不同的\
C. 训练集和测试集通常会有一样的分布\
答案为：A、B ，如果类别在测试集中出现，但没有在训练集中出现，独热码将不能进行类别编码，这是主要困难。如果训练集和测试集的频率分布不相同，我们需要多加小心。

**310.假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出「-0.0001」。X 可能是以下哪一个激活函数？**\
A. ReLU\
B. tanh\
C. SIGMOID\
D. 以上都不是\
答案为：B，该激活函数可能是 tanh，因为该函数的取值范围是 (-1,1)。


## [BAT机器学习面试1000题系列（311-315）](https://zhuanlan.zhihu.com/p/33404937)

**311、下面哪些对「类型 1（Type-1）」和「类型 2（Type-2）」错误的描述是正确的？**

A. 类型 1 通常称之为假正类，类型 2 通常称之为假负类。\
B. 类型 2 通常称之为假正类，类型 1 通常称之为假负类。\
C. 类型 1 错误通常在其是正确的情况下拒绝假设而出现。\
答案为(A)和(C)：在统计学假设测试中，I 类错误即错误地拒绝了正确的假设即假正类错误，II 类错误通常指错误地接受了错误的假设即假负类错误。

**312、在下面的图像中，哪一个是多元共线（multi-collinear）特征？**

![](https://pic2.zhimg.com/80/v2-cd90ae18e0595816874d304c5fb6245b_hd.jpg)

A. 图 1 中的特征\
B. 图 2 中的特征\
C. 图 3 中的特征\
D. 图 1、2 中的特征\
E. 图 2、3 中的特征\
F. 图 1、3 中的特征\
答案为（D）：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。

**313、鉴别了多元共线特征。那么下一步可能的操作是什么？**\
A. 移除两个共线变量

B. 不移除两个变量，而是移除一个\
C. 移除相关变量可能会导致信息损失，可以使用带罚项的回归模型（如 ridge 或 lasso regression）。\
答案为（B）和（C）：因为移除两个变量会损失一切信息，所以我们只能移除一个特征，或者也可以使用正则化算法（如 L1 和 L2）。

**314、给线性回归模型添加一个不重要的特征可能会造成？**\
A. 增加 R-square

B. 减少 R-square\
答案为（A）：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。

**315、假定目标变量的类别非常不平衡，即主要类别占据了训练数据的 99%。现在你的模型在测试集上表现为 99% 的准确度。那么下面哪一项表述是正确的？**\
A. 准确度并不适合于衡量不平衡类别问题\
B. 准确度适合于衡量不平衡类别问题\
C. 精确率和召回率适合于衡量不平衡类别问题\
D. 精确率和召回率不适合于衡量不平衡类别问题\
答案为（A）和（C）


## [BAT机器学习面试1000题系列（316-320）](https://zhuanlan.zhihu.com/p/33430550)

**316、什么是偏差与方差？**

泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。

偏差：

![](https://pic1.zhimg.com/80/v2-097a71d4fea5d0d4de8034579b4424ba_hd.jpg)

方差：

![](https://pic4.zhimg.com/80/v2-f025aba189e1290b954c77da0dc0004f_hd.jpg)

**317、解决bias和Variance问题的方法是什么？**

交叉验证\
High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征\
High Variance解决方案：agging、简化模型、降维

**318.采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？**\
用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。

**319、xgboost怎么给特征评分？**

在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。[python]

```
# feature importance
print(model.feature_importances_)
# plot  pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)
pyplot.show()  ==========
# plot feature importance
plot_importance(model)
pyplot.show()

```

Python是最好的语言 ------鲁迅

向所有的程序员致敬

![](https://pic4.zhimg.com/80/v2-b19687767acd268e8a6e6da7e0400aa1_hd.jpg)

**320、什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？**

bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为袋外数据oob（out of bag）,它可以用于取代测试集误差估计方法。\
袋外数据(oob)误差的计算方法如下：\
对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类,因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O;这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。


## [BAT机器学习面试1000题系列（321-325）](https://zhuanlan.zhihu.com/p/33455911)

**321、假设张三的mp3里有1000首歌，现在希望设计一种随机算法来随机播放。与普通随机模式不同的是，张三希望每首歌被随机到的概率是与一首歌的豆瓣评分（0~10分）成正比的，如朴树的《平凡之路》评分为8.9分，逃跑计划的《夜空中最亮的星》评分为9.5分，则希望听《平凡之路》的概率与《夜空中最亮的星》的概率比为89:95。现在我们已知这1000首歌的豆瓣评分：**

**（1）请设计一种随机算法来满足张三的需求。**

**（2）写代码实现自己的算法。**

```
#include <iostream>

```

#include <time.h>

#include <stdlib.h>

using namespace std;

int findIdx(double songs[],int n,double rnd){

int left=0;

int right=n-1;

int mid;

while(left<=right){

mid=(left+right)/2;

if((songs[mid-1]<=rnd) && (songs[mid]>=rnd))

return mid;

if(songs[mid]>rnd)

right=mid-1;

else

left=mid+1;

}

// return mid;

}

int randomPlaySong(double sum_scores[],int n){

double mx=sum_scores[n-1];

double rnd= rand()*mx/(double)(RAND_MAX);

return findIdx(sum_scores,n,rnd);

}

int main()

{

srand(time(0));

double scores[]={5.5,6.5,4.5,8.5,9.5,7.5,3.5,5.0,8.0,2.0};

int n=sizeof(scores)/sizeof(scores[0]);

double sum_scores[n];

sum_scores[0]=scores[0];

for(int i=1;i<n;i++)

sum_scores[i]=sum_scores[i-1]+scores[i];

cout<<"Calculate the probability of each song: "<<endl;

int totalScore=sum_scores[n-1];

for(int i=0;i<n;i++)

cout<<scores[i]/totalScore<<" ";

cout<<endl;

int counts[n];

for(int i=0;i<n;i++)

counts[i]=0;

int i=0;

int idx;

int MAX_ITER=100000000;

while(i<MAX_ITER){

idx=randomPlaySong(sum_scores,n);

counts[idx]++;

i++;

}

cout<<"After simulation, probability of each song: "<<endl;

for(int i=0;i<n;i++)

cout<<1.0*counts[i]/MAX_ITER<<" ";

cout<<endl;

return 0;

}

**322.对于logistic regession问题：prob（t|x）=1/（1+exp（w*x+b））且label y=0或1，请给出loss function和权重w的更新公式及推导。**

Logistic regression 的loss function 是log loss, 公式表达为：

![](https://pic2.zhimg.com/80/v2-49b582d6c83f60f8afeabd28890073a9_hd.jpg)

w的更新公式可以由最小化loss function得到，即：

![](https://pic2.zhimg.com/80/v2-7abdd7410082f4f30e208efdb0b49597_hd.jpg)

其中大括号里面的部分，等价于逻辑回归模型的对数似然函数，所以也可以用极大似然函数方法求解，

根据梯度下降法，其更新公式为：

![](https://pic4.zhimg.com/80/v2-d2c7fe4aebccd81f4f60d6f0bd01ff06_hd.jpg)

**323.决策树的父节点和子节点的熵的大小关系是什么？**

A. 决策树的父节点更大

B. 子节点的熵更大

C. 两者相等

D. 根据具体情况而定

正确答案：B。在特征选择时，应该给父节点信息增益最大的节点，而信息增益的计算为 IG(Y|X) = H(Y) - H(Y/X)，H(Y/X) 为该特征节点的条件熵， H(Y/X) 越小，即该特征节点的属性对整体的信息表示越"单纯"，IG更大。 则该属性可以更好的分类。H(Y/X) 越大，属性越"紊乱"，IG越小，不适合作为分类属性。

**324.欠拟合和过拟合的原因分别有哪些？如何避免？**

欠拟合的原因：模型复杂度过低，不能很好的拟合所有的数据，训练误差大；

避免欠拟合：增加模型复杂度，如采用高阶模型（预测）或者引入更多特征（分类）等。

过拟合的原因：模型复杂度过高，训练数据过少，训练误差小，测试误差大；

避免过拟合：降低模型复杂度，如加上正则惩罚项，如L1，L2，增加训练数据等。

**325.语言模型的参数估计经常使用MLE（最大似然估计）。面临的一个问题是没有出现的项概率为0，这样会导致语言模型的效果不好。为了解决这个问题，需要使用：**

A. 平滑

B. 去噪

C. 随机插值

D. 增加白噪音


## [BAT机器学习面试1000题（326~330题）](https://zhuanlan.zhihu.com/p/33637683)

**326.下面关于Hive的说法正确的是( )**

A. Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文本映射为一张数据库表，并提供简单的SQL查询功能

B. Hive可以直接使用SQL语句进行相关操作

C. Hive能够在大规模数据集上实现低延迟快速的查询

D. Hivez在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下

正确答案：A

Hive使用类sql语句进行相关操作，称为HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。

Hive 构建在基于静态批处理的Hadoop 之上，Hadoop 通常都有较高的延迟并且在作业提交和调度的时候需要大量的开销。因此，Hive 并不能够在大规模数据集上实现低延迟快速的查询，例如，Hive在几百MB 的数据集上执行查询一般有分钟级的时间延迟。

Hive 并不适合那些需要低延迟的应用，例如，联机事务处理（OLTP）。Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的HiveQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive 并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是大数据集的批处理作业，例如，网络日志分析。

**327.关于input split和block的描述正确的是( )**

A. Mapreduce 的input split就是一个block

B. input split是一种记录的逻辑划分,而block是对输入数据的物理分割,两者之间有着本质的区别

C. 由于Block是本地的,DFSCline可以不用向DataNode建立连接,直接读磁盘上的文件

D. 为了发挥计算本地化性能,应该尽量使inputSplit大小与block大小相当

正确答案：B

1\. 一个split不会包含零点几或者几点几个Block，一定是包含大于等于1个整数个Block

2\. 一个split不会包含两个File的Block,不会跨越File边界

3\. split和Block的关系是一对多的关系

4\. maptasks的个数最终决定于splits的长度

**328.推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到。**

根据贝叶斯公式P(c|d)=（P(c)P(d|c)/P(d)）。

这里，分母P(d)不必计算，因为对于每个类都是相等的。 分子中，P(c)是每个类别的先验概率，可以从训练集直接统计，

P(d|c)根据独立性假设，可以写成如下 P(d|c)=￥P(wi|c)（￥符号表示对d中每个词i在c类下概率的连乘），

P(wi|c)也可以从训练集直接统计得到。 至此，对未知类别的d进行分类时，类别为c=argmaxP(c)￥P(wi|c)。

**329.逻辑回归与多元回归分析有哪些不同？**

A. 逻辑回归预测某事件发生的概率\
B. 逻辑回归有较高的拟合效果\
C. 逻辑回归回归系数的评估\
D. 以上全选

答案：D\
逻辑回归是用于分类问题，我们能计算出一个事件/样本的概率；一般来说，逻辑回归对测试数据有着较好的拟合效果；建立逻辑回归模型后，我们可以观察回归系数类标签(正类和负类)与独立变量的的关系。

**330."过拟合是有监督学习的挑战，而不是无监督学习"以上说法是否正确：**\
A. 正确\
B. 错误

答案：B\
我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数.








