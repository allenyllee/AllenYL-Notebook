# 機器學習理論2-1_深度學習

[toc]
<!-- toc --> 

## Some Ideas

- super resolution for text image




## Back Propergation

- [三十分钟理解计算图上的微积分：Backpropagation，反向微分 - CSDN博客](https://blog.csdn.net/xbinworld/article/details/56523063)

    > 神经网络的训练算法，目前基本上是以Backpropagation (BP) 反向传播为主（加上一些变化），NN的训练是在1986年被提出，但实际上，BP 已经在不同领域中被重复发明了数十次了（参见 Griewank (2010)[1]）。更加一般性且与应用场景独立的名称叫做：反向微分 (reverse-mode differentiation)。本文是看了资料[2]中的介绍，写的蛮好，自己记录一下，方便理解。
    > 
    > 从本质上看，BP 是一种快速求导的技术，可以作为一种不单单用在深度学习中并且可以胜任大量数值计算场景的基本的工具。
    > 
    > 计算图
    > ---
    > 
    > 必须先来讲一讲计算图的概念，计算图出现在Bengio 09年的《Learning Deep Architectures for AI》，\
    > Bengio使用了有向图结构来描述神经网络的计算:
    > 
    > ![这里写图片描述](https://i.imgur.com/xr10GF1.jpg)
    > 
    > 整张图可看成三部分：输入结点、输出结点、从输入到输出的计算函数。上图很容易理解，就是output=sin(a*x+b) * x
    > 
    > 计算图上的导数
    > -------
    > 
    > 有向无环图在计算机科学领域到处可见，特别是在函数式程序中。他们与依赖图（dependency graph）或者调用图（call graph）紧密相关。同样他们也是大部分非常流行的深度学习框架背后的核心抽象。
    > 
    > 下文以下面简单的例子来描述：
    > 
    > ![这里写图片描述](https://i.imgur.com/PKdkrti.jpg)
    > 
    > 假设 a = 2, b = 1，最终表达式的值就是 6。\
    > 为了计算在这幅图中的偏导数，我们需要 和式法则（sum rule ）和 乘式法则（product rule）：
    > 
    > ![这里写图片描述](https://i.imgur.com/EpXcBUr.jpg)
    > 
    > 下面，在图中每条边上都有对应的导数了：\
    > ![这里写图片描述](https://i.imgur.com/2PsrGvT.jpg)
    > 
    > 那如果我们想知道哪些没有直接相连的节点之间的影响关系呢？假设就看看 e 如何被 a 影响的。如果我们以 1 的速度改变 a，那么 c 也是以 1 的速度在改变，导致 e 发生了 2 的速度在改变。因此 e 是以 1 * 2 的关于 a 变化的速度在变化。\
    > 而一般的规则就是对一个点到另一个点的所有的可能的路径进行求和，每条路径对应于该路径中的所有边的导数之积。因此，为了获得 e 关于 b 的导数，就采用路径求和：
    > 
    > ![这里写图片描述](https://i.imgur.com/cuyVDGs.jpg)
    > 
    > 这个值就代表着 b 改变的速度通过 c 和 d 影响到 e 的速度。聪明的你应该可以想到，事情没有那么简单吧？是的，上面例子比较简单，在稍微复杂例子中，路径求和法很容易产生路径爆炸：
    > 
    > ![这里写图片描述](https://i.imgur.com/Bc8CbTc.jpg)
    > 
    > 在上面的图中，从 X 到 Y 有三条路径，从 Y 到 Z 也有三条。如果我们希望计算 dZ/dX，那么就要对 3 * 3 = 9 条路径进行求和了：
    > 
    > ![这里写图片描述](https://i.imgur.com/P8coBQj.jpg)
    > 
    > 该图有 9 条路径，但是在图更加复杂的时候，路径数量会指数级地增长。相比于粗暴地对所有的路径进行求和，更好的方式是进行因式分解：
    > 
    > ![这里写图片描述](https://i.imgur.com/hJzrVUn.jpg)
    > 
    > 有了这个因式分解，就出现了高效计算导数的可能------通过在每个节点上反向合并路径而非显式地对所有的路径求和来大幅提升计算的速度。实际上，两个算法对每条边的访问都只有一次！
    > 
    > 前向微分和反向微分
    > ---------
    > 
    > 前向微分**从图的输入开始**，一步一步到达终点。在每个节点处，对输入的路径进行求和。每个这样的路径都表示输入影响该节点的一个部分。通过将这些影响加起来，我们就得到了输入影响该节点的全部，也就是关于输入的导数。
    > 
    > ![这里写图片描述](https://i.imgur.com/ibZ33TE.jpg)
    > 
    > 相对的，反向微分是从图的输出开始，反向一步一步抵达最开始输入处。在每个节点处，会合了所有源于该节点的路径。
    > 
    > ![这里写图片描述](https://i.imgur.com/CM40UJP.jpg)
    > 
    > 前向微分 跟踪了输入如何改变每个节点的情况。反向微分 则跟踪了每个节点如何影响输出的情况。也就是说，前向微分应用操作 d/dX 到每个节点，而反向微分应用操作 dZ/d 到每个节点。
    > 
    > **让我们重新看看刚开始的例子：**\
    > ![这里写图片描述](https://i.imgur.com/DSSIWVg.jpg)
    > 
    > 我们可以从 b 往上使用前向微分。这样获得了每个节点关于 b 的导数。（写在边上的导数我们已经提前算高了，这些相对比较容易，只和一条边的输入输出关系有关）
    > 
    > ![这里写图片描述](https://i.imgur.com/uxeeoQk.jpg)
    > 
    > 我们已经计算得到了 de/db，**输出关于一个输入 b 的导数**。但是如果我们从 e 往回计算反向微分呢？这会得到 **e 关于每个节点的导数：**
    > 
    > ![这里写图片描述](https://i.imgur.com/tiReB9V.jpg)
    > 
    > 反向微分给出了 e 关于每个节点的导数，这里的确是每一个节点。我们得到了 de/da 和 de/db，e 关于输入 a 和 b 的导数。（当然中间节点都是包括的），**前向微分给了我们输出关于某一个输入的导数，而反向微分则给出了所有的导数。**
    > 
    > **想象一个拥有百万个输入和一个输出的函数。前向微分需要百万次遍历计算图才能得到最终的导数，而反向微分仅仅需要遍历**一次**就能得到所有的导数！速度极快！**
    > 
    > 训练神经网络时，我们将衡量神经网络表现的代价函数看做是神经网络参数的函数。我们希望计算出代价函数关于所有参数的偏导数，从而进行梯度下降（gradient descent）。现在，常常会遇到百万甚至千万级的参数的神经网络。所以，反向微分，也就是 BP，在神经网络中发挥了关键作用！所以，其实BP的本质就是链式法则。
    > 
    > （有使用前向微分更加合理的场景么？当然！因为反向微分得到一个输出关于所有输入的导数，前向微分得到了所有输出关于一个输入的导数。如果遇到了一个有多个输出的函数，前向微分肯定更加快速）
    > 
    > BP 也是一种理解导数在模型中如何流动的工具。在推断为何某些模型优化非常困难的过程中，BP 也是特别重要的。典型的例子就是在 Recurrent Neural Network 中理解 vanishing gradient 的原因。


- [Automatic differentiation - Wikiwand](https://www.wikiwand.com/en/Automatic_differentiation)


    > ![Figure 2: Example of forward accumulation with computational graph](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/ForwardAccumulationAutomaticDifferentiation.png/600px-ForwardAccumulationAutomaticDifferentiation.png) 
    > 
    > Figure 2: Example of forward accumulation with computational graph
    > 
    > ![Figure 3: Example of reverse accumulation with computational graph](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/ReverseaccumulationAD.png/600px-ReverseaccumulationAD.png) 
    > 
    > Figure 3: Example of reverse accumulation with computational graph
    > 


### softmax/corss entropy

- [derivative - Backpropagation with Softmax / Cross Entropy - Cross Validated](https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy)

    The cross entropy error function is

    $$
    E=-\sum_jt_j\log o_j
    $$

    with $t$   and $o$   as the target and output at neuron $j$  , respectively. The sum is over each neuron in the output layer. $o_j$    itself is the result of the softmax function:

    $$
    o_j=softmax(z_j)=\frac{e^{z_j}}{\sum_j e^{z_j}}
    $$

    Again, the sum is over each neuron in the output layer and $z_j$ is the input to neuron $j$:

    $$
    z_j=\sum_i w_{ij}y_i+b_j
    $$

    That is the sum over all neurons in the previous layer with their corresponding output $o_i$ and weight $w_ij$ towards neuron $j$ plus a bias $b$.

    Finally, to get the gradient of $E$ with respect to the weight-matrix $w$, giving the final expression (assuming a one-hot $t$  , i.e. $τ=1$  )

    $$
    \frac{\partial E}{\partial w_{ij}}=y_i(o_j-t_j)
    $$

    where $y$ is the input on the lowest level (of your example).


- [machine learning - Cross entropy function (python) - Stack Overflow](https://stackoverflow.com/questions/47377222/cross-entropy-function-python)

    > ```python
    > def cross_entropy(predictions, targets, epsilon=1e-12):
    >     """
    >     Computes cross entropy between targets (encoded as one-hot vectors)
    >     and predictions. 
    >     Input: predictions (N, k) ndarray
    >            targets (N, k) ndarray        
    >     Returns: scalar
    >     """
    >     predictions = np.clip(predictions, epsilon, 1. - epsilon)
    >     N = predictions.shape[0]
    >     ce = -np.sum(np.sum(targets*np.log(predictions+1e-9)))/N
    >     return ce
    > 
    > predictions = np.array([[0.25,0.25,0.25,0.25],
    >                         [0.01,0.01,0.01,0.96]])
    > targets = np.array([[0,0,0,1],
    >                    [0,0,0,1]])
    > ans = 0.71355817782  #Correct answer
    > x = cross_entropy(predictions, targets)
    > print(np.isclose(x,ans))
    > ```
    > 
    > Here, I think it's a little clearer if you stick with np.sum(). Also, I added 1e-9 into the np.log() to avoid the possibility of having a log(0) in your computation. Hope this helps!


## Activation Function


### Softmax

### Hierarchical Softmax

- [類神經網路 -- Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax) « MARK CHANG'S BLOG](http://cpmarkchang.logdown.com/posts/276263--hierarchical-probabilistic-neural-networks-neural-network-language-model)

- [word2vec原理(二) 基于Hierarchical Softmax的模型 - 刘建平Pinard - 博客园](https://www.cnblogs.com/pinard/p/7243513.html)



### sigmoid

### tanh

### relu

### selu

- [[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)

- [最新激活神經元:Self-Normalization Neural Network (SELU) | Learning by Hacking](https://data-sci.info/2017/06/11/%E6%9C%80%E6%96%B0%E6%BF%80%E6%B4%BB%E7%A5%9E%E7%B6%93%E5%85%83self-normalization-neural-network-selu/)

    > 在訓練深度神經網路時會碰到gradient vanishing的問題，這個問題在Weight初始值或Activation  Neuron設定不當時會更嚴重，以下引用Andrej Karpathy在[Standford CS231n](http://cs231n.stanford.edu/)上的說明:
    > 
    > [[](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.17.22.png)](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.17.22.png)
    > 
    > 圖為個訓練10層神經網路，每層500個hidden unit實驗，Weight初始值是從Gaussian(0,0.01)隨機抽，Activation Neuron採用tanh。上圖由左到下分別為每層輸出平均、標準差和分佈。我們可以看到在第三層以後輸出幾乎都在0附近，這會造成最後面幾層的Gradient非常小，非常小的Gradient再往前傳會更小(可以想像成一個<0的數一直乘)，於是到最後gradient就幾乎等於0，我們稱這個叫做Gradient Vanishing。Gradient Vanishing後Weight就失去了更新的方向，於是model就train不動了。
    > 
    > 為了改善這個問題，前人做過很多的努力，以下簡單帶過如下(細節請參考[Standford CS231n 2017: Training Neural Network Part I](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf))
    > 
    > 1.  Xavier Initialization[1]: 這個方法設計了一個Initialization的方式，讓輸出層的值在線性的activation neuron會有長得像Gaussian(0,1)的分佈。但是遇到非線性的activation neuron像是relu就會失效(mean ~0, std ~ 0 -> Gradient Vanishing)。
    > 2.  He Initialization[2]: 也是一個特殊的Initialization方式，雖然改善了前者讓Gradient Vanishing情況變好，但不是Gaussian(0,1)。
    > 3.  Batch Normalization[3]:簡單來說就是既然想要Gaussian(0,1)，那就每層都做一個吧。直覺上解釋就是每層去normalize他，此外又加上了兩個model可以學出來的參數來決定normalize的程度。這是目前在訓練深度神經網路中最常見的做法。
    > 
    > 那麼本篇的主角Selu，到底是什麼呢？ 他從本質上去改進Activation Neuron，讓他在數學上具有自動讓輸出值收斂到 mean = 0, std =1，即便是在有noise的情況下。而就算std不收斂到1，作者們也給出了上下界。那這神奇的Selu長什麼樣子呢？
    > 
    > [[](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.57.28-300x63.png)](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.57.28.png)
    > 
    > Source: Paper
    > 
    > 其中的λ=1.0507009873554804934193349852946, α= 1.6732632423543772848170429916717
    > 
    > 讀者看到這可能以為小編在亂寫，但是這些數字真的是作者用各種數學證明算出來的(主要是Banach Fix Point Theorem)，程式碼也是這個數字。而他的的Performance上贏過了許多現有的baselines，小編在此取其一給讀者
    > 
    > [[](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午10.06.58.png)](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午10.06.58.png)
    > 
    > 上圖是將SELU + Feed Forward Neuron(FNN) 和 BatchNorm + FNN分別在MNIST和CIFAR上的表現。我們可以看到還不到很深層SELU的表現已經勝過Batch Norm。更多細節請參考
    > 
    > -   原始論文([Arxiv](https://arxiv.org/pdf/1706.02515.pdf)) (題外話:共同作者之一Hochreiter也是LSTM的共同作者)
    > -   code([Github](https://github.com/bioinf-jku/SNNs))
    > -   作者教你怎麼算λ,α的Code([Github](https://github.com/bioinf-jku/SNNs/blob/master/getSELUparameters.ipynb))
    > 
    > 另外已經有強者用Tensorflow在CIFAR、SVHN、MNIST上做出SELU,RELU,leaky RELU的輸出值的比較，非常適合想要實際玩玩SELU的讀者:
    > 
    > -   Activation Visualization Histogram ([Github](https://github.com/shaohua0116/Activation-Visualization-Histogram))
    > 

## initializer

#### Xavier
- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)

#### He
#### Lecun


## optimizer

- Gradient Descent
    $$
    \begin{aligned}
    &\theta_t : \text{目前參數值所成向量}&\\
    &g_t = \nabla_\theta L(\theta_t)&\\
    &\\
    &\theta_{t+1}=\theta_t+\eta g_t&\\
    \end{aligned}
    $$
    
    
- adam

## Regularization

- dropout
- L2 Regularization

通常 dropout, L2 regularization 選⼀個就好


## Convolution

- 公式推導

    $$
    \begin{aligned}
    & L: Origin\ Width &\\
    & p: padding &\\
    & f: filter\ Width &\\
    & s: stride &\\
    & N: New\ Width &\\
    &\\
    &L+2p = f \times N -(f-s)(N-1) &\\
    & \Longrightarrow L = f + s(N-1) - 2p &\\
    & \Longrightarrow N = \frac{L-f+2p}{s}+1 &\\
    \end{aligned}
    $$

    ![](https://screenshotscdn.firefoxusercontent.com/images/4ca43278-1643-4e84-8aa4-b9f4bb8545b1.png)


- [[1603.07285] A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)



## 侷限性 Limitation 

- [深度学习的局限性](https://zhuanlan.zhihu.com/p/27955150)

    > 如果你开发了一个能控制人身体的神经网络，并希望它能够在不被任何车碰撞的情况下游览整个城市，那么神经网路控制的人在各种情况下都要死掉数千次，直到可以判断出车辆的情况以及各种危险，并制定执行回避行为。而再去一个新的城市，神经网络必须重新学习大部分的知识。反过来，人类能够学习安全的行为，而没有用死亡试错 的过程，这要归功于人类假设情境的抽象建模。
    > 
    > ![](https://pic3.zhimg.com/80/v2-01c43aec1debaa7f6b7fb5c18915c519_hd.jpg)
    > 
    > （图：同样的经验，左侧是机器学习的局部泛化，缺乏抽象能力。右侧是人类的极端泛华，可以通过假设而抽象建模，不必真的遍历）
    > 
    > 简而言之，尽管我们在机器感知上取得了进步，但我们仍然远离感性的人文 AI：我们的模型只能执行*局部泛化*，适应与过去数据非常接近的新情况，而人类认知能够*极端泛化*，迅速适应大胆新奇的情况，或为长远未来的情况进行规划。
    > 
    > ---
    > 
    > ## 结论
    > 
    > 这是你应该记住的：到目前为止，深度学习的唯一真正的成功是使用连续几何变换将空间 X 映射到空间 Y 的能力，但还要给出了大量的人为注释的数据。做好这一切，基本上能改变每一个行业的游戏规则，但是距离更人性化的 AI 还有很长一段路要走。
    > 
    > 为了让 AI 解决这些限制，并开始与人类大脑竞争，我们需要跳出「简单的输入到输出映射」，关注*推理*和*抽象*。
    > 
    > > 原文：[The limitations of deep learning](https://link.zhihu.com/?target=https%3A//blog.keras.io/the-limitations-of-deep-learning.html)
    > 

## 剪枝 pruning


### 推翻剪枝固有觀點？清華、伯克利提出NN過參數化真的不重要

- [[1810.05270] Rethinking the Value of Network Pruning](https://arxiv.org/abs/1810.05270)

- [推翻剪枝固有觀點？清華、伯克利提出NN過參數化真的不重要 - 幫趣](http://bangqu.com/973584.html)


    > 網絡剪枝的典型過程包括三步：1）訓練一個大的過參數化模型，2）根據某個標準修剪訓練好的模型，3）微調修剪後的模型，以恢復損失的性能。
    > 
    > ![](http://i2.bangqu.com/j/news/20181022/97358415401844294749t928.png)
    > 
    > *圖 1：典型的三步網絡剪枝步驟。*
    > 
    > 一般來說，剪枝過程背後有兩個共識：一，大家都認爲首先訓練一個大的過參數化模型很重要（Luo et al., 2017），因爲這樣會提供高性能模型（更強的表徵和優化能力），而人們可以安全地刪除冗餘參數而不會對準確率造成顯著損傷。因此，這種觀點很普遍，人們認爲該方法比從頭開始直接訓練更小的網絡（一種常用的基線方法）更優秀。二，剪枝後的架構和相關的權重被認爲是獲得最終高效模型的關鍵。因此大多現有的剪枝技術選擇微調剪枝後的模型，而不是從頭開始訓練它。剪枝後保留下來的權重通常被視爲至關重要，因爲準確地選擇一組重要的權重並不容易。
    > 
    > 在本研究中，我們認爲上面提到的兩種觀點都未必正確。針對多個數據集及多個網絡架構，我們對當前最優剪枝算法進行了大量實證評估，得出了兩個令人驚訝的觀察結果。首先，對於具備預定義目標網絡架構的剪枝算法（圖 2），從隨機初始化直接訓練小目標模型能實現與使用經典三步流程相同的性能。在這種情況下，我們不需要從訓練大規模模型開始，而是可以直接從頭訓練剪枝後模型。其次，對於沒有預定義目標網絡的剪枝算法，從頭開始訓練剪枝後的模型也可以實現與微調相當甚至更好的性能。
    > 
    > 這一觀察結果表明，對於這些剪枝算法而言，重要的是獲得的網絡架構而不是保留的權重，儘管這些目標架構還是需要訓練大模型才能獲得。我們的結果和文獻中結果的矛盾之處在於超參數的選擇、數據增強策略和評估基線模型的計算力限制。
    > 
    > ![](http://i2.bangqu.com/j/news/20181022/9735841540184429872z37AK.png)
    > 
    > *圖 2：預定義和未預定義（自動發現）的目標架構之間的區別。稀疏度 x 是用戶指定的，a、b、c、d 由剪枝算法決定。*
    > 
    > 該論文的結果表明，我們需要重新審視現有的網絡剪枝算法。第一階段的過參數化似乎不像之前想象得那麼有效。此外，繼承大模型的權重不一定是最優選擇，而且可能導致剪枝後的模型陷入局部極小值，即使這些權重按剪枝標準來看是「重要的」。該論文的結果顯示自動剪枝算法的價值可能在於識別高效結構、執行隱性架構搜索，而不是選擇「重要的」權重。
    > 


## Course

### CS231n
- Notes
    - [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)
    - [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/)
- Slides
    - [Syllabus | CS 231N](http://cs231n.stanford.edu/syllabus.html)
- Lecture Video
    - [Lecture Collection | Convolutional Neural Networks for Visual Recognition (Spring 2017) - YouTube - YouTube](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

## Datasets

- [CIFAR-10 and CIFAR-100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html)
- [ImageNet](http://www.image-net.org/)
- [COCO - Common Objects in Context](http://cocodataset.org/#home)
- 

## CNN 

### State of the art

- MNIST, CIFAR-10, CIFAR-100, STL-10, SVHN, ILSVRC2012 task 1 [Classification datasets results](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)

### Papers

- AlexNet [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
- VGGNet [[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
- GoogLeNet [[1409.4842] Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)
- ResNet [[1512.03385] Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

### Articles

- [#Deep Learning回顾#之LeNet、AlexNet、GoogLeNet、VGG、ResNet - 我爱机器学习 - 博客园](https://www.cnblogs.com/52machinelearning/p/5821591.html)
- [深度学习方法（五）：卷积神经网络CNN经典模型整理Lenet，Alexnet，Googlenet，VGG，Deep Residual Learning - CSDN博客](http://blog.csdn.net/xbinworld/article/details/45619685)
- [残差网络ResNet笔记 - 简书](https://www.jianshu.com/p/e58437f39f65)
- [无需数学背景，读懂 ResNet、Inception 和 Xception 三大变革性架构 | 机器之心](https://www.jiqizhixin.com/articles/2017-08-19-4)
- [The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3) – Adit Deshpande – CS Undergrad at UCLA ('19)](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
- [Exploring Deep Learning & CNNs - RSIP Vision](https://www.rsipvision.com/exploring-deep-learning/)
- [A Beginner's Guide To Understanding Convolutional Neural Networks – Adit Deshpande – CS Undergrad at UCLA ('19)](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)

### Topics

#### 破解 capcha

- [使用深度学习来破解 captcha 验证码](https://zhuanlan.zhihu.com/p/26078299)
    - [ypwhs/captcha_break: 验证码识别](https://github.com/ypwhs/captcha_break)
- [rickyhan/SimGAN-Captcha: Solve captcha without manually labeling a training set](https://github.com/rickyhan/SimGAN-Captcha)
- [JasonLiTW/simple-railway-captcha-solver: 實作基於CNN的台鐵訂票驗證碼辨識以及驗證性高的訓練集產生器 (Simple captcha solver based on CNN and a training set generator by imitating the style of captcha)](https://github.com/JasonLiTW/simple-railway-captcha-solver)
- [裤҉裆҉里҉的҉霸҉气҉/verification-decoder - 碼雲 Gitee.com](https://gitee.com/kdldbq/verification-decoder?from=weekmail)


#### Image Caption

- [karpathy/neuraltalk2: Efficient Image Captioning code in Torch, runs on GPU](https://github.com/karpathy/neuraltalk2)

    <iframe src="https://player.vimeo.com/video/146492001" width="640" height="400" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
    <p><a href="https://vimeo.com/146492001">NeuralTalk and Walk</a> from <a href="https://vimeo.com/kylemcdonald">Kyle McDonald</a> on <a href="https://vimeo.com">Vimeo</a>.</p>

- [[1703.09137] Where to put the Image in an Image Caption Generator](https://arxiv.org/abs/1703.09137)


#### Ultrasound image with deep learning

- [[1710.10006] Deep Learning for Accelerated Ultrasound Imaging](https://arxiv.org/abs/1710.10006)
    - [[1710.06304] Towards CT-quality Ultrasound Imaging using Deep Learning](https://arxiv.org/abs/1710.06304)

- [Ultrasound DL](http://deepultrasound.ai/)
- [Automating Breast Cancer Detection with Deep Learning](https://blog.insightdatascience.com/automating-breast-cancer-detection-with-deep-learning-d8b49da17950)

## GAN

### State of the art

- [the-gan-zoo/gans.tsv at master · hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv)


#### 二次元妹子

- [MakeGirlsMoe - Create Anime Characters with A.I.!](https://make.girls.moe/)
- 


### Papers

- [Timnit Gebru 在 Twitter："Does someone have a list like the 10 or even 20 GAN related papers I should read this year or something like this? I can't keep up. @goodfellow_ian ?"](https://twitter.com/timnitGebru/status/968242968007200769)

    > 1\. Progressive GANs: [ https://arxiv.org/abs/1710.10196  ](https://t.co/UEFhewds2M "https://arxiv.org/abs/1710.10196") (probably the highest quality images so far)
    > 
    > 2\. Spectral normalization: [ https://openreview.net/forum?id=B1QRgziT-&noteId=BkxnM1TrM …](https://t.co/tt2os9H1Py "https://openreview.net/forum?id=B1QRgziT-&noteId=BkxnM1TrM") (got GANs working on lots of classes, which has been hard)
    > 
    > 3\. Projection discriminator: [ https://openreview.net/forum?id=ByS1VpgRZ …](https://t.co/qG1xwu1PuX "https://openreview.net/forum?id=ByS1VpgRZ") (from the same lab as #2, both techniques work well together, overall give very good results with 1000 classes) Here's the video of putting the two methods together: [https://www.youtube.com/watch?time_continue=3&v=r6zZPn-6dPY](https://www.youtube.com/watch?time_continue=3&v=r6zZPn-6dPY)
    > 
    > 4\. pix2pixHD (GANs for 2-megapixel video) [https://arxiv.org/abs/1711.11585](https://arxiv.org/abs/1711.11585)
    > [https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be](https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be) 
    > 
    > 
    > 5\. Are GANs created equal? [ https://arxiv.org/abs/1711.10337  ](https://t.co/4dIIOjPBC3 "https://arxiv.org/abs/1711.10337") A big empirical study showing the importance of good rigorous empirical work and how a lot of the GAN variants don't seem to actually offer improvements in practice 
    > 
    > 
    > 6\. WGAN-GP [ https://arxiv.org/abs/1704.00028  ](https://t.co/zJ6ZDSdz7w "https://arxiv.org/abs/1704.00028") : probably the most popular GAN variant today and seems to be pretty good in my opinion. Caveat: the baseline GAN variants should not perform nearly as badly as this paper claims, especially the text one
    > 
    > 7\. StackGAN++: [ https://arxiv.org/abs/1710.10916  ](https://t.co/ccOlTNW43F "https://arxiv.org/abs/1710.10916") High quality text-to-image synthesis with GANs
    > 
    > 8\. Making all ML algorithms differentially private by training them on fake private data generated by GANs [https://www.biorxiv.org/content/early/2017/07/05/159756](https://www.biorxiv.org/content/early/2017/07/05/159756)
    > 
    > 9\. You should be a little bit aware of the "GANs with encoders" space, one of my favorites is [ https://arxiv.org/abs/1701.04722](https://t.co/2uWTwu6kes "https://arxiv.org/abs/1701.04722")
    > 
    > 10\. You should be a little bit aware of the "theory of GAN convergence" space, one of my favorites is [ https://arxiv.org/abs/1706.04156](https://t.co/JpCKaHy9im "https://arxiv.org/abs/1706.04156")
    > 
    > [name=Ian Goodfellow]


#### 對抗樣本

- [[1412.6572] Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)

- [[1801.02610] Generating Adversarial Examples with Adversarial Networks](https://arxiv.org/abs/1801.02610)

#### 防禦對抗樣本

- [Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models | OpenReview](https://openreview.net/forum?id=BkJ3ibb0-&noteId=SJwPXJaHG)

#### Convergence and Stability

- [[1705.07215] On Convergence and Stability of GANs](https://arxiv.org/abs/1705.07215)

- [[1703.10717] BEGAN: Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/abs/1703.10717)

#### GAN + RL



- [Learning to write programs that generate images | DeepMind](https://deepmind.com/blog/learning-to-generate-images/)

    - [DeepMind提出SPIRAL：使用強化對抗學習，實現會用畫筆的智能體 - 幫趣](http://bangqu.com/Sr63zI.html)




### Articles

- [Gan的数学推导 | Sherlock Blog](https://sherlockliao.github.io/2017/06/20/gan_math/)

- [amazing_gans_pycon2017 - Google 簡報](https://docs.google.com/presentation/d/14jJL6MR2uf4CD7PmsgMtivX8WfK-QMdKdwrLEYH9qDA/edit#slide=id.g22c42edce6_0_644)

- [[1701.04862] Towards Principled Methods for Training Generative Adversarial Networks](https://arxiv.org/abs/1701.04862)



#### Training giude

- [soumith/ganhacks: starter from "How to Train a GAN?" at NIPS2016](https://github.com/soumith/ganhacks)

- [[1701.00160] NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)
- 

#### Mode colapse

<iframe width="560" height="315" src="https://www.youtube.com/embed/ktxhiKhWoEE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

- [Mode collapse in GANs - Aiden Nibali](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/)

    > Addressing mode collapse
    > ------------------------
    > 
    > Mode collapse is a well-recognised problem, and researchers have made a few attempts at addressing it. I have identified 4 broad approaches to tackling mode collapse, which are described below.
    > 
    > ### Directly encourage diversity
    > 
    > It is impossible to determine output diversity by considering individual samples in isolation. This leads to a very logical next step of using batches of samples to directly assess diversity. Minibatch discrimination and feature mapping [1](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/#fn:imprgan) are two techniques which fall into this category.
    > 
    > Minibatch discrimination gives the discriminator the power of comparing samples across a batch to help determine whether the batch is real or fake.
    > 
    > Feature matching modifies the generator cost function to factor in the diversity of generated batches. It does this by matching statistics of discriminator features for fake batches to those of real batches. I had some success combining feature matching with the traditional GAN generator loss function to form a hybrid objective.
    > 
    > ### Anticipate counterplay
    > 
    > One way to prevent the cat-and-mouse game of hopping between modes is to peek into the future and anticipate counterplay when updating parameters. This approach should be familiar to those who know a bit about game theory (eg [minimax](https://en.wikipedia.org/wiki/Minimax)). Intuitively, this prevents players of the GAN game from making moves which are easily countered.
    > 
    > Unrolled GANs [2](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/#fn:unrolled) take this kind of approach by allowing the generator to “unroll” updates of the discriminator in a fully differentiable way. Now instead of the generator learning to fool the current discriminator, it learns to maximally fool the discriminator _after it has a chance to respond_, thus taking counterplay into account. Downsides of this approach are increased training time (each generator update has to simulate multiple discriminator updates) and a more complicated gradient calculation (backprop through an optimiser update step can be difficult).
    > 
    > ### Use experience replay
    > 
    > Hopping back and forth between modes can be minimised by showing old fake samples to the discriminator every so often. This prevents the discriminator from becoming too exploitable, but only for modes that have already been explored by the generator in the past.
    > 
    > A similar kind of effect can be achieved by occasionally substituting in an old discriminator/generator for a few iterations.
    > 
    > ### Use multiple GANs
    > 
    > Rather than fight mode collapse we could simply accept that the GAN will cover only a subset of the modes in the dataset, and train multiple GANs for different modes. When combined, these GANs cover all of the modes. AdaGAN [3](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/#fn:adagan) takes this approach. The major downside here is that training multiple GANs takes a lot of time. Furthermore, using a combination of GANs is generally more unwieldy than working with just one.




### Topics

#### Variational Autoencoder

- [[1606.05908] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)


#### WGAN

- [[1701.07875] Wasserstein GAN](https://arxiv.org/abs/1701.07875)
- [Wasserstein GAN and the Kantorovich-Rubinstein Duality - Vincent Herrmann](https://vincentherrmann.github.io/blog/wasserstein/)
- [A Hitchhikers guide to Wasserstein.pdf](http://n.ethz.ch/~gbasso/download/A%20Hitchhikers%20guide%20to%20Wasserstein/A%20Hitchhikers%20guide%20to%20Wasserstein.pdf)
- [f-divergence - Wikiwand](https://www.wikiwand.com/en/F-divergence)

- [ComputationalOT.pdf](https://optimaltransport.github.io/pdf/ComputationalOT.pdf)

- [Duality (optimization) - Wikiwand](https://www.wikiwand.com/en/Duality_(optimization))

- [[1704.00028] Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)

#### LSGAN

- [[1611.04076] Least Squares Generative Adversarial Networks](https://arxiv.org/abs/1611.04076)




#### Nash equilibrium

- [[1706.08500] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500)

- [[1705.02894] Geometric GAN](https://arxiv.org/abs/1705.02894)

- [[1708.08819] Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields](https://arxiv.org/abs/1708.08819)

#### image style transfer

- [NVIDIA/FastPhotoStyle: Style transfer, deep learning, feature transform](https://github.com/NVIDIA/FastPhotoStyle)

#### 2D to 3D

- [Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression](http://aaronsplace.co.uk/papers/jackson2017recon/)
    - [AaronJackson/vrn: Code for "Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression"](https://github.com/AaronJackson/vrn)


#### sample code

- [tjwei/DIY_AI](https://github.com/tjwei/DIY_AI)

- [tjwei/GANotebooks: wgan, wgan2(improved, gp), infogan, and dcgan implementation in lasagne, keras, pytorch](https://github.com/tjwei/GANotebooks)


## RNN

### State of the art

### Papers

### Articles

- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    - [The unreasonable effectiveness of Character-level Language Models (and why RNNs are still cool) - Jupyter Notebook Viewer](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)
    - [The Unreasonable Effectiveness of Recurrent Neural Networks：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/36s673/the_unreasonable_effectiveness_of_recurrent/)

        > > In particular, setting temperature very near zero will give the most likely thing that Paul Graham might say:
        > > 
        > > "is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same"
        > > 
        > > looks like we've reached an infinite loop about startups.
        > 
        > Great article, but this part is fundamentally incorrect, and probably the reason the sample is so loopy.
        > 
        > It may be counter-intuitive, but if you pick the most likely next character at every step, you will not necessarily end up with the most likely sequence. In other words, the greedy solution is not necessarily optimal.
        > 
        > Consider:
        > 
        > ```
        > P(00) = 0.4
        > P(01) = 0.0
        > P(10) = 0.3
        > P(11) = 0.3
        > 
        > ```
        > 
        > `1` is the most likely first character, but `00` is the most likely sequence.
        > 
        > Back in college, my differential equations professor had this to say: If you eat as much as you can every single day, you probably won't maximize your total food consumption.


- [Understanding LSTM Networks -- colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

- [LSTM RNN 循环神经网络 (LSTM) - 有趣的机器学习 | 莫烦Python](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-4-LSTM/)

- [(33 条消息)CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？ - 知乎](https://www.zhihu.com/question/34681168)

- [遞歸神經網路和長短期記憶模型 RNN & LSTM · 資料科學・機器・人](https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_rnns_lstm_work.html)

- [A Beginner's Guide to Recurrent Networks and LSTMs - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM](https://deeplearning4j.org/lstm.html)

#### Recursive NN

- [用 Recursive Neural Networks 得到分析树 - 简书](https://www.jianshu.com/p/403665b55cd4)

- [Recursive (not Recurrent!) Neural Networks in TensorFlow](https://www.kdnuggets.com/2016/06/recursive-neural-networks-tensorflow.html)

    > In RNNs, at each time step the network takes as input its previous state s(t-1) and its current input x(t) and produces an output y(t) and a new hidden state s(t). TreeNets, on the other hand, don’t have a simple linear structure like that. With RNNs, you can ‘unroll’ the net and think of it as a large feedforward net with inputs x(0), x(1), …, x(T), initial state s(0), and outputs y(0),y(1),…,y(T), with T varying depending on the input data stream, and the weights in each of the cells tied with each other. You can also think of TreeNets by unrolling them – the weights in each branch node are tied with each other, and the weights in each leaf node are tied with each other. The TreeNet illustrated above has different numbers of inputs in the branch nodes. Usually, we just restrict the TreeNet to be a binary tree – each node either has one or two input nodes. There may be different _types_ of branch nodes, but branch nodes of the same type have tied weights.
    > 
    > The advantage of TreeNets is that they can be very powerful in learning hierarchical, tree-like structure.
    > 
    > The disadvantages are, firstly, that the tree structure of every input sample must be known at training time. We will represent the tree structure like this (lisp-like notation):
    >
    > (S (NP that movie) (VP was) (ADJP cool))
    >
    > In each sub-expression, the _type_ of the sub-expression must be given – in this case, we are parsing a sentence, and the type of the sub-expression is simply the part-of-speech (POS) tag.
    >
    > The second disadvantage of TreeNets is that training is hard because the tree structure changes for each training sample and it’s not easy to map training to mini-batches and so on.
    > 
    - [Implementation in TensorFlow - subexpr.py](https://gist.github.com/anj1/504768e05fda49a6e3338e798ae1cddd)


- [machine learning - Recurrent vs Recursive Neural Networks: Which is better for NLP? - Cross Validated](https://stats.stackexchange.com/questions/153599/recurrent-vs-recursive-neural-networks-which-is-better-for-nlp)




### Topics

#### NLP

- [下一步研究目標：盤點NLP領域最具潛力的六大方向 - 幫趣](http://bangqu.com/TTN8wD.html)

- [从文本生成看Seq2Seq模型](https://zhuanlan.zhihu.com/p/29967933)

- [中美两位 AI 大师的“巅峰对话”：为何 NLP 领域难以出现“独角兽”？ | 独家](https://zhuanlan.zhihu.com/p/33970936)

- [deep-learning-with-keras-notebooks/8.0-using-word-embeddings.ipynb at master · erhwenkuo/deep-learning-with-keras-notebooks](https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/8.0-using-word-embeddings.ipynb)

- [Understanding Convolutional Neural Networks for NLP – WildML](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)

- [從零開始的 Sequence to Sequence | 雷德麥的藏書閣](https://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/)

#### Adversiral lstm

- [Recurrent Generative Adversarial Networks?：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/2ttq4g/recurrent_generative_adversarial_networks/)

- [對抗式神經翻譯機 Adversarial Neural Machine Translation | Learning by Hacking](https://data-sci.info/2017/04/30/adversarial-neural-machine-translation/)

#### 英文 word stem 

- [Word stem - Wikiwand](https://www.wikiwand.com/en/Word_stem)
    The stem of the [verb](https://www.wikiwand.com/en/Verb) **wait** is **wait**: it is the part that is common to all its inflected variants.

#### 有毒留言分析

- [rnn_tutorial/toxic_tutorial.ipynb at master · IKMLab/rnn_tutorial](https://github.com/IKMLab/rnn_tutorial/blob/master/toxic_tutorial.ipynb)

#### Word2Vec

- [理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)

- Skip-gram: 給定中間的字來預測上下文
    - [skipgram/Skip-Gram Practice.ipynb at master · IKMLab/skipgram](https://github.com/IKMLab/skipgram/blob/master/Skip-Gram%20Practice.ipynb)

- Con.nuous Bag of Words (CBOW): 給定前後文來預測中間的字


#### 中文斷詞

[中研院 - 中文斷詞系統](http://ckipsvr.iis.sinica.edu.tw/)

#### 情意分析

- [Deep-Learning-MOOC/04-1. 用RNN做情意分析.ipynb at master · yenlung/Deep-Learning-MOOC](https://github.com/yenlung/Deep-Learning-MOOC/blob/master/04-1.%20%E7%94%A8RNN%E5%81%9A%E6%83%85%E6%84%8F%E5%88%86%E6%9E%90.ipynb)

#### 自然語言推論

- [IKMLab/arct: Code for NLITrans at SemEval18 Task12](https://github.com/IKMLab/arct)


#### 無須平行文本之無監督翻譯

- [Notes on Unsupervised Neural Machine Translation](https://zhuanlan.zhihu.com/p/30649985)
    - [《UNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLY》阅读笔记](https://zhuanlan.zhihu.com/p/32375955)
    - [Machine Translation Without the Data – buZZrobot](https://buzzrobot.com/machine-translation-without-the-data-21846fecc4c0)
    - [【Science】無監督式機器翻譯，不需要人類干預和平行文本 - 壹讀](https://read01.com/Rnzx84N.html)
    - [[1711.00043] Unsupervised Machine Translation Using Monolingual Corpora Only](https://arxiv.org/abs/1711.00043)
    - [[1710.11041] Unsupervised Neural Machine Translation](https://arxiv.org/abs/1710.11041)


- [无平行文本照样破解密码，CipherGAN有望提升机器翻译水平](https://zhuanlan.zhihu.com/p/33672256)
    - [[1801.04883] Unsupervised Cipher Cracking Using Discrete GANs](https://arxiv.org/abs/1801.04883)
- [密码学家百年来无法辨认，500年前古怪手稿的加密希伯来语被AI算法破译](https://zhuanlan.zhihu.com/p/34063499)
    - [Decoding Anagrammed Texts Written in an Unknown Language and Script | Hauer | Transactions of the Association for Computational Linguistics](https://transacl.org/ojs/index.php/tacl/article/view/821)

#### 語音合成

##### WaveNet

- [[1712.05884] Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)
    - [谷歌新一代WaveNet ：深度學習怎麼生成語音？ | 2分鐘論文 - 幫趣](http://bangqu.com/3C6864.html)
        在原先Google的WaveNet論文中，我們爲了解決語音合成難題，創造了擴張卷積，這個網絡結構跳躍性地輸入數據，由此使我們我們有了更好的全局視野。這有點像增加我們眼睛的感受野，讓我們能夠感受整個景觀，而不是照片中只有樹的狹窄的視角。
    
        新框架利用梅爾聲譜作爲WaveNet的輸入，這種聲譜是一種基於人類感知的中間媒介，它不僅記錄了不同的單詞如何發音，而且還記錄了預期的音量和語調。

    - [r9y9/wavenet_vocoder: WaveNet vocoder](https://github.com/r9y9/wavenet_vocoder)

- [[1609.03499] WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)
    - [(1) WaveNet by Google DeepMind | Two Minute Papers #93 - YouTube](https://www.youtube.com/watch?v=CqFIVCD1WWo)
    - [WaveNet: A Generative Model for Raw Audio | DeepMind](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
    - [WaveNet launches in the Google Assistant | DeepMind](https://deepmind.com/blog/wavenet-launches-google-assistant/)
    - [DeepMind: WaveNet - A Generative Model for Raw Audio：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/51sr9t/deepmind_wavenet_a_generative_model_for_raw_audio/)
    - [Deepmind 打造語音生成模型 WaveNet，比傳統音檔生成速度快 1000 倍，聲音更擬真？ | TechOrange](https://buzzorange.com/techorange/2017/10/11/deepmind-wavenet-1000-times-faster/)

- [ibab/tensorflow-wavenet: A TensorFlow implementation of DeepMind's WaveNet paper](https://github.com/ibab/tensorflow-wavenet)
- [tomlepaine/fast-wavenet: Speedy Wavenet generation using dynamic programming](https://github.com/tomlepaine/fast-wavenet)
- [buriburisuri/speech-to-text-wavenet: Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition based on DeepMind's WaveNet and tensorflow](https://github.com/buriburisuri/speech-to-text-wavenet)

#### 音樂生成

- [821760408-sp/the-wavenet-pianist: A Wavenet generative model in TensorFlow, trained with Western Classical solo piano canon with global and local conditioning](https://github.com/821760408-sp/the-wavenet-pianist)

- [Can music be generated using generative adversarial networks? - Quora](https://www.quora.com/Can-music-be-generated-using-generative-adversarial-networks)
    > Yes. Just this AAAI, [Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473) combines GANs with Policy gradient (reinforcement learning) to generate music.
    > 
    > The remaining answer might be useful/useless for different audience:
    > 
    > However, unlike images where GANs clearly beat RNNs in generation quality IMO, [\[1612.04357\] Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.04357) vs [openai/pixel-cnn](https://github.com/openai/pixel-cnn) , it appears the other way round in music.
    > 
    > Check out these RNN modifications:
    > 
    > [An Unconditional End-to-End Neural Audio Generation Model](https://arxiv.org/abs/1612.07837)
    > 
    > [soroushmehr/sampleRNN_ICLR2017](https://github.com/soroushmehr/sampleRNN_ICLR2017)
    > 
    > [SampleRNN](https://soundcloud.com/samplernn)
    > 
    > and
    > 
    > [WaveNet: A Generative Model for Raw Audio | DeepMind](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)


- [CodePen - Neural Drum Machine](https://codepen.io/allenyllee/full/ZxGBXP/)
- [CodePen - Deep Roll](https://codepen.io/allenyllee/full/zWGRRg/)

- [Learning to generate lyrics and music with Recurrent Neural Networks](https://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/)

- [`pretty_midi` tutorial](https://nbviewer.jupyter.org/github/craffel/pretty-midi/blob/master/Tutorial.ipynb)

- [MusicVAE: Creating a palette for musical scores with machine learning.](https://magenta.tensorflow.org/music-vae)



#### 股價預測

- [lucko515/tesla-stocks-prediction: The implementation of LSTM in TensorFlow used for the stock prediction.](https://github.com/lucko515/tesla-stocks-prediction)


## Bayesian Neural Networks(BNN)

- [Engineering Uncertainty Estimation in Neural Networks](https://eng.uber.com/neural-networks-uncertainty-estimation/)

- [[1801.07710] Bayesian Neural Networks](https://arxiv.org/abs/1801.07710)

- [What is a Bayesian Neural Network? - Quora](https://www.quora.com/What-is-a-Bayesian-Neural-Network)

    > A Bayesian neural network (BNN) refers to extending standard networks with posterior inference. Standard NN training via optimization is (from a probabilistic perspective) equivalent to maximum likelihood estimation (MLE) for the weights.
    > 
    > For many reasons this is unsatisfactory. One reason is that it lacks proper theoretical justification from a probabilistic perspective: why maximum likelihood? Why just point estimates? Using MLE ignores any uncertainty that we may have in the proper weight values. From a practical standpoint, this type of training is often susceptible to overfitting, as NNs often do.
    > 
    > One partial fix for this is to introduce regularization. From a Bayesian perspective, this is equivalent to inducing priors on the weights (say Gaussian distributions if we are using L2 regularization). Optimization in this case is akin to searching for MAP estimators rather than MLE. Again from a probabilistic perspective, this is not the *right* thing to do, though it certainly works well in practice.
    > 
    > The correct (i.e., theoretically justifiable) thing to do is posterior inference, though this is very challenging both from a modelling and computational point of view. BNNs are neural networks that take this approach. In the past this was all but impossible, and we had to resort to poor approximations such as Laplace’s method (low complexity) or MCMC (long convergence, difficult to diagnose). However, lately there have been some super-interesting results on using variational inference to do this \[1\], and this has sparked a great deal of interest in the area.
    > 
    > BNNs are important in specific settings, especially when we care about uncertainty very much. Some examples of these cases are decision making systems, (relatively) smaller data settings, Bayesian Optimization, model-based reinforcement learning and others.
    > 
    > \[1\] -\[[1505.05424\] Weight Uncertainty in Neural Networks](https://arxiv.org/abs/1505.05424)
    > 
    > 


- [What are the advantages of using a Bayesian neural network - Cross Validated](https://stats.stackexchange.com/questions/141879/what-are-the-advantages-of-using-a-bayesian-neural-network)

    > Bayesian neural nets are useful for solving problems in domains where data is scarce, as a way to prevent overfitting.
    > 
    > They often beat all other methods in such situations. Example applications are molecular biology ([for example this paper](http://bioinformatics.oxfordjournals.org/content/early/2011/07/29/bioinformatics.btr444.full.pdf+html)) and medical diagnosis (areas where data often come from costly and difficult expiremental work).
    > 
    > ---
    > 
    > Let's define our BNN prediction as $\bar{f}(x′|x,t)=∫f(x′,ω)p(ω|x,t)dω$ , where $f$ is the NN function, $x′$ are your inputs, $ω$ are the NN parameters, and $x,t$ are the training inputs and targets. This should be compatible with the syntax used by Neal in the links provided by @forecaster. Then we can calculate a standard deviation of the posterior predictive distribution, which I would naively use as an accuracy on the prediction : 
    > $$
    > \sigma(x′)=\sqrt{∫[f(x′,ω)−\bar{f}(x′|x,t)]^2p(ω|x,t)dω}
    > $$
    > 
    > 


## Reinforcement Learning

### State of the art

### Papers

- [Actor-Critic Algorithms](https://papers.nips.cc/paper/1786-actor-critic-algorithms)
- 

### Articles

- [RL Course by David Silver - Lecture 2: Markov Decision Process - YouTube](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
- [强化学习 Reinforcement Learning 教程系列 | 莫烦Python](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/)
- [106 Fall - ADL x MLDS, NTU](https://www.csie.ntu.edu.tw/~yvchen/f106-adl/syllabus.html)

#### DQN

- [DQN从入门到放弃6 DQN的各种改进 - 知乎](https://zhuanlan.zhihu.com/p/21547911)
- 

### Topics

###### tags: `execrise`
- [Deep RL Bootcamp - Labs](https://sites.google.com/view/deep-rl-bootcamp/labs)

#### 下棋
- [Zeta36/chess-alpha-zero: Chess reinforcement learning by AlphaGo Zero methods.](https://github.com/Zeta36/chess-alpha-zero)

#### System ML

- [SysML Conference](http://www.sysml.cc/)



## Computer Vision

### State of the art

### Papers

#### Action and gesture recognition
- [Learning Adaptive Hidden Layers for Mobile Gesture Recognition
](http://cvlab.citi.sinica.edu.tw/ProjectWeb/AHL/#top)

### Articles

#### CNN 架構比較
- [LeNet、AlexNet、GoogLeNet、VGG、ResNetInception-ResNet-v2、FractalNet、DenseNet - 程序园](http://www.voidcn.com/article/p-rewgmeze-bcq.html)

- [VGGNet | 简说](http://simtalk.cn/2016/09/25/VGGNet/)

- [比ResNet更加有效的網路架構 DenseNet: Densely Connected Convolutional Networks | Learning by Hacking](https://data-sci.info/2017/07/26/%E6%AF%94resnet%E6%9B%B4%E5%8A%A0%E6%9C%89%E6%95%88%E7%9A%84%E7%B6%B2%E8%B7%AF%E6%9E%B6%E6%A7%8B-densenet-densely-connected-convolutional-networks/)

- [adl_1103 - 161103_ConvolutionalNN.pdf](https://www.csie.ntu.edu.tw/~yvchen/f105-adl/doc/161103_ConvolutionalNN.pdf)

### Topics

#### Image matching and alignment

###### tags `SIFT`
- [SIFT特征提取分析 - CSDN博客](http://blog.csdn.net/abcjennifer/article/details/7639681)

###### tags `complementary descriptor`

- [shamangary/DeepCD: [ICCV17] DeepCD: Learning Deep Complementary Descriptors for Patch Representations](https://github.com/shamangary/DeepCD)



#### Image Inpainting

- [基于CNN的图像修复（CNN-based Image Inpainting） - CSDN博客](http://blog.csdn.net/YhL_Leo/article/details/56674833)

#### Image style transfer

###### tags: `Gram matrices`

- [[1701.01036] Demystifying Neural Style Transfer](https://arxiv.org/abs/1701.01036)
    the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel


- [Neural Artistic Style Transfer: A Comprehensive Look](https://medium.com/artists-and-machine-intelligence/neural-artistic-style-transfer-a-comprehensive-look-f54d8649c199)


#### Object Detection

- [COCO - Common Objects in Context](http://cocodataset.org/#detections-leaderboard)

- [[1711.07240] MegDet: A Large Mini-Batch Object Detector](https://arxiv.org/abs/1711.07240)

- [政府在看你，2000萬個攝影機時刻監視，中國最大AI獨角獸商湯：這樣更安全｜產業｜科技｜2018-01-17｜即時｜天下雜誌](https://www.cw.com.tw/article/article.action?id=5087696)

###### tags: `RCNN`
- [Selective Search for Object Recognition](https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&bib=all.bib)
- [【目标检测】RCNN算法详解 - CSDN博客](http://blog.csdn.net/shenxiaolu1984/article/details/51066975)
- [RCNN- 将CNN引入目标检测的开山之作 - 知乎](https://zhuanlan.zhihu.com/p/23006190)
- [CS231n Lecture 8 - Localization and Detection - YouTube](https://www.youtube.com/watch?v=_GfPYLNQank&t=574s)

###### tags: `Fast RCNN`
- [【目标检测】Fast RCNN算法详解 - CSDN博客](http://blog.csdn.net/shenxiaolu1984/article/details/51036677)
- [fast-rcnn - fast-rcnn-slides.pdf](http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf)

###### tags: `Faster RCNN`
- [Tutorial: Deep Learning for Objects and Scenes - Part 1 - YouTube](https://www.youtube.com/watch?v=jHv37mKAhV4)
- [目标检测之RCNN，SPP-NET，Fast-RCNN，Faster-RCNN | 冰蓝记录思考的地方](http://lanbing510.info/2017/08/24/RCNN-FastRCNN-FasterRCNN.html)
- [keras版faster-rcnn算法详解（1.RPN计算） - 知乎](https://zhuanlan.zhihu.com/p/28585873)
- [How does the region proposal network (RPN) in Faster R-CNN work? - Quora](https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work)


###### tags: `YOLO`

- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/)

- [[1506.02640] You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
- [[1612.08242] YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242)

- [YOLO 升級到 v3 版，速度相比 RetinaNet 快 3.8 倍 - 幫趣](http://bangqu.com/zLN4cl.html)


#### Image segmentation

###### tags: `U-Net`

- [[1505.04597] U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
- [[1605.06211] Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)


## 新聞

- [深度學習專家很缺嗎？ - EE Times Taiwan 電子工程專輯網](https://www.eettaiwan.com/news/article/20180410NT01-Are-We-Short-of-Deep-Learning-Experts?utm_source=EETT%20Article%20Alert&utm_medium=Email&utm_campaign=2018-04-12)

    > 深度學習「可說是一門跨學科的領域，你不僅要有數學和演算法方面的專家，也需要知道如何在平台上建置軟體的電腦系統專家。然後，我們也會需要熟悉資料管線的專家來調整和管理資料組合。」
    > 





$$E(t,o)=-\sum_j t_j \log o_j$$
