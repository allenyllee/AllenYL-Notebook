# Docker__å¿«é€Ÿä¸Šæ‰‹

[toc]
<!-- toc --> 

## What is docker

-   Docker æ˜¯ä¸€ç¨®è¼•é‡ç´šçš„ä½œæ¥­ç³»çµ±è™›æ“¬åŒ–è§£æ±ºæ–¹æ¡ˆï¼Œå…¶æ‰€å‰µé€ å‡ºçš„è™›æ“¬ç’°å¢ƒåˆç¨±ç‚ºå®¹å™¨(container)ã€‚
    
-   å‚³çµ±çš„è™›æ“¬åŒ–æŠ€è¡“ï¼Œå¦‚VirtureBoxç­‰ï¼Œä¸»è¦æ˜¯ç”¨è»Ÿé«”è™›æ“¬å‡ºä¸€æ•´å¥—ç¡¬é«”ï¼Œç„¶å¾Œåœ¨å…¶ä¸Šé‹è¡Œä¸€å¥—å®Œæ•´çš„OSã€‚å¯æƒ³è€ŒçŸ¥ï¼Œé‹è¡Œæ•ˆçŽ‡ç›¸ç•¶ä½Žè½ã€‚
    
-   Dockerçš„å‰µæ–°åœ¨æ–¼æƒ³åˆ°: æ—¢ç„¶åŽŸæœ¬å°±æœ‰OS äº†ï¼Œç‚ºä½•ä¸å¥½å¥½åˆ©ç”¨å‘¢ï¼Ÿç”±æ–¼Linux OS ä¸åŒç‰ˆæœ¬é–“çš„ kernel æ˜¯å¯ä»¥å…±ç”¨çš„ï¼Œåªè¦åœ¨ä¸Šé¢å¯¦ä½œä¸€å±¤è™›æ“¬ APIï¼Œæ‡‰ç”¨ç¨‹å¼çœ‹èµ·ä¾†å°±å¥½åƒè·‘åœ¨ä¸åŒæ©Ÿå™¨ä¸Šäº†å‘¢ï¼

![](https://screenshotscdn.firefoxusercontent.com/images/d6bb2271-9180-449a-960f-c14bd3c0adcc.png)


### docker command

- [ä»€éº¼æ˜¯ Docker Â· ã€ŠDocker â€”â€” å¾žå…¥é–€åˆ°å¯¦è¸ã€‹æ­£é«”ä¸­æ–‡ç‰ˆ](https://philipzheng.gitbooks.io/docker_practice/content/introduction/what.html)

- [ä»€ä¹ˆæ˜¯ Docker Â· Docker â€”â€” ä»Žå…¥é—¨åˆ°å®žè·µ](https://yeasy.gitbooks.io/docker_practice/content/introduction/what.html)

#### æ“ä½œå®¹å™¨
    
-   docker run ubuntu:14.04 /bin/echo 'Hello world'
    
-   docker run -t -i ubuntu:14.04 /bin/bash #äº¤äº’æ¨¡å¼
    
-   docker run ubuntu:17.10 /bin/sh -c "while true; do echo hello world; sleep 1; done" #å¾Œå°æ¨¡å¼
    
-   docker container ls #åˆ—å‡ºå®¹å™¨
    
-   docker exec -i \[container_id\] bash #é€²å…¥å®¹å™¨
    
-   docker container stop \[container_id\] #çµ‚æ­¢å®¹å™¨
    
-   docker container rm \[container_name\] #åˆªé™¤å®¹å™¨
    

#### æ“ä½œé¡åƒ
    
-   åœ¨ Dockerfile æ–‡ä»¶æ‰€åœ¨ç›®å½•æ‰§è¡Œï¼šdocker build -t \[image_name\]:\[tag\] . #è£½ä½œé¡åƒ
    
-   docker pull \[image\]:\[tag\] #å¾žDockerhubæ‹‰å–é¡åƒ


### Container Architecture

- [Architecting Containers Part 1: Why Understanding User Space vs. Kernel Space Matters â€“ Red Hat Enterprise Linux Blog](https://rhelblog.redhat.com/2015/07/29/architecting-containers-part-1-user-space-vs-kernel-space/)

    > before diving head-first into a discussion about the architecture and deployment of containers in a production environment, there are three important things that developers, architects, and systems administrators, need to know :
    > 
    > 1.  All applications, inclusive of containerized applications, rely on the underlying kernel
    > 2.  The kernel provides an API to these applications via system calls
    > 3.  Versioning of this API matters as itâ€™s the â€œglueâ€ that ensures deterministic communication between the user space and kernel space
    > 
    > While containers are sometimes treated like virtual machines, it is important to note, unlike virtual machines, the kernel is the only layer of abstraction between programs and the resources they need access to. Letâ€™s see why.
    > 
    > All processes make system calls:
    > 
    > [![User Space vs. Kernel Space - Simple User Space](https://rhelblog.files.wordpress.com/2015/07/user-space-vs-kernel-space-simple-user-space.png?w=640&h=253)](https://rhelblog.files.wordpress.com/2015/07/user-space-vs-kernel-space-simple-user-space.png) As containers are processes, they also make system calls:
    > 
    > [![User Space vs. Kernel Space - Simple Container](https://rhelblog.files.wordpress.com/2015/07/user-space-vs-kernel-space-simple-container.png?w=640&h=253)](https://rhelblog.files.wordpress.com/2015/07/user-space-vs-kernel-space-simple-container.png)
    > 
    > OK, so you understand what a process is, and that containers are processes, but what about the files and programs that live inside a container image? These files and programs make up what is known as [user space](http://www.linfo.org/user_space.html). When a container is started, a program is loaded into memory from the container image. Once the program in the container is running, it still needs to make system calls into [kernel space](http://www.linfo.org/kernel_space.html). The ability for the user space and kernel space to communicate in a deterministic fashion is critical.

### Container Perfomance

- [What is the runtime performance cost of a Docker container - Stack Overflow](https://stackoverflow.com/questions/21889053/what-is-the-runtime-performance-cost-of-a-docker-container)

    > [Here](http://domino.research.ibm.com/library/cyberdig.nsf/papers/0929052195DD819C85257D2300681E7B/$File/rc25482.pdf) is an excellent 2014 IBM research paper titled "An Updated Performance Comparison of Virtual Machines and Linux Containers" by Felter et al. that provides a comparison between bare metal, KVM, and Docker containers. **The general result is that Docker is nearly identical to Native performance and faster than KVM in every category.**
    > 
    > The exception to this is Docker's NAT - if you use port mapping (e.g. `docker run -p 8080:8080`) then you can expect a minor hit in latency, as shown below. However, you can now use the host network stack (e.g. `docker run --net=host`) when launching a Docker container, which will perform identically to the `Native` column (as shown in the Redis latency results lower down).
    > 
    > ![Docker NAT overhead](https://i.stack.imgur.com/4yRh1m.png)
    > 
    > 
    > 
    > Taking a look at Disk IO:
    > 
    > ![IO docker vs kvm vs native](https://i.stack.imgur.com/2Ftytm.png)
    > 
    > Now looking at CPU overhead:
    > 
    > ![docker cpu overhead](https://i.stack.imgur.com/wZZH6m.png)
    > 
    > Now some examples of memory (read the paper for details, memory can be extra tricky)
    > 
    > ![docker memory comparison](https://i.stack.imgur.com/aHPVkm.png)
    > 


## install docker


### install docker in bash on windows

- [Running Docker containers on Bash on Windows - Jayway](https://blog.jayway.com/2017/04/19/running-docker-on-bash-on-windows/)

- [Installing the Docker client on Windows Subsystem for Linux (Ubuntu)](https://medium.com/@sebagomez/installing-the-docker-client-on-ubuntus-windows-subsystem-for-linux-612b392a44c4)

- [Can you run Docker natively on the new Windows 10 (Ubuntu) bash userspace? - Server Fault](https://serverfault.com/questions/767994/can-you-run-docker-natively-on-the-new-windows-10-ubuntu-bash-userspace#_=_)


## Docker swarm vs. Kubernetes vs. Mesos vs. Amazon ECS


- [Kubernetes Vs Docker Swarm - A Comparison of Containerization Platform](https://vexxhost.com/blog/kubernetes-vs-docker-swarm/)

    > **Benefits & drawbacks of Kubernetes**
    > 
    > **_Benefits of Kubernetes:_**
    > 
    > -   Kubernetes is backed by the Cloud Native Computing Foundation (CNCF).
    > -   Kubernetes have an impressively huge community among container orchestration tools. Over 50,000 commits and 1200 contributors.
    > -   Kubernetes is an open source and modular tool that works with any OS.
    > -   Kubernetes provides easy service organization with pods
    > 
    > **_Drawbacks of Kubernetes_**
    > 
    > -   When doing it yourself, Kubernetes installation can be quite complex with steep learning curve. An option to solve this issue is to opt for a managed [Kubernetes-as-a-service](https://vexxhost.com/public-cloud/container-services/kubernetes/) such as ours.
    > -   In Kubernetes, it is required to have a separate set of tools for management, including kubectl CLI.
    > -   It is Incompatible with existing Docker CLI and Compose tools
    > 
    > **Benefits & drawbacks of Docker Swarm**
    > 
    > **_Benefits of Docker Swarm_**
    > 
    > -   Docker Swarm is easy to install with a fast setup
    > -   Docker Swarm is a lightweight installation. It is simpler to deploy and Swarm mode is included in the Docker engine.
    > -   Docker Swarm has an easier learning curve.
    > -   Docker Swarm smoothly integrates with Docker Compose and Docker CLI. Thatâ€™s because these are native Docker tools. Most of the Docker CLI commands will work with Swarm.
    > 
    > **_Drawbacks of Docker Swarm_**
    > 
    > -   Docker Swarm provides limited functionality.
    > -   Docker Swarm has limited fault tolerance.
    > -   Docker Swarm have smaller community and project as compared to Kubernetes community
    > -   In Docker Swarm, services can be scaled manually.
    > 

    > Docker Swarm is preferred in environments where simplicity and fast development is favored. Whereas Kubernetes is suitable for environments where medium to large clusters are running complex applications.

- [å·…å³°å¯¹å†³ä¹‹Swarmã€Kubernetesã€Mesos - DockOne.io](http://dockone.io/article/1138)

    > ### Apache Mesos & Mesosphere Marathon
    > 
    > Mesosçš„ç›®çš„å°±æ˜¯å»ºç«‹ä¸€ä¸ªé«˜æ•ˆå¯æ‰©å±•çš„ç³»ç»Ÿï¼Œå¹¶ä¸”è¿™ä¸ªç³»ç»Ÿèƒ½å¤Ÿæ”¯æŒå¾ˆå¤šå„ç§å„æ ·çš„æ¡†æž¶ï¼Œä¸ç®¡æ˜¯çŽ°åœ¨çš„è¿˜æ˜¯æœªæ¥çš„æ¡†æž¶ï¼Œå®ƒéƒ½èƒ½æ”¯æŒã€‚è¿™ä¹Ÿæ˜¯çŽ°ä»Šä¸€ä¸ªæ¯”è¾ƒå¤§çš„é—®é¢˜ï¼šç±»ä¼¼Hadoopå’ŒMPIè¿™äº›æ¡†æž¶éƒ½æ˜¯ç‹¬ç«‹å¼€çš„ï¼Œè¿™å¯¼è‡´æƒ³è¦åœ¨æ¡†æž¶ä¹‹é—´åšä¸€äº›ç»†ç²’åº¦çš„åˆ†äº«æ˜¯ä¸å¯èƒ½çš„ã€‚\[35\]  
    >   
    > å› æ­¤Mesosçš„æå‡ºå°±æ˜¯ä¸ºäº†åœ¨åº•éƒ¨æ·»åŠ ä¸€ä¸ªè½»é‡çš„èµ„æºå…±äº«å±‚ï¼ˆresource-sharing layerï¼‰ï¼Œè¿™ä¸ªå±‚ä½¿å¾—å„ä¸ªæ¡†æž¶èƒ½å¤Ÿé€‚ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æŽ¥å£æ¥è®¿é—®é›†ç¾¤èµ„æºã€‚Mesoså¹¶ä¸è´Ÿè´£è°ƒåº¦è€Œæ˜¯è´Ÿè´£å§”æ´¾æŽˆæƒï¼Œæ¯•ç«Ÿå¾ˆå¤šæ¡†æž¶éƒ½å·²ç»å®žçŽ°äº†å¤æ‚çš„è°ƒåº¦ã€‚  
    >   
    > å–å†³äºŽç”¨æˆ·æƒ³è¦åœ¨é›†ç¾¤ä¸Šè¿è¡Œçš„ä½œä¸šç±»åž‹ï¼Œå…±æœ‰å››ç§ç±»åž‹çš„æ¡†æž¶å¯ä¾›ä½¿ç”¨\[52\]ã€‚å…¶ä¸­æœ‰ä¸€äº›æ”¯æŒåŽŸç”Ÿçš„Dockerï¼Œæ¯”å¦‚è¯´Marathon\[39\]ã€‚Dockerå®¹å™¨çš„æ”¯æŒè‡ªä»ŽMesos 0.20.0å°±å·²ç»è¢«åŠ å…¥åˆ°Mesosä¸­äº†\[51\]ã€‚  
    >   
    > æˆ‘ä»¬æŽ¥ä¸‹æ¥å°†ä¼šé‡ç‚¹å…³æ³¨å¦‚ä½•åœ¨è®©Mesoså’ŒMarathonä¸€èµ·å·¥ä½œï¼Œæ¯•ç«ŸMarathonä¸»è¦æ˜¯ç”±Mesosphereç»´æŠ¤\[41\]ï¼Œå¹¶ä¸”æä¾›äº†å¾ˆå¤šå…³äºŽè°ƒåº¦çš„åŠŸèƒ½ï¼Œæ¯”å¦‚è¯´çº¦æŸï¼ˆconstraintsï¼‰\[38\]ï¼Œå¥åº·æ£€æŸ¥ï¼ˆhealth checksï¼‰\[40\]ï¼ŒæœåŠ¡å‘çŽ°ï¼ˆservice discoveryï¼‰å’Œè´Ÿè½½å‡è¡¡ï¼ˆload balancingï¼‰\[42\]ã€‚  
    > 
    > [![04.png](http://dockone.io/uploads/article/20160320/bba30f8f454ec423e10e10d080c5d581.png "04.png")](http://dockone.io/uploads/article/20160320/bba30f8f454ec423e10e10d080c5d581.png)
    > 
    >   
    > Apache Mesos architecture using Marathon, Â© Adrian Mouat \[49\]  
    >   
    > æˆ‘ä»¬å¯ä»¥ä»Žå›¾ä¸Šçœ‹åˆ°ï¼Œé›†ç¾¤ä¸­ä¸€å…±å‡ºçŽ°äº†4ä¸ªæ¨¡å—ã€‚ZooKeeperå¸®åŠ©MarathonæŸ¥æ‰¾Mesos masterçš„åœ°å€\[53\]ï¼ŒåŒæ—¶å®ƒå…·æœ‰å¤šä¸ªå®žä¾‹å¯ç”¨ï¼Œä»¥æ­¤åº”ä»˜æ•…éšœçš„å‘ç”Ÿã€‚Marathonè´Ÿè´£å¯åŠ¨ï¼Œç›‘æŽ§ï¼Œæ‰©å±•å®¹å™¨ã€‚Mesos maseråˆ™ç»™èŠ‚ç‚¹åˆ†é…ä»»åŠ¡ï¼ŒåŒæ—¶å¦‚æžœæŸä¸€ä¸ªèŠ‚ç‚¹æœ‰ç©ºé—²çš„CPU/RAMï¼Œå®ƒå°±ä¼šé€šçŸ¥Marathonã€‚Mesos slaveè¿è¡Œå®¹å™¨ï¼Œå¹¶ä¸”æŠ¥å‘Šå½“å‰å¯ç”¨çš„èµ„æºã€‚  
    > 
    > #### çº¦æŸï¼ˆConstraintsï¼‰
    > 
    > çº¦æŸä½¿å¾—æ“ä½œäººå‘˜èƒ½å¤Ÿæ“æŽ§åº”ç”¨åœ¨å“ªäº›èŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œå®ƒä¸»è¦ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šä¸€ä¸ªå­—æ®µåï¼ˆfield nameï¼‰ï¼ˆå¯ä»¥æ˜¯slavveçš„hostnameæˆ–è€…ä»»ä½•Mesos slaveå±žæ€§ï¼‰ï¼Œä¸€ä¸ªæ“ä½œç¬¦å’Œä¸€ä¸ªå¯é€‰çš„å€¼ã€‚5ä¸ªæ“ä½œç¬¦å¦‚ä¸‹ï¼š  
    >   
    > æ“ä½œç¬¦ï¼šè§’è‰²ï¼ˆroleï¼‰  
    > 
    > -   UNIQUEï¼šä½¿å¾—å±žæ€§å”¯ä¸€ï¼Œæ¯”å¦‚è¯´è¶Šè‹\[â€œhostnameâ€,â€UNIQUEâ€\]ä½¿å¾—æ¯ä¸ªhostä¸Šåªæœ‰ä¸€ä¸ªåº”ç”¨åœ¨è¿è¡Œã€‚
    > -   CLUSTERï¼šä½¿å¾—è¿è¡Œåº”ç”¨çš„slaveså¿…é¡»å…±äº«åŒä¸€ä¸ªç‰¹å®šå±žæ€§ã€‚æ¯”å¦‚è¯´çº¦æŸ \[â€œrack idâ€, â€œCLUSTERâ€, â€œrack-1â€\] å¼ºåˆ¶åº”ç”¨å¿…é¡»è¿è¡Œåœ¨rack-1ä¸Šï¼Œæˆ–è€…å¤„äºŽæŒ‚èµ·çŠ¶æ€çŸ¥é“rack-1æœ‰äº†ç©ºä½™çš„CPU/RAMã€‚
    > -   GROUP_BYï¼šæ ¹æ®æŸä¸ªç‰¹æ€§çš„å±žæ€§ï¼Œå°†åº”ç”¨å¹³å‡åˆ†é…åˆ°èŠ‚ç‚¹ä¸Šï¼Œæ¯”å¦‚è¯´ç‰¹å®šçš„hostæˆ–è€…rackã€‚
    > -   LIKEï¼šä½¿å¾—åº”ç”¨åªè¿è¡Œåœ¨æ‹¥æœ‰ç‰¹å®šå±žæ€§çš„slavesä¸Šã€‚å°½ç®¡åªæœ‰CLUSTERå¯ç”¨ï¼Œä½†ç”±äºŽå‚æ•°æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œå› æ­¤å¾ˆå¤šçš„å€¼éƒ½èƒ½å¤Ÿè¢«åŒ¹é…åˆ°ã€‚
    > -   UNLIKEï¼šå’ŒLIKEç›¸åã€‚
    > 
    >   
    > 
    > #### å¥åº·æ£€æŸ¥ï¼ˆHealth checksï¼‰
    > 
    > å¥åº·æ£€æŸ¥æ˜¯åº”ç”¨ä¾èµ–çš„ï¼Œéœ€è¦è¢«æ‰‹åŠ¨å®žçŽ°ã€‚è¿™æ˜¯å› ä¸ºåªæœ‰å¼€å‘è€…çŸ¥é“ä»–ä»¬è‡ªå·±çš„åº”ç”¨å¦‚ä½•åˆ¤æ–­å¥åº·çŠ¶æ€ã€‚ï¼ˆè¿™æ˜¯ä¸€ä¸ªSwarmå’ŒMesosä¹‹é—´çš„ä¸åŒç‚¹ï¼‰  
    >   
    > Mesosæä¾›äº†å¾ˆå¤šé€‰é¡¹æ¥å£°æ˜Žæ¯ä¸ªå¥åº·æ£€æŸ¥ä¹‹é—´éœ€è¦ç­‰å¾…å¤šå°‘ç§’ï¼Œæˆ–è€…å¤šå°‘æ¬¡è¿žç»­çš„å¥åº·æ£€æŸ¥å¤±è´¥åŽï¼Œè¿™ä¸ªä¸å¥åº·çš„ä»»åŠ¡éœ€è¦è¢«ç»ˆç»“ã€‚  
    > 
    > #### æœåŠ¡å‘çŽ°å’Œè´Ÿè½½å‡è¡¡ï¼ˆService discovery and load balancingï¼‰
    > 
    > ä¸ºäº†èƒ½å¤Ÿå‘é€æ•°æ®åˆ°æ­£åœ¨è¿è¡Œçš„åº”ç”¨ï¼Œæˆ‘ä»¬éœ€è¦æœåŠ¡å‘çŽ°ã€‚Apache Mesosæä¾›äº†åŸºäºŽDNSçš„æœåŠ¡å‘çŽ°ï¼Œç§°ä¹‹ä¸ºMesos-DNS\[44\]ï¼Œå®ƒèƒ½å¤Ÿåœ¨å¤šä¸ªæ¡†æž¶ï¼ˆä¸ä»…ä»…æ˜¯Marathonï¼‰ç»„æˆçš„é›†ç¾¤ä¸­å¾ˆå¥½çš„å·¥ä½œã€‚  
    >   
    > å¦‚æžœä¸€ä¸ªé›†ç¾¤åªç”±è¿è¡Œå®¹å™¨çš„èŠ‚ç‚¹ç»„æˆï¼ŒMarathonè¶³ä»¥æ‰¿å½“èµ·ç®¡ç†çš„ä»»åŠ¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸»æœºå¯ä»¥è¿è¡Œä¸€ä¸ªTCPçš„ä»£ç†ï¼Œå°†é™æ€æœåŠ¡ç«¯å£çš„è¿žæŽ¥è½¬å‘åˆ°ç‹¬ç«‹çš„åº”ç”¨å®žä¾‹ä¸Šã€‚Marathonç¡®ä¿æ‰€æœ‰åŠ¨æ€åˆ†é…çš„æœåŠ¡ç«¯å£éƒ½æ˜¯å”¯ä¸€çš„ï¼Œè¿™ç§æ–¹å¼æ¯”æ‰‹åŠ¨æ¥åšå¥½çš„å¤šï¼Œæ¯•ç«Ÿå¤šä¸ªæ‹¥æœ‰åŒæ ·é•œåƒçš„å®¹å™¨éœ€è¦åŒä¸€ä¸ªç«¯å£ï¼Œè€Œè¿™äº›å®¹å™¨å¯ä»¥è¿è¡Œåœ¨åŒä¸€ä¸ªä¸»æœºä¸Šã€‚  
    >   
    > Marathonæä¾›äº†ä¸¤ä¸ªTCP/HTTPä»£ç†ã€‚ä¸€ä¸ªç®€å•çš„shellè„šæœ¬\[37\]è¿˜æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„è„šæœ¬ï¼Œç§°ä¹‹ä¸ºmarathon-ldï¼Œå®ƒæ‹¥æœ‰æ›´å¤šçš„åŠŸèƒ½\[43\]ã€‚
    > 

- [Container Wars: Kubernetes vs. Docker Swarm vs. Amazon ECS | Caylent](https://caylent.com/containers-kubernetes-docker-swarm-amazon-ecs/)

    > Elastic Container Service (Amazon ECS)
    > --------------------------------------
    > 
    > AWSâ€™s own container management service, [Amazon ECS](https://aws.amazon.com/ecs/) is a Docker-compatible service which allows you to run containerized applications on EC2 instances and is an alternative to both Kube and Swarm.
    > 
    > While Docker has won everyone over with its simplicity, Amazon ECS is a comparatively complex tool as you have to learn a whole new platform. Components within ECS consist of:
    > 
    > -   **ECS clusters:** Groups of EC2 instances which run tasks.
    > -   **Task Definition:** A text file, in JSON format, which includes much the same information as a â€˜docker runâ€™ command. Plus, details including which containers should run on on one host.
    > -   **Service:** Your tool for running and maintaining specified numbers of task definition instances across your cluster.
    > -   **Service Scheduler:** Keeps watch over running tasks and makes sure that the correct number is up. Plus, the feature reschedules tasks if they have failed.
    > -   **Container Agents:** This feature allows you to connect your cluster instances to your container.
    > 
    > Amazon ECS provides maximum value for those looking for seamless integration between containers and other AWS services. Itâ€™s a fully managed service which offers high availability, scalability, and security. On top of that, your AWS support plan includes it as standard.
    > 
    > Plus, if you are already on AWS, there is no need to install any extra software to operate your cluster.
    > 

- [Docker vs. Kubernetes vs. Apache Mesos: Why What You Think You Know is Probably Wrong - Mesosphere](https://mesosphere.com/blog/docker-vs-kubernetes-vs-apache-mesos/?utm_source=twitter&utm_medium=organic)

    > Letâ€™s start with Dockerâ€¦
    > ------------------------
    > 
    > Docker Inc., today started as a Platform-as-a-Service startup named dotCloud. The dotCloud team found that managing dependencies and binaries across many applications and customers required significant effort. So they combined some of the capabilities of Linux [cgroups](https://en.wikipedia.org/wiki/Cgroups) and namespaces into a single and easy to use package so that applications can consistently run on any infrastructure. This package is [the Docker image](https://docs.docker.com/engine/docker-overview/), which provides the following capabilities:
    > 
    > -   **Packages the application and the libraries in a single package** (the Docker Image), so applications can consistently be deployed across many environments;
    > -   **Provides Git-like semantics**, such as â€œdocker pushâ€, â€œdocker commitâ€ to make it easy for application developers to quickly adopt the new technology and incorporate it in their existing workflows;
    > -   **Define Docker images as immutable layers**, enabling immutable infrastructure. Committed changes are stored as an individual read-only layers, making it easy to re-use images and track changes. Layers also save disk space and network traffic by only transporting the updates instead of entire images;
    > -   **Run Docker containers by instantiating the immutable image** with a writable layer that can temporarily store runtime changes, making it easy to deploy and scale multiple instances of the applications quickly.
    > ![](https://mesosphere.com/wp-content/uploads/2017/07/docker-host.png)
    > 
    > Enter Kubernetes
    > ----------------
    > 
    > Google recognized the potential of the Docker image early on and sought to deliver container orchestration â€œas-a-serviceâ€ on the Google Cloud Platform. Google had tremendous experience with containers (they introduced cgroups in Linux) but existing internal container and distributed computing tools like Borg were directly coupled to their infrastructure. So, instead of using any code from their existing systems, Google designed Kubernetes from scratch to orchestrate Docker containers. Kubernetes was released in February 2015 with the following goals and considerations:
    > 
    > -   **Empower application developers** with a powerful tool for Docker container orchestration without having to interact with the underlying infrastructure;
    > -   **Provide standard deployment interface** and primitives for a consistent app deployment experience and APIs across clouds;
    > -   **Build on a Modular API core** that allows vendors to integrate systems around the core Kubernetes technology.
    > 
    > ![](https://mesosphere.com/wp-content/uploads/2017/07/kubernetes-architecture.png)
    > 
    > 
    > 
    > Apache Mesos
    > ------------
    > 
    > Apache Mesos started as a UC Berkeley project to create a next-generation cluster manager, and apply the lessons learned from cloud-scale, distributed computing infrastructures such as [Googleâ€™s Borg](https://research.google.com/pubs/pub43438.html) and [Facebookâ€™s Tupperware](https://www.youtube.com/watch?v=C_WuUgTqgOc). While Borg and Tupperware had a monolithic architecture and were closed-source proprietary technologies tied to physical infrastructure, Mesos introduced a modular architecture, an open source development approach, and was designed to be completely independent from the underlying infrastructure. Mesos was quickly adopted by [Twitter](https://youtu.be/F1-UEIG7u5g), [Apple(Siri)](http://www.businessinsider.com/apple-siri-uses-apache-mesos-2015-8), [Yelp](https://engineeringblog.yelp.com/2015/11/introducing-paasta-an-open-platform-as-a-service.html), [Uber](http://highscalability.com/blog/2016/9/28/how-uber-manages-a-million-writes-per-second-using-mesos-and.html), [Netflix](https://medium.com/netflix-techblog/distributed-resource-scheduling-with-apache-mesos-32bd9eb4ca38), and many leading technology companies to support everything from microservices, big data and real time analytics, to elastic scaling.
    > 
    > As a cluster manager, Mesos was architected to solve for a very different set of challenges:
    > 
    > -   **Abstract data center resources** into a single pool to simplify resource allocation while providing a consistent application and operational experience across private or public clouds;
    > -   **Colocate diverse workloads** on the same infrastructure such analytics, stateless microservices, distributed data services and traditional apps to improve utilization and reduce cost and footprint;
    > -   **Automate day-two operations** for application-specific tasks such as deployment, self healing, scaling, and upgrades; providing a highly available fault tolerant infrastructure;
    > -   **Provide evergreen extensibility** to run new application and technologies without modifying the cluster manager or any of the existing applications built on top of it;
    > -   **Elastically scale** the application and the underlying infrastructure from a handful, to tens, to tens of thousands of nodes.
    > 
    > ![](https://mesosphere.com/wp-content/uploads/2017/07/mesos-two-level-scheduler.png)
    > 




## Docker swarm

- [twtrubiks/docker-swarm-tutorial: Docker Swarm åŸºæœ¬æ•™å­¸ - å¾žç„¡åˆ°æœ‰ Docker-Swarm-Beginners-GuideðŸ“](https://github.com/twtrubiks/docker-swarm-tutorial)

## Mesos

- [Mesosphereé‡‹å‡ºé–‹æºè³‡æ–™ä¸­å¿ƒä½œæ¥­ç³»çµ±DC/OSï¼Œèª“è¨€è®“è³‡æ–™ä¸­å¿ƒç®¡ç†åƒä½¿ç”¨å€‹äººé›»è…¦ | iThome](https://www.ithome.com.tw/news/105437)

- [ç”¨30å¤©ä¾†å»ºæ§‹å’Œæ“ä½œApache Mesos ç³»åˆ—æ–‡ç« åˆ—è¡¨ - iT é‚¦å¹«å¿™::ä¸€èµ·å¹«å¿™è§£æ±ºé›£é¡Œï¼Œæ‹¯æ•‘ IT äººçš„ä¸€å¤©](https://ithelp.ithome.com.tw/users/20103456/ironman/1063)

- [mesos åŽŸç†ä¸Žæž¶æž„ Â· Docker â€”â€” ä»Žå…¥é—¨åˆ°å®žè·µ](https://yeasy.gitbooks.io/docker_practice/content/mesos/architecture.html)

    > ### æž¶æž„
    > 
    > ä¸‹é¢è¿™å¼ åŸºæœ¬æž¶æž„å›¾æ¥è‡ª Mesos å®˜æ–¹ã€‚
    > 
    > ![mesos çš„åŸºæœ¬æž¶æž„](https://yeasy.gitbooks.io/docker_practice/content/mesos/_images/mesos-architecture.png)
    > 
    > å›¾ 1.22.3.1 - mesos çš„åŸºæœ¬æž¶æž„
    > 
    > å¯ä»¥çœ‹å‡ºï¼ŒMesos é‡‡ç”¨äº†ç»å…¸çš„ä¸»-ä»Žï¼ˆmaster-slaveï¼‰æž¶æž„ï¼Œå…¶ä¸­ä¸»èŠ‚ç‚¹ï¼ˆç®¡ç†èŠ‚ç‚¹ï¼‰å¯ä»¥ä½¿ç”¨ zookeeper æ¥åš HAã€‚
    > 
    > Mesos master æœåŠ¡å°†è¿è¡Œåœ¨ä¸»èŠ‚ç‚¹ä¸Šï¼ŒMesos slave æœåŠ¡åˆ™éœ€è¦è¿è¡Œåœ¨å„ä¸ªè®¡ç®—ä»»åŠ¡èŠ‚ç‚¹ä¸Šã€‚
    > 
    > è´Ÿè´£å®Œæˆå…·ä½“ä»»åŠ¡çš„åº”ç”¨æ¡†æž¶ä»¬ï¼Œè·Ÿ Mesos master è¿›è¡Œäº¤äº’ï¼Œæ¥ç”³è¯·èµ„æºã€‚
    > 
    > ### åŸºæœ¬å•å…ƒ
    > 
    > Mesos ä¸­æœ‰ä¸‰ä¸ªåŸºæœ¬çš„ç»„ä»¶ï¼šç®¡ç†æœåŠ¡ï¼ˆmasterï¼‰ã€ä»»åŠ¡æœåŠ¡ï¼ˆslaveï¼‰ä»¥åŠåº”ç”¨æ¡†æž¶ï¼ˆframeworkï¼‰ã€‚
    > 
    > #### ç®¡ç†æœåŠ¡ \- master
    > 
    > è·Ÿå¤§éƒ¨åˆ†åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç±»ä¼¼ï¼Œä¸»èŠ‚ç‚¹èµ·åˆ°ç®¡ç†ä½œç”¨ï¼Œå°†çœ‹åˆ°å…¨å±€çš„ä¿¡æ¯ï¼Œè´Ÿè´£ä¸åŒåº”ç”¨æ¡†æž¶ä¹‹é—´çš„èµ„æºè°ƒåº¦å’Œé€»è¾‘æŽ§åˆ¶ã€‚åº”ç”¨æ¡†æž¶éœ€è¦æ³¨å†Œåˆ°ç®¡ç†æœåŠ¡ä¸Šæ‰èƒ½è¢«ä½¿ç”¨ã€‚
    > 
    > ç”¨æˆ·å’Œåº”ç”¨éœ€è¦é€šè¿‡ä¸»èŠ‚ç‚¹æä¾›çš„ API æ¥èŽ·å–é›†ç¾¤çŠ¶æ€å’Œæ“ä½œé›†ç¾¤èµ„æºã€‚
    > 
    > #### ä»»åŠ¡æœåŠ¡ \- slave
    > 
    > è´Ÿè´£æ±‡æŠ¥æœ¬ä»ŽèŠ‚ç‚¹ä¸Šçš„èµ„æºçŠ¶æ€ï¼ˆç©ºé—²èµ„æºã€è¿è¡ŒçŠ¶æ€ç­‰ç­‰ï¼‰ç»™ä¸»èŠ‚ç‚¹ï¼Œå¹¶è´Ÿè´£éš”ç¦»æœ¬åœ°èµ„æºæ¥æ‰§è¡Œä¸»èŠ‚ç‚¹åˆ†é…çš„å…·ä½“ä»»åŠ¡ã€‚
    > 
    > éš”ç¦»æœºåˆ¶ç›®å‰åŒ…æ‹¬å„ç§å®¹å™¨æœºåˆ¶ï¼ŒåŒ…æ‹¬ LXCã€Docker ç­‰ã€‚
    > 
    > #### åº”ç”¨æ¡†æž¶ \- framework
    > 
    > åº”ç”¨æ¡†æž¶æ˜¯å®žé™…å¹²æ´»çš„ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼š
    > 
    > -   è°ƒåº¦å™¨ï¼ˆschedulerï¼‰ï¼šæ³¨å†Œåˆ°ä¸»èŠ‚ç‚¹ï¼Œç­‰å¾…åˆ†é…èµ„æºï¼›
    > -   æ‰§è¡Œå™¨ï¼ˆexecutorï¼‰ï¼šåœ¨ä»ŽèŠ‚ç‚¹ä¸Šæ‰§è¡Œæ¡†æž¶æŒ‡å®šçš„ä»»åŠ¡ï¼ˆæ¡†æž¶ä¹Ÿå¯ä»¥ä½¿ç”¨ Mesos è‡ªå¸¦çš„æ‰§è¡Œå™¨ï¼ŒåŒ…æ‹¬ shell è„šæœ¬æ‰§è¡Œå™¨å’Œ Docker æ‰§è¡Œå™¨ï¼‰ã€‚
    > 
    > åº”ç”¨æ¡†æž¶å¯ä»¥åˆ†ä¸¤ç§ï¼šä¸€ç§æ˜¯å¯¹èµ„æºçš„éœ€æ±‚æ˜¯ä¼šæ‰©å±•çš„ï¼ˆæ¯”å¦‚ Hadoopã€Spark ç­‰ï¼‰ï¼Œç”³è¯·åŽè¿˜å¯èƒ½è°ƒæ•´ï¼›ä¸€ç§æ˜¯å¯¹èµ„æºéœ€æ±‚å¤§å°æ˜¯å›ºå®šçš„ï¼ˆMPI ç­‰ï¼‰ï¼Œä¸€æ¬¡ç”³è¯·å³å¯ã€‚
    > 

- [Apache Mesos: The True OS For The Software Defined Data Center? â€“ Cloud Architect Musings](https://cloudarchitectmusings.com/2015/03/23/apache-mesos-the-true-os-for-the-software-defined-data-center/)

- [DCOSåˆ°åº•æ˜¯å•¥ï¼Ÿçœ‹å®Œè¿™ç¯‡ä½ å°±æ‡‚äº†~ - 51CTO.COM](http://cloud.51cto.com/art/201603/506805.htm)

    > ä¸‹é¢ä»‹ç»ä¸‰ç§åŸºäºŽMesosè¡ç”Ÿçš„ç”Ÿæ€ç³»ç»Ÿï¼š Mesosphere DCOS ä»ŽMesosphereå®˜ç½‘äº†è§£åˆ°ï¼ŒMesosphere DCOSæ˜¯ä»¥ Mesosä¸ºâ€œæ ¸å¿ƒâ€ï¼Œä¸Žå…¶å‘¨è¾¹æœåŠ¡åŠåŠŸèƒ½ç»„ä»¶æ‰€ç»„æˆçš„ä¸€ä¸ªç”Ÿæ€ç³»ç»Ÿã€‚å®ƒè·¨è¶Šæ•°æ®ä¸­å¿ƒæˆ–äº‘çŽ¯å¢ƒä¸­çš„æ‰€æœ‰ä¸»æœºï¼Œå°†æ‰€æœ‰ä¸»æœºçš„èµ„æºæ”¾å…¥ä¸€ä¸ªèµ„æºæ± ï¼Œä½¿æ‰€æœ‰ä¸»æœºçš„è¡Œä¸ºæ•´ä½“ä¸Šåƒä¸€ä¸ªå¤§è®¡ç®—æœºã€‚ Mesosphere DCOSå†…éƒ¨æž¶æž„å›¾
    > 
    > ![](http://s5.51cto.com/wyfs02/M00/7C/CD/wKiom1bYCDzxFp3EAACDVAFWONA866.jpg)
    > 
    > ç”±å›¾å¯è§ï¼ŒMesosphere DCOSé™¤äº†å†…æ ¸Mesosï¼Œè¿˜æœ‰ä¸¤ä¸ªå…³é”®ç»„ä»¶Marathonå’ŒChronosã€‚å…¶ä¸­ï¼ŒMarathon(ååˆ†å¸ƒå¼çš„init)æ˜¯ä¸€ä¸ªç”¨äºŽå¯åŠ¨é•¿æ—¶é—´è¿è¡Œåº”ç”¨ç¨‹åºå’ŒæœåŠ¡çš„æ¡†æž¶ï¼ŒChronos(åˆååˆ†å¸ƒå¼çš„cron)æ˜¯ä¸€ä¸ªåœ¨Mesosä¸Šè¿è¡Œå’Œç®¡ç†è®¡åˆ’ä»»åŠ¡çš„æ¡†æž¶ã€‚æ­¤å¤–ï¼ŒMesosphere DCOSè¿˜æœ‰Mesos-DNSè¿™æ ·çš„æ’ä»¶æ¨¡å—ï¼Œå®ƒç±»ä¼¼ä¸€ä¸ªCLIï¼Œä¸€ä¸ªGUIåˆæˆ–è€…æ˜¯æä¾›ä½ æƒ³è¿è¡Œçš„æ‰€æœ‰çš„åŒ…çš„ä»“åº“ç­‰å·¥å…·ã€‚ Mesosphere DCOS å¯ä»¥è¿è¡Œåœ¨ä»»æ„çš„çŽ°ä»£LinuxçŽ¯å¢ƒï¼Œå…¬æœ‰æˆ–ç§æœ‰äº‘ï¼Œè™šæ‹Ÿæœºç”šè‡³æ˜¯è£¸æœºçŽ¯å¢ƒï¼Œå½“å‰æ”¯æŒçš„å¹³å°æœ‰äºšé©¬é€ŠAWSï¼Œè°·æ­ŒGCEï¼Œå¾®è½¯Azureï¼ŒOpenstackç­‰ç­‰ã€‚æ®Mesosphereå®˜ç½‘æ˜¾ç¤ºï¼ŒMesosphere DCOSåœ¨å…¶å…¬æœ‰ä»“åº“ä¸Šå·²æä¾›äº†40å¤šç§æœåŠ¡ç»„ä»¶ï¼Œæ¯”å¦‚Hadoopï¼ŒSparkï¼ŒCassandra, Jenkins, Kafka, MemSQLç­‰ç­‰ã€‚ æµ™æ±Ÿç§»åŠ¨ä¸Žå¤©çŽ‘è”åˆç ”å‘çš„DCOS ä¸‹å›¾ä¸ºè¯¥DCOSå†…éƒ¨æž¶æž„ç¤ºæ„
    > 
    > ![](http://s3.51cto.com/wyfs02/M00/7C/CC/wKioL1bYCLbAFMh4AAB6tbAFsOs988.jpg)
    > 
    > ç”±å›¾å¯è§ï¼Œâ€œæ ¸å¿ƒâ€Mesosè´Ÿè´£é›†ç¾¤ä¸­æ‰€æœ‰èŠ‚ç‚¹èµ„æºçš„åŠ¨æ€è°ƒåº¦ä¸Žç®¡ç†ã€‚å…¶ä¸Šè¿˜åŒ…æ‹¬DCOSç®¡æŽ§å¹³å°ï¼Œå®¹å™¨åº”ç”¨æ¡†æž¶ç­‰é‡è¦åŠŸèƒ½ç»„ä»¶ã€‚è¯¥è¿è¥å•†è¡¨ç¤ºï¼Œä¸Šè¿°DCOSå¹³å°ä¸ä»…å…·å¤‡çµæ´»å¼¹æ€§çš„ä¼¸ç¼©èƒ½åŠ›ï¼Œä¸ºç³»ç»Ÿæä¾›é«˜æ•ˆçš„å¹³è¡Œæ‰©å±•æ¥åº”å¯¹çªå‘çš„ä¸šåŠ¡é«˜å³°ï¼Œè€Œä¸”Mesosä¸ŽDockerçš„ç»“åˆæžå¤§ç®€åŒ–ä¸šåŠ¡è¿ç»´å¤æ‚åº¦ï¼Œå®žçŽ°è‡ªåŠ¨åŒ–éƒ¨ç½²ä¸Žåº”ç”¨ç¨‹åºå‡çº§ï¼ŒMesosè¿˜å¯ä¸ºèµ„æºç®¡ç†æä¾›é«˜å®¹é”™æ€§ï¼Œè‡ªåŠ¨è¾¨åˆ«æœåŠ¡å™¨ã€æœºæž¶æˆ–ç½‘ç»œå‡ºçŽ°çš„æ•…éšœç­‰ã€‚
    > 

- [è«‡è«‡Apache Mesoså’ŒMesosphere DCOSï¼šæ­·å²ã€æž¶æ§‹ã€ç™¼å±•å’Œæ‡‰ç”¨ - å£¹è®€](https://read01.com/xA44K.html)

    > Mesos çš„è¨­è¨ˆå®—æ—¨åœ¨æ–¼å˜—è©¦å’Œæé«˜é›†ç¾¤çš„åˆ©ç”¨æ•ˆçŽ‡å’Œæ€§èƒ½ï¼Œä»–å€‘èªç‚ºå°æ–¼æ•¸æ“šä¸­å¿ƒè³‡æºçš„å–®ç´”éœæ…‹åŠƒåˆ†å’Œä½¿ç”¨çš„é€™æ¨£ä¸€å€‹æ–¹å¼æ˜¯å€¼å¾—é‡æ–°è€ƒé‡çš„ï¼Œèˆ‰å€‹ä¾‹å­ä¾†èªªï¼š  
    >   
    > æˆ‘å€‘å‡è¨­ä½ çš„æ•¸æ“šä¸­å¿ƒè£¡æ“æœ‰9å€‹ä¸»æ©Ÿï¼š
    > 
    > ![](https://i1.read01.com/SIG=2hrde6o/3041705658323030.jpg)
    > 
    > å¦‚æžœæŠŠå®ƒéœæ…‹çš„åŠƒåˆ†é–‹ä¾†ï¼Œä¸¦ä¸”æŒ‡å®šæ¯ä¸‰å€‹ä¸»æ©Ÿæ‰¿è¼‰ä¸€å€‹æ‡‰ç”¨ï¼Œé€™æ¨£ä¸€ä¾†ç¸½å…±æ˜¯3å€‹æ‡‰ç”¨ï¼ˆé€™è£¡æ˜¯Hadoopã€Spark å’Œ Ruby on Railsï¼‰ã€‚
    > 
    > ![](https://i2.read01.com/SIG=2tar2cp/3041705658323031.jpg)
    > 
    > é¡¯è€Œæ˜“è¦‹çš„ä¸€å€‹å•é¡Œæ˜¯é€™äº›ä¸»æ©Ÿçš„è³‡æºåˆ©ç”¨çŽ‡ä¸¦ä¸æœƒå¾ˆé«˜ï¼›
    > 
    > ![](https://i3.read01.com/SIG=28o1miq/3041705658323032.jpg)
    > 
    > å› æ­¤å¦‚æžœä½ æƒ³ä½¿ç”¨å…¨éƒ¨çš„è³‡æºï¼Œå³é€™è£¡ä¾‹å­ä¸­çš„å…¨éƒ¨9å°ä¸»æ©Ÿï¼Œé‚£éº¼å°±éœ€è¦å°‡å…¶æŠ½è±¡æˆä¸€å€‹å…±äº«è³‡æºæ± ï¼Œè€Œä½ å¯ä»¥æŒ‰éœ€è¨ˆåŠƒé…ç½®ï¼Œé€™æ¨£çš„è©±ï¼Œåˆ©ç”¨çŽ‡è‡ªç„¶å¯ä»¥å¾—åˆ°ç›¸æ‡‰çš„æå‡ï¼›
    > 
    > ![](https://i1.read01.com/SIG=249nqor/3041705658323033.jpg)
    > 
    > Mesosåœ˜éšŠçš„ç¬¬äºŒå€‹è§€é»žåœ¨æ–¼ä»–å€‘è¦ºå¾—éœ€è¦ç‚ºåˆ†å¸ƒå¼ç³»çµ±é‡èº«å®šè£½ä¸€å¥—æ–°çš„ç³»çµ±ï¼Œæ›å¥è©±èªªï¼Œä»–å€‘è¦ºå¾—MapReduceä¸¦ä¸æ˜¯é©ç”¨æ–¼æ‰€æœ‰çš„å ´æ™¯ï¼ˆé€™ä¹Ÿå°Žè‡´äº†Sparkçš„èª•ç”Ÿï¼Œè€Œå®ƒåˆæ˜¯å¦å¤–ä¸€å€‹æ•…äº‹äº†ï¼‰ï¼Œè€Œæˆ‘å€‘éœ€è¦ä¸€å€‹æ–°çš„æ›´ç°¡å–®å’Œæ›´å…·æœ‰é€šç”¨æ€§çš„å°ˆç‚ºåˆ†å¸ƒå¼ç³»çµ±æä¾›æœå‹™çš„é€™æ¨£ä¸€å€‹æ¡†æž¶ã€‚
    > 
    > #### Mesos æ¡†æž¶ï¼ˆåˆ†å¸ƒå¼ç³»çµ±ï¼‰åˆ°åº•æ˜¯ä»€éº¼?
    > 
    > ä¸€èˆ¬ä¾†èªªï¼Œä¸€å€‹åˆ†å¸ƒå¼ç³»çµ±ä½ éœ€è¦æœ‰ä¸€å€‹Coordinatorï¼ˆèª¿åº¦å™¨ï¼‰å’Œ å¤šå€‹Workerï¼ˆåŸ·è¡Œä»»å‹™ï¼‰ã€‚èª¿åº¦å™¨ä»¥åŒæ­¥ï¼ˆåˆ†å¸ƒå¼ï¼‰çš„æ–¹å¼é‹è¡Œé€²ç¨‹/ä»»å‹™ï¼Œè™•ç†ç¨‹åºéŒ¯èª¤ï¼ˆå®¹éŒ¯ï¼‰ï¼Œä¸¦ä¸”è² è²¬å„ªåŒ–æ€§èƒ½ï¼ˆå³å½ˆæ€§ä¼¸ç¸®ï¼‰ã€‚æ›å¥è©±èªªï¼Œå®ƒè² è²¬å”èª¿åœ¨æ•¸æ“šä¸­å¿ƒåŽ»å¯¦éš›åŸ·è¡Œä½ æƒ³è¦é‹è¡Œçš„ä»£ç¢¼ï¼ˆä¸éœ€è¦æ˜¯ä¸€å€‹å®Œæ•´çš„ç¨‹åºï¼Œå®ƒä¹Ÿå¯ä»¥æ˜¯æŸäº›ç¨®é¡žçš„é‹ç®—ï¼‰ã€‚æ­£å¦‚ä¹‹å‰æ‰€æåˆ°çš„é‚£æ¨£ï¼ŒMesoså°‡å…¶ç¨±ä¹‹ç‚ºè¯åˆèª¿åº¦ã€‚
    > 
    > ![](https://i2.read01.com/SIG=33tkves/3041705658323034.jpg)
    > 
    > æˆ–è€…ä¹Ÿå¯ä»¥é€™éº¼èªªï¼ŒMesosæ˜¯ä¸€å€‹å¸¶æœ‰èª¿åº¦å™¨çš„åˆ†å¸ƒå¼ç³»çµ±ã€‚
    > 
    > ![](https://i3.read01.com/SIG=3fc2j4t/3041705658323035.jpg)
    > 
    > é‚£éº¼Mesosçš„çœŸæ­£å®šä½æ˜¯ä»€éº¼å‘¢? ç•¶ä½ å˜—è©¦åŽ»åŸ·è¡Œå®ƒçš„ä»»å‹™æ™‚ä½ å¯ä»¥ç†è§£ç‚ºå®ƒå¯¦éš›ä¸Šå°±æ˜¯æ©Ÿå™¨å’Œèª¿åº¦å™¨ä¹‹é–“çš„ä¸€å±¤æŠ½è±¡ã€‚  
    >   
    > å› æ­¤åœ¨Mesosé‡Œï¼Œèª¿åº¦å™¨æ˜¯å’ŒMesoså±¤ï¼ˆé€šéŽAPIç­‰ï¼‰é€šä¿¡ï¼Œè€Œä¸æ˜¯ç›´æŽ¥è·Ÿç‰©ç†æ©Ÿå™¨æ‰“äº¤é“ã€‚Mesosé€™è£¡é€šéŽé€™æ¨£çš„æ–¹å¼å˜—è©¦è§£æ±ºçš„å³æ˜¯è³‡æºçš„éœæ…‹åŠƒåˆ†å•é¡Œï¼Œé€™æ„å‘³è‘—ä½ ä¸å†éœ€è¦é‡å°æ¯å€‹ç‰¹å®šçš„é‹è¡Œæ™‚åˆ†é…ä¸€å€‹å°æ‡‰çš„èª¿åº¦å™¨åŽ»æ±ºå®šå¯¦éš›åŽ»åŸ·è¡Œå®ƒçš„workersï¼Œè€Œå–è€Œä»£ä¹‹çš„æ˜¯ï¼Œä½ æœ‰ä¸€å€‹èª¿åº¦å™¨åŽ»å’ŒMesosé€šä¿¡ï¼Œè€Œå®ƒæœƒåéŽä¾†ä¾æ“šæ•´å€‹è³‡æºæ± çš„å‰©é¤˜è³‡æºåšèª¿åº¦ã€‚
    > 
    > ![](https://i1.read01.com/SIG=3quo7qu/3041705658323036.jpg)
    > 
    > é€™æ¨£åšå¸¶ä¾†çš„æœ€é¡¯è€Œæ˜“è¦‹çš„å¥½è™•å°±æ˜¯ä½ å¯ä»¥åœ¨ä¸€æ‰¹æ©Ÿå™¨ä¸Šé‹è¡Œå¤šå€‹ä¸åŒçš„åˆ†å¸ƒå¼ç³»çµ±ä¸¦ä¸”æ›´æœ‰æ•ˆçš„ï¼ˆä¸å†æ˜¯éœæ…‹åŠƒåˆ†ï¼‰å‹•æ…‹åŠƒåˆ†å’Œå…±äº«é€™äº›è³‡æºã€‚
    > 
    > ![](https://i2.read01.com/SIG=3mfebgv/3041705658323037.jpg)
    > 
    > å…¶æ¬¡ï¼Œä¹‹æ‰€ä»¥é€™æ¨£æŠ½è±¡è¨­è¨ˆçš„å¦å¤–ä¸€å€‹é‡è¦åŽŸå› åœ¨æ–¼å®ƒèƒ½å¤ æä¾›ä¸€å€‹é€šç”¨åŠŸèƒ½é›†ï¼ˆæ•…éšœæª¢æ¸¬ã€åˆ†å¸ƒå¼ä»»å‹™ã€ä»»å‹™å•Ÿå‹•ã€ä»»å‹™ç›£æŽ§ã€çµæŸä»»å‹™ã€æ¸…ç†ä»»å‹™ç­‰ï¼‰ï¼Œé€™æ¨£ä¸€ä¾†å°±ç„¡éœ€æ¯å€‹åˆ†å¸ƒå¼ç³»çµ±éƒ½å„è‡ªé‡è¤‡çš„åŽ»å¯¦ç¾é€™æ¨£ä¸€å¥—é‚è¼¯ã€‚
    > 
    > #### Mesos é©åˆä½œç‚ºæ•¸æ“šä¸­å¿ƒçš„å“ªä¸€å±¤çš„æŠ½è±¡?
    > 
    > Mesos é€™ä¸€å±¤æŠ½è±¡å¯¦ç¾çš„ç›®çš„å³æ˜¯æƒ³è¦å˜—è©¦é€šéŽä½¿ç”¨ä¸¦æ›´å¥½çš„èª¿åº¦è³‡æºä½¿å¾—é‹è¡Œåœ¨å…¶ä¹‹ä¸Šçš„é€™äº›æ¡†æž¶è®Šå¾—æ›´åŠ æ˜“æ–¼æ§‹å»ºå’Œé‹è¡Œã€‚
    > 
    > ![](https://i3.read01.com/SIG=1lmucmg/3041705658323038.jpg)
    > 
    > IaaSçš„æŠ½è±¡çš„æ˜¯æ©Ÿå™¨ï¼Œä¾‹å¦‚ä½ çµ¦å®ƒæŒ‡å®šä¸€å€‹æ•¸å­—ï¼Œå®ƒä¾¿æœƒç”Ÿæˆä¸€å †çš„æ©Ÿå™¨è€Œé€™ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯Mesosæ¦‚å¿µæ¨¡åž‹æ›´åº•å±¤åŒ–çš„ä¸€å€‹æŠ½è±¡ã€‚PaaSå‰‡è€ƒæ…®çš„æ›´å¤šæ˜¯éƒ¨ç½²å’Œç®¡ç†æ‡‰ç”¨/æœå‹™ï¼Œå®ƒä¸¦ä¸é—œå¿ƒåº•å±¤çš„é‚£äº›åŸºç¤Žæž¶æ§‹ï¼Œè€Œä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯Mesosæ¦‚å¿µæ¨¡åž‹çš„ä¸€å€‹æ›´é«˜å±¤é¢çš„æŠ½è±¡ã€‚åœ¨äº¤äº’æ–¹é¢ï¼ŒPaaSå¯èƒ½æ˜¯å’Œé–‹ç™¼è€…ç›´æŽ¥äº¤äº’ï¼Œè€ŒMesoså‰‡æ˜¯ä»¥APIçš„å½¢å¼å’Œè»Ÿé«”ç¨‹åºäº¤äº’ã€‚  
    >   
    > æ›å¥è©±èªªï¼Œä½ å¯ä»¥åŸºæ–¼Mesosä¹‹ä¸Šæ§‹å»ºä¸€å€‹Paasç³»çµ±ï¼ˆä¾‹å¦‚åƒMarathon - å®ƒå¥½åƒä»»ä½•åœ°æ–¹éƒ½æ¯”ä¸€å€‹çœŸæ­£çš„Paasç³»çµ±æ›´åƒPaaSï¼‰ï¼ŒåŒæ™‚ä½ å¯ä»¥åœ¨ä¸€å€‹IaaSä¸Šé‹è¡ŒMesosï¼ˆä¾‹å¦‚OpenStackï¼‰ã€‚  
    >   
    > å¦‚æžœä½ å°‡ä½ çš„Mesosé‹è¡Œåœ¨ä¸€å€‹çµ„åˆç³»çµ±ï¼ˆä¾‹å¦‚å°±åƒOpenstack + ç‰©ç†ç¡¬é«” + è™›æ“¬æ©Ÿï¼‰ä¹‹ä¸Šï¼Œé‚£éº¼ä½ å¯ä»¥å¾ˆç›´è§€çš„å†æ¬¡é«”æœƒåˆ°å‹•æ…‹åŠƒåˆ†è³‡æºçš„å¥½è™•ï¼Œé‚£ä¾¿æ˜¯ä½ èƒ½å¤ è·¨è¶Šé€™äº›åº•å±¤çµ„ä»¶è€Œç›´æŽ¥çš„åŽ»ç®¡ç†å’Œè¨ˆåŠƒä½ çš„å·¥ä½œè² è¼‰ï¼ŒæŸç¨®æ„ç¾©ä¸Šä¾†èªªï¼Œä½ å¯ä»¥èªç‚ºMesosé¡žä¼¼æ–¼æ˜¯ä¸€å€‹æ•¸æ“šä¸­å¿ƒçš„å…§æ ¸ï¼Œå³å®ƒè² è²¬å°‡ç‰©ç†æ©Ÿå™¨æŠ½è±¡æˆè³‡æºï¼Œå¾žè€Œä½¿å¾—ä½ èƒ½å¤ å¿½ç•¥åº•å±¤çµ„ä»¶çš„å­˜åœ¨ï¼Œé€šéŽæ¶ˆè²»Mesosçš„æŠ½è±¡è³‡æºä¾†æ§‹å»ºåˆ†å¸ƒå¼ç³»çµ±ã€‚  
    >   
    > å› æ­¤æˆ‘å€‘å¯ä»¥èªªï¼ŒApache Mesosæ˜¯ç‚ºæ§‹å»ºå’Œé‹è¡Œå…¶å®ƒåˆ†å¸ƒå¼ç³»çµ±ï¼ˆä¾‹å¦‚åƒSparkï¼‰æä¾›æœå‹™çš„åˆ†å¸ƒå¼ç³»çµ±ã€‚
    > 
    > #### Mesosæž¶æ§‹å…§å¹•
    > 
    > åœ¨ Mesos é‡Œï¼Œä¸€å€‹æ¡†æž¶ç¨‹åºï¼ˆæˆ–è€…èªªåˆ†å¸ƒå¼ç³»çµ±ï¼‰ç™¼èµ·çš„ä¸€æ¬¡è«‹æ±‚æœƒåœ¨è¢«æŽ¥æ”¶åˆ°çš„é‚£å€‹æ™‚åˆ»ç”±èª¿åº¦å™¨æ‰¿æŽ¥å’Œåˆ†é…ã€‚é€™è·Ÿå‚³çµ±åˆ†å¸ƒå¼ç³»çµ±ä¸€èˆ¬äººç‚ºç™¼èµ·è«‹æ±‚çš„æ–¹å¼ä¸å¤ªä¸€æ¨£ï¼ˆå†å¼·èª¿ä¸€ä¸‹ï¼ŒMesoså°‡æœƒè®“æ¡†æž¶ç¨‹åºç™¼èµ·è«‹æ±‚ï¼Œè€Œä¸æ˜¯äººå·¥æ“ä½œï¼‰ï¼Œå‚³çµ±çš„æ–¹å¼å³éœ€è¦åœ¨äººç‚ºç™¼èµ·è«‹æ±‚æ™‚è¨­å®šå¥½éœ€è¦åˆ†é…çš„ç‰¹å®šè³‡æºï¼Œç„¶å¾Œå†åŽ»çœŸæ­£è«‹æ±‚å’Œç²å–é€™äº›è³‡æºï¼Œé€™é¡žæƒ…æ³ä¸­æœ€å…¸åž‹çš„èŽ«éŽæ–¼éœ€æ±‚å ´æ™¯çš„è®Šæ›ï¼ˆè¨­æƒ³åœ¨Map/Reduceçš„å ´æ™¯ä¸‹ï¼Œæ¯”å¦‚åœ¨Mapå’ŒReduceéšŽæ®µåˆ‡æ›ä¹‹éš›ç”¢ç”Ÿçš„ä¸€å€‹éœ€æ±‚è³‡æºçš„è®ŠåŒ–ï¼‰
    > 
    > ![](https://i1.read01.com/SIG=1p780sh/3041705658323039.jpg)
    > 
    > èˆ‡å‚³çµ±åˆ†å¸ƒå¼ç³»çµ±ä¸ä¸€æ¨£çš„æ˜¯ï¼ŒMesos å°‡æœƒç«‹é¦¬ç‚ºå…¶åˆ†é…æ‰€èƒ½åˆ†é…çš„æœ€å¤§è³‡æºï¼Œè€Œä¸æ˜¯å‚»å‚»çš„åœ¨é‚£ç­‰åˆ°æ»¿è¶³è©²è«‹æ±‚çš„è³‡æºå®Œæˆ/å®Œå…¨åˆ°ä½ï¼ˆåœ¨é€™è£¡å®ƒæƒ³è¦å¯¦ç¾çš„ä¾¿æ˜¯åœ¨çµ•å¤§å¤šæ•¸æƒ…æ³ä¸‹ååˆ†å¥æ•ˆçš„ç„¡é˜»å¡žå¼è³‡æºåˆ†é…ç­–ç•¥ï¼Œå³ä½ ç„¡é ˆç«‹é¦¬æ¶ˆè²»é æœŸè«‹æ±‚çš„å…¨é‡è³‡æºçš„é€™æ¨£çš„æƒ…æ™¯ï¼‰ã€‚  
    >   
    > ç•¶ç„¶ï¼Œç¾åœ¨æ¡†æž¶é¡žæ‡‰ç”¨ï¼ˆåˆ†å¸ƒå¼ç³»çµ±ï¼‰ä¹Ÿå¯ä»¥ä½¿ç”¨Mesosæä¾›çš„è³‡æºå®Œæˆä»–å€‘è‡ªå·±çš„èª¿åº¦ï¼Œé€™ä¾¿æ˜¯æ‰€è¬‚çš„ ã€ŒäºŒæ¬¡è³‡æºèª¿åº¦ã€ã€‚
    > 
    > ![](https://i3.read01.com/SIG=2vu57os/3041705658323041.jpg)
    > 
    > æœ€çµ‚é”åˆ°çš„æ•ˆæžœå³æ˜¯ä½ ä¸‹ç™¼çš„ä¸€å€‹ä»»å‹™å¯ä»¥åœ¨æ•´å€‹æ•¸æ“šä¸­å¿ƒçš„ä»»æ„ä¸€å€‹åœ°æ–¹æäº¤ä¸¦ä¸”é‹è¡Œã€‚  
    >   
    > æ§‹å»ºé€™æ¨£çš„ã€ŒäºŒæ¬¡è³‡æºèª¿åº¦ã€ç³»çµ±çš„åŽŸå› åœ¨æ–¼å®ƒå¯ä»¥åœ¨åŒä¸€æ™‚é–“å…§æ”¯æŒå¤šå€‹åˆ†å¸ƒå¼ç³»çµ±ã€‚åŒæ¨£ä»¥ä¸Šé¢çš„ä¾‹å­ä¾†è§£é‡‹ï¼ŒMesosç‚ºSparkæä¾›å’Œåˆ†é…æ‰€éœ€çš„è³‡æºã€‚è€Œé€™è£¡ï¼ŒSparkå‰‡è² è²¬æ±ºç­–å’Œåˆ†é…é€™äº›å¯ç”¨è³‡æºåŽ»é‹è¡Œå¯¦éš›ä»»å‹™ï¼ˆå³å› ç‚ºå¯ç”¨çš„è³‡æºå¾—ä»¥æ»¿è¶³éœ€æ±‚ï¼Œæ‰€ä»¥æˆ‘æ‰èƒ½å¤ å¯¦éš›åŽ»é‹è¡Œé€™äº›mapä»»å‹™ï¼‰ã€‚
    > 
    > ![](https://i1.read01.com/SIG=2acvj6v/3041705658323042.jpg)
    > 
    > æ‰€ä»¥ä¸€æ—¦ä¸€å€‹ä»»å‹™è¢«æ¡†æž¶æ‡‰ç”¨æäº¤åˆ°Mesosï¼Œé‚£éº¼é€™äº›ä»»å‹™å°±å¿…é ˆè¢«å¯¦éš›åŸ·è¡Œã€‚Mesos master è² è²¬æŒ‡æ´¾ä»»å‹™çµ¦æ¯å€‹slaveï¼Œè€Œæ¯å€‹slaveé€šéŽä¸Šé¢è·‘è‘—çš„agentä¾†ç®¡ç†å’Œé‹è¡Œé€™äº›ä»»å‹™ã€‚ï¼ˆé€™å³æ˜¯èªªå¦‚æžœé€™å€‹ä»»å‹™æ˜¯å°æ‡‰çš„ä¸€å€‹å‘½ä»¤ï¼Œé‚£éº¼å®ƒæœƒåŽ»åŸ·è¡Œå®ƒï¼Œå¦‚æžœå®ƒéœ€è¦ä¸€äº›ç‰¹å®šçš„è³‡æºä¾†å®Œæˆé€™å€‹ä»»å‹™ï¼Œæ¯”å¦‚åƒjaråŒ…ï¼Œé‚£éº¼å®ƒæœƒå…ˆç²å–æ‰€éœ€çš„è³‡æºï¼Œç„¶å¾Œåœ¨ä¸€å€‹æ²™ç›’è£¡åŸ·è¡Œå®ƒï¼Œæœ€å¾Œæ‰ç™¼èµ·é€™å€‹ä»»å‹™ï¼‰  
    >   
    > æˆ–è€…èªªä½ ä¹Ÿå¯ä»¥é€™æ¨£ï¼Œæ¡†æž¶æ‡‰ç”¨å¯ä»¥é€šéŽä¸€å€‹åŸ·è¡Œå™¨ï¼ˆæ¡†æž¶æ‡‰ç”¨éœ€è¦ä¸€å€‹ä¸­é–“å±¤ï¼Œé€™å€‹ä¸­é–“å±¤å¯ä»¥ç”¨ä¾†å¤šç·šç¨‹åŸ·è¡Œä»»å‹™ï¼‰ä¾†éˆæ´»çš„æ±ºå®šå®ƒæƒ³è¦åŸ·è¡Œçš„ä»»å‹™ã€‚
    > 
    > ![](https://i2.read01.com/SIG=26t9vcu/3041705658323043.jpg)
    > 
    > ç‚ºäº†ä¿è­‰è³‡æºçš„ç›¸å°éš”é›¢æ€§ï¼ŒMesos å° Kernelçš„cgroupså’Œnamespaces æä¾›äº†å…§ç½®çš„åŽŸç”Ÿæ”¯æŒï¼Œç•¶ç„¶ä½ ä¹Ÿå¯ä»¥å°‡ä¸€å€‹Dockerå®¹å™¨ç•¶åšä¸€å€‹ä»»å‹™åŽ»é‹è¡Œã€‚é€™æ¨£ä¸€ä¾†ï¼Œå®ƒä¾¿çµ¦ä½ æä¾›äº†ä¸€å€‹å¤šç§Ÿæˆ¶çš„ï¼ˆæ¡†æž¶ï¼‰è³‡æºæ± çš„è¨ªå•æ©Ÿåˆ¶ï¼ˆè·¨ä¸»æ©Ÿå’Œä¸»æ©Ÿå…§éƒ¨çš„é€²ç¨‹é–“é€šä¿¡ï¼‰ã€‚  
    >   
    > ä½ å¯ä»¥é è«‹æ±‚ä½ æ‰€éœ€çš„è³‡æºï¼Œç•¶ç„¶é€™æ¨£ä½ ä¹Ÿå°±å›žåˆ°äº†è³‡æºå›ºå®šåŠƒåˆ†çš„æ™‚ä»£ã€‚å¦‚æžœä½ æœ‰ä¸€äº›æœ‰ç‹€æ…‹çš„æ‡‰ç”¨ï¼Œé‚£éº¼ä½ éœ€è¦é å®šä¸€äº›è³‡æºï¼ˆé€™é¡žä»»å‹™é€šå¸¸éœ€è¦åœ¨åŒä¸€å°ä¸»æ©Ÿä¸Šé‹è¡Œï¼‰ä¸¦ä¸”éœ€è¦ä¸€äº›æŒä¹…åŒ–çš„å­˜å„²å·ï¼ˆæ•¸æ“šéœ€è¦èƒ½å¤ æ”¯æŒæ•…éšœé·ç§»å’Œæ¢å¾©ï¼‰ï¼Œè€Œé€™é¡žéœ€æ±‚MesosåŒæ¨£èƒ½å¤ æ”¯æŒã€‚DCOSï¼ˆæ•¸æ“šä¸­å¿ƒä½œæ¥­ç³»çµ±ï¼‰å³æ˜¯Mesosçš„ã€Œæ ¸å¿ƒã€èˆ‡å…¶å‘¨é‚Šçš„æœå‹™åŠåŠŸèƒ½çµ„ä»¶æ‰€çµ„æˆçš„ä¸€å€‹ç”Ÿæ…‹ç³»çµ±ã€‚ä¾‹å¦‚åƒmesos-dnsé€™æ¨£çš„æ’ä»¶æ¨¡å¡Šï¼Œé¡žä¼¼ä¸€å€‹CLIï¼Œä¸€å€‹GUIåˆæˆ–è€…æ˜¯æä¾›ä½ æƒ³é‹è¡Œçš„æ‰€æœ‰çš„åŒ…çš„å€‰åº«ç­‰å·¥å…·ï¼Œä»¥åŠåƒMarathonï¼ˆåˆååˆ†å¸ƒå¼çš„initï¼‰ã€Chronosï¼ˆåˆååˆ†å¸ƒå¼çš„cronï¼‰é€™æ¨£çš„æ¡†æž¶ç­‰ç­‰ã€‚  
    > 
    > 

### Install Mesos

- [å®‰è£…ä¸Žä½¿ç”¨ Â· Docker â€”â€” ä»Žå…¥é—¨åˆ°å®žè·µ](https://yeasy.gitbooks.io/docker_practice/content/mesos/installation.html#)

- [é€šè¿‡Dockeræ¥éƒ¨ç½²Mesosé›†ç¾¤ - DockOne.io](http://dockone.io/article/136)

    > é€šè¿‡Dockeræ¥éƒ¨ç½²Mesosé›†ç¾¤
    > ==================
    > 
    > ã€ç¼–è€…çš„è¯ã€‘Apache Mesosç³»ç»Ÿæ˜¯ä¸€å¥—èµ„æºç®¡ç†è°ƒåº¦é›†ç¾¤ç³»ç»Ÿï¼Œå¯ä»¥ç”¨æ¥ç®¡ç†Dockeré›†ç¾¤ï¼Œæ¢ä¸ªæ€è·¯ï¼Œæœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡Dockerå®¹å™¨æ¥éƒ¨ç½²ä¸€ä¸ªå•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹çš„Mesosé›†ç¾¤ï¼Œæ•´ä¸ªè¿‡ç¨‹éžå¸¸ç®€å•ï¼Œåªéœ€è¦ä¸ƒä¸ªå‘½ä»¤å³å¯å®Œæˆï¼Œæ•´ä¸ªæ­¥éª¤ä½œè€…ä¹Ÿè®°å½•åˆ°äº†GitHubï¼ŒæŽ¨èå­¦ä¹ ã€‚  
    >   
    > è¿™ç¯‡æ–‡ç« å°†æ•™ä½ å¦‚ä½•ä½¿ç”¨Dockerå®¹å™¨éƒ¨ç½²ä¸€ä¸ªå•èŠ‚ç‚¹çš„[Mesos](http://mesosphere.com/)é›†ç¾¤ï¼Œæ•´ä¸ªéƒ¨ç½²è¿‡ç¨‹éžå¸¸ç®€å•ï¼Œåªéœ€è¦ä¸ƒä¸ªå‘½ä»¤ã€‚åœ¨éƒ¨ç½²ä¹‹å‰ä½ éœ€è¦å‡†å¤‡ä¸€ä¸ªè£…æœ‰Dockerçš„çŽ¯å¢ƒï¼Œè¿™ä¸ªéžå¸¸ç®€å•ï¼Œæˆ‘ä¸èµ˜è¿°ã€‚æˆ‘ä»¬æ€»å…±éœ€è¦å¯åŠ¨å››ä¸ªå®¹å™¨ï¼Œåˆ†åˆ«æ˜¯ï¼š  
    > 
    > -   ZooKeeper
    > -   Meso Master
    > -   Marathon
    > -   Mesos Slave Container
    > 
    >   
    > å¦‚ä¸Šé¢æåˆ°çš„ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªå¯ä»¥è¿è¡Œçš„Docker Serverï¼Œä½ å¯ä»¥é€šè¿‡ä»»ä½•ä½ å–œæ¬¢çš„æ–¹å¼æ¥èŽ·å¾—Dockerï¼Œæ¯”å¦‚åœ¨[æœ¬åœ°çš„Vagrantä¸­å®‰è£…Docker](https://docs.vagrantup.com/v2/provisioning/docker.html)ã€ä½¿ç”¨[Boot2Docker](http://boot2docker.io/)ã€ä½¿ç”¨[CoreOS](https://coreos.com/)æˆ–è€…åœ¨AWSå®‰è£…ã€‚æ•´ä¸ªçš„éƒ¨ç½²è¿‡ç¨‹æˆ‘éƒ½æ”¾åˆ°äº†[GitHub](https://github.com/sekka1/mesosphere-docker)ä¸Šäº†ï¼ŒåŒ…æ‹¬æ‰€æœ‰çš„å®¹å™¨æž„å»ºçš„Dockerfileæ–‡ä»¶ï¼Œ ä½ å¯ä»¥æœ¬åœ°åŽ»æž„å»ºè¿™äº›é•œåƒï¼Œ æˆ–è€…ä»ŽDocker Hubä¸Šä¸‹è½½å·²ç»æž„å»ºå¥½çš„é•œåƒã€‚æˆ‘ä»¬æ‰€ä½¿ç”¨çš„æ˜¯Docker Hubä¸Šçš„é•œåƒæ˜¯ï¼š  
    > 
    > -   [ZooKeeper](https://registry.hub.docker.com/u/garland/zookeeper/)
    > -   [Meso Master](https://registry.hub.docker.com/u/garland/mesosphere-docker-mesos-master/)
    > -   [Marathon](https://registry.hub.docker.com/u/garland/mesosphere-docker-marathon/)
    > 
    >   
    > 
    > #### éƒ¨ç½²æ­¥éª¤
    > 
    > **ç¬¬ä¸€æ­¥ï¼š**èŽ·å–Docker Serverçš„IPï¼Œå¹¶èµ‹å€¼åˆ°HOST_IPå˜é‡ä¸­ï¼Œåœ¨æŽ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­æˆ‘ä»¬è¿˜ä¼šç”¨åˆ°ã€‚  
    > 
    > root@docker-server:/#Â HOST_IP=10.11.31.7
    > 
    >   
    > **ç¬¬äºŒæ­¥ï¼š**å¯åŠ¨ZooKeeperå®¹å™¨ã€‚  
    > 
    > dockerÂ runÂ -dÂ \
    > -pÂ 2181:2181Â \
    > -pÂ 2888:2888Â \
    > -pÂ 3888:3888Â \
    > garland/zookeeper
    > 
    >   
    > **ç¬¬ä¸‰æ­¥ï¼š** å¯åŠ¨Mesos Masterã€‚  
    > 
    > dockerÂ runÂ --net="host"Â \
    > -pÂ 5050:5050Â \
    > -eÂ "MESOS\_HOSTNAME=${HOST\_IP}"Â \
    > -eÂ "MESOS\_IP=${HOST\_IP}"Â \
    > -eÂ "MESOS\_ZK=zk://${HOST\_IP}:2181/mesos"Â \
    > -eÂ "MESOS_PORT=5050"Â \
    > -eÂ "MESOS\_LOG\_DIR=/var/log/mesos"Â \
    > -eÂ "MESOS_QUORUM=1"Â \
    > -eÂ "MESOS\_REGISTRY=in\_memory"Â \
    > -eÂ "MESOS\_WORK\_DIR=/var/lib/mesos"Â \
    > -dÂ \
    > garland/mesosphere-docker-mesos-master
    > 
    >   
    > **ç¬¬å››æ­¥ï¼š** å¯åŠ¨Marathonã€‚  
    > 
    > dockerÂ runÂ -dÂ \
    > -pÂ 8080:8080Â \
    > garland/mesosphere-docker-marathonÂ --masterÂ zk://${HOST_IP}:2181/mesosÂ \
    > --zkÂ zk://${HOST_IP}:2181/marathon
    > 
    >   
    > **ç¬¬äº”æ­¥ï¼š** å¯åŠ¨Mesos Slaveã€‚  
    > 
    > dockerÂ runÂ -dÂ \
    > --nameÂ mesos\_slave\_1Â \
    > --entrypoint="mesos-slave"Â \
    > -eÂ "MESOS\_MASTER=zk://${HOST\_IP}:2181/mesos"Â \
    > -eÂ "MESOS\_LOG\_DIR=/var/log/mesos"Â \
    > -eÂ "MESOS\_LOGGING\_LEVEL=INFO"Â \
    > garland/mesosphere-docker-mesos-master:latest
    > 
    >   
    > **ç¬¬å…­æ­¥ï¼š** è®¿é—® Mesos é¡µé¢ã€‚  
    > Mesos Web é¡µé¢åœ°å€æ˜¯ï¼š  
    > 
    > http://${HOST_IP}:5050
    > 
    >   
    > 
    > [![mesos.png](http://dockone.io/uploads/article/20150112/8b66e0df677cdaea0990511ebbf940eb.png)](http://dockone.io/uploads/article/20150112/8b66e0df677cdaea0990511ebbf940eb.png)
    > 
    >   
    > **ç¬¬ä¸ƒæ­¥ï¼š** é€šè¿‡Marathonçš„Webé¡µé¢å¯åŠ¨ä¸€ä¸ªJobã€‚Marathon Webé¡µé¢åœ°å€æ˜¯ï¼šhttp://${HOST_IP}:8080ã€‚  
    >   
    > Marathon å¯ä»¥è®©ä½ éƒ¨ç½²é•¿æœŸè¿è¡Œçš„Jobåˆ°Mesos Slaveå®¹å™¨ä¸Šï¼Œ è¿™ä¸ªå¯ä»¥å¸®åŠ©ä½ åŽ»æ£€æŸ¥ä½ çš„é›†ç¾¤æ˜¯å¦å¯åŠ¨ï¼Œå¹¶ä¸”å¤„äºŽrunningçš„çŠ¶æ€ï¼Œ æ‰“å¼€ä¸Šé¢çš„åœ°å€åŽä½ ä¼šçœ‹åˆ°ä¸‹é¢çš„é¡µé¢ï¼š  
    > 
    > [![marathon1.png](http://dockone.io/uploads/article/20150112/96a351240ed6bdddc7570710e65c8025.png)](http://dockone.io/uploads/article/20150112/96a351240ed6bdddc7570710e65c8025.png)
    > 
    >   
    > ç‚¹å‡»å³ä¸Šè§’çš„â€œNew Appâ€æŒ‰é’®ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„Job/Taskï¼Œ æˆ‘ä»¬è¿™é‡Œåªæ˜¯è¾“å‡ºä¸€ä¸ªâ€helloâ€œåˆ°æ–‡ä»¶"/tmp/output.txt"é‡Œé¢ï¼Œç„¶åŽæˆ‘ä»¬å¯ä»¥åˆ°å®¹å™¨ä¸­æŸ¥çœ‹æ–‡ä»¶æ˜¯å¦åˆ›å»ºï¼Œå¹¶æ£€æŸ¥ä¸‹è¿™ä¸ªJobæ˜¯ä¸æ˜¯ä¸€ç›´åœ¨è¿è¡Œã€‚  
    > 
    > [![marathon2.png](http://dockone.io/uploads/article/20150112/4fbac4e14542397efd8a12018e7e90af.png "marathon2.png")](http://dockone.io/uploads/article/20150112/4fbac4e14542397efd8a12018e7e90af.png)
    > 
    >   
    > **ç¬¬å…«æ­¥ï¼š** æ£€æŸ¥Job/Taskæ˜¯ä¸æ˜¯åœ¨è¿è¡Œ  
    > æŽ¥ä¸‹æ¥è®©æˆ‘ä»¬æ£€æŸ¥ä¸‹Job/Taskæ˜¯ä¸æ˜¯ä¸€ç›´åœ¨Mesos Slaveä¸Šé¢è¿è¡Œã€‚  
    > åœ¨Docker Serverä¸Šè¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œ è¿™ä¸ªå‘½ä»¤ä¼šè®©ä½ è¿›åˆ°Mesos Slave å®¹å™¨ä¸­ï¼Œç„¶åŽå†ä½¿ç”¨`tail`å‘½ä»¤æŸ¥çœ‹/tmp/output.txtæ–‡ä»¶é‡Œé¢çš„å†…å®¹ã€‚  
    > 
    > dockerÂ execÂ -itÂ mesos\_slave\_1Â /bin/bash
    > root@ca83bf0ea76a:/#Â tailÂ -fÂ /tmp/output.txt
    > 
    >   
    > ä½ ä¼šçœ‹åˆ°æ¯éš”ä¸€ç§’é’Ÿâ€œhelloâ€å°±ä¼šè¢«è¿½åŠ åˆ°è¿™ä¸ªæ–‡ä»¶ä¸­ã€‚  
    >   
    > æ³¨æ„ï¼šå¤šèŠ‚ç‚¹çš„MesosçŽ¯å¢ƒéƒ¨ç½²æ­¥éª¤è¯·å‚è€ƒ[è¿™é‡Œ](https://github.com/sekka1/mesosphere-docker#multi-node-setup)ã€‚  
    >   
    > **åŽŸæ–‡é“¾æŽ¥ï¼š[Deploy a Mesos Cluster with 7 Commands Using Docker](https://medium.com/@gargar454/deploy-a-mesos-cluster-with-7-commands-using-docker-57951e020586)ï¼ˆç¿»è¯‘ï¼šå·¦ä¼Ÿ æ ¡å¯¹ï¼šæŽé¢–æ°ï¼‰**
    > 



### Using Mesos with nvidia-docker for GPU cluster
    
- [Apache Mesos - Nvidia GPU Support](https://mesos.apache.org/documentation/latest/gpu-support/)

    > Overview
    > --------
    > 
    > Mesos exposes GPUs as a simple `SCALAR` resource in the same way it always has for CPUs, memory, and disk. That is, a resource offer such as the following is now possible:
    > 
    > ```
    > cpus:8; mem:1024; disk:65536; gpus:4;
    > 
    > ```
    > 
    > However, unlike CPUs, memory, and disk, _only_ whole numbers of GPUs can be selected. If a fractional amount is selected, launching the task will result in a `TASK_ERROR`.
    > 

    > Agent Flags
    > -------------------------------------------------------------------------------------
    > 
    > The following isolation flags are required to enable Nvidia GPU support on an agent.
    > 
    > ```
    > --isolation="filesystem/linux,cgroups/devices,gpu/nvidia"
    > 
    > ```
    > 
    > The `filesystem/linux` flag tells the agent to use Linux-specific commands to prepare the root filesystem and volumes (e.g., persistent volumes) for containers that require them.
    > 
    > The `cgroups/devices` flag tells the agent to restrict access to a specific set of devices for each task that it launches (i.e., a subset of all devices listed in `/dev`). When used in conjunction with the `gpu/nvidia` flag, the `cgroups/devices` flag allows us to grant / revoke access to specific GPUs on a per-task basis.
    > 



    > if you want to exclude a specific GPU device because an unwanted Nvidia graphics card is listed alongside a more powerful set of GPUs. When this is required, the following additional agent flags can be used to accomplish this:
    > 
    > ```
    > --nvidia_gpu_devices="<list_of_gpu_ids>"
    > 
    > --resources="gpus:<num_gpus>"
    > 
    > ```
    > 
    > For the `--nvidia_gpu_devices` flag, you need to provide a comma separated list of GPUs, as determined by running `nvidia-smi` on the host where the agent is to be launched ([see below](https://mesos.apache.org/documentation/latest/gpu-support/#external-dependencies) for instructions on what external dependencies must be installed on these hosts to run this command). Example output from running `nvidia-smi` on a machine with four GPUs can be seen below:
    > 
    > ```
    > +------------------------------------------------------+
    > | NVIDIA-SMI 352.79     Driver Version: 352.79         |
    > |-------------------------------+----------------------+----------------------+
    > | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    > | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    > |===============================+======================+======================|
    > |   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
    > | N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
    > +-------------------------------+----------------------+----------------------+
    > |   1  Tesla M60           Off  | 0000:05:00.0     Off |                    0 |
    > | N/A   35C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
    > +-------------------------------+----------------------+----------------------+
    > |   2  Tesla M60           Off  | 0000:83:00.0     Off |                    0 |
    > | N/A   38C    P0    40W / 150W |     34MiB /  7679MiB |      0%      Default |
    > +-------------------------------+----------------------+----------------------+
    > |   3  Tesla M60           Off  | 0000:84:00.0     Off |                    0 |
    > | N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |     97%      Default |
    > +-------------------------------+----------------------+----------------------+
    > 
    > ```
    > 
    > The GPU `id` to choose can be seen in the far left of each row. Any subset of these `ids` can be listed in the `--nvidia_gpu_devices` flag (i.e., all of the following values of this flag are valid):
    > 
    > ```
    > --nvidia_gpu_devices="0"
    > --nvidia_gpu_devices="0,1"
    > --nvidia_gpu_devices="0,1,2"
    > --nvidia_gpu_devices="0,1,2,3"
    > --nvidia_gpu_devices="0,2,3"
    > --nvidia_gpu_devices="3,1"
    > etc...
    > 
    > ```
    > 
    > For the `--resources=gpus:<num_gpus>` flag, the value passed to `<num_gpus>` must equal the number of GPUs listed in `--nvidia_gpu_devices`. If these numbers do not match, launching the agent will fail.
    > 


    > ### Minimal Setup With Support for Docker Containers[](https://mesos.apache.org/documentation/latest/gpu-support/#minimal-setup-with-support-for-docker-containers)
    > 
    > The commands below show a minimal example of bringing up a GPU-capable Mesos cluster on `localhost` and running a docker container on it. The required agent flags are set as described above, and the `mesos-execute` command has been told to enable the `GPU_RESOURCES` framework capability so it can receive offers containing GPU resources. Additionally, the required flags to enable support for docker containers (as described [here](https://mesos.apache.org/documentation/latest/container-image/)) have been set up as well.
    > 
    > ```
    > $ mesos-master \
    >       --ip=127.0.0.1 \
    >       --work_dir=/var/lib/mesos
    > 
    > $ mesos-agent \
    >       --master=127.0.0.1:5050 \
    >       --work_dir=/var/lib/mesos \
    >       --image_providers=docker \
    >       --executor_environment_variables="{}" \
    >       --isolation="docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia"
    > 
    > $ mesos-execute \
    >       --master=127.0.0.1:5050 \
    >       --name=gpu-test \
    >       --docker_image=nvidia/cuda \
    >       --command="nvidia-smi" \
    >       --framework_capabilities="GPU_RESOURCES" \
    >       --resources="gpus:1"
    > 
    > ```
    > 
    > If all goes well, you should see something like the following in the `stdout` out of your task.
    > 
    > ```
    > +------------------------------------------------------+
    > | NVIDIA-SMI 352.79     Driver Version: 352.79         |
    > |-------------------------------+----------------------+----------------------+
    > | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    > | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    > |===============================+======================+======================|
    > |   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
    > | N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
    > +-------------------------------+----------------------+----------------------+
    > 
    > ```
    > 


- [åŸºäºŽ Mesosã€Docker å’Œ Nvidia GPU çš„æ·±åº¦å­¦ä¹ å¹³å°å®žè·µ-DockerInfo](http://www.dockerinfo.net/3697.html)

    > NVIDIA GPU å¯ä»¥å¤§å¤§åŠ å¿« Deep Learning ä»»åŠ¡çš„è¿è¡Œé€Ÿåº¦ï¼›åŒæ—¶ï¼ŒGPUèµ„æºåˆæ˜¯ååˆ†æ˜‚è´µçš„ï¼Œéœ€è¦å°½å¯èƒ½æé«˜ GPU èµ„æºçš„åˆ©ç”¨çŽ‡ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨ [Mesos](http://www.dockerinfo.net/docker/mesos "æ›´å¤šå…³äºŽ Mesos çš„æ–‡ç« ") å°† GPU èµ„æºæ±‡èšæˆèµ„æºæ± æ¥å®žçŽ°èµ„æºå…±äº«ï¼Œå¹¶å€Ÿç”¨ Docker äº¤ä»˜æ·±åº¦å­¦ä¹ çš„ runtime çŽ¯å¢ƒï¼Œå¾ˆå¥½çš„è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚
    > 
    > å¹³å°æž¶æž„
    > ----
    > 
    > è¿™é‡Œä»ŽèŠ‚ç‚¹å†…éƒ¨å’Œé›†ç¾¤ä¸¤å±‚ä»‹ç»å¹³å°çš„æž¶æž„ã€‚
    > 
    > åœ¨èŠ‚ç‚¹å†…éƒ¨ï¼Œæˆ‘ä»¬åˆ©ç”¨nvidia-docker ( https://github.com/NVIDIA/nvidia-docker ) æ¥å¸®åŠ©å®¹å™¨å†…éƒ¨çš„ç¨‹åºè°ƒç”¨å¤–é¢ä¸»æœºä¸Šçš„CUDA driverã€‚å¦‚ä¸‹å›¾ä¸€æ‰€ç¤ºï¼ŒCUDA Driver åŠGPU Driverå®‰è£…åœ¨å¤–éƒ¨Host ä¸Šï¼›CUDA Toolkitã€å…¶å®ƒæ·±åº¦å­¦ä¹ ç»„ä»¶åŠç”¨æˆ·åº”ç”¨ç¨‹åºè¿è¡Œåœ¨Docker å®¹å™¨ä¸­ã€‚è¿™æ ·æ—¢èƒ½å¿«é€Ÿé…ç½®çŽ¯å¢ƒï¼Œåˆä¿è¯äº†HOSTä¸å—ç”¨æˆ·åº”ç”¨ç¨‹åºæ±¡æŸ“ã€‚
    > 
    > ![20161115160457](http://img.dockerinfo.net/2016/11/20161115160457.jpg)
    > 
    > 
    > åœ¨æ•´ä¸ªé›†ç¾¤å±‚é¢ï¼Œåˆ©ç”¨Mesoså°†èµ„æºæ± åŒ–ï¼Œç”¨æˆ·å¯é€šè¿‡Marathonç»Ÿä¸€å…¥å£éƒ¨ç½²è‡ªå·±çš„æ·±åº¦å­¦ä¹ Docker Packageã€‚å¦‚ä¸‹å›¾äºŒæ‰€ç¤ºï¼Œé›†ç¾¤åŒ…æ‹¬3ä¸ªMesos ç®¡ç†ï¼ˆMasterï¼‰èŠ‚ç‚¹å’Œå¤šä¸ªå·¥ä½œï¼ˆslaveï¼‰èŠ‚ç‚¹ï¼Œä»»åŠ¡åˆ†å‘åˆ°slaveåŽï¼Œslaveé€šè¿‡ executor ä¸Ž Docker daemon é€šä¿¡ï¼ŒDocker daemon æŒ‰éœ€ä»Žå†…éƒ¨é•œåƒä»“åº“æ‹‰å–å¹¶åŠ è½½Docker imageã€‚å¯¹åº”åˆ°çœŸå®žåº”ç”¨åœºæ™¯ï¼Œå¸¦æœ‰æ·±åº¦å­¦ä¹ ä¾èµ–ç»„ä»¶çš„ Docker image å°†è¢«pull&runï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡sshæˆ–è€…jupyterçš„webå…¥å£ä¸Žç»„ä»¶è¿›è¡Œäº¤äº’ã€‚
    > 
    > ![20161115160511](http://img.dockerinfo.net/2016/11/20161115160511.jpg)

- [klueska-mesosphere/mesos-gpu-docker](https://github.com/klueska-mesosphere/mesos-gpu-docker)
    
    > Clone this repo on an Nvidia GPU equipped machine with at least 2 GPUs (g2.8xlarge instances will do nicely).
    > 
    > Then run:
    > 
    > ```
    > ./build.sh
    > ./run.sh
    > ./deploy-tasks.sh
    > 
    > ```
    > 
    > This will launch 2 marathon applications. The first application runs `nvidia-smi` every 60 seconds without a docker container, and the second one runs `nvidia-smi` every 60 seconds inside an `nvidia/cuda` container.

- [Tutorial: Deep learning with TensorFlow, Nvidia and Apache Mesos (DC/OS), Part 1 - Mesosphere](https://mesosphere.com/blog/tutorial-deep-learning-with-tensorflow-nvidia-and-apache-mesos-dcos-part-1/)

    [Part 1: Deep Learning w/ TensorFlow, Nvidia & DC/OS (Apache Mesos) - YouTube](https://www.youtube.com/watch?v=hrXiqKGb7OQ&feature=youtu.be)

    > -   Tutorial #1: Run a Tensor flow Docker image on your laptop and run a machine learning model with and without GPUs.
    >     
    > -   Tutorial #2: Run a Tensoflow Docker image on a DC/OS cluster with and without GPUs. See GPU isolation and Jupyter in action.
    >     
    > -   Tutorial #3: Deploy a dynamic, distributed TensorFlow on DC/OS from the Universe. See how TensorFlow on DC/OS dynamically consumes and releases resources on the cluster when done. Run multiple TensorFlows on the same cluster with different resource requirements.


- [Docker on Mesos - Mesosphere](https://mesosphere.com/blog/docker-on-mesos/)

- [Tutorial: Deep learning with TensorFlow, Nvidia and Apache Mesos (DC/OS), Part 2 - Mesosphere](https://mesosphere.com/blog/tutorial-deep-learning-with-tensorflow-nvidia-and-apache-mesos-dcos-part-2/)

    [Part 2: Deep Learning w/ TensorFlow, Nvidia & DC/OS (Apache Mesos) - YouTube](https://www.youtube.com/watch?v=VzHVM0SCF44)
    
    > ### Run TensorFlow on DC/OS with GPUs
    > 
    > #### Deploy the TensorFlow service with GPUs
    > 
    > Now that youâ€™ve got TensorFlow examples running on your cluster, letâ€™s see how performance compares when you configure your service to use GPUs.
    > 
    > 1.  Go to the Services tab of the DC/OS UI.
    > 2.  Click **+** to add a service.
    > 3.  Choose **Single Container**.
    > 4.  Toggle to the **JSON Editor** and paste the following [application definition](https://docs.mesosphere.com/1.9/deploying-services/creating-services/) into the editor.
    >     
    >     {
    >       "id": "tensorflow-gpus-1",
    >       "acceptedResourceRoles": \["slave_public"\],
    >       "cpus": 4,
    >       "gpus": 4,
    >       "mem": 2048,
    >       "disk": 0,
    >       "instances": 1,
    >       "container": {
    >         "type": "MESOS",
    >         "docker": {
    >           "image": "tensorflow/tensorflow:latest-gpu"
    >         }
    >       }
    >     }
    >     
    >     This application definition is largely the same as the last one, except, here, youâ€™re requesting 4 GPUs and specifying the TensorFlow Docker image thatâ€™s configured for GPUs.
    >     
    > 5.  Click **Review and Run**, then **Run Service**.
    > 
    > #### Verify access to GPUs
    > 
    > Youâ€™ll recall that we created a cluster with a public agent that has 8 GPUs, but only requested access to 4. Letâ€™s verify that the node has 8 GPUs, and that our service has access to only 4 of them.
    > 
    > 1.  First, use `dcos task exec` to run a command inside of the container to get the public IP address of the agent node the container is running on.
    >     
    >     dcos task exec tensorflow-gpus-1 curl -s ifconfig.co
    >     
    > 2.  Now, use that public IP to SSH into the node and run `nvidia-smi` to verify the number of GPUs the node has.
    >     
    >     ssh <public-ip> nvida-smi
    >     
    >     You should see 8 GPUs installed and running on the machine. The container for your service, however, should only be able to see 4 of those GPUs.
    >     
    > 3.  Run `dcos task exec` with the `bash` option to get a shell inside of your serviceâ€™s container.
    >     
    >     dcos task exec -it tensorflow-gpus-1 bash
    >     
    > 4.  Set up environment variables so you can run `nvida-smi` from within this shell.
    >     
    >     export LD\_LIBRARY\_PATH=/usr/local/nvidia/lib64 export PATH=$PATH:/usr/local/nvidia/bin
    >     
    > 5.  Run `nvidia-smi` to verify that even though you have 8 GPUs installed on the _machine_, you only have access to four of them inside this container.
    >     
    >     nvidia-smi
    >     
    > 
    > #### Run a TensorFlow example with GPUs
    > 
    > Now that youâ€™ve installed TensorFlow and verified your access to 4 GPUs, letâ€™s run the same example as before.
    > 
    > 1.  If you exited the `tensorflow-gpus-1` container, reenter it and set up the environment variables by following the steps in the last section.
    > 2.  Install git and clone the TensorFlow-Examples repository.
    >     
    >     apt-get update; apt-get install -y git git clone https://github.com/aymericdamien/TensorFlow-Examples
    >     
    > 3.  Run and time the same example you ran earlier, the convolutional network example.
    >     
    >     cd TensorFlow-Examples/examples/3\_NeuralNetworks time python convolutional\_network.py
    >     
    > 4.  Watch the code find the GPUs and execute.
    > 
    > This took my DC/OS cluster about 2 minutes: about 5 times faster than before!
    > 
    > #### Launch Two TensorFlow Instances
    > 
    > Youâ€™ll recall that we have a cluster with _8_ GPUs, but we only requested access to 4 of them. Now, letâ€™s launch a second TensorFlow instance that will consume the remaining 4 GPUs in parallel with the first.
    > 
    > Running more than one TensorFlow instance in parallel shows that you can have multiple users on the same cluster with _isolated access_ to the GPUs on it.
    > 
    > 1.  Add a third service to your DC/OS cluster with the following application definition, which is similar to the first application definition with GPUs.
    >     
    >     {
    >       "id": "tensorflow-gpus-2",
    >       "acceptedResourceRoles": \["slave_public"\],
    >       "cpus": 4,
    >       "gpus": 4,
    >       "mem": 2048,
    >       "disk": 0,
    >       "instances": 1,
    >       "container": {
    >         "type": "MESOS",
    >         "docker": {
    >           "image": "tensorflow/tensorflow:latest-gpu"
    >         }
    >       }
    >     }
    >     
    > 2.  Verify that your second TensorFlow instance is running by accessing the Jupyter notebook that runs by default on the TensorFlow Docker image. In the application definition above, the `acceptedResourceRoles` parameter is set to `slave_public`, which gives us access to the public IP of the agents where the containers are running.
    >     1.  Get the public IP of the agent where the task has been launched.
    >         
    >         dcos task exec tensorflow-gpus-2 curl -s ifconfig.co
    >         
    >     2.  Go to the STDERR log of the service to get the Jupyter URL. **Services** \> **tensorflow-gpus-2** \> **task-id** \> **paper icon** \> **ERROR (STDERR)**. You will see this a message similar to the following.
    >         
    >          Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:10144/?token=d4f3d8f80eb97299e74b5254d1600c480c3f042d548e51f5        
    >         
    >     3.  Replace `localhost` with the public IP you found earlier to see the Jupyter notebook.
    >     4.  Click the `Getting Started` notebook and run some commands.
    > 
    > Thanks for playing along at home!


- [douban/tfmesos: Tensorflow in Docker on Mesos #tfmesos #tensorflow #mesos](https://github.com/douban/tfmesos)


- [Training Your Custom Model with the DC/OS TensorFlow package - Mesosphere](https://mesosphere.com/blog/tensorflow-custom-model/)

- [Distributed TensorFlow with GPU Support on Mesosphere DC/OS](https://mesosphere.com/blog/tensorflow-gpu-support-deep-learning/)

    <iframe width="560" height="315" src="https://www.youtube.com/embed/vsyU9R3y7VA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

    > ### Benefits of running Distributed TensorFlow on DC/OS
    > 
    > The new beta release of TensorFlow on DC/OS helps solve each of the problems outlined above and more. Specifically it helps to:
    > 
    > 1.  **Simplify the deployment of distributed TensorFlow:** Deploying a distributed TensorFlow cluster with all of its components on any infrastructure, whether itâ€™s baremetal, virtual or public cloud is as simple as passing a JSON file to a single CLI command. Updating and tweaking parameters to fine-tune and optimize becomes trivial.
    > 2.  **Share infrastructure across teams:** DC/OS allows multiple teams to share the infrastructure and launch multiple TensorFlow jobs while maintaining complete resource isolation. Once a TensorFlow job is done, capacity is released and made available to other teams.
    > 3.  **Deploy different TensorFlow versions on the same cluster:** As with many DC/OS services, you can easily deploy multiple instances of a services, each with a different version, on the same cluster. This means that when a new version of TensorFlow is released, one team can take advantage of the latest features and capabilities without running the risk of breaking another teamâ€™s code.
    > 4.  **Allocate GPUs dynamically:** GPUs greatly increase the speed of deep learning models, especially during training. However, GPUs are precious resources that must be efficiently utilized. Since DC/OS automatically detects all GPUs on a cluster, [GPU-based scheduling](https://mesosphere.com/blog/accelerating-machine-learning-with-gpus-and-dcos/) can be used to allow TensorFlow to request all or some of the GPU resources on a per job basis (similar to requesting CPU, memory, and disk resources). Once the job is complete, the GPU resources are released and made available to other jobs.
    > 5.  **Focus on model development, not deployment:** DC/OS separates the model development from the cluster configuration by eliminating the need to manually introduce a ClusterSpec in the model code. Instead, the person deploying the TensorFlow package specifies the properties of the various workers and parameter servers they would like their model to run with, and the package generates a unique ClusterSpec for it at deploy-time. Under the hood the package finds a set of machines to run each worker / parameter server on, populates a ClusterSpec with the appropriate values, starts each parameter server and worker task, and passes it the generated ClusterSpec. The developer simply writes his code expecting this object to be populated, and the package takes care of the rest.The figure below shows a JSON snippet that can be used to deploy a TensorFlow package from the DC/OS CLI with a mix of CPU and GPU workers.![It's easy to deploy distributed TensorFlow on DC/OS using the CLI.](https://mesosphere.com/wp-content/uploads/2017/10/tensorflow-dcos-7.png?v2)
    >     
    >     The command to launch TensorFlow with this config would be:
    >     
    >     dcos package install beta-tensorflow --options=<path/to/config.json>
    >     
    >     The package can also be deployed from the DC/OS service catalog by specifying these parameters in the UI.
    >     
    > 6.  **Automate failure recovery:** The TensorFlow package is written using the DC/OS SDK and leverages built-in resiliency features including automatic restart so that failed tasks effectively self-heal.
    > 7.  **Deploy job configuration parameters securely at runtime:** The DC/OS secrets service dynamically deploys credentials and confidential configuration options to each TensorFlow instance at runtime. Operators can easily add credentials to access confidential information or specific configuration URLs without exposing them in the model code.
    > 
    > 


- [Day1: ä½¿ç”¨Apache Mesosçš„ç›®çš„ç‚ºä½•ï¼Ÿ - iT é‚¦å¹«å¿™::ä¸€èµ·å¹«å¿™è§£æ±ºé›£é¡Œï¼Œæ‹¯æ•‘ IT äººçš„ä¸€å¤©](https://ithelp.ithome.com.tw/articles/10184643)

    > **å®ƒèƒ½å¹«åŠ©æˆ‘å€‘åšåˆ°å“ªäº›äº‹æƒ…ï¼Ÿ**
    > 
    > ![http://ithelp.ithome.com.tw/upload/images/20161201/20103456RrxGTPlpbk.png](http://ithelp.ithome.com.tw/upload/images/20161201/20103456RrxGTPlpbk.png)
    > 
    > ä¸Šåœ–æ˜¯ä¸€å€‹åœ¨å‚³çµ±éƒ¨ç½²serviceæœ€ç°¡å–®çš„æž¶æ§‹ï¼Œå¦‚æœ‰å››å°æ©Ÿå™¨æœ‰2å°æ˜¯éƒ¨ç½²Tomcat Serviceå¦å¤–2å°æ˜¯Jetty Serviceï¼Œåœ¨éƒ¨ç½²é€™äº›serviceä¸Šç³»çµ±ç®¡ç†è€…å¿…é ˆè¦æ¸…ç¤Žçš„çŸ¥é“å“ªäº›serverä¸Šæœ‰å“ªäº›Serviceï¼Œä¸¦ä¸”è¦äº†è§£åˆ°æ¯å°serverä¸Šçš„è³‡æºï¼Œå¦‚æžœé€™æ™‚æœ‰æ–°çš„éœ€æ±‚å¦‚è¦åœ¨åŠ å…¥ä¸€å°æ–°çš„Jetty Serviceï¼Œé€™æ™‚ç³»çµ±ç®¡ç†å“¡æœƒåœ¨é€™å››å°æ©Ÿå™¨ä¸­é¸æ“‡ä¸€å°è³‡æºä½¿ç”¨çŽ‡è¼ƒä½Žçš„serveré€²è¡Œéƒ¨ç½²ã€‚ä¹‹å¾Œhost1æœ‰å¯èƒ½å› ç‚ºç¡¬é«”æ•…éšœå°Žè‡´Tomcat Serviceç„¡æ³•å•Ÿå‹•ï¼Œè€Œå¤§å®¶åªèƒ½é€£åˆ°host2æœ‰å¯èƒ½å°Žè‡´æµé‡éŽå¤§è€Œé€ æˆhost2è¨˜æ†¶é«”ä¸å¤ è€Œç•¶æ©Ÿï¼Œä¸€é€£ä¸²çš„æƒ¡æ€§å¾ªç’°è€Œé€ æˆç³»çµ±ç®¡ç†å“¡éŽç´¯å·¥ä½œæ•ˆçŽ‡é™ä½Žï¼Œè€Œä¸‹åœ–æ”¹ç”¨Apache Mesosçš„æž¶æ§‹ä¾†è§£æ±ºæ­¤å•é¡Œï¼š  
    > ![http://ithelp.ithome.com.tw/upload/images/20161201/20103456XODssn8JE9.png](http://ithelp.ithome.com.tw/upload/images/20161201/20103456XODssn8JE9.png)
    > 
    > æ”¹æˆApache Mesosæž¶æ§‹æœƒå¦‚ä¸Šåœ–ï¼ŒMesosè©³ç´°çš„æž¶æ§‹åœ¨æœªä¾†é‚„æœƒç¹¼çºŒä»‹ç´¹ï¼Œä»¥ä¸Šåœ–ä¾†èªªåœ¨å»ºæ§‹Tomcat Serviceå’ŒJetty Serviceçš„æ“ä½œæ˜¯é€éŽMesos Frameworkå¦‚Marathonçš„æ–¹å¼ï¼Œæˆ‘å€‘åªè¦é€éŽrestfulçš„æ–¹å¼æŠŠå¯«å¥½çš„jsonè¨­å®šå‚³åˆ°Marathonå°±å¯ä»¥åŸ·è¡Œå•Ÿå‹•serviceï¼Œjsonçš„è¨­å®šæª”å…§å®¹å¦‚ä¸‹ï¼ŒMarathon Frameworkä¹Ÿåœ¨æœªä¾†æœƒåšä»‹ç´¹ï¼Œç¾åœ¨åªæ˜¯ç°¡å–®demo mesoså°æˆ‘å€‘ä½¿ç”¨ä¸Šçš„æ–¹ä¾¿æ€§ã€‚
    > 
    > **èƒ½å¤ ä½¿æˆ‘å€‘å¾—åˆ°å“ªäº›å¥½è™•å‘¢ï¼Ÿ**
    > 
    > ![http://ithelp.ithome.com.tw/upload/images/20161201/20103456lZY8QVdJ2Q.png](http://ithelp.ithome.com.tw/upload/images/20161201/20103456lZY8QVdJ2Q.png)
    > 
    > åŸ·è¡Œrestfulçš„æŒ‡ä»¤å¦‚ä¸‹ï¼š
    > 
    > ```
    > curl -X POST -H "Content-type: application/json" http://172.17.0.4:8080/v2/apps -d @Tomcat.json
    > ```
    > 
    > ä¸Šé¢çš„jsonè¨­å®šå¯ä»¥è¨­å®štomcat serviceè¦åŸ·è¡Œåœ¨2å°çš„serverä¸Šï¼Œå¦‚æžœæœ‰ä¸€å°çš„serviceè¢«ç æŽ‰æˆ–ç•¶æŽ‰æœƒåœ¨å…¶å®ƒçš„serverä¸Šå•Ÿå‹•serviceã€å¯ä»¥è¨­å®šè¦åœ¨å“ªå¹¾å°ç‰¹å®šçš„serverä¸ŠåŸ·è¡Œserviceã€å¦å¤–ä¹Ÿå¯ä»¥åŽ»æŒ‡å®šserviceéœ€è¦å¤šå°‘çš„è³‡æº(å¦‚ï¼šCPU coreæ•¸ã€è¨˜æ†¶é«”å¤§å°)â€¦ç­‰ç­‰çš„åŠŸèƒ½ï¼Œé€™äº›åœ¨æœªä¾†éƒ½æœƒåšä»‹ç´¹ï¼Œåœ¨éƒ¨ç½²serverä¸Šåªè¦æŠŠè¨­å®šæª”å¯«å¥½å°±å¯ä»¥å¿«é€Ÿçš„æŠŠserviceå•Ÿå‹•èµ·ä¾†ï¼Œè®“æˆ‘å€‘çš„å·¥ä½œæ•ˆçŽ‡å¢žåŠ ä¹ŸæœŸæœ›è®“æˆ‘å€‘åœ¨éƒ¨ç½²serviceå’Œç®¡ç†æ•´å€‹clusterçš„è³‡æºæ›´åŠ çš„è‡ªå‹•åŒ–ã€‚
    > 



## HyperPilot
- [å®¹å™¨å¢é›†æ•ˆèƒ½æˆæ–°è­°é¡Œï¼ŒHyperPiloté–‹æºé‡‹å‡ºï¼Œè¦é æ©Ÿå™¨å­¸ç¿’è‡ªå‹•å„ªåŒ–å®¹å™¨å¢é›† | iThome](https://www.ithome.com.tw/news/121876)

    > è—‰åŠ©å®¹å™¨å¢é›†ï¼Œä¾†æ”¯æ´å¤§æ•¸æ“šæˆ–æ©Ÿå™¨å­¸ç¿’åˆ†æžæ‰€éœ€çš„å¤§åž‹å¢é›†ï¼Œå·²æ˜¯è³‡æ–™ç§‘å­¸å®¶å¸¸ç”¨çš„æ–¹æ³•ä¹‹ä¸€ï¼Œä½†æƒ³è¦è‡ªå·±åœ¨é›²ç«¯éƒ¨ç½²Dockerå®¹å™¨å¢é›†ï¼Œç¬¬ä¸€å€‹æŒ‘æˆ°å°±æ˜¯ï¼Œè¦æ€Žéº¼ç§Ÿç”¨VMï¼Œæ‰èƒ½çœéŒ¢åˆèƒ½ç¬¦åˆéœ€è¦çš„æ•ˆèƒ½ï¼Ÿå¾—åŒæ™‚è€ƒé‡VMè¦æ ¼ã€å®¹å™¨é…ç½®å’Œæ‡‰ç”¨ç¨‹å¼çš„é…ç½®éœ€æ±‚ç­‰å¤šé …è¤‡é›œè®Šå› ï¼Œä¾†è¡¡é‡æˆæœ¬ï¼Œå¾€å¾€å¾—é è³‡æ·±çš„é›²ç«¯æž¶æ§‹å¸«æ‰æœ‰èƒ½åŠ›æ‹¿æå¾—ç•¶ã€‚
    > 
    > å‰Mesosphereåˆ†æ•£å¼ç³»çµ±é¦–å¸­å·¥ç¨‹å¸«Timothy Chen[è¿‘æ—¥é–‹æºé‡‹å‡º](https://medium.com/@tnachen/hyperpilot-open-sourced-100-of-its-products-18d0e018fe45)äº†ä¸€å¥—å®¹å™¨å¢é›†é…ç½®è‡ªå‹•åŒ–å·¥å…·Hyperpilotï¼Œé€™æ˜¯åˆ©ç”¨æ©Ÿå™¨å­¸ç¿’æŠ€è¡“ä¸­çš„Bayesian Optimizationæœ€ä½³åŒ–æŠ€è¡“ï¼Œä¾æ“šä½¿ç”¨è€…æä¾›çš„æ¢ä»¶ï¼Œä¾†æ‰¾å‡ºæœ€ä½³çš„å®¹å™¨å¢é›†æ‰€éœ€è¦çš„VMæž¶æ§‹é…ç½®ï¼Œç›®å‰åªèƒ½é‡å°AWSä¸Šçš„VMè¦æ ¼ç‚ºå„ªåŒ–å°è±¡ã€‚
    > 
    > é€™å¥—å·¥å…·é‚„æä¾›äº†ä¸€å€‹è³‡æºç“¶é ¸åˆ†æžå·¥å…·HyperPathï¼Œå¯ä»¥å¾žCPUã€è¨˜æ†¶é«”ã€ç¶²è·¯ã€I/Oçš„ä¾†è©•ä¼°è³‡æºç“¶é ¸ï¼Œé–‹ç™¼è€…å·²é…ç½®çš„å–®ä¸€å®¹å™¨æˆ–å–®ä¸€ç¯€é»žçš„æ•ˆèƒ½ä¸Šé™ï¼Œæ–¹ä¾¿é–‹ç™¼è€…ä¾†è¡¡é‡æ‡‰ç”¨ç¨‹å¼åœ¨é€™æ¨£è¦æ ¼çš„å®¹å™¨æˆ–ç¯€é»žä¸Šçš„åŸ·è¡Œæ•ˆèƒ½ã€‚
    > 
    > Timothy ChençŽ‡é ˜çš„åœ˜éšŠä¹Ÿé–‹ç™¼äº†æ–°çš„Heracles æ•ˆèƒ½è©•ä¼°æ¼”ç®—æ³•ï¼Œå¯ä¾›å®¹å™¨å¢é›†æŽ§åˆ¶å™¨ä½¿ç”¨ï¼Œä¾†å‹•æ…‹èª¿æ•´é…ç½®çµ¦æ‡‰ç”¨ç¨‹å¼çš„è³‡æºï¼Œæ ¹æ“šTimothy Chené‡‹å‡ºçš„æ¸¬è©¦ä¾‹å­ï¼Œå¯å°‡ä¸€å€‹Sparkåˆ†æžå¢é›†çš„æ•ˆèƒ½åˆ©ç”¨çŽ‡æé«˜2ï½ž3å€ã€‚

- [Hyperpilot open sourced 100% of its products â€“ Timothy Chen â€“ Medium](https://medium.com/@tnachen/hyperpilot-open-sourced-100-of-its-products-18d0e018fe45)

