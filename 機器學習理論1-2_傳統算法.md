# 機器學習理論1-2_傳統算法

[toc]
<!-- toc --> 


# 夏農熵 (Shannon entropy)

- [Shannon entropy in the context of machine learning and AI](https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32)



# 奇異值分解

- [The SIngular Value Decomposition (SVD) song: It Had To Be U - YouTube](https://www.youtube.com/watch?v=fKVRSbFKnEw)



# Boosted tree

- [SGHMC+DDP Learning - BoostedTree.pdf](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)


# Blending and Bagging

- [207_handout.pdf](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)



# 類聚 Clustering

## K-means

- [K means 演算法 | 學習堅持，堅持學習 - 點部落](https://dotblogs.com.tw/dragon229/2013/02/04/89919)

    > 「K means」是一種聚類 (Cluster) 的方式
    > 
    > 聚類基本上就是依照著「**物以類聚**」的方式在進行
    > 
    > (或許我們也可能想成，相似的東西有著相似的特徵)
    > 
    > **給予一組資料，將之分為k類** (k由使用者設定) 就是「K means」的用處
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/201324113322839.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/201324113322839.gif)
    > 
    > 從數學式來看，「K means」主要是為了將下列公式最小化
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/201324111729246.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/201324111729246.gif)
    > 
    > 所有資料點 x~j~ 到其對應群中心 u~i~ 的距離總合是最小的
    > 
    > 目的就是要**找到最佳的群中心 u~i~ 及 x~j~ 所屬的群**來符合上面的要求
    > 
    > **「K means」流程**
    > 
    > 為了解決
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/201324112225730.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/201324112225730.gif)
    > 
    > 這個公式
    > 
    > 「K means」使用**兩步驟**的疊代方式求解 (直接微分難度有點高阿...)
    > 
    > 完整的流程如下
    > 
    > 1.  隨機選取資料組中的k筆資料當作初始群中心u~1~~u~k~\
    >     [![](https://az787680.vo.msecnd.net/user/dragon229/1302/20132411442964.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/20132411442964.gif)
    > 
    > 2.  計算每個資料xi 對應到最短距離的群中心 (**固定 u~i~ 求解所屬群 S~i~**)\
    >     ![](https://az787680.vo.msecnd.net/user/dragon229/1302/201324114734261.gif)\
    >                                                                                                                           (有框的為群中心)
    > 
    > 3.  利用目前得到的分類重新計算群中心 (**固定 S~i~ 求解群中心 u~i~**)\
    >     [![](https://az787680.vo.msecnd.net/user/dragon229/1302/201324114947714.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/201324114947714.gif)
    > 
    > 4.  重複step 2,3直到收斂 (達到最大疊代次數 or 群心中移動距離很小)
    > 
    > **初始值設定**
    > 
    > 大概了解了「K means」的流程之後
    > 
    > 我們可以發現**一開始的群中心是不固定的**
    > 
    > 也就是說同一筆資料用「K means」跑10次，10次的結果可能都不同
    > 
    > 讓我們來看一個極端點的例子
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/20132412452964.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/20132412452964.gif)
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/20132412458793.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/20132412458793.gif)
    > 
    > 由上圖可以發現
    > 
    > 如果**初始群中心設定的不好可能導致不會的結果**
    > 
    > 但就正常情況而已，
    > 
    > 「K means」算是"速度快"、效果又不差的演算法
    > 
    > **延伸**
    > 
    > 基本上「K means」算是一個非常簡單的算法，
    > 
    > 這個算法除了讓我們**將一組資料丟入，得到每個資料對應的群及群中心**還可以做什麼呢?
    > 
    > 換個想法，
    > 
    > 1.  如果我們先用一組資料來訓練出群中心 (訓練階段)
    > 2.  之後每丟入一個資料我們都可以找到其對應的群中心 (回想階段)
    > 
    > 在這個情況「K means」已經有點像分類器在使用了
    > 
    > 差別在我們**只知道丟入的資料和哪個群相似**，但**不知道該資料明確的分類**
    > 
    > 當然之後每丟入一個資料我們也可以再次更新群心中 (online K means版本)
    > 
    > 另一個想法，
    > 
    > 如果我們有一組特徵資料，我們**想知道用這組特徵來做分類是好還是不好**
    > 
    > 我們可以用「K means」來做簡單的確認
    > 
    > 在這情況下
    > 
    > 我們有特徵資料及每個資料所屬的類別
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/201324133341402.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/201324133341402.gif)
    > 
    > (原始特徵資料對應的分類)
    > 
    > 接下來我們利用「K means」重新進行聚類
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/20132413349746.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/20132413349746.gif)
    > 
    > (利用K means的聚類結果)
    > 
    > 就可以知道這組特徵是否容易將不同類別的資料聚在一起 (上圖聚類結果很好，沒有將分屬不同類別的資料聚在一起)
    > 
    > 如果如下面的情況
    > 
    > [![](https://az787680.vo.msecnd.net/user/dragon229/1302/201324133535418.gif)](https://az787680.vo.msecnd.net/user/dragon229/1302/201324133535418.gif)
    > 
    > (原始特徵資料對應的分類)
    > 
    > 我們可以發現原始類別和「K means」的聚類結果差距很大
    > 
    > 實際上我們可以解讀為
    > 
    > 這組特徵或許沒有很好的表達能力 (目標類別的表達)
    > 
    > 因為正常情況下
    > 
    > 同類別的東西會有相似的特徵 (特徵相似 = 資料點鄰近)
    > 
    > 而上圖中看不出這個情況
    > 
    > 因此我們可以認為這組特徵或許不適合用來表達目標分類情況

## K-means, K-means++ vs. KNN

- [Kmeans、Kmeans++和KNN算法比较 - CSDN博客](https://blog.csdn.net/chlele0105/article/details/12997391)

    > K-Means介绍
    > ---------
    > 
    >    K-means算法是聚类分析中使用最广泛的算法之一。它把n个对象根据他们的属性分为k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。其聚类过程可以用下图表示：
    > 
    > ![](https://i.imgur.com/RJT7VWr.jpg)
    > 
    >    如图所示，数据样本用圆点表示，每个簇的中心点用叉叉表示。(a)刚开始时是原始数据，杂乱无章，没有label，看起来都一样，都是绿色的。(b)假设数据集可以分为两类，令K=2，随机在坐标上选两个点，作为两个类的中心点。(c-f)演示了聚类的两种迭代。先划分，把每个数据样本划分到最近的中心点那一簇；划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。这样不断进行"划分---更新---划分---更新"，直到每个簇的中心不在移动为止。
    > 
    > 该算法过程比较简单，但有些东西我们还是需要关注一下，此处，我想说一下"求点中心的算法"
    > 
    > 一般来说，求点群中心点的算法你可以很简的使用各个点的X/Y坐标的平均值。也可以用另三个求中心点的的公式：
    > 
    > **1）Minkowski Distance 公式 ------** λ 可以随意取值，可以是负数，也可以是正数，或是无穷大。
    > 
    > $$
    > d_{ij}=\sqrt[\lambda]{\sum_{k=1}^{n}{ {\lvert x_{ik} }-x_{jk} \rvert}^{\lambda} }
    > $$
    > 
    > **2）Euclidean Distance 公式** ------ 也就是第一个公式 λ=2 的情况
    > 
    > $$
    > d_{ij}=\sqrt{\sum_{k=1}^{n}{ { \Big( x_{ik} }-x_{jk} \Big) }^{2} }
    > $$
    > 
    > **3）CityBlock Distance 公式** ------ 也就是第一个公式 λ=1 的情况
    > 
    > $$
    > d_{ij}=\sum_{k=1}^{n}{ {\lvert x_{ik} }-x_{jk} \rvert}
    > $$
    > 
    > 
    > 这三个公式的求中心点有一些不一样的地方，我们看下图（对于第一个 λ 在 0-1之间）。
    > 
    > ![](https://i.imgur.com/33D9aG6.jpg)![](https://i.imgur.com/tWC2tMr.jpg)![](https://i.imgur.com/J3sNfoK.jpg)
    > 
    > **（1）Minkowski Distance （2）Euclidean Distance （3）CityBlock Distance**
    > 
    > 上面这几个图的大意是他们是怎么个逼近中心的，第一个图以星形的方式，第二个图以同心圆的方式，第三个图以菱形的方式。
    > 
    > **Kmeans算法的缺陷**
    > 
    > -   聚类中心的个数K 需要事先给定，但在实际中这个 K 值的选定是非常难以估计的，很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适
    > -   Kmeans需要人为地确定初始聚类中心，不同的初始聚类中心可能导致完全不同的聚类结果。（可以使用Kmeans++算法来解决）
    > 
    > 针对上述第2个缺陷，可以使用Kmeans++算法来解决
    > 
    > #### K-Means ++ 算法
    > 
    > ####  k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。
    > 
    > 1.  从输入的数据点集合中随机选择一个点作为第一个聚类中心
    > 2.  对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
    > 3.  选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
    > 4.  重复2和3直到k个聚类中心被选出来
    > 5.  利用这k个初始的聚类中心来运行标准的k-means算法
    > 
    >  从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下：
    > 
    > 1.  先从我们的数据库随机挑个随机点当"种子点"
    > 2.  对于每个点，我们都计算其和最近的一个"种子点"的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。
    > 3.  然后，再取一个随机值，用权重的方式来取计算下一个"种子点"。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其<=0，此时的点就是下一个"种子点"。
    > 4.  重复2和3直到k个聚类中心被选出来
    > 5.  利用这k个初始的聚类中心来运行标准的k-means算法
    > 
    > 可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。至于为什么原因比较简单，如下图 所示：  
    > 
    >    ![](https://i.imgur.com/3YYxYXl.jpg)
    > 
    >    假设A、B、C、D的D(x)如上图所示，当算法取值Sum(D(x))*random时，该值会以较大的概率落入D(x)较大的区间内，所以对应的点会以较大的概率被选中作为新的聚类中心。
    > 
    > #### k-means++代码：<http://rosettacode.org/wiki/K-means%2B%2B_clustering>
    > 
    > KNN(K-Nearest Neighbor)介绍
    > -------------------------
    > 
    > 算法思路：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。
    > 
    > 看下面这幅图：
    > 
    > ![](https://i.imgur.com/hZVUo1G.jpg)
    > 
    > KNN的算法过程是是这样的：
    > 
    >    从上图中我们可以看到，图中的数据集是良好的数据，即都打好了label，一类是蓝色的正方形，一类是红色的三角形，那个绿色的圆形是我们待分类的数据。
    > 
    >    如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形
    > 
    >    如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形
    > 
    >    我们可以看到，KNN本质是基于一种数据统计的方法！其实很多机器学习算法也是基于数据统计的。
    > 
    >    KNN是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。
    > 
    > 具体是每次来一个未知的样本点，就在附近找K个最近的点进行投票。
    > 
    > 再举一个例子，Locally weighted regression (LWR)也是一种 memory-based 方法，如下图所示的数据集。
    > 
    > ![](https://i.imgur.com/th38fl2.jpg)
    > 
    > 用任何一条直线来模拟这个数据集都是不行的，因为这个数据集看起来不像是一条直线。但是每个局部范围内的数据点，可以认为在一条直线上。每次来了一个位置样本x，我们在X轴上以该数据样本为中心，左右各找几个点，把这几个样本点进行线性回归，算出一条局部的直线，然后把位置样本x代入这条直线，就算出了对应的y，完成了一次线性回归。也就是每次来一个数据点，都要训练一条局部直线，也即训练一次，就用一次。LWR和KNN很相似，都是为位置数据量身定制，在局部进行训练。
    > 
    > KNN和K-Means的区别
    > --------------
    > 
    > | KNN                                                                                                                                            | K-Means                                                                                                                  |
    > |------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|
    > | 1.KNN是分类算法<br> 2.监督学习<br> 3.喂给它的数据集是带label的数据，<br>已经是完全正确的数据                                                               | 1.K-Means是聚类算法<br> 2.非监督学习<br> 3.喂给它的数据集是无label的数据，<br>是杂乱无章的，经过聚类后才变得有点顺序，<br>先无序，后有序 |
    > | 没有明显的前期训练过程，<br>属于memory-based learning                                                                                              | 有明显的前期训练过程                                                                                                     |
    > | K的含义：来了一个样本x，要给它分类，<br>即求出它的y，就从数据集中，<br>在x附近找离它最近的K个数据点，<br>这K个数据点，类别c占的个数最多，<br>就把x的label设为c | K的含义：K是人工固定好的数字，<br>假设数据集合可以分为K个簇，<br>由于是依靠人工定好，需要一点先验知识                            |
    > | 相似点：都包含这样的过程，给定一个点，<br>在数据集中找离它最近的点。<br>即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。                     |                                                                                                                          |
    > 



## Quantum Mechanics to Cluster Time Series

- [[1805.01711] Using Quantum Mechanics to Cluster Time Series](https://arxiv.org/abs/1805.01711)

# Regression

## Locally weighted regression

- [Day 97: Locally weighted regression – 100 days of algorithms – Medium](https://medium.com/100-days-of-algorithms/day-97-locally-weighted-regression-c9cfaff087fb)

    > [Locally weighted regression](https://en.wikipedia.org/wiki/Local_regression) is a very powerful non-parametric model used in statistical learning.
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*wCaQjroVOCfWs0hp6A8_2w.png)
    > 
    > To explain how it works, we can begin with a linear regression model and [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares).
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*1NAfP3i3XSce8JtouWaNKw.png)
    > 
    > Given a *dataset* **X**, **y**, we attempt to find a *linear model* **h(x)** that minimizes *residual sum of squared errors*. The solution is given by *Normal equations*.
    > 
    > Linear model can only fit a straight line, however, it can be empowered by polynomial features to get more powerful models. Still, we have to *decide* and *fix* the number and types of features ahead.
    > 
    > Alternate approach is given by locally weighted regression.
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*emprpI55Xdu7muftbz1yEg.png)
    > 
    > Given a *dataset* **X**, **y**, we attempt to find a *model* **h(x)** that minimizes *residual sum of* ***weighted*** *squared errors*. The weights are given by a *kernel function* which can be chosen arbitrarily and in my case I chose a Gaussian kernel. The solution is very similar to *Normal equations*, we only need to insert diagonal weight matrix **W**.
    > 
    > What is interesting about this particular setup? By adjusting meta-parameter **τ** you can get a non-linear model that is as strong as polynomial regression of any degree.
    > 
    > And if you are interested, you can find an excellent explanation of what kernel really does in Andrew Ng's [Machine learning course](https://www.youtube.com/watch?v=HZ4cvaztQEs&t=720s).
    > 
    > This time the notebook contains interactive plot. You can adjust meta-parameter **τ** and watch in realtime its influence on the model. Have fun!
    > 
    > <https://github.com/coells/100days>
    > 
    > <https://notebooks.azure.com/coells/libraries/100days>
    > 
    > #### algorithm
    > ```python
    > def local_regression(x0, X, Y, tau):\
    >     # add bias term\
    >     x0 = np.r_[1, x0]\
    >     X = np.c_[np.ones(len(X)), X]
    > 
    >     # fit model: normal equations with kernel\
    >     xw = X.T * radial_kernel(x0, X, tau)\
    >     beta = np.linalg.pinv(xw @ X) @ xw @ Y
    > 
    >     # predict value\
    >     return x0 @ beta
    > 
    > def radial_kernel(x0, X, tau):\
    >     return np.exp(np.sum((X - x0) ** 2, axis=1) / (-2 * tau * tau))
    > ```
    > 
    > #### data
    > ```python
    > n = 1000
    > 
    > # generate dataset\
    > X = np.linspace(-3, 3, num=n)\
    > Y = np.log(np.abs(X ** 2 - 1) + .5)
    > 
    > # jitter X\
    > X += np.random.normal(scale=.1, size=n)
    > ```
    > #### fit & plot models
    > ```python
    > def plot_lwr(tau):\
    >     # prediction\
    >     domain = np.linspace(-3, 3, num=300)\
    >     prediction = [local_regression(x0, X, Y, tau) for x0 in domain]
    > 
    >     plot = figure(plot_width=400, plot_height=400)\
    >     plot.title.text = 'tau=%g' % tau\
    >     plot.scatter(X, Y, alpha=.3)\
    >     plot.line(domain, prediction, line_width=2, color='red')
    > 
    >     return plot
    > 
    > show(gridplot([\
    >     [plot_lwr(10.), plot_lwr(1.)],\
    >     [plot_lwr(0.1), plot_lwr(0.01)]\
    > ]))
    > ```
    > #### interactive plot
    > 
    > *Download and run the* [*notebook*](https://github.com/coells/100days) *to interact with the plot.*

# 降維方法

## PCA （principal component analysis）主成份分析

- [淺談降維方法中的 PCA 與 t-SNE – DnD mag – Medium](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

    > > *將一個具有 n 個特徵空間的樣本，轉換為具有 k 個特徵空間的樣本，其中 k < n*
    > 
    > 以下是 PCA 的主要步驟：
    > 
    > 1.  將數據標準化
    > 2.  建立**共變異數矩陣（covariance matrix）**
    > 3.  利用**奇異值分解（SVD）**求得**特徵向量（eigenvector）**跟**特徵值（eigenvalue）**
    > 4.  通常特徵值會由大到小排列，選取 k 個特徵值與特徵向量
    > 5.  將原本的數據投影（映射）到特徵向量上，得到新的特徵數
    > 
    > PCA 最重要的部分就是奇異值分解，因此接下來的章節讓我們來談談**奇異值分解**
    > 
    > #### 直觀理解奇異值分解
    > 
    > 在矩陣分解當中，奇異值分解是個相當有名的方法。矩陣分解在高中數學當中最常見的用途就是解方程式（如 LU 分解），從奇異值分解的公式當中我們可以直觀地了解：
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*EW7y-TDCOHClCKINXFtlZQ.png)
    > 
    > 
    > 
    > 其中 A 為一個 m x n 的矩陣，𝑈 跟 V 都為正交矩陣，**𝛴 為奇異值矩陣**。奇異值矩陣為矩陣 A 對應的特徵值，在 PCA 當中又叫做**主成份**，代表對保存訊息的重要程度，通常由大到小遞減排列在對角中，是個對稱矩陣。
    > 
    > 那麼這邊的 A 對應什麼呢？當然就是我們的特徵，只是特別要注意的是這邊的 A 我們通常使用**共變異數矩陣（covariance martix）**來求算，記得資料一定要先正規化後在進行奇異值分解
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*xk7Vhzw5dI77BLNl3Dsdlw.png)
    > 
    > 
    > 
    > 因為共變異數矩陣**常用 Sigma 表示，不要跟上面的 𝛴 搞混囉**。因此如果要降維，我們可以用 U 的前 k 列乘上對應 𝛴 當中的特徵向量，就可以得出新的特徵了，而從幾何的角度來看
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*3nASowA14C1z-hIshmfg7w.png)
    > 
    > 
    > 
    > 這樣子的運算在幾何當中，其實是將 X 投影到 U 的前 k 個向量
    > 

## t-SNE 

- [淺談降維方法中的 PCA 與 t-SNE – DnD mag – Medium](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

    > t-SNE 主要是將高維的數據用高斯分佈的機率密度函數近似，而低維數據的部分使用 t 分佈的方式來近似，在使用 KL 距離計算相似度，最後再以梯度下降（或隨機梯度下降）求最佳解 。
    > 
    > #### 高斯分佈的機率密度函數
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*9CSkBUO3tvJAhq8Wzv9s2Q.png)
    > 
    > 
    > 
    > 其中，X 為隨機變量，𝝈 為變異數，𝜇 為平均。
    > 
    > 因此原本高維的數據可以這樣表示：
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*KxxzB_L8Wi1L-FgznjhkyA.png)
    > 
    > 
    > 
    > 而低維的數據用 t 分布的機率密度函數可以這樣表示（自由度為 1）
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*OwvAZgYHAfYvKehREGH9WQ.png)
    > 
    > 
    > 
    > 其中，x 為高維當中的數據，y 為低維當中的數據。P, Q 分別代表機率分佈。
    > 
    > 為什麼會使用 t 分佈來近似低維的數據呢？主要是因為轉換成低維之後一定會丟失許多訊息，所以為了不被異常值影響可以使用 t 分佈。
    > 
    > t 分佈在樣本數較少時，可以比較好模擬母體分布的情形，不容易被異常值所影響。
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1500/1*hkZuHGVNMmS4_MHfxoIDdA.jpeg) 
    > T 分佈與高斯分佈的機率密度函數
    > 
    > #### 兩個分佈之間的相似度
    > 
    > 求算兩個分佈之間的相似度，經常用 KL 距離（Kullback-Leibler Divergence）來表示，也叫做相對熵（Relative Entropy）。
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*geKaV3PDC2idMGUCXroJzw.png)
    > 
    > 
    > 
    > 在 t-SNE 中使用了困惑度（Perp）來當作超參數。
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*BiQ-gYxdbZsGP5KfYUF9Kg.png)
    > 
    > 
    > 
    > 論文中提出通常困惑度在 5 ~ 50 之間。
    > 
    > #### Cost function
    > 
    > 用 KL 距離計算 Cost
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*2XSnUoPaYB2_FMQ8xMzOuQ.png)
    > 
    > 
    > 求梯度可以寫成
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*MzouyFbnSKdeBA7nLJVCPw.png)
    > 
    > 
    > 最後再利用梯度下降法（或隨機梯度下降法）就可以找到最小值了。
    > 
    > ---
    > 
    > #### 實測：使用 MNIST 測試
    > 
    > 測試集可以到[這裡](http://yann.lecun.com/exdb/mnist/)下載，首先我們先用 PCA 降到二維看看。
    > 
    > **PCA**
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1250/1*Be6oSLuXgG0cDi2gJEL-wA.jpeg)
    > 
    > 
    > **t-SNE**
    > 
    > 接下來使用 t-SNE 測試
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1750/1*5i8McPBKmFOMOcCDjl8w4Q.jpeg)
    > 
    > ---
    > 
    > ### 小結
    > 
    > * 當特徵數量過多時，使用 PCA 可能會造成降維後的特徵欠擬合（underfitting），這時可以考慮使用 t-SNE 來降維
    > * t-SNE 的需要比較多的時間執行
    >
    > 
