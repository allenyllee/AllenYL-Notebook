# NLP__資料收集3-1_文本建模1

[toc]
<!-- toc --> 

## Secquence Modeling

### GTMM
- [DeepMind丨深度學習最新生成記憶模型，遠超RNN的GTMM](http://www.bigdatafinance.tw/index.php/tech/557-deepmind-rnn-gtmm)

### RNN+CNN

- [Representing Language with Recurrent and Convolutional Layers: An Authorship Attribution Example](https://hergott.github.io/language-representation-rnn-cnn/)

### [1803.01271] An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (TCN)

- [[1803.01271] An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)

### [1812.10464] Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond

- [[1812.10464] Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond](https://arxiv.org/abs/1812.10464)



## 文章向量

### 概觀

- [當前最好的詞句嵌入技術概覽：從無監督學習到監督、多任務學習 - 幫趣](http://bangqu.com/59R436.html#utm_source=Facebook_PicSee&utm_medium=Social)

    - [📚The Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)

    > FastText 相對於原始的 word2vec 向量最主要的提升是它引入了 n 元字符（n-gram），這使得對沒有在訓練數據中出現的單詞（詞彙表外的單詞）計算單詞的表徵成爲了可能。
    >     
    > ---
    >     
    > 深度上下文單詞表徵（ELMo）在很大的程度上提高了目前最先進的詞嵌入模型的性能。它們由 Allen 人工智能研究所研發，並將在 6 月初的 NAACL 2018 （ https://arxiv.org/abs/1802.05365 ） 中展示。
    > 
    > ELMo 模型會爲每一個單詞分配一個表徵，該表徵是它們所屬的整個語料庫中的句子的一個函數。詞嵌入將從一個兩層的雙向語言模型（LM）的內部狀態中計算出來，因此該模型被命名爲「ELMo」： Embeddings from Language Models（E 代表「嵌入」，LM 代表「語言模型」）。
    > 
    > ELMo 模型的特點：
    > 
    > - ELMo 模型的輸入是字符而不是單詞。因此，它們可以利用子詞單元的優勢來計算有意義的單詞表示，即使這些單詞可能在詞彙表之外（就像 FastText 一樣）。
    > 
    > - ELMo 是在雙向語言模型中的一些層上的激勵函數的串接。一個語言模型的不同層會對一個單詞的不同類型的信息進行編碼（例如，詞性標註（Part-Of-Speech tagging）由雙向 LSTM（biLSTM）的較低層很好地預測，而詞義排歧則由較高層更好地進行編碼）。將所有的層串接起來使得自由組合各種不同的單詞表徵成爲了可能，從而在下游任務中得到更好的模型性能。
    > 
    > ---
    > 
    > 在這個領域有一個廣泛的共識（ http://arxiv.org/abs/1805.01070 ） ，那就是：直接對句子的詞嵌入取平均（所謂的詞袋模型（Bag-of-Word，BoW））這樣簡單的方法可以爲許多下游任務提供一個很強大的對比基線。
    > 
    > Arora 等人在 ICLR 2017 上提出了「A Simple but Tough-to-Beat Baseline for Sentence Embeddings」 （ https://openreview.net/forum?id=SyK00v5xx ） ，這是一個很好的能夠被用於計算這個基線（BoW）的算法，算法的大致描述如下：選擇一個流行的詞嵌入方法，通過詞向量的線性的加權組合對一個句子進行編碼，並且刪除共有的部分（刪除它們的第一個主成分上的投影）。
    > 
    > ---
    > 
    > 除了簡單的詞向量平均，第一個主要的提議是使用無監督學習訓練目標，這項工作是起始於 Jamie Kiros 和他的同事們在 2015 年提出的「Skip-thought vectors」（ https://arxiv.org/abs/1506.06726 ） 。
    > 
    > 「Skip-thoughts vector」是一個典型的學習無監督句子嵌入的案例。它可以被認爲相當於爲詞嵌入而開發的「skip-gram」模型的句子向量，我們在這裏試圖預測一個給定的句子周圍的句子，而不是預測一個單詞周圍的其他單詞。該模型由一個基於循環神經網絡的編碼器—解碼器結構組成，研究者通過訓練這個模型從當前句子中重構周圍的句子。
    > 
    > Skip-Thoughts 的論文中最令人感興趣的觀點是一種詞彙表擴展方案：Kiros 等人通過在他們的循環神經網絡詞嵌入空間和一個更大的詞嵌入空間（例如，word2vec）之間學習一種線性變換來處理訓練過程中沒有出現的單詞。
    > 
    > 「Quick-thoughts vectors」（ https://openreview.net/forum?id=rJvJXZb0W ） 是研究人員最近對「Skip-thoughts vectors」的一個改進，它在今年的 ICLR 上被提出。在這項工作中，在給定前一個句子的條件下預測下一個句子的任務被重新定義爲了一個分類問題：研究人員將一個用於在衆多候選者中選出下一個句子的分類器代替瞭解碼器。它可以被解釋爲對生成問題的一個判別化的近似。
    > 
    > 該模型的運行速度是它的優點之一（與 Skip-thoughts 模型屬於同一個數量級），使其成爲利用海量數據集的一個具有競爭力的解決方案。
    > 
    > ![](http://i2.bangqu.com/j/news/20180606/59R4361528261210081r28S2.png)*「Quick-thoughts」分類任務示意圖。分類器需要從一組句子嵌入中選出下一個句子。圖片來自 Logeswaran 等人所著的「An efficient framework for learning sentence representations」。*
    > 
    > ---
    > 
    > 在很長一段時間內，人們認爲監督學習技術比無監督學習技術得到的句子嵌入的質量要低一些。然而，這種假說最近被推翻了，這要部分歸功於「InferSent」（ https://arxiv.org/abs/1705.02364 ） 的提出。
    > 
    > InferSent 具有非常簡單的架構，這使得它成爲了一種非常有趣的模型。它使用 Sentence Natural Language Inference（NLI）數據集（該數據集包含 570,000 對帶標籤的句子，它們被分成了三類：中立、矛盾以及蘊含）訓練一個位於句子編碼器頂層的分類器。兩個句子使用同一個編碼器進行編碼，而分類器則是使用通過兩個句子嵌入構建的一對句子表徵訓練的。Conneau 等人採用了一個通過最大池化操作實現的雙向 LSTM 作爲編碼器。
    > 
    > ![](http://i2.bangqu.com/j/news/20180606/59R4361528261211217433N2.png)
    > 
    > 
    > ---
    >
    > 在 ICLR 2018 上發表的描述 MILA 和微軟蒙特利爾研究院的工作的論文《Learning General Purpose Distributed Sentence Representation via Large Scale Multi-Task Learning》（ https://arxiv.org/abs/1804.00079 ）中，Subramanian 等人觀察到，爲了能夠在各種各樣的任務中泛化句子表徵，很有必要將一個句子的多個層面的信息進行編碼。
    > 
    > 因此，這篇文章的作者利用了一個一對多的多任務學習框架，通過在不同的任務之間進行切換去學習一個通用的句子嵌入。被選中的 6 個任務（對於下一個/上一個句子的 Skip-thoughts 預測、神經機器翻譯、組別解析（constituency parsing），以及神經語言推理）共享相同的由一個雙向門控循環單元得到的句子嵌入。實驗表明，在增添了一個多語言神經機器翻譯任務時，句法屬性能夠被更好地學習到，句子長度和詞序能夠通過一個句法分析任務學習到，並且訓練一個神經語言推理能夠編碼語法信息。
    > 
    > 谷歌在 2018 年初發布的的通用句子編碼器（ https://arxiv.org/abs/1803.11175 ）也使用了同樣的方法。他們的編碼器使用一個在各種各樣的數據源和各種各樣的任務上訓練的轉換網絡，旨在動態地適應各類自然語言理解任務。該模型的一個預訓練好的版本可以在 TensorFlow 獲得。









### DSSM

#### 原理

- [A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval - Microsoft Research](https://www.microsoft.com/en-us/research/publication/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval/)


- [DSSM - Microsoft Research](https://www.microsoft.com/en-us/research/project/dssm/)


- [炼丹师读源码之细究DSSM Embedding实现](https://zhuanlan.zhihu.com/p/37082976)


- [用于Sentence Embedding的DSSM与LSTM：管中窥豹 – D&C](https://boweihe.me/2016/08/26/dssm%E4%B8%8Elstm/)


- [学习记录一下深度语义匹配模型-DSSM | Kubi Code'Blog](http://kubicode.me/2017/04/21/Deep%20Learning/Study-With-Deep-Structured-Semantic-Model/)

- [Deep Learning for Web Search and Natural Language Processing - wsdm2015.v3.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/wsdm2015.v3.pdf)



#### 實作

- [airalcorn2/Deep-Semantic-Similarity-Model: My Keras implementation of the Deep Semantic Similarity Model (DSSM)/Convolutional Latent Semantic Model (CLSM) described here: http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf.](https://github.com/airalcorn2/Deep-Semantic-Similarity-Model)

- [Model DSSM on Tensorflow](https://liaha.github.io/models/2016/06/21/dssm-on-tensorflow.html)
    - [liaha/dssm](https://github.com/liaha/dssm)






### Neural Bag-of-Ngrams

- [Neural Bag-of-Ngrams - Association for the Advancement of Artificial ...](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14513/14079)

- [libofang/Neural-BoN: code for AAAI-17 paper "Neural Bag-of-Ngrams"](https://github.com/libofang/Neural-BoN)

- [Neural Bag-of-Ngrams - CSDN博客](https://blog.csdn.net/qq_22548549/article/details/78213004)

    > 该文一共提出了三种ngram向量创建的方法。分别是Context guided N-gram representation, Text Guided N-gram representation以及Label guided N-gram representation. 其中前两个模型都是无监督的，最后一个模型是监督算法，需要对文本打标签。\
    > ![](https://img-blog.csdn.net/20171012123832474?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    > 
    > Context guided N-gram representation
    > ------------------------------------
    > 
    > Context guided N-gram representation简称CGNR，借鉴的是Skip-gram模型生成词向量的方法。CGNR用当前ngram向量预测其上下文中的ngram向量。其形式化表示如下\
    > ![](https://img-blog.csdn.net/20171012124622467?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > 其中g表示其是第i篇文章第j个ngram，vg是其对应的向量，c表示g的第q个上下文中的ngram。其中上下文集合表示如下\
    > ![](https://img-blog.csdn.net/20171012124917166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > 其中win是上下文窗口的大小，m是最大的gram值。上下文集合创建的例子如下图所示\
    > ![](https://img-blog.csdn.net/20171012125347703?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > 其中绿色表示目标gram，蓝色表示该gram的上下文集合，gram最大值是3，即包含1gram, 2gram和3gram。\
    > CGNR依然存在一些问题，因为有相似上下文的词它们的词义不一定相似，可能刚好相反，如词"good"和"bad"。所以作者又提出了Text Guided N-gram representation以及Label guided N-gram representation.
    > 
    > Text Guided N-gram representation
    > ---------------------------------
    > 
    > 一般而言，同一篇文章里出现的词，他们的词义也是相似的，尤其是有关评论的文章里，经常会出现"good", "not bad"等词。他们所表达的词义是一样的。因此，该文提出要用ngram向量预测该ngram所属的文本，形式化表述如下\
    > ![](https://img-blog.csdn.net/20171012130421982?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > 其中ti表示第i篇文档。TGNR模型适合于长文本，比如在一篇消极的电影影评中，可能会同时出现"terrible", "waste of time"等词，TGNR模型能将这些词聚集在一起，而对于短文本，可能并不会出现这么多词。由此，该文进一步提出了Label guided N-gram representation模型。
    > 
    > Label guided N-gram representation
    > ----------------------------------
    > 
    > 当词"good"和"perfect"出现在不同的文档里是，CGNR和TGNR可能都不能达到较好的效果。但是当两篇文档归属于同一个标签时，其中的词可能具有相似的语义。由此该文提出了Label guided N-gram representation方法，用当前ngram预测其所属的文档的标签，形式化表示如下\
    > ![](https://img-blog.csdn.net/20171012131351485?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > 相对而言，LGNR更适合短文本。


### Neural Network 處理動態長度的方法

- [machine learning - How can neural networks deal with varying input sizes? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/2008/how-can-neural-networks-deal-with-varying-input-sizes)

    > Others already mentioned:
    > 
    > - zero padding
    > - RNN
    > - recursive NN
    > - using convolutions different number of times depending on the size of input. 



### Universal Sentence Encoder

- [通用句子語義編碼器，谷歌在語義文本相似性上的探索 - 幫趣](http://bangqu.com/gY7194.html)

    > **語義文本相似度**
    > 
    > 在「Learning Semantic Textual Similarity from Conversations」這篇論文中，我們引入一種新的方式來學習語義文本相似的句子表示。直觀的說，如果句子的回答分佈相似，則它們在語義上是相似的。例如，「你多大了？」以及「你的年齡是多少？」都是關於年齡的問題，可以通過類似的回答，例如「我 20 歲」來回答。相比之下，雖然「你好嗎？」和「你多大了？」包含的單詞幾乎相同，但它們的含義卻大相徑庭，所以對應的回答也相去甚遠。
    > 
    > 論文地址：https://arxiv.org/abs/1804.07754
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY719415273972350175UWgr.png)
    > 
    > *如果句子可以通過相同的答案來回答，那麼句子在語義上是相似的。否則，它們在語義上是不同的。*
    > 
    > 這項工作中，我們希望通過給回答分類的方式學習語義相似性：給定一個對話輸入，我們希望從一批隨機選擇的回覆中分類得到正確的答案。但是，任務的最終目標是學習一個可以返回表示各種自然語言關係（包括相似性和相關性）的編碼模型。我們提出了另一預測任務（此處是指 SNLI 蘊含數據集），並通過共享的編碼層同時推進兩項任務。利用這種方式，我們在 STSBenchmark 和 CQA task B 等相似度度量標準上取得了更好的表現，究其原因，是簡單等價關係與邏輯蘊含之間存在巨大不同，後者爲學習複雜語義表示提供了更多可供使用的信息。
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397235681nLm72.png)
    > 
    > *對於給定的輸入，分類可以認爲是一種對所有可能候選答案的排序問題。*
    > 
    > **通用句子編碼器**
    > 
    > 「Universal Sentence Encoder」這篇論文介紹了一種模型，它通過增加更多任務來擴展上述的多任務訓練，並與一個類似 skip-thought 的模型聯合訓練，從而在給定文本片段下預測句子上下文。然而，我們不使用原 skip-thought 模型中的編碼器 - 解碼器架構，而是使用一種只有編碼器的模型，並通過共享編碼器來推進預測任務。利用這種方式，模型訓練時間大大減少，同時還能保證各類遷移學習任務（包括情感和語義相似度分類）的性能。這種模型的目的是爲儘可能多的應用（釋義檢測、相關性、聚類和自定義文本分類）提供一種通用的編碼器。
    > 
    > 論文地址：https://arxiv.org/abs/1803.11175
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397237211r96N9.png)
    > 
    > *成對語義相似性比較，結果爲 TensorFlow Hub 通用句子編碼器模型的輸出。*
    > 
    > 正如文中所說，通用句子編碼器模型的一個變體使用了深度平均網絡（DAN）編碼器，而另一個變體使用了更加複雜的自注意力網絡架構 Transformer。
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397238079233wo.png)
    > 
    > *「Universal Sentence Encoder」一文中提到的多任務訓練。各類任務及結構通過共享的編碼層/參數（灰色框）進行連接。*
    > 
    > 隨着其體系結構的複雜化，Transformer 模型在各種情感和相似度分類任務上的表現都優於簡單的 DAN 模型，且在處理短句子時只稍慢一些。然而，隨着句子長度的增加，使用 Transformer 的計算時間明顯增加，但是 DAN 模型的計算耗時卻幾乎保持不變。
    > 
    > **新模型**
    > 
    > 除了上述的通用句子編碼器模型之外，我們還在 TensorFlow Hub 上共享了兩個新模型：大型通用句型編碼器通和精簡版通用句型編碼器。
    > 
    > -   大型：https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-large/1
    > 
    > -   精簡：https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-lite/1
    > 
    >  這些都是預訓練好的 Tensorflow 模型，給定長度不定的文本輸入，返回一個語義編碼。這些編碼可用於語義相似性度量、相關性度量、分類或自然語言文本的聚類。
    > 
    > 大型通用句型編碼器模型是用我們介紹的第二篇文章中提到的 Transformer 編碼器訓練的。它針對需要高精度語義表示的場景，犧牲了速度和體積來獲得最佳的性能。
    > 
    > 精簡版模型使用 Sentence Piece 詞彙庫而非單詞進行訓練，這使得模型大小顯著減小。它針對內存和 CPU 等資源有限的場景（如小型設備或瀏覽器）。
    > 
    > 我們很高興與大家分享這項研究以及這些模型。這只是一個開始，並且仍然還有很多問題亟待解決，如將技術擴展到更多語言上（上述模型目前僅支持英語）。我們也希望進一步地開發這種技術，使其能夠理解段落甚至整個文檔。在實現這些目標的過程中，很有可能會產生出真正的「通用」編碼器。
    > 
    > *原文鏈接：https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html*


- [《Universal Sentence Encoder》论文分享](https://zhuanlan.zhihu.com/p/35174235)

    > 论文主要是提出了一个统一的句子编码框架，句子级别的encode比Word2vec使用起来会更加的方便，因为可以直接拿来做句子分类等任务。
    > 
    > 本文主要提出了两个句子encode的框架，一个是之前《attention is all you need》里面的一个encode框架，另一个是DAN（deep average network）的encode方式。两个的训练方式较为类似，都是通过多任务学习，将encode用在不同的任务上，包括分类，生成等等，以不同的任务来训练一个编码器，从而实现泛化的目的。
    > 
    > Transformer：
    > 
    > 第一个句子编码模型使用了transformer里面的一个子图，这个子图通过attention来一个句子中词语的context表示，同时考虑所有的词并将其转化为一个fixed length向量。具体了解可以去看那篇论文，这篇论文本身也没有给出具体的图。
    > 
    > ![](https://pic4.zhimg.com/80/v2-540dc62cdb5fe8939c5185a0eeb14c7b_hd.jpg)
    > 
    > DAN：
    > 
    > 第二个句子编码模型使用了较为简单的一个DAN模型（[Mohit Iyyer](https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/P/P15/P15-1162.pdf)），通过对embedding求平均后输入到feed-forward network，然后不加softmax即可得到隐层表示，然后把这个向量类似于上面的方法，用多任务来进行训练模型。
    > 
    > ![](https://pic3.zhimg.com/80/v2-bb79f8465693d9834e45a616b6a53631_hd.jpg)
    > 
    > 论文之所以提出两个是考虑到模型的准确度和复杂程度，以及训练的资源消耗。
    > 
    > attention那个encode模型准确率高，但是模型复杂度高，训练时间长，消耗gpu资源多
    > 
    > 而DAN模型虽然准确度低，但是训练快，模型复杂度低。
    > 
    > 文中也给出了样例，有了一个初始训练好的模型，用户可以根据自己的需求再加入一些文本进行再次训练。（[使用地址](https://link.zhihu.com/?target=https%3A//www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1)）
    > 
    > ![](https://pic4.zhimg.com/80/v2-51aac31df9e31c1d907171f20e96fc0f_hd.jpg)
    > 
    > 文中还提出了比较两个句子向量之间的相似度时候不用原生的cos距离，而是转化为弧度之后计算，效果要好于原生的：
    > 
    > ![](https://pic3.zhimg.com/80/v2-cb699c1869f91223cffe3d77cc524f79_hd.jpg)
    > 


# Bag-of-Word based

## TF-IDF

- [tf–idf - Wikiwand](https://www.wikiwand.com/en/Tf%E2%80%93idf)



- [[文件探勘] TF-IDF 演算法：快速計算單字與文章的關聯 – David's Perspective](https://taweihuang.hpd.io/2017/03/01/tfidf/)

    > ### BoW (Bag of Words) 與詞彙數量-文件矩陣
    > 
    > 假設現在有 ![D](https://s0.wp.com/latex.php?latex=D&bg=ffffff&fg=000&s=0 "D") 篇文件 (document)，而所有文件中總共使用了 ![~T~](https://s0.wp.com/latex.php?latex=%7ET%7E&bg=ffffff&fg=000&s=0 "~T~") 個詞彙 (term)，我們就可以將文章轉換成以下類型的矩陣，其中第一欄第一列的「12」代表的是「文件 1」 中出現了12個「文字 1」。如此一來，我們可以用 ![[12,~0,~3,~\cdots,~2]](https://s0.wp.com/latex.php?latex=%5B12%2C%7E0%2C%7E3%2C%7E%5Ccdots%2C%7E2%5D&bg=ffffff&fg=000&s=0 "[12,~0,~3,~\cdots,~2]") 這個向量來代表「文件 1」，同理也可用「文件 D」也可以用 ![[0,~2,~8,~\cdots,~0]](https://s0.wp.com/latex.php?latex=%5B0%2C%7E2%2C%7E8%2C%7E%5Ccdots%2C%7E0%5D&bg=ffffff&fg=000&s=0 "[0,~2,~8,~\cdots,~0]") 來表示。
    > 
    > ![圖片1](https://taweihuang.hpd.io/wp-content/uploads/2017/03/圖片1.png)
    > 
    > 這樣的方法就是「BoW (Bag of Word)演算法」，這種方法雖然很簡單，但有2個主要的問題 ─ 一是每篇文章的總字數不一樣，比如說文字 2在文件 2中出現9次，在文件 D中卻只出現2次，這樣是否代表文字 2 對文件 2 比較重要，對文件 D 比較不重要呢？答案是否定的，說不定文件2有10000個字，而文件D只有50個字，如此一來文字2應該對文件D比較重要才對。
    > 
    > 另一個問題是，時常重複出現的慣用詞彙對一個文件的影響很大。比如說，上圖中的文字 3在每個文件中都出現好多次，可能是「the」之類的常用字，如此一來「文件 D」的向量就會被  the 這個字所主導，但 the 這個字其實沒什麼特別的意義。
    > 
    > 為了處理以上兩個問題，歷史悠久但非常好用的 TF-IDF演算法就被發明出來了。
    > 
    > ### TF-IDF 演算法
    > 
    > TF-IDF 演算法包含了兩個部分：**詞頻**（term frequency，TF）跟**逆向文件頻率**（inverse document frequency，IDF）。詞頻指的是某一個給定的詞語在該文件中出現的頻率，第 ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")個詞出現在第 ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") 篇文件的頻率記做 ![~tf_{t,d}~](https://s0.wp.com/latex.php?latex=%7Etf_%7Bt%2Cd%7D%7E&bg=ffffff&fg=000&s=0 "~tf_{t,d}~")，舉例來說，如果文件 1 總共有100個字，而第 1 個字在文件 1 出現的次數是12次，因此![~tf_{1,1}=12/100~](https://s0.wp.com/latex.php?latex=%7Etf_%7B1%2C1%7D%3D12%2F100%7E&bg=ffffff&fg=000&s=0 "~tf_{1,1}=12/100~")，如此一來，我們就可以針對上述的第一個問題進行修正，以頻率而不是次數來看待文字的重要性，讓文章與文章之間比較有可比較性。
    > 
    > 而逆向文件頻率則是用來處理常用字的問題。假設詞彙 ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t") 總共在 ![d_t](https://s0.wp.com/latex.php?latex=d_t&bg=ffffff&fg=000&s=0 "d_t") 篇文章中出現過，則詞彙 ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t") 的 IDF 定義成 ![~idf_t = \log\left(\frac{D}{d_t}\right)~](https://s0.wp.com/latex.php?latex=%7Eidf_t+%3D+%5Clog%5Cleft%28%5Cfrac%7BD%7D%7Bd_t%7D%5Cright%29%7E&bg=ffffff&fg=000&s=0 "~idf_t = \log\left(\frac{D}{d_t}\right)~")。比如說，假設文字 1 總共出現在 25 篇不同的文件，則 ![~~idf_1 = \log\left(\frac{D}{25}\right)~~](https://s0.wp.com/latex.php?latex=%7E%7Eidf_1+%3D+%5Clog%5Cleft%28%5Cfrac%7BD%7D%7B25%7D%5Cright%29%7E%7E&bg=ffffff&fg=000&s=0 "~~idf_1 = \log\left(\frac{D}{25}\right)~~")。如果詞彙 ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")在非常多篇文章中都出現過，就代表 ![~d_t~](https://s0.wp.com/latex.php?latex=%7Ed_t%7E&bg=ffffff&fg=000&s=0 "~d_t~") 很大，此時![~idf_t~](https://s0.wp.com/latex.php?latex=%7Eidf_t%7E&bg=ffffff&fg=000&s=0 "~idf_t~") 就會比較小。
    > 
    > 而一個字對於一篇文件重要性的分數 (score) 就可以透過TF與IDF兩個指標計算出來，我們將第 ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")個詞彙對於第 ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") 篇文件的TF-IDF權重定義為 ![~w_{t,d} =  tf_{t,d} \times  idf_t ~](https://s0.wp.com/latex.php?latex=%7Ew_%7Bt%2Cd%7D+%3D%C2%A0+tf_%7Bt%2Cd%7D+%5Ctimes+%C2%A0idf_t+%7E&bg=ffffff&fg=000&s=0 "~w_{t,d} =  tf_{t,d} \times  idf_t ~")。如此一來，當詞彙 ![~t~](https://s0.wp.com/latex.php?latex=%7Et%7E&bg=ffffff&fg=000&s=0 "~t~") 很常出現在文件  ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") 時，他的 ![~tf_{t,d} ~](https://s0.wp.com/latex.php?latex=%7Etf_%7Bt%2Cd%7D+%7E&bg=ffffff&fg=000&s=0 "~tf_{t,d} ~") 就會比較大，而如果詞彙![~~ t](https://s0.wp.com/latex.php?latex=%7E%7Et&bg=ffffff&fg=000&s=0 "~~t") 也很少出現在其他篇文章，則 ![~idf_t~](https://s0.wp.com/latex.php?latex=%7Eidf_t%7E&bg=ffffff&fg=000&s=0 "~idf_t~") 也會比較大，使 ![~w_{t,d}~](https://s0.wp.com/latex.php?latex=%7Ew_%7Bt%2Cd%7D%7E&bg=ffffff&fg=000&s=0 "~w_{t,d}~")整體來說比較大，也就是說詞彙![~t~](https://s0.wp.com/latex.php?latex=%7Et%7E&bg=ffffff&fg=000&s=0 "~t~") 對於文件  ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") 來說是很重要的。如此一來，我們就可以計算出 TF-IDF 矩陣，如下圖所示。
    > 
    > ![圖片1](https://taweihuang.hpd.io/wp-content/uploads/2017/03/圖片1-1.png)
    > 
    > 另一方面， TF-IDF 時常被用來作資訊檢索 (information retrieval)，比如說，給定一串指令 (query)「文字 1 + 文字 3 + 不在現有詞彙裡面的文字」，則這串指令跟第 ![d](https://s0.wp.com/latex.php?latex=d&bg=ffffff&fg=000&s=0 "d") 的關聯分數就可以定義文 ![tf_{1,d} + tf_{3,d} ](https://s0.wp.com/latex.php?latex=tf_%7B1%2Cd%7D+%2B+tf_%7B3%2Cd%7D%C2%A0&bg=ffffff&fg=000&s=0 "tf_{1,d} + tf_{3,d} ")。
    > 
    > ### 大鼻是怎麼用 TF-IDF？
    > 
    > TF-IDF 常被我用在3個地方，一個是作為 baseline model的特徵 (feature)，比如說作文件分類 (text classification) 時，我就會把 tf 跟 idf 都當作文件的特徵(所以一篇文章總夠會有 ![2T](https://s0.wp.com/latex.php?latex=2T&bg=ffffff&fg=000&s=0 "2T")個特徵)，去跑分類模型，作為 baseline。有時候我會把一個詞彙對於每篇的文章的 tf-idf 值當作該詞彙的特徵，去跑文字的分群。還有一個是大鼻好友ㄐㄓ告訴我的妙招，就是如果我們想要用word2vec得出來的詞向量去表示一個文件的話，可以用tf-idf的值當作權重，再把該文件中每個詞向量用tf-idf當作權重加起來，效果也很不錯喔！

### 實作

- [[AI] The fastest way to identify keywords in news articles — TFIDF with Wikipedia (Python version)](https://hackernoon.com/the-fastest-way-to-identify-keywords-in-news-articles-tfidf-with-wikipedia-python-version-baf874d7eb16)



### the advantages and disadvantages of TF-IDF

- [What are the advantages and disadvantages of TF-IDF? - Quora](https://www.quora.com/What-are-the-advantages-and-disadvantages-of-TF-IDF)

    > Advantages:
    > - Easy to compute
    > - You have some basic metric to extract the most descriptive terms in a document
    > - You can easily compute the similarity between 2 documents using it
    > 
    > Disadvantages:
    > - TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc.
    > - For this reason, TF-IDF is only useful as a lexical level feature
    > - Cannot capture semantics (e.g. as compared to topic models, word embeddings)
    > 
    > So it depends a lot for what you want to use TF-IDF.











# Topic model

## course

### Topic Models - David Blei

- [Topic Models - VideoLectures.NET](http://videolectures.net/mlss09uk_blei_tm/)

## metrics

### Palmetto Online Demo

- [Palmetto Online Demo](http://palmetto.aksw.org/palmetto-webapp/?coherence=umass)

    > CV is based on a sliding window, a one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosinus similarity.
    > 
    > This coherence measure retrieves cooccurrence counts for the given words using a sliding window and the window size 110. The counts are used to calculated the NPMI of every top word to every other top word, thus, resulting in a set of vectors—one for every top word. The one-set segmentation of the top words leads to the calculation of the similarity between every top word vector and the sum of all top word vectors. As similarity measure the cosinus is used. The coherence is the arithmetic mean of these similarities. (Note that this was the best coherence measure in our evalution.)
    > 
    > ---
    > 
    > CUMass is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure.
    > 
    > The main idea of this coherence is that the occurrence of every top word should be supported by every top preceding top word. Thus, the probability of a top word to occur should be higher if a document already contains a higher order top word of the same topic. Therefore, for every word the logarithm of its conditional probability is calculated using every other top word that has a higher order in the ranking of top words as condition. The probabilities are derived using document cooccurrence counts. The single conditional probabilities are summarized using the arithmetic mean. (Note that in the original publication only the sum of these values is calculated)


## LSA (Latent Semantic Analysis)

- [Latent Semantic Analysis(LSA/ LSI)算法简介 - 潘的博客 - 博客园](https://www.cnblogs.com/kemaswill/archive/2013/04/17/3022100.html)

    > 我们希望找到一种模型，能够捕获到单词之间的相关性。如果两个单词之间有很强的相关性，那么当一个单词出现时，往往意味着另一个单词也应该出现(同义词)；反之，如果查询语句或者文档中的某个单词和其他单词的相关性都不大，那么这个词很可能表示的是另外一个意思(比如在讨论互联网的文章中，Apple更可能指的是Apple公司，而不是水果)  。
    > 
    >   LSA(LSI)使用SVD来对单词-文档矩阵进行分解。SVD可以看作是从单词-文档矩阵中发现不相关的索引变量(因子)，将原来的数据映射到语义空间内。在单词-文档矩阵中不相似的两个文档，可能在语义空间内比较相似。
    > 
    >   SVD，亦即奇异值分解，是对矩阵进行分解的一种方法，一个t*d维的矩阵(单词-文档矩阵)X，可以分解为T*S*D^T^，其中T为t*m维矩阵，T中的每一列称为左奇异向量(left singular bector)，S为m*m维对角矩阵，每个值称为奇异值(singular value)，D为d*m维矩阵,D中的每一列称为右奇异向量。在对单词文档矩阵X做SVD分解之后，我们只保存S中最大的K个奇异值，以及T和D中对应的K个奇异向量，K个奇异值构成新的对角矩阵S'，K个左奇异向量和右奇异向量构成新的矩阵T'和D'：X'=T'*S'*D'^T^形成了一个新的t*d矩阵。
    > 
    > ---
    > 
    > 在 **查询** 时，对与每个给定的查询，我们根据这个查询中包含的单词(X~q~)构造一个伪文档：D~q~=X~q~TS^-1^，然后该伪文档和D'中的每一行计算相似度(余弦相似度)来得到和给定查询最相似的文档。
    > 




## LDA

### 原理解說


> - [LDA原理解說-ppt](https://docs.google.com/presentation/d/1xGGetms4CyirQAWbKM4i9I1sVPty8S9j_8n_GR6cFqg/edit?usp=sharing) [name=Ya-Lun Li]



- [Latent Dirichlet Allocation](http://www.jmlr.org/papers/v3/blei03a.html)

- [LDA-math-汇总 LDA数学八卦 | 我爱自然语言处理](http://www.52nlp.cn/lda-math-%e6%b1%87%e6%80%bb-lda%e6%95%b0%e5%ad%a6%e5%85%ab%e5%8d%a6)


- [LDA(Latent Dirichlet Allocation)主题模型 - CSDN博客](https://blog.csdn.net/aws3217150/article/details/53840029)

    > LDA于2003年由 David Blei, Andrew Ng和 Michael I. Jordan提出，因为模型的简单和有效，掀起了主题模型研究的波浪。虽然说LDA模型简单，但是它的数学推导却不是那么平易近人，一般初学者会深陷数学细节推导中不能自拔。于是牛人们看不下去了，纷纷站出来发表了各种教程。国内方面rickjin有著名的《[LDA数学八卦](http://www.52nlp.cn/author/rickjin)》，国外的Gregor Heinrich有著名的《[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)》。其实有了这两篇互补的通俗教程，大家沉住心看个4、5遍，基本就可以明白LDA为什么是简单的了。那么其实也没我什么事了，然而心中总有一种被大牛点播的豁然开朗的快感，实在是不吐不快啊。
    > 
    > 什么是主题
    > =====
    > 
    > 因为LDA是一种主题模型，那么首先必须明确知道LDA是怎么看待主题的。对于一篇新闻报道，我们看到里面讲了昨天NBA篮球比赛，那么用大腿想都知道它的主题是关于体育的。为什么我们大腿会那么聪明呢？这时大腿会回答因为里面出现了"科比"、"湖人"等等关键词。那么好了，我们可以定义主题是一种关键词集合，如果另外一篇文章出现这些关键词，我们可以直接判断他属于某种主题。但是，亲爱的读者请你想想，这样定义主题有什么弊端呢？按照这种定义，我们会很容易给出这样的条件：一旦文章出现了一个球星的名字，那么那篇文章的主题就是体育。可能你马上会骂我在瞎说，然后反驳说不一定，文章确实有球星的名字，但是里面全部在讲球星的性丑闻，和篮球没半毛钱关系，此时主题是娱乐还差不多。所以一个词不能硬性地扣一个主题的帽子，如果说一篇文章出现了某个球星的名字，我们只能说有很大概率他属于体育的主题，但也有小概率属于娱乐的主题。于是就会有这种现象：**同一个词，在不同的主题背景下，它出现的概率是不同的**。并且我们都可以基本确定，一个词不能代表一种主题，那么到底什么才是主题呢？耐不住性子的同学会说，既然一个词代表不了一种主题，那我就把所有词都用来代表一种主题，然后你自己去慢慢意会。没错，这样确实是一种完全的方式，主题本来就蕴含在所有词之中，这样确实是最保险的做法，但是你会发现这样等于什么都没做。老奸巨猾的LDA也是这么想的，但他狡猾之处在于它用非常圆滑手段地将主题用所有词汇表达出来。怎么个圆滑法呢？手段便是概率。LDA认为天下所有文章都是用基本的词汇组合而成，此时假设有词库V={v1,v2,....,vn}
    > 
    > ，那么如何表达主题k
    > 
    > 呢？LDA说通过词汇的概率分布来反映主题！多么狡猾的家伙。我们举个例子来说明LDA的观点。假设有词库
    > 
    > {科比，篮球，足球，奥巴马，希拉里，克林顿}
    > 
    > 假设有两个主题
    > 
    > {体育，政治}
    > 
    > LDA说体育这个主题就是：
    > 
    > {科比:0.3，篮球:0.3，足球:0.3，奥巴马:0.03，希拉里:0.03，克林顿:0.04}
    > 
    > (数字代表某个词的出现概率)，而政治这个主题就是：
    > 
    > {科比:0.03，篮球:0.03，足球:0.04，奥巴马:0.3，希拉里:0.3，克林顿:0.3}
    > 
    > LDA就是这样说明什么是主题的，竟说得我无言以对，细思之下也是非常合理。
    > 
    > 文章在讲什么
    > ======
    > 
    > 给你一篇文章读，然后请你简要概括文章在讲什么，你可能会这样回答：80%在讲政治的话题，剩下15%在讲娱乐，其余都是废话。这里大概可以提炼出三种主题：政治，娱乐，废话。也就是说，对于某一篇文章，很有可能里面不止在讲一种主题，而是几种主题混在一起的。读者可能会问，LDA是一种可以从文档中提炼主题模型，那他在建模的时候有没有考虑这种情形啊，他会不会忘记考虑了。那您就大可放心了，深谋远虑的LDA早就注意到这些了。LDA认为，文章和主题之间并不一定是一一对应的，也就是说，文章可以有多个主题，一个主题可以在多篇文章之中。这种说法，相信读者只能点头称是。假设现在有K
    > 
    > 个主题，有M
    > 
    > 篇文章，那么每篇文章里面不同主题的组成比例应该是怎样的呢？由于上一小节我们知道不能硬性的将某个词套上某个主题，那么这里我们当然不能讲某个主题套在某个文章中，也就是有这样的现象：**同一个主题，在不同的文章中，他出现的比例(概率)是不同的**，看到这里，读者可能已经发现，文档和主题之间的关系和主题和词汇的关系是多么惊人的类似！LDA先人一步地将这一发现说出来，它说，上一节我们巧妙地用词汇的分布来表达主题，那么这一次也不例外，我们巧妙地用主题的分布来表达文章！同样，我们举个例子来说明一下，假设现在有两篇文章：
    > 
    > 《体育快讯》，《娱乐周报》
    > 
    > 有三个主题
    > 
    > 体育，娱乐，废话
    > 
    > 那么
    > 
    > 《体育快讯》是这样的:[废话,体育,体育,体育,体育,....,娱乐,娱乐]
    > 
    > 而
    > 
    > 《娱乐周报》是这样的:[废话,废话,娱乐,娱乐,娱乐,....,娱乐,体育]
    > 
    > 也就是说，一篇文章在讲什么，通过不同的主题比例就可以概括得出。
    > 
    > 文章是如何生成的
    > ========
    > 
    > 在前面两小节中，LDA认为，每个主题会对应一个词汇分布，而每个文档会对应一个主题分布，那么一篇文章是如何被写出来的呢？读者可能会说靠灵感+词汇。LDA脸一沉，感觉完蛋，灵感是什么玩意？LDA苦思冥想，最后没办法，想了个馊主意，它说
    > 
    > 灵感=随机
    > 
    > 这也是没办法中的办法。因此对于某一篇文章的生产过程是这样的：
    > 
    > 1.  确定主题和词汇的分布
    > 2.  确定文章和主题的分布
    > 3.  随机确定该文章的词汇个数N
    > 
    > -\
    >     -   如果当前生成的词汇个数小于N
    > 
    > 1.  执行第5步，否则执行第6步
    > 2.  由文档和主题分布随机生成一个主题，通过该主题由主题和词汇分布随机生成一个词，继续执行第4步
    > 3.  文章生成结束
    > 
    > 只要确定好两个分布(主题与词汇分布，文章与主题分布)，然后随机生成文章各个主题比例，再根据各个主题随机生成词，词与词之间的顺序关系被彻底忽略了，这就是LDA眼中世间所有文章的生成过程！聪明的读者肯定觉得LDA完全就是一个骗子，这样认为文章的生成未免也太过天真了吧。然而事实就是如此，这也是为什么说LDA是一个很简单的模型。幸好我们这是用LDA来做主题分析，而不是用来生成文章，而从上上节的分析我们知道，主题其实就是一种词汇分布，这里并不涉及到词与词的顺序关系，所以LDA这种BOW(bag of words)的模型也是有它的简便和实用之处的。
    > 
    > LDA数学分析
    > =======
    > 
    > 上一小节，我们忽略了一个问题，就是如何确定主题和词汇分布，还有文档与词汇的分布，但是要搞明白这个问题，就避免不了一些数学分析了。再次强烈推荐rickjin的《[LDA数学八卦](http://www.52nlp.cn/author/rickjin)》还有Gregor Heinrich有著名的《[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)》。此处我只做一些关键扼要的理论推导 :-)
    > 
    > 多项式分布
    > -----
    > 
    > 回忆一下概率论学的东西，假设一个硬币正面朝上的概率是p
    > 
    > ，如果重复抛n次硬币，正面朝上的次数为k
    > 
    > 的概率分布就是二项分布，也就是
    > 
    > Cknpk(1-p)n-k
    > 
    > 如果抛n次k个不同的硬币，假设每个硬币正面朝上的概率分别是p1,p2,....,pk,那么抛n次之后，各个硬币正面朝上的次数n1,n2,....,nk的概率分布就是多项式分布了，记为
    > 
    > n!n1!n2!,....,nk!p1n1p2n2....pknk
    > 
    > LDA出来说话了：**主题和词汇的分布就是多项式分布**！因为一个主题里面不同词汇出现的概率不同，如果只有1个词汇，那么它就是二项分布，但是词汇不可能只有1个，所以它理所当然就是符合多项式分布。这是挺合理的假设，反正我们现在研究的内容是为词汇按主题给出不同的概率分布，其实也就是给出不同词汇在不同主题下的出现比例，我们并不关系词汇之间的顺序关系，所以多项式分布已经很好地刻画出这种关系了。LDA又说：**文章和主题之间的分布也是符合多项式分布**！因为一篇文章要确定不同主题的出现概率，和主题要确定每个词汇的出现概率是完全可以类比的！思考一下我们也接受了LDA的说法，接下来我们介绍一些记号：
    > 
    > -   V
    > 
    > -   代表我们字典的词汇个数-   K-   代表主题的个数-   M-   代表文章的个数-   ϕk→代表第k个主题的多项式分布参数，长度为V，因此Φ是一个K∗V-   的矩阵，每一行代表一个主题的多项式分布参数-   θm→代表第m篇文章的多项式分布参数，长度为K，因此Θ是一个M∗K-   的矩阵，没一行代表一篇文章的多项式分布参数-   Nm代表第m-   篇文章的长度-   zm,n代表第m篇文章第n-   个词由哪个主题产生的-   wm,n代表第m篇文章第n
    > 
    > -   个词
    > 
    > 对于第m
    > 
    > 篇文章，令z
    > 
    > 代表一次实验产生的主题随机变量，那么它就服从:
    > 
    > z∼Multi(z|θm→)\
    > 那么对于第k个主题，令w代表一次实验产生的词随机变量，那么它就服从：
    > 
    > w∼Multi(w|ϕk→)
    > 
    > 然后为了产生第m篇文章，只要简单的按顺序利用Multi(z|θm→)随机生成zm,1,zm,2,zm,3,...,zm,Nm，然后对号入座，再利用Multi(w|ϕzm,n→)，生成wm,1,wm,2,wm,3,...,wm,Nm即可。
    > 
    > Dirichlet分布
    > -----------
    > 
    > 忘记告诉大家，LDA属于江湖中的贝叶斯学派，在贝叶斯学派眼中，像上面提到的ϕk→
    > 
    > 和θm→
    > 
    > 都是随机变量，随机变量怎么可以没有先验概率分布呢？这岂不是贻笑大方吗？所以LDA整理了一下衣领，马上提出了他们的先验概率分布：
    > 
    > ϕ⃗ ∼Dirichlet(α⃗ )
    > 
    > 而
    > 
    > θ⃗ ∼Dirichlet(β⃗ )
    > 
    > 为什么LDA要说它的先验分布是Dirichlet分布呢？其中最大的原因是因为多项式分布和Dirichlet分布是一对共轭分布，共轭分布有什么好处呢？好处在于计算后验概率有极大的便利！说到底是LDA看中它计算方便。增加了先验概率分布，那么在确定文章与主题分布还有主题与词汇分布的时候，就由先验概率分布先随机生成确定多项式分布的参数。用一个联合概率分布来描述第m篇文章生成过程：
    > 
    > p(zm→,wm→,θm→,Φ|α⃗ ,β⃗ )=∏nNmp(wm,n|ϕzm,n→)p(zm,n|θm→)p(θm→|α⃗ )p(Φ|β⃗ )\
    > 对于习惯了使用极大似然法的同学，为了使用极大似然法，我们必须将隐含变量消除，那么对于第m篇文章其生成的边缘概率为：
    > 
    > p(wm→|α⃗ ,β⃗ )=∫θm∫Φ∫zm→∏nNmp(wm,n|ϕzm,n→)p(zm,n|θm→)p(θm→|α⃗ )p(Φ|β⃗ )=∫θm∫Φ∑zm→∏nNmp(wm,n|ϕzm,n→)p(zm,n|θm→)p(θm→|α⃗ )p(Φ|β⃗ )=∫θm∫Φ∑zm1∑zm2...∑zmNm∏nNmp(wm,n|ϕzm,n→)p(zm,n|θm→)p(θm→|α⃗ )p(Φ|β⃗ )=∫θm∫Φ∏nNm∑zmnp(wm,n|ϕzm,n→)p(zm,n|θm→)p(θm→|α⃗ )p(Φ|β⃗ )\
    > 以上可以看到，边缘概率分布实在是太复杂了，靠普通极大似然法来求解基本无望。不过这并不能难倒我们聪明的计算机科学家，下面我们来介绍一种近似求解法。
    > 
    > Gibbs 采样算法
    > ----------
    > 
    > ### 采样算法的思想
    > 
    > 对于一个概率分布p(x⃗ )
    > 
    > ，我们想得到它得概率分布，无奈有些概率分布形式实在复杂，我们无法直接求解，那么怎么办呢？假设我们的随机变量为：
    > 
    > x⃗ =[x1,x2,x3,...,xi,...,xn]\
    > 其中xi∈{0,1}，假设我们知道p(x⃗ )的概率分布，那么我们可以通过采样的方式得到每一次的样本值：
    > 
    > x⃗ (1)=[0,0,0,0,....0]x⃗ (2)=[1,1,0,0,....0]x⃗ (3)=[1,0,0,0,....0]x⃗ (4)=[1,1,0,0,....0].......x⃗ (N)=[1,1,0,0,....0]\
    > 那么我们可以统计每个样本出现的次数，然后除以总的采样次数N来得到相应概率分布。比如我们观察样本值[1,1,0,0,....]出现了K次，那么样本出现的概率就可以如下求得：
    > 
    > p(x1=1,x2=1,x3=0,....,xn=0)=KN\
    > 这是采样的基本思想，但是这里有令人匪夷所思的地方，首先我们不知道概率分布想得到概率分布我们必须通过采样方法来求得，但是采样方法又依赖于我们知道对应的概率分布才能得到相应的采样值，这是一个"鸡生蛋，蛋生鸡"的矛盾，足以令人百思不得其解，此时神奇的Gibbs采样更加令人充满敬畏。
    > 
    > ### 神奇的Gibbs采样
    > 
    > 实际应用中，我们通常是有一堆文章，然后要我们去自动提取主题，或者通过现有的文章训练出LDA模型，然后预测新的文章所属主题分类。也就是我们的兴趣点在于求出Θ
    > 
    > 和Φ
    > 
    > 的后验概率分布。虽然LDA的模型思想很简单，但是要精确求出参数的后验概率分布却是不可行的，只能通过近似的方式求解。幸好前人已经发现了很多近似求解方法，其中比较简单的就是Gibbs采样，Gibbs的采样精神很简单，对于一个很复杂的高维概率分布:
    > 
    > p(x⃗ )\
    > 我们想获得p(x⃗ )的样本，从而确定p(x⃗ )的参数值，也就是我们无法直接求取概率分布的参数，但是我们可以通过神奇的Gibbs采样，获得需要求解的概率分布的样本值，从而反过来确定概率分布。Gibbs采样很简单，它说如果你能很容易求解（通常都是很容易求解，因为此时的分布是一个一维的条件分布）
    > 
    > p(xi|x¬i→)\
    > 这样的条件分布，那么想获得联合分布的样本，只需执行如下过程：
    > 
    > -   随机初始化x⃗ 0={x01,x02,...,x0N}
    > 
    > -\
    >     -   对于 t=1,2,3,4,....T:
    > 
    >     -   xt1∼p(x1|xt-1¬1→)-\
    >     -   xt2∼p(x2|xt-1¬2→)-\
    >     -   xt3∼p(x3|xt-1¬3→)-\
    >     -   ....-   xtN∼p(xN|xt-1¬N→)-\
    >     -   得到一个样本值x⃗ (t)=[xt1,xt2,xt3,....,xtN]
    > 
    > -   -
    > 
    > 当采样过程收敛之后，通过以上采样得到的样本就是真实的 p(x⃗ )
    > 
    > 样本。为什么可以如此神奇地操作，再次推荐rickjin的《LDA数学八卦》！
    > 
    > Collapsed Gibbs Sampler
    > -----------------------
    > 
    > 对于LDA的Inference问题，有了万能的Gibbs采样算法，问题求解变得简单了。上面我们已经知道LDA模型的文档建模的联合分布：
    > 
    > p(zm→,wm→,θm→,Φ|α⃗ ,β⃗ )\
    > 为了采样方便，对于参数θm→，Φ，它们本身就是关联z⃗ 和w⃗ 的，也就是我们如果得到z⃗ 和w⃗ 的采样，参数θm→，Φ自然可以得知。那么可以积分化简，得到最终要采样的模型：
    > 
    > p(z⃗ ,w⃗ |α⃗ ,β⃗ )\
    > 有了联合分布，Gibbs万能算法就可以套用了，首先它必须先解决一个问题(为了表达方便，超参数α⃗ ,β⃗ 省略)：
    > 
    > p(zm,i|zm,¬i,wm→)
    > 
    > 其中zm→={zm,i＝k,zm,¬i}，zm,¬i代表第m篇文章里面去除主题zm,i的其他所有主题。其具体推导一开始推荐的文章都有详尽严格的过程，这里实在没有必要在赘述，直接给出结果(为了表达方便，这里去除下标m)：
    > 
    > p(zi=k|z¬i,w⃗ )=p(w⃗ ,z⃗ )p(w⃗ ,z¬i→)=p(w⃗ |z⃗ )p(w¬i|z¬i→)p(wi)p(z⃗ )p(z¬i)→∝p(w⃗ |z⃗ )p(w¬i|z¬i→)p(z⃗ )p(z¬i)→∝n(t)k,¬i+βt∑Vt=1n(t)k,¬i+βtn(k)m,¬i+αk∑Kk=1n(k)m,¬i+αk∝n(t)k,¬i+βt∑Vt=1n(t)k,¬i+βt(n(k)m,¬i+αk)
    > 
    > 其中n(t)k代表第m篇文章中词汇t属于主题k出现的次数，n(t)k,¬i代表除去其中zi的剩余个数，也就是说
    > 
    > n(t)k,¬i=n(t)k-1
    > 
    > 类似的，n(k)m代表第m篇文章中主题k出现的次数，n(k)m,¬i代表除去其中zi的剩余个数，也就是说
    > 
    > n(k)m,¬i=n(k)m-1
    > 
    > 所以计算p(zi|z¬i,w⃗ )变成简单地在计数的问题。通过Gibbs采样算法后，最后的模型参数可以这样得出：
    > 
    > ϕk,t=n(t)k+βt∑Vt=1n(t)k+βt
    > 
    > θm,k=n(k)m+αk∑Kk=1n(k)m+αk
    > 
    > 判断新文章的主题分布
    > ----------
    > 
    > 由上一小节，我们可以通过大量文章求解出LDA这个模型，那么对于一篇新的文章，如何计算它的主题分布呢？一个方式是将文章加入到原来的训练集合里面{z~⃗ ,z⃗ ;w~⃗ ,w⃗ }
    > 
    > ，然后得到它的采样条件概率为：
    > 
    > p(zi~=k|z~⃗ ¬i,z⃗ ,w⃗ ,w~⃗ )∝n(t)k+n~(t)k,¬i+βt∑Vt=1n(t)k+n~(t)k,¬i+βtn(k)m~,¬i+αk∑Kk=1n(k)m~,¬i+αk
    > 
    > 再重新跑一次Gibbs采样，然后得到它的分布。实际上这种做法确实是最优的，但是太慢了，怎么办呢？因为新来的文章，它的词汇是固定的，我们上节已经求出ϕk,t，也就是词汇和主题的分布已经是确定的，我们没必要再浪费计算力了，也就是我们认为对一篇新文章，它已经难以撼动之前成千上万文章的统计结果了，也就是说：
    > 
    > n(t)k+n~(t)k,¬i+βt∑Vt=1n(t)k+n~(t)k,¬i+βt≈n(t)k+βt∑Vt=1n(t)k+βt
    > 
    > 所以我们新的条件采样概率变成了：
    > 
    > p(zi~=k|w~i=t,z~⃗ ¬i,z⃗ ,w⃗ ,w~¬i→)∝ϕk,t∗(n(k)m~,¬i+αk)
    > 
    > 然后我们又可以开动Gibbs采样发动机，得到最终的分布结果：
    > 
    > θm~,k=n(k)m~+αk∑Kk=1n(k)m~+αk
    > 
    > LDA的Gibbs采样实现
    > =============
    > 
    > 对于程序员来说，看惯代码，没有代码有点无所适从，有没有简单LDA的实现漂亮代码呢？答案是有的，LingPipe里面的LatentDirichletAllocation这个类，完整地按照Gregor Heinrich有著名的《[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)》介绍的算法实现了，代码非常简单，并且可读性极高，建议抓来一看，必然大有毗益。此处我们贴出Gregor中提供的伪代码，以供查看：
    > 
    > -   初始化阶段，出乎意料的简单，只要初始化4个统计量，分别是:
    > 
    >     -   文档m
    > 
    > 对应主题k的计数： nkm-\
    >     -   文档m的词汇数：nm-\
    >     -   主题k对应的词汇为t的计数：ntk-\
    >     -   主题k的词汇数：nk-   -\
    >         -   将nkm,nm,ntk,nk内存清0，然后根据以下程序随机初始化值：
    > 
    > -   遍历每一个文档m∈[1,M]
    > 
    > -   遍历每个词汇n∈[1,Nm]
    > 
    > -   从多项式分布Mult(1/K)
    > 
    > 得到一个采样值：zmn=k-\
    >     -   nkm=nkm+1-\
    >     -   nm=nm+1-\
    >     -   ntk=ntk+1-\
    >     -   nk=nk+1-   -   -   -\
    >                 -   Gibbs采样过程，以下是一个采样周期的执行过程：
    > 
    >     -   遍历每一个文档m∈[1,M]
    > 
    > -   遍历每个词汇n∈[1,Nm]
    > 
    > -   对于当前的wm,n
    > 
    > 的主题k对应的词汇t执行：
    > 
    > -   nkm=nkm-1
    > 
    > -\
    >     -   nm=nm-1-\
    >     -   ntk=ntk-1-\
    >     -   nk=nk-1-\
    >     -   根据p(zi=k|z¬i,w⃗ )获得一个采样值：k^=zm,n并执行：
    > 
    > -   nk^m=nk^m+1
    > 
    > -\
    >     -   nm=nm+1-\
    >     -   ntk^=ntk^+1-\
    >     -   nk^=nk^+1
    > 
    > -   -   -   -   -   -
    > 
    > 难以置信的简单，一般在收敛之前，需要跑一定数量的采样次数让采样程序大致收敛，这个次数一般称为：burnin period。我们希望从采样程序中获得独立的样本，但是显然以上的采样过程明显依赖上一个采样结果，那么我们可以在上一次采样后，先抛弃一定数量的采样结果，再获得一个采样结果，这样可以大致做到样本独立，这样真正采样的时候，总有一定的滞后次数，这样的样本与样本的间隔称为：SampleLag。
    > 
    > LDA为什么能work
    > ===========
    > 
    > 如果您反复读了前面反复强调的两篇LDA科普大作，并清楚了解它的实现细节，有一些问题可能会慢慢萦绕在心中，挥之不去------为什么LDA能够work？为什么LDA能产生如下结果：
    > ![这里写图片描述](https://img-blog.csdn.net/20170831143634112?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXdzMzIxNzE1MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    > 为什么它的结果会这么神奇，每个主题下面的词分布基本符合我们的直觉。比如computers这个主题下面的词汇分布是"computer，model，information，data，......"。
    > 前面我们提到，LDA模型生成过程很简单，基本上是极其naive的，但是为什么它就能产生这样符合直觉的结果呢？（虽然说从直觉上，判断一篇文章的主题，即使文章词的顺序打乱了，我们还是能大致判断主题的，也就是LDA这种BOW模型有它的合理性，但是这并不能解释它为什么能产生如上结果）
    > 
    > 现在我们再回顾一下，对于LDA模型，我们推断的目标是它背后的隐结构，也就是"文档-主题分布"和"主题-词汇分布"，那么我们再来仔细观察它的后验概率分布（原谅我为了表达方便，改变了一些记号）：
    > 
    > p(z|w,θ)∝p(w,z,θ|α)=∏mp(θm|α)∏np(zmn|θm)p(wmn|zmn,Φ)
    > 首先需要明确的是**隐变量的后验概率分布是正比于联合概率分布**的。在极大似然法中，**我们会希望模型越接近实际越好，也就是模型在数据上的概率越大越好**。我们先只考虑上面表达式的数据似然项p(wmn|zmn,Φ)，为了使这一似然项越大越好，我们会希望某个**主题下对应的词的概率越大越好**，由于主题-词汇分布由Φ决定，理想情况下，我们希望Φ矩阵应该是长下面这个样子：
    > ![这里写图片描述](https://img-blog.csdn.net/20170831181756008?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXdzMzIxNzE1MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    > 横向代表主题，总共有K行，列向代表词汇，总共有V列。
    > 
    > 我们希望每个主题的词汇概率质量如上图蓝色方块所示（方块的概率和为1），也就是词汇按照主题个数不重叠地切割，这样可以保证词汇在所属主题下概率值最大，因为每一行概率和为1，我们希望概率密度集中在某些词上面，而不是分散地落在每个词上面（请读者思考一下为什么这样会使最终的概率值增大，这里比较难以用文字表达清楚，可以设想一下，假如每个主题的概率密度均匀分配到每个词上面，那么最终每一项p(wmn|zmn,Φ)
    > 
    > 会很小，这与我们的愿望是违背的）。为了使模型在数据上的概率最大，算法会倾向于将主题词汇分布按照上图的形式拟合，也就是"主题-词汇"分布会倾向于成为一个稀疏的分布。
    > 
    > 有了上面的讨论，我们再来想想，每个主题会选哪些词汇作为自己的主题词呢？ 也就是主题应该选哪些词来将自己的概率质量散落在他们身上。答案是那些经常出现在一起的词。假设主题A下面有主题词：a1,a2,...,an
    > 
    > ，为了使概率值变大，那么这些词一定是同时出现在很多个文档里面，并且在多个文档中，这些词大部分都同时出现。可以想想为什么要这样，假设不是这样，比如在文档1种主题A的词汇是a1,a2,a3,a4，在文档2中主题A的词汇是a5,a6,a7,a8，在文档3中主题A的词汇是a9,a10,a11,a12
    > 
    > ，以此类推，如果主题A的词汇真的是这个样子------文档中同一主题都没有在另一个文档中出现，那么主题A下面的词会很多，但是我们上面分析了，为了使模型概率值大，每个主题下面的词必须越少越好，所以这也是有违背愿望的。因此**主题下面的词都会倾向于同时出现在多个文档中**，到这里为止，读者应该可以大概明白为什么LDA可以产生那么符合直觉的词汇分布了，因为在我们人类自己的概念中，主题词汇就是这样的东西，他们会经常一起出现，比如"银行"，"存款"这些金融主题词汇会很频繁同时出现在多个场合中，所以我们会将他们归为一个主题，而LDA恰恰能捕获这种特性。
    > 
    > Edwin Chen的博客《[Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)》也比较直观地介绍了LDA的直觉特性。至于怎么从理论上来说明为什么具有稀疏性质， [Quora上面有一个相对直观的解释](https://www.quora.com/Why-does-LDA-work)，我大概总结一下，由于LDA用Dirichlet作为Prior分布，而作为Prior的Dirichlet在其分布参数α⃗ 
    > 
    > 取很小的时候（一般α取K/50, β
    > 
    > 取值0.01），可以使得其采样的多项式参数变得稀疏。LDA有两对稀疏对抗，主题与文档的稀疏性和主题与词汇的稀疏性之间的对抗，而LDA会从数据中学习到一个权衡结果。为什么Dirichlet会有稀疏性质呢？可以参照以这篇：
    > 《[Notes on Multinomial sparsity and the Dirichlet concentration parameter α](http://www.cs.cmu.edu/~dbamman/notes/dirichletConcentration.pdf)》
    > 这篇note提到的Dirichlet其实可以看成几个Gamma分布变换而来，具体变换证明可以参照Quora另一个解答：[Construction of Dirichlet distribution with Gamma distribution](http://stats.stackexchange.com/questions/36093/construction-of-dirichlet-distribution-with-gamma-distribution)。另一个好处是，多了先验分布的模型比pLSA更加健壮，不容易导致overfiting，如果看回上面推导Gibbs Sampling的公式，Dirichlet其实起到一种Smooth的效果。可以再参考Kevin Gimpel写的《[Modeling Topics](http://www.cs.cmu.edu/~nasmith/LS2/gimpel.06.pdf)》，对于几种常见基础的主题模型的对比，也可以解除不少困惑。
    > 
    > 参考文献
    > ====
    > 
    > -   《[LDA数学八卦](http://www.52nlp.cn/author/rickjin)》
    > -   《[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)》
    > -   《[gibbs sampling for the uninitiated](http://wwwold.cs.umd.edu/~hardisty/papers/gsfu.pdf)》
    > -   《[text mining and topic models](http://cseweb.ucsd.edu/~elkan/250B/topicmodels.pdf)》
    > -   《[A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling](http://u.cs.biu.ac.il/~89-680/darling-lda.pdf)》
    > -   《[Probabilistic Latent Semantic Analysis](http://dl.acm.org/citation.cfm?id=2073829)》
    > -   [LingPipe](http://alias-i.com/lingpipe/)
    > -   《[Modeling Topics](http://www.cs.cmu.edu/~nasmith/LS2/gimpel.06.pdf)》
    > -   《[Notes on Multinomial sparsity and the Dirichlet concentration parameter α](http://www.cs.cmu.edu/~dbamman/notes/dirichletConcentration.pdf)》
    > -   [Why does LDA works？](https://www.quora.com/Why-does-LDA-work)
    > -   [Construction of Dirichlet distribution with Gamma distribution](http://stats.stackexchange.com/questions/36093/construction-of-dirichlet-distribution-with-gamma-distribution)
    > -   Dave Blei's video lecture on topic models: < http://videolectures.net/mlss09uk_blei_tm>

- [Conjugate prior - Wikiwand](https://www.wikiwand.com/en/Conjugate_prior)

    > Example
    > -------
    > 
    > The form of the conjugate prior can generally be determined by inspection of the [probability density](https://www.wikiwand.com/en/Probability_density_function) or [probability mass function](https://www.wikiwand.com/en/Probability_mass_function) of a distribution. For example, consider a [random variable](https://www.wikiwand.com/en/Random_variable) which consists of the number of successes ![s](https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632) in ![n](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) [Bernoulli trials](https://www.wikiwand.com/en/Bernoulli_trial) with unknown probability of success ![q](https://wikimedia.org/api/rest_v1/media/math/render/svg/06809d64fa7c817ffc7e323f85997f783dbdf71d) in [0,1]. This random variable will follow the [binomial distribution](https://www.wikiwand.com/en/Binomial_distribution), with a probability mass function of the form
    > 
    > ![{\displaystyle p(s)={n \choose s}q^{s}(1-q)^{n-s))](https://wikimedia.org/api/rest_v1/media/math/render/svg/db06457d8f50347a044b7672740227e722331976)
    > 
    > The usual conjugate prior is the [beta distribution](https://www.wikiwand.com/en/Beta_distribution) with parameters (![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3), ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8)):
    > 
    > ![p(q)={q^{\alpha -1}(1-q)^{\beta -1} \over \mathrm {B} (\alpha ,\beta )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c924d010fed83e219416a1ffb41331aa429ada9c)
    > 
    > where ![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) and ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) are chosen to reflect any existing belief or information (![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) = 1 and ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) = 1 would give a [uniform distribution](https://www.wikiwand.com/en/Uniform_distribution_(continuous) "Uniform distribution (continuous)")) and *Β*(![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3), ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8)) is the [Beta function](https://www.wikiwand.com/en/Beta_function) acting as a [normalising constant](https://www.wikiwand.com/en/Normalising_constant).
    > 
    > In this context, ![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) and ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) are called *[hyperparameters](https://www.wikiwand.com/en/Hyperparameter)* (parameters of the prior), to distinguish them from parameters of the underlying model (here *q*). It is a typical characteristic of conjugate priors that the dimensionality of the hyperparameters is one greater than that of the parameters of the original distribution. If all parameters are scalar values, then this means that there will be one more hyperparameter than parameter; but this also applies to vector-valued and matrix-valued parameters. (See the general article on the [exponential family](https://www.wikiwand.com/en/Exponential_family), and consider also the [Wishart distribution](https://www.wikiwand.com/en/Wishart_distribution "Wishart distribution"), conjugate prior of the [covariance matrix](https://www.wikiwand.com/en/Covariance_matrix) of a [multivariate normal distribution](https://www.wikiwand.com/en/Multivariate_normal_distribution), for an example where a large dimensionality is involved.)
    > 
    > If we then sample this random variable and get *s* successes and *f* failures, we have
    > 
    > ![{\displaystyle {\begin{aligned}P(s,f\mid q=x)&={s+f \choose s}x^{s}(1-x)^{f},\\P(x)&={x^{\alpha -1}(1-x)^{\beta -1} \over \mathrm {B} (\alpha ,\beta )},\\P(q=x\mid s,f)&={\frac {P(s,f\mid x)P(x)}{\int P(s,f\mid x)P(x)dx))\\&=(({s+f \choose s}x^{s+\alpha -1}(1-x)^{f+\beta -1}/\mathrm {B} (\alpha ,\beta )} \over \int _{y=0}^{1}\left({s+f \choose s}y^{s+\alpha -1}(1-y)^{f+\beta -1}/\mathrm {B} (\alpha ,\beta )\right)dy}\\&={x^{s+\alpha -1}(1-x)^{f+\beta -1} \over \mathrm {B} (s+\alpha ,f+\beta )},\\\end{aligned))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cb14cab68a14e4ae9b49c0d3acea79ecf00c97ca)
    > 
    > which is another Beta distribution with parameters (![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) + *s*, ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) + *f*). This posterior distribution could then be used as the prior for more samples, with the hyperparameters simply adding each extra piece of information as it comes.

### LDA 數學推導

$$
\begin{split}
&遊戲中的K個topic-word骰子，可記為\vec{\varphi_{1}},...,\vec{\varphi_{K}}，\\
&對於包含M篇文檔的語料庫C=(d_1,d_2,...,d_M)\\
&中的每篇文檔d_m，都有一個特定的doc-topic骰子\vec{\theta_{m}}，\\
&所有對應的骰子記為\vec{\theta_{1}},...,\vec{\theta_{M}}\\
&\\
&由條件機率，第m篇文檔d_m中的每個詞的生成機率可展開為\\
&p(w|d_m)=\displaystyle\sum_{z=1}^{K}p(w|z)p(z|d_m)=\sum_{z=1}^{K}\varphi_{zw}\theta_{mz}\\
&所以整篇文檔的生成機率為\\
&p(\vec{w}|d_m)=\displaystyle\prod_{i=1}^{n}\sum_{z=1}^{K}p(w_i|z)p(z|d_m)=\prod_{i=1}^{n}\sum_{z=1}^{K}\varphi_{zw_i}\theta_{dz}
\end{split}
$$

---

$$
\begin{split}
&\vec{\theta_m} \sim Dir(\vec{\alpha})\\
&\vec{\varphi_k} \sim Dir(\vec{\beta})
\end{split}
$$

---

$$
\begin{split}
&\vec{\alpha} \rightarrow \vec{\theta_m} \rightarrow z_{m,n} : 這個過程表示如何生成第m篇文檔\\
&第n個詞的topic編號。從第一個罈子抽出M個骰子，\\
&其中的第m個骰子作為這篇文檔的doc-topic骰子\vec{\theta_m}，\\
&然後投擲這個骰子生成文檔中第n個詞的topic編號z_{m,n}\\
\end{split}
$$

---

$$
\begin{split}
&\vec{\beta} \rightarrow \vec{\varphi_k} \rightarrow w_{m,n}|k=z_{m,n} : 這個過程表示如何\\
&生成第m篇文檔的第n個詞。從第二個罈子抽出K個骰子，\\
&其中的第k=z_{m,n}個骰子作為這個詞的topic-word\\
&骰子\vec{\varphi_k}，然後投擲這個骰子生成文檔中第n個詞w_{m,n}
\end{split}
$$

---
$$
\vec{\alpha} \underbrace{\longrightarrow}_{Dirichlet} \vec{\theta_m} \underbrace\longrightarrow_{Multinomial} \vec{z_m}
$$

---


$$
\begin{split}
&p(\vec{z_m}|\vec{\alpha})=\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta\vec{\alpha}}\\
&其中\vec{n_m}=(n_m^{(1)},...,n_m^{(K)})，n_m^{(k)}表示第m篇文檔中\\
&第k個topic產生的詞的個數
\end{split}
$$

---

$$
\begin{split}
p(\vec{z}|\vec{\alpha})&=\prod_{m=1}^{M}p(\vec{z_m}|\vec{\alpha})\\
&=\prod_{m=1}^{M}\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta\vec{\alpha}}\\
\end{split}
$$

---

$$
\vec{\beta} \underbrace{\longrightarrow}_{Dirichlet} \vec{\varphi_k} \underbrace\longrightarrow_{Multinomial} \vec{w_{(k)}}
$$

---

$$
\begin{split}
&p(\vec{w_{(k)}}|\vec{\beta})=\frac{\Delta(\vec{n_k}+\vec{\beta})}{\Delta\vec{\beta}}\\
&其中\vec{n_k}=(n_k^{(1)},...,n_k^{(K)})，n_k^{(t)}表示第k個topic產生的詞中\\
&word\ t 的個數
\end{split}
$$

---

$$
\begin{split}
p(\vec{w}|\vec{z},\vec{\beta})&=\prod_{k=1}^{K}p(\vec{w_{(k)}}|\vec{z_{(k)},\beta})\\
&=\prod_{k=1}^{K}\frac{\Delta(\vec{n_k}+\vec{\beta})}{\Delta\vec{\beta}}\\
\end{split}
$$

---

$$
\begin{split}
p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})&=p(\vec{w}|\vec{z},\vec{\beta})p(\vec{z}|\vec{\alpha})\\
&=\prod_{k=1}^{K}\frac{\Delta(\vec{n_k}+\vec{\beta})}{\Delta\vec{\beta}}\prod_{m=1}^{M}\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta\vec{\alpha}}\\
\end{split}
$$

---

$$
P(X_{t+1}=x|X_t,X_t-1,...)=P(X_{t+1}=x|X_t)
$$

---

$$
π_0P^n=πP=π
$$

---

$$
\begin{split}
&由於\vec{w}是觀測到的已知數據，只有\vec{z}是隱含的變量，\\
&所以我們真正需要採樣的是分布p(\vec{z}|\vec{w})。語料庫\vec{z}中\\
&的第i個詞對應的topic 我們記為z_i，其中i=(m,n)，\\
&表示第m篇文檔的第n個詞，我們用\neg i 表示去除下標為\\
&i的詞。那麼，按照Gibbs Sampling算法的要求，\\
&我們要求得任意座標軸i對應的條件分布\\
&p(z_i=k|\vec{z}_{\neg i},\vec{w})\\
&\\
&假設已觀測到的詞w_i=t，則由貝葉斯法則，\\
&可以得到\\
\end{split}
$$

---

$$
\begin{split}
p(z_i=k|\vec{z}_{\neg i},\vec{w}) &\propto p(z_i=k,w_i=t|\vec{z}_{\neg i},\vec{w}_{\neg i})\\
&=\int p(z_i=k,w_i=t,\vec{\theta}_m,\vec{\varphi}_k|\vec{z}_{\neg i},\vec{w}_{\neg i})d\vec{\theta}_md\vec{\varphi}_k\\
&...\\
&=\int \theta_{mk} Dir(\vec{\theta}_m|\vec{n}_{m,\neg i}+\vec{\alpha}) d \vec{\theta}_m 
\cdot \int \varphi_{kt} Dir(\vec{\varphi}_k|\vec{n}_{k,\neg i}+\vec{\beta})  d \vec{\varphi}_k\\
&=E(\theta_{mk}) \cdot E(\varphi_{kt})\\
&=\hat{\theta}_{mk} \cdot \hat{\varphi}_{kt}\\
\end{split}
$$

---

$$
\begin{split}
\hat{\theta}_{mk}&=\frac{n_{m,\neg i}^{(k)}+\alpha_k}{\sum_{k=1}^{K}(n_{m,\neg i}^{(k)}+\alpha_k)}\\
\hat{\varphi}_{kt}&=\frac{n_{k,\neg i}^{(t)}+\beta_t}{\sum_{t=1}^{V}(n_{k,\neg i}^{(t)}+\beta_t)}\\
\end{split}
$$

---

$$
\begin{split}
p(z_i=k|\vec{z}_{\neg i},\vec{w}) &\propto \underbrace{\frac{n_{m,\neg i}^{(k)}+\alpha_k}{\sum_{k=1}^{K}(n_{m,\neg i}^{(k)}+\alpha_k)}}_{p(topic|doc)} \cdot \underbrace{\frac{n_{k,\neg i}^{(t)}+\beta_t}{\sum_{t=1}^{V}(n_{k,\neg i}^{(t)}+\beta_t)}}_{p(word|topic)}\\
\end{split}
$$

### topic coherence 

- [topic_coherence_tutorial](https://markroxor.github.io/gensim/static/notebooks/topic_coherence_tutorial.html)

- [Coherence Score u_mass : learnmachinelearning](https://www.reddit.com/r/learnmachinelearning/comments/9bcr77/coherence_score_u_mass/)

    > [Palmetto (link)](http://palmetto.aksw.org/palmetto-webapp/?coherence=umass) states that in u_mass, "for every word the logarithm of its conditional probability is calculated using every other top word that has a higher order in the ranking of top words as condition" - so it is not tested against every other word, but only high ranking words.
    > 
    > Also, in [this tutorial (link)](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda) the optimal topic number is being calculated using coherence score, so I feel like it is a valid approach.


- [Gensim Topic Modeling - A Guide to Building Best LDA models](https://web.archive.org/web/20180524091929/https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda)

    > 17\. How to find the optimal number of topics for LDA?
    > ------------------------------------------------------
    > 
    > My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.
    > 
    > Choosing a 'k' that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.
    > 
    > If you see the same keywords being repeated in multiple topics, it's probably a sign that the 'k' is too large.
    > 
    > The `compute_coherence_values()` (see below) trains multiple LDA models and provides the models and their corresponding coherence scores.
    > 
    > ```python
    > def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    >     """
    >     Compute c_v coherence for various number of topics
    > 
    >     Parameters:
    >     ----------
    >     dictionary : Gensim dictionary
    >     corpus : Gensim corpus
    >     texts : List of input texts
    >     limit : Max num of topics
    > 
    >     Returns:
    >     -------
    >     model_list : List of LDA topic models
    >     coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    >     """
    >     coherence_values = []
    >     model_list = []
    >     for num_topics in range(start, limit, step):
    >         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
    >         model_list.append(model)
    >         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
    >         coherence_values.append(coherencemodel.get_coherence())
    > 
    >     return model_list, coherence_values
    > ```
    > 
    > ```python
    > # Can take a long time to run.
    > model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)
    > ```
    > 
    > ```python
    > # Show graph
    > limit=40; start=2; step=6;
    > x = range(start, limit, step)
    > plt.plot(x, coherence_values)
    > plt.xlabel("Num Topics")
    > plt.ylabel("Coherence score")
    > plt.legend(("coherence_values"), loc='best')
    > plt.show()
    > ```
    > 
    > [![Choosing the optimal number of LDA topics](https://i.imgur.com/QVCloCa.jpg)](https://i.imgur.com/QVCloCa.jpg)
    > 
    > Choosing the optimal number of LDA topics
    > 
    > ```python
    > # Print the coherence scores
    > for m, cv in zip(x, coherence_values):
    >     print("Num Topics =", m, " has Coherence Value of", round(cv, 4))
    > ```
    > 
    > ```python
    > Num Topics = 2  has Coherence Value of 0.4451
    > Num Topics = 8  has Coherence Value of 0.5943
    > Num Topics = 14  has Coherence Value of 0.6208
    > Num Topics = 20  has Coherence Value of 0.6438
    > Num Topics = 26  has Coherence Value of 0.643
    > Num Topics = 32  has Coherence Value of 0.6478
    > Num Topics = 38  has Coherence Value of 0.6525
    > ```
    > 
    > If the coherence score seems to keep increasing, it may make better sense to pick the model that gave the highest CV before flattening out. This is exactly the case here.
    > 

    > ---
    > 
    > 18\. Finding the dominant topic in each sentence
    > ------------------------------------------------
    > 
    > One of the practical application of topic modeling is to determine what topic a given document is about.
    > 
    > To find that, we find the topic number that has the highest percentage contribution in that document.
    > 
    > The `format_topics_sentences()` function below nicely aggregates this information in a presentable table.
    > 
    > ```python
    > def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):
    >     # Init output
    >     sent_topics_df = pd.DataFrame()
    > 
    >     # Get main topic in each document
    >     for i, row in enumerate(ldamodel[corpus]):
    >         row = sorted(row, key=lambda x: (x[1]), reverse=True)
    >         # Get the Dominant topic, Perc Contribution and Keywords for each document
    >         for j, (topic_num, prop_topic) in enumerate(row):
    >             if j == 0:  # => dominant topic
    >                 wp = ldamodel.show_topic(topic_num)
    >                 topic_keywords = ", ".join([word for word, prop in wp])
    >                 sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
    >             else:
    >                 break
    >     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
    > 
    >     # Add original text to the end of the output
    >     contents = pd.Series(texts)
    >     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    >     return(sent_topics_df)
    > 
    > df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)
    > 
    > # Format
    > df_dominant_topic = df_topic_sents_keywords.reset_index()
    > df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
    > 
    > # Show
    > df_dominant_topic.head(10)
    > ```
    > 
    > [![Dominant Topic For Each Document](https://i.imgur.com/DZTDFlx.jpg)](https://i.imgur.com/DZTDFlx.jpg)
    > 
    > Dominant Topic For Each Document
    > 
    > 19\. Find the most representative document for each topic
    > ---------------------------------------------------------
    > 
    > Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!
    > 
    > ```python
    > # Group top 5 sentences under each topic
    > sent_topics_sorteddf_mallet = pd.DataFrame()
    > 
    > sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')
    > 
    > for i, grp in sent_topics_outdf_grpd:
    >     sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,
    >                                              grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],
    >                                             axis=0)
    > 
    > # Reset Index
    > sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)
    > 
    > # Format
    > sent_topics_sorteddf_mallet.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", "Text"]
    > 
    > # Show
    > sent_topics_sorteddf_mallet.head()
    > ```
    > 
    > [![Most Representative Topic For Each Document](https://i.imgur.com/D8D09c9.jpg)](https://i.imgur.com/D8D09c9.jpg)
    > 
    > Most Representative Topic For Each Document
    > 
    > The tabular output above actually has 20 rows, one each for a topic. It has the topic number, the keywords, and the most representative document. The `Perc_Contribution` column is nothing but the percentage contribution of the topic in the given document.
    > 
    > 20\. Topic distribution across documents
    > ----------------------------------------
    > 
    > Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information.
    > 
    > ```python
    > # Number of Documents for Each Topic
    > topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()
    > 
    > # Percentage of Documents for Each Topic
    > topic_contribution = round(topic_counts/topic_counts.sum(), 4)
    > 
    > # Topic Number and Keywords
    > topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]
    > 
    > # Concatenate Column wise
    > df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)
    > 
    > # Change Column names
    > df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']
    > 
    > # Show
    > df_dominant_topics
    > ```
    > 
    > [![Topic Volume Distribution](https://i.imgur.com/dm8F3sf.jpg)](https://i.imgur.com/dm8F3sf.jpg)
    > 
    > Topic Volume Distribution
    > 
    > 



### LDA and TFIDF

- [How do you combine LDA and tf-idf? - Quora](https://www.quora.com/How-do-you-combine-LDA-and-tf-idf)

    > LDA requires data in the form of integer counts. So modifying feature values using TF-IDF and then using with LDA doesn't really fit in. You might instead want to try some of the NMF algorithms, which aren't MCMC usually, but they work with general non-negative data. I've seen nice results in CCA this way. However, you can use TF-IDF as a way of screening features/words to then use in the LDA.
    > 
    > Also, in most IR experiments I've done, LDA weights for a document are usually dominated (in value for retrieval) by TF-IDF weights, so not much good.
    > 
    > Having said that, check out:
    > [Why is the performance improved by using TFIDF instead of bag-of-words in LDA clustering?](https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering)
    > where they talk about doing it and [Tanmoy Mukherjee](https://www.quora.com/profile/Tanmoy-Mukherjee-1) gives some insights.


- [Why is the performance improved by using TFIDF instead of bag-of-words in LDA clustering? - Quora](https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering)

    > tf part of tf-idf is the bag of word assumption. Let me add some points where one might use tf-idf to get better performance
    > LDA is similar to matrix factorization i.e it takes a term document matrix and gives two matrices topic by word and document by topic matrix. According to standard approach it is assumed all words are *equally*
    > important for calculating the conditional probability. However certain terms comes with a certain frequency i.e a stop word would have more frequency than certain keywords. So if one were to weight in  the importance of a term
    > you would replace the sampling eqn 2.26 of [1]
    > where you would replace all the counts of the term N with a new count N' which gives you the importance of a term. The remaining steps should remain similar.
    > 
    > [1][http://cxwangyi.files.wordpress....](http://cxwangyi.files.wordpress.com/2012/01/llt.pdf)
    > Distributed Gibbs Sampling of Latent Topic
    > Models: The Gritty Details
    > 
    > P.S I have been lazy to skip details but a good place to initiate this discussion should be in the topic models mailing list.

- [Necessary to apply TF-IDF to new documents in gensim LDA model? - Stack Overflow](https://stackoverflow.com/questions/44781047/necessary-to-apply-tf-idf-to-new-documents-in-gensim-lda-model)

    > tf-idf is used in the latent dirichlet allocation to some extent. As can be read in the paper [Topic Models by Blei and Lafferty](http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf) (e.g. p.6 - Visualizing Topics and p.12), the tf-idf score can be very useful for LDA. It can be used to visualize topics or to chose the vocabulary. "It is often computationally expensive to use the entire vocabulary. Choosing the top V words by TFIDF is an effective way to prune the vocabulary".
    > 
    > This said, LDA does not need tf-idf to infer topics, but it can be useful and it can improve your results.
    > 


## interactive topic modeling/Incorporating Domain Knowledge into Topic Modeling

- [How do I remove non-sense topics generated using LDA? - Quora](https://www.quora.com/How-do-I-remove-non-sense-topics-generated-using-LDA)

    > While the paper on Interactive topic modeling would be something you should definitely look into [Page on nih.gov](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2943854/) where the authors use the concept of must links and cannot links and creating an appropriate Dirichet tree prior.
    > Have a look at these slides where the authors also use first order logic for the same
    > [http://pages.cs.wisc.edu/~andrze...](http://pages.cs.wisc.edu/~andrzeje/publications/ijcai-2011-slides.pdf)
    > Chck some related work by Dan Roth on constrained models although they are not Mixture models
    > Hope that helps
    > 
    > ---
    > 
    > At the risk of self-promotion, you may want to consider interactive topic modeling:
    > [Page on colorado.edu](http://www.cs.colorado.edu/~jbg/docs/mlj_2013_itm.pdf)
    > 
    > More generally, here's a book chapter on care and feeding of topic models:
    > [Page on colorado.edu](http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf)


## nonnegative matrix factorization (NMF)/probabilistic latent semantic analysis (PLSA)

- [What is the difference between NMF and LDA? Why are the priors of LDA sparse-induced? - Quora](https://www.quora.com/What-is-the-difference-between-NMF-and-LDA-Why-are-the-priors-of-LDA-sparse-induced)

    > The first thing to note is that nonnegative matrix factorization (NMF) can be shown to be equivalent to optimizing the same objective function as the one from probabilistic latent semantic analysis (PLSA) ([Ding et al., 2008](http://core.ac.uk/download/pdf/22118.pdf)).  The two approaches only differ in how inference proceeds, but the underlying model is the same.
    > 
    > This makes it easier to compare NMF to latent Dirichlet allocation (LDA), as PLSA is simply the special case of LDA where we assume the Dirichlet prior in the data generating process for LDA is uniform.
    > 
    > On sparseness: This follows directly from the properties of the Dirichlet distribution. David Blei's [Topic Models](http://videolectures.net/mlss09uk_blei_tm/) lecture at 54:30 provides a good intuition for this.


## LFM vs LSA(LSI) vs pLSA(pLSI) vs LDA

- [LSA vs pLSA vs LDA : MachineLearning](https://www.reddit.com/r/MachineLearning/comments/10mdtf/lsa_vs_plsa_vs_lda/)

    > each of them is used to describe incoming set of bag-of-word distribution data into a lower dimensional bag-of-topic data.
    > 
    > LSA -> uses SVD, and as a result the topics are assumed to be orthogonal.
    > 
    > pLSA -> Treats topics as word distributions, uses probabilistic methods, and topics are allowed to be non-orthogonal.
    > 
    > LDA -> similar to pLSA, but with dirichlet priors for the document-topic and topic-word distributions. This prevents over-fitting, and gives better results.
    > 


- [topic model (LSA、PLSA、LDA) - lmm6895071的专栏 - CSDN博客](https://blog.csdn.net/lmm6895071/article/details/74999129)

    > # Topic模型
    > 
    > **概要：**
    > 
    > > [LFM](https://www.baidu.com/s?wd=LFM&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)（依赖于矩阵分解）  
    > > LSA(LSI)（[SVD](https://www.baidu.com/s?wd=SVD&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)分解）  
    > > PLSI（EM算法优化，频率学派，参数未知但固定）  
    > > LDA（在PLSA基础上加上贝叶斯框架，$\alpha$, $\beta$ ~dirichlet分布,分别作为主题-文档和词-主题的先验分布；贝叶斯学派的特点是参数是随机变化的，但是服从某个分布，不断的学习新的知识，形成后验）
    > 
    > **介绍：**
    > 
    > > LFM、LSI、PLSI、LDA都是隐含语义分析技术，是同一类概念；在本质上是相通的，都是找出潜在的主题或特征。这些技术首先在文本挖掘领域中被提出来，近些年也被不断应用到其他领域中，并得到了不错的应用效果。  
    > > 在推荐系统中它能够基于用户的行为对item进行自动聚类，也就是把item划分到不同类别/主题，这些主题/类别可以理解为用户的兴趣。对文本信息进行隐含主题发掘以提取必要特征，譬如LDA获得主题分布之后，可以实现对文档的降维。在论文推荐领域，次LDA+PMF模型实现协同主题回归模型（CTR)。
    > 
    > * * *
    > 
    > ## LFM （隐语义模型）
    > 
    > 例子：  
    > 将用户评分矩阵（混淆矩阵）分解R=P* Q  
    > P矩阵代表了 user-class  
    > Q矩阵代表了class-item  
    > **class:根据自动聚类算法获得几个类标签**；  
    > P、Q中的参数通过模型学习得到：  
    > 最后计算平方损失函数，利用随机梯度下降法，使得损失值最小；  
    > ![矩阵分解](https://i.imgur.com/3lJui0i.jpg)  
    > ![损失函数](https://i.imgur.com/zCMSkta.jpg)
    > 
    > [参考文献](http://blog.csdn.net/harryhuang1990/article/details/9924377)
    > 
    > * * *
    > 
    > ## LSA模型
    > 
    > Latent Semantic Analysis (Latent Semantic Indexing)
    > 
    > > 背景  
    > > 传统的信息检索中：将单词作为特征，构造特征向量；计算查询单词与文档间的相似度；但是没有考虑到语义、同义词等相关信息；在基于单词的检索方法中，同义词会降低检索算法的召回率(Recall)，而多义词的存在会降低检索系统的准确率(Precision)。  
    > > 我们希望找到一种模型，能够捕获到单词之间的相关性。如果两个单词之间有很强的相关性，那么当一个单词出现时，往往意味着另一个单词也应该出现(同义 词)；反之，如果查询语句或者文档中的某个单词和其他单词的相关性都不大，那么这个词很可能表示的是另外一个意思(比如在讨论互联网的文章中，[Apple](https://www.baidu.com/s?wd=Apple&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd) 更可能指的是Apple公司，而不是水果) 。  
    > > $LSA(LSI)使用SVD$来对**单词-文档矩阵**进行分解。$SVD$可以看作是从单词-文档矩阵中发现不相关的索引变量(因子)，将原来的数据映射到语义空间内。在单词-文档矩阵中不相似的两个文档，可能在语义空间内比较相似。
    > 
    > $SVD$，亦即奇异值分解 ，一个$t*d$维的(单词-文档矩阵)$X$，可以分解为:
    > 
    > $$X=T*S*D^T$$
    > 
    > 其中$T$为$t*m$维矩阵，$T$中的每一列称为左奇异向量(left singular vector)，$S$为$m*m$维对角矩阵，每个值称为奇异值(singular value)，$D$为$d*m$维矩阵,$D$中的每一列称为右奇异向量。在对单词文档矩阵$X$做$SVD$分解之后，我们只保存$S$中最大的$K$个奇异值，得到$D^{'}$、$T'$、$S'$；则形成了一个新的$t*d$矩阵：
    > 
    > $$X'=T^{'}*S'*D^{'T}$$
    > 
    > 还原后的X’与X差别很大，这是因为我们认为之前X存在很大的噪音，X’是对X处理过同义词和多义词后的结果。
    > 
    > 在查询时，对与每个给定的查询q，我们根据这个查询中包含的单词($X_q$)构造一个伪文档：$D_q=X_qTS^{-1}$，然后该伪文档和$D'$中的每一行计算相似度(余弦相似度)来得到和给定查询最相似的文档。
    > 
    > [参考文献](http://www.cnblogs.com/kemaswill/archive/2013/04/17/3022100.html)
    > 
    > * * *
    > 
    > > 下面介绍主题模型，PLSA，LDA；  
    > > 这里需要介绍一部分基础知识：共轭分布，频率学派，贝叶斯学派；  
    > > **频率学派思想：** 参数未知，但是固定，可以通过样本，计算最大似然估计获得；  
    > > **贝叶斯学派思想：** 参数未知，是个随机变量，但是服从某个分布；参数服从某个先验分布，然后我们通过现有数据修正模型，获得后验分布；  
    > > 先验知识+数据知识 ———>后验分布；  
    > > **共轭分布**：先验分布的形式和后验分布的形式一样；  
    > > 比如：先验是Beta分布，数据分布是伯努利分布（0-1分布），那么后验分布仍然是Beta分布；  
    > > Dirichlet分布+多项式分布=Dirichlet分布  
    > > ![这里写图片描述](https://i.imgur.com/Y7AugpF.jpg)
    > 
    > ## PLSA模型
    > 
    > > 首先，回顾一元模型，然后引出贝叶斯学派的一元模型；  
    > > ![这里写图片描述](https://i.imgur.com/5k5FVsB.jpg)  
    > > 如图示：  
    > > 一元模型中，不存在潜在主题，我们产生word的过程，相当于投骰子（V面）；那么整个文档集的分布是：(文档直接独立，word之间独立)  
    > > 
    > > $$p(W)=\prod_d^D \prod_i^N p(w_i)=\prod_d^D \prod_v^V p(w_v)^{c_v}$$
    > > 
    > >   
    > > 然后通过最大似然方法获得参数，$\hat{p(w_i)}=\frac{c_i}{C}$,$C$是总的頻数；
    > > 
    > > * * *
    > > 
    > > 混合一元模型：  
    > > 这里，我们假定，一篇文档有一个主题z，因此，  
    > > 
    > > $$p(W,z|d)=p(z|d)\prod_i^Np(w_i|z)\\ p(W|d)=\sum_z p(z|d)\prod_i^Np(w_i| z)$$
    > > 
    > > * * *
    > > 
    > > 以上频率学派思想，现在，利用贝叶斯学派思想，重新思考模型：  
    > > 现在有一个坛子，里面有无穷多个骰子（V面）;现在，我们首先得抽取一个骰子，然后才能进行计算；我们假定选取过程是服从Dirichlet分布的（先验)，因为我们知道，投骰子时，获得word的頻数是服从多项式分布的；这样后验概率也是Dirichlet分布；  
    > > 这里先验参数是$\theta$,那么  
    > > 
    > > $$p(W,\theta)= p(\theta)p(W|\theta) \\ p(W)=\int p(\theta)p(W|\theta)d\theta = \int p(\theta)\prod p(w_i|\theta)d\theta$$
    > 
    > 我们回顾了基础知识；现在我们来分析一下PLSA模型，概率图模型如图C所示；可以看到，**每一篇文档含有多个主题；**；  
    > 现在，我们生成文档的过程是：我们投骰子（K面，代表文档-主题概率）获得主题z，然后寻找到主题为z的那个主题-word骰子，然后投骰子获得word;  
    > 即：  
    > 
    > $$p(w_i|d_m)=\sum_z p(w_i|z)p(z|d_m) \\ p(W|d_m)=\prod_i^N \sum_z p(w_i|z)p(z|d_m)=\prod_i^N\sum_z \theta_{w_i,z}\phi_{d_m}$$
    > 
    > 这里可以使用EM算法，最大似然方法进行模型估计；
    > 
    > * * *
    > 
    > ## LDA模型
    > 
    > > PLSA 模型本质上是频率学派思想，我们现在利用贝叶斯思想进行考虑；  
    > > 引入Dirichlet先验，$\alpha,\beta$是Dirichlet分布的参数；  
    > > 这样，先根据先验获得一个主题-文档分布的参数，然后从多项式分布得到一个主题，即$\alpha \thicksim\theta_m \thicksim z_{m,n}$；  
    > > 同时从$\beta$先验中，获得多项式分布，然后根据具体主题获得word,即：$\beta \thicksim \phi_k \thicksim w_{m,n}|k=z_{m,n}$  
    > > ![这里写图片描述](https://i.imgur.com/vbhhBoO.jpg)  
    > > 数据知识仍然是多项分布（词频）；  
    > > 这样的话，可以得到参数的后验概率：$Dir(\theta_m|n_m+\alpha)$  
    > > 所以topic的后验概率是：  
    > > 
    > > $$p(Z|\alpha)=\prod_m^M \int p(z_m|\theta_m)p(\theta_m|\alpha)d_{\theta_m}\\ =\prod_m^M\int \prod_n^Np(\theta_{m,n})^{n_{z_n}} Dir(\theta_m|\alpha)d_{\theta_m} \\ =\prod_m^M \int \prod_n^Np(\theta_{m,n})^{n_{z_n}}\frac{1}{\bigtriangleup(\alpha) }\prod_n^Np(\theta_{m,n})^{\alpha-1}d_{\theta_m}\\ =\prod_m^M\frac{1}{\bigtriangleup(\alpha)}\int \prod_n^N p(\theta_{{m,n}})^{n_{n}+\alpha-1}d_{\theta_m}= \prod_m^M\frac{\bigtriangleup(n_m+\alpha)}{\bigtriangleup(\alpha)}$$
    > 
    > 注意：n_m是向量表示，代表伪计数；  
    > 同理，可以获得word-topic 的分布的后验概率是$Dir(\phi_k|n_k+\beta)$,  
    > 
    > $$p(W|Z,\beta)=\prod_k^Kp(W_{(k)}|Z_{(k)},\beta)\\ =\prod_k^K \frac{\bigtriangleup(n_k+\beta)}{\bigtriangleup(\beta)}$$
    > 
    > **然后计算联合概率：**  
    > 
    > $$p(W,Z|\alpha,\beta)=p(Z|\alpha)p(W|Z,\beta)\\ =\prod_k^K \frac{\bigtriangleup(n_k+\beta)}{\bigtriangleup(\beta)}\prod_m^M\frac{\bigtriangleup(n_m+\alpha)}{\bigtriangleup{(\alpha)}}$$
    > 
    >   
    > **由于W是观测变量，因此我们可以获得隐变量Z的条件概率；**  
    > （注：这里可以使用变分EM模型解耦，然后估计隐变量Z的分布；另一种是使用gibbs 采样进行估计）；
    > 
    > > ![这里写图片描述](https://i.imgur.com/2w2qO3B.jpg)
    > 
    > gibbs 采样需要已知条件概率，所以我们继续推导如下：  
    > ![这里写图片描述](https://i.imgur.com/cYl8Ad4.jpg)![这里写图片描述](https://i.imgur.com/UtFeiXM.jpg)![这里写图片描述](https://i.imgur.com/3vWJVsm.jpg)![这里写图片描述](https://i.imgur.com/B7btwFh.jpg)![这里写图片描述](https://i.imgur.com/zGOR9dk.jpg)
    > 
    > 参考文献：  
    > [http://blog.csdn.net/baimafujinji/article/details/53946367](http://blog.csdn.net/baimafujinji/article/details/53946367)  
    > [http://blog.csdn.net/pipisorry/article/details/51525308](http://blog.csdn.net/pipisorry/article/details/51525308)  
    > [http://www.cnblogs.com/pinard/p/6873703.html](http://www.cnblogs.com/pinard/p/6873703.html)  
    > LDA数学八卦



## 用 neural network 加速 LDA 推論

- [[1508.01011] Learning from LDA using Deep Neural Networks](https://arxiv.org/abs/1508.01011)


## LDA2vec

- [[1605.02019] Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec](https://arxiv.org/abs/1605.02019)

- [Introducing our Hybrid lda2vec Algorithm | Stitch Fix Technology – Multithreaded](https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&lambda=1&term=)

    ![](https://multithreaded.stitchfix.com/assets/posts/2016-05-27-lda2vec/lda2vec_network_publish_text_header.gif)

    > The final ingredient is to encourage the weights to look like a sparse Dirichlet distribution. Sampling from the [Dirichlet can get involved](http://stiglerdiet.com/blog/2015/Jul/28/dirichlet-distribution-and-dirichlet-process/), but conveniently [measuring and optimizing the likelihood](http://www.msr-waypoint.com/en-us/um/people/minka/papers/dirichlet/minka-dirichlet.pdf) is extremely simple:
    > 
    > $$\begin{equation} \Sigma_k (\alpha_k - 1) log p_k \end{equation}$$
    > 
    > (Note that we’ve thrown out the terms independent of document weights. Furthermore,  $\alpha_k$ is usually a constant set to $\frac{1}{number\ of\ documents}$, and so the only variable to optimize is the document-to-topic proportion, $p_k$.)
    > 
    > This simple likelihood makes projections onto our latent topic basis sparse. Without this sparsity-inducing term the document weights tend to have evenly spread out mass which makes reading the document vectors as difficult as word vectors. Furthermore, the topic vectors that the document weights couple to are also junk when not imposing a Dirichlet likelihood. Curiously, without this term the topic vectors are poorly defined and seem to produce incoherent groups of words.

- [How to easily do Topic Modeling with LSA, PSLA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)

- [《Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec》阅读笔记](https://zhuanlan.zhihu.com/p/23767128)

- [LDA2vec: Word Embeddings in Topic Models (article) - DataCamp](https://www.datacamp.com/community/tutorials/lda2vec-topic-model)


# Word2Vec based

## Doc2Vec

### 原理



- [深度学习笔记——Word2vec和Doc2vec原理理解并结合代码分析 - CSDN博客](https://blog.csdn.net/mpk_no1/article/details/72458003)


- [How does doc2vec represent feature vector of a document? Can anyone explain mathematically how the process is done? - Quora](https://www.quora.com/How-does-doc2vec-represent-feature-vector-of-a-document-Can-anyone-explain-mathematically-how-the-process-is-done)

    - [[1405.4053] Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)

    > Doc2Vec explores the above observation by adding additional input nodes representing documents as additional context. Each additional node can be thought of just as an id for each input document.
    > 
    > ![](https://i.imgur.com/gXTwNOU.jpg)
    > 
    > D represent the features representing the document context and W
    > 
    > represent the word context in a window surrounding the target word. Training is similar to word2vec, with additional document context. The objective of doc2vec learning is
    > 
    > max∑∀(tar,con,doc)logP
    > 
    > (target word|context words, document context)
    > 
    > At the end of the training process, you will have word embeddings, W
    > 
    > and document embedding D
    > 
    > for documents in the training corpus.
    > 
    > Great! We got document embeddings for input corpus but what about new unseen documents that appear in the test set. The idea is to learn their representation at test time by solving an optimization problem for inference.
    > 
    > ![](https://i.imgur.com/WUCU8W5.jpg)
    > 
    > The optimization problem is not any different from the training problem. max∑∀(tar,con)logP
    > 
    > (target word|context words, document = test doc). One may however, choose to keep W and W′ fixed and learn variable D as document embedding.
    > 
    > The other resource is my (unfinished and unedited) notes on W2V and P2V derivations: "[Understanding Word2Vec and Paragraph2Vec](http://piyushbhardwaj.github.io/documents/w2v_p2vupdates.pdf)".
    > 

- [Distributed representations of sentences and documents | the morning paper](https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/)

    > the word vectors are asked to contribute to a prediction task about the next word in the sentence.
    > 
    > The paragraph vectors are also asked to contribute to the prediction task of the next word given many contexts sampled from the paragraph. 
    > 
    > ![](https://adriancolyer.files.wordpress.com/2016/05/paragraph-vectors-fig-2.png?w=566&zoom=2)
    > 
    > The only change compared to word vector learning is that the paragraph vector is concatenated with the word vectors to predict the next word in a context. Contexts are fixed length and sampling from a sliding window over a paragraph. Paragraph vectors are shared for all windows generated from the same paragraph, but not across paragraphs.
    > 
    > It acts as a memory that remembers what is missing from the current context – or the topic of the paragraph. For this reason, we often call this model the Distributed Memory Model of Paragraph Vectors (PV-DM).
    > 
    > In summary, the algorithm itself has two key stages: 1) training to get word vectors W, softmax weights U, b and paragraph vectors D on already seen paragraphs; and 2) “the inference stage” to get paragraph vectors D for new paragraphs (never seen before) by adding more columns in D and gradient descending on D while holding W, U, b fixed. We use D to make a prediction about some particular labels using a standard classifier, e.g., logistic regression. 
    > 
    > A variation on the above scheme is ignore context words in the input (i.e., do away with the sliding window), and instead force the model to predict words randomly sampled from the paragraph in the output.
    > 
    > we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector… We name this version the Distributed Bag of Words version of Paragraph Vector (PV-DBOW)
    > 
    > ![](https://adriancolyer.files.wordpress.com/2016/05/paragraph-vectors-fig-3.png?w=566&zoom=2)
    > 
    > PV-DM performs better than PV-DBOW, but in tests combining both PV-DM and PV-DBOW gives the best results of all

- [Word2vec 句向量模型PV-DM与PV-DBOW原论文翻译 - 微雨轻寒的博客 - CSDN博客](https://blog.csdn.net/liaocyintl/article/details/50369158)


    > 2.2 句向量：一个分布记忆模型
    > ----------------
    > 
    > 在我们的句（Paragraph）向量模型中，每一个句子都被映射成一个独立的向量，这个句向量作为矩阵 D 的一列；同时，每一个词也被映射成一个独立的向量，这个词向量作为矩阵 W 的一列。对这个句向量和这些词向量求平均或者首尾相连，用来预测文本中的下一个词。在本研究的试验中，我们选用首尾相连来组合这些矩阵。
    > 严格的说，与公式1（Word2vec的公式）相比，唯一的不同点在于这里从 W 和D 两个矩阵中构造 h 。
    > 句子的标识（Token）被当做另外一个"词"看待。它扮演一个"Memory"的角色，用来记忆当前文本或文章主题中漏掉了什么。因此，我们把这个模型称为"句向量的分布记忆模型"(PV-DM: Distributed Memory Model of Paragraph Vectors)。
    > 上下文是固定长度的，从句子的一个滑动窗口中取样。句向量被限制在一个句子的所有上下文里面，但不超越句子。但是词向量矩阵 W 是超越句子的。比如说，"powerful"的词向量也对所有的句子有效。
    > 我们通过 **随机梯度下降法** 来训练这些句向量和词向量，在此过程中通过反向传播获得梯度。在随机梯度下降的每一步，都可以从一个随机的句子中抽取一个定长的上下文，如图2从网络中计算出梯度误差，然后更新模型的参数。
    > 在预测阶段，需要执行一个"推断（inference）"步骤计算新句子的句向量。他也是通过梯度上升来获取。在这个阶段，其余的模型参数、词向量矩阵 W 和 softmax 权重是固定的。
    > 假设语料库中有 N 个句子，字典里有 M 个词汇；我们试图将每一个句子映射到 p 维空间，每一个词映射到 q 维空间，于是这个模型就有总共 N×p+M×q 个参数（包括softmax参数）。即使句子的数量会随着 N 的增大而增大，训练中的更新还是稀疏且高效。
    > 
    > ![](https://img-blog.csdn.net/20151221160314787)
    > 图2：这是一个句向量框架。这个框架类似于图1中的框架。唯一不同点在于增加了一个句子标识（Token），这个标识被映射到矩阵 D 的一个向量里。在这个模型里，通过对上下文三个词向量的首尾相接或求均值，来预测第四个词。这个句向量表示从当前上下文而来的缺失的信息，被当做一个关于句子主题的存储器。
    > 
    > 经过训练，这些句向量就可以当做句子的特征使用。我们可以把这些特征直接用于传统的机器学习技术，比如逻辑回归、支持向量机或者K-means聚类。
    > 总而言之，这个算法有两个关键阶段：1）通过训练获得词向量矩阵 W, softmax权重 U, b 以及 句向量 D 从已知的句子里；2）第二个阶段是推断阶段，用于取得一个新句子（没有出现过）的句向量 D，通过增加更多的列在矩阵 D 里，并保持 W, U, b 不变的情况下在矩阵 D 上进行梯度下降。我们使用 D 通过一个基础的分类器给句子加上标签。
    > **句向量的优点：** 句向量的一个重要的优点在于，它的训练集是没有被加上标签的数据，因此它可以被用于一些训练样本标签不足的任务。
    > 句向量也解决了词袋模型的一些关键的弱点。第一，它传承了词向量的一个重要特性------词和词之间的语义。在语义里，"强有力"比起"巴黎"来说，和"强壮"更接近。句向量的第二个优点在于它考虑到了"词序（word order）"，n-gram模型则需要设置一个较大的n才能做到。这一点很重要，因为n-gram模型保存了句子中大量的信息，包括词序。也就是说，我们的模型优于词袋n-gram模型因为后者会表现出一个极高的维度，这会影响效率。
    > 
    > 2.3 无词序句向量：分布词袋模型
    > -----------------
    > 
    > 上面的方法讨论了在一个文本窗口内，通过句向量和词向量的首尾相接来预测下一个词。另一种方法不把上下文中的词作为输入，而是强制这个模型在输出中从句子中随机抽取词汇来进行预测。实际上，其意义在于在每一个随机梯度下降的循环中，我们抽取一个文本窗口，然后从这个文本窗口中抽取一个词，然后通过一个分类任务得到句向量。这项技术如图3所示。我们把这个版本称为句向量的分布词袋（PV-DBOW: Distributed Bag of Words version of Paragraph Vector）版本，相比于上一节提到的PV-DM版本。
    > 
    > ![](https://img-blog.csdn.net/20151221174010394)\
    > 图3：句向量的分布词袋版本。在这个版本中，句向量被训练出来，用来预测在一个小窗口中的词汇。
    > 
    > 除了在概念上简单以外，这个模型只需要存储少量的数据。相比于上一个模型需要存储softmax权重和词向量，这个模型只需要存储softmax权重。同样的，这个模型也近似于Skip-gram模型。\
    > 在我们的试验中，每一个句向量都是两个向量的组合：一个通过PV-DM训练，另一个通过PV-DBOW训练。PV-DM能够很好地执行多种任务，但是它结合PV-DBOW后，常常能够更加出色完成任务，此我们强烈推荐这种做法。


### 實作

- [基于gensim的Doc2Vec简析 - CSDN博客](https://blog.csdn.net/lenbow/article/details/52120230)

- [基于jieba和doc2vec的中文情感语料分类 - 个人文章 - SegmentFault 思否](https://segmentfault.com/a/1190000012203525)

- [用 Doc2Vec 得到文档／段落／句子的向量表达 - 简书](https://www.jianshu.com/p/854a59b93e09)



- [jhlau/doc2vec: Python scripts for training/testing paragraph vectors](https://github.com/jhlau/doc2vec)

- [RaRe-Technologies/gensim: Topic Modelling for Humans](https://github.com/RaRe-Technologies/gensim)
    - [gensim/doc2vec-lee.ipynb at develop · RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)
    - [gensim/doc2vec-IMDB.ipynb at develop · RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)
    - [gensim/doc2vec-wikipedia.ipynb at develop · RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb)

- [abtpst/Doc2Vec: Doc2Vec algorithm for solving moview review sentiment analysis](https://github.com/abtpst/Doc2Vec)

- [hiyijian/doc2vec: C++ implement of Tomas Mikolov's word/document embedding](https://github.com/hiyijian/doc2vec)

- [fbkarsdorp/doc2vec: Tutorial and review of word2vec / doc2vec](https://github.com/fbkarsdorp/doc2vec)

- [ibrahimsharaf/doc2vec: Text classification using Doc2Vec & Random Forest](https://github.com/ibrahimsharaf/doc2vec)

- [Doc2vec tutorial | RARE Technologies](https://rare-technologies.com/doc2vec-tutorial/)




- [情感分析的新方法——基于Word2Vec/Doc2Vec/Python - OPEN 开发经验库](http://www.open-open.com/lib/view/open1444351655682.html)

- [doc2vec计算文档相似度 - 程序园](http://www.voidcn.com/article/p-fiwnobtj-dx.html)

- [Doc2Vec tutorial using Gensim – Andreas Klintberg – Medium](https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1)


- [jhlau/doc2vec: Python scripts for training/testing paragraph vectors](https://github.com/jhlau/doc2vec)


- [TensorFlow-Machine-Learning-Cookbook/doc2vec.py at master · PacktPublishing/TensorFlow-Machine-Learning-Cookbook](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py#L112:1)


- [Coding doc2vec - Luminis Amsterdam](https://amsterdam.luminis.eu/2017/02/21/coding-doc2vec/)

    - [blog-doc2vec/pvdbow.ipynb at master · luminis-ams/blog-doc2vec](https://github.com/luminis-ams/blog-doc2vec/blob/master/pvdbow.ipynb)

    > Training the network
    > --------------------
    > 
    > In Tensorflow, training a network is done in two steps. First, you define the model. You can think of the model as a graph. Second, you run the model. We'll take a look at the first step, how our model is defined. First: the input.
    > 
    > 
    > ```python=
    > # Input data
    > 
    > dataset = tf.placeholder(tf.int32, shape=[BATCH_SIZE])
    > 
    > labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])
    > ```
    >  
    > 
    > The `dataset` is defined as a placeholder with the shape of a simple array that contains ints. When we run the model, it will contain document ids. Nothing more, nothing less. It will contain as many document ids as we want to feed the stochastic gradient descent algorithm in a single iteration. The `labels` placeholder is also a vector. It will contain integers that represent words from the vocabulary. So, basically, for each document id we want to predict a word that occurs in it. In our implementation, we make sure that a batch contains one or more text windows. So if we use a text window of size 8, a batch will contain one or more sequences of eight consecutive words. Next, we take a look at the weights in our neural network:
    > 
    > 
    > ```python=
    > # Weights
    > 
    > embeddings = tf.Variable(
    > 
    >     tf.random_uniform([len(doclens), EMBEDDING_SIZE],
    >                     -1.0, 1.0))
    > 
    > softmax_weights = tf.Variable(
    >     tf.truncated_normal(
    >             [vocab_size, EMBEDDING_SIZE],
    >             stddev=1.0 / np.sqrt(EMBEDDING_SIZE)))
    > 
    > softmax_biases = tf.Variable(tf.zeros([vocab_size]))
    > ```
    > 
    > 
    > You can think of `embeddings` as the transpose of the matrix ![D](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-4b9ef1bbd23fd1b198de883813285620_l3.svg "Rendered by QuickLaTeX.com") from our [previous post](https://amsterdam.luminis.eu/2017/01/30/implementing-doc2vec/) [2]. In its rows, it has a document vector of length `EMBEDDING_SIZE` for each document id. This document vector is also called an "input vector". You can also think of `embeddings` as the weights between the input layer and the middle layer of our small doc2vec network. When we run the session, we will initialize the `embeddings` variable with random weights between ![-1.0](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-ee8fa04fc2bf1dbbff787fc33f469ffe_l3.svg "Rendered by QuickLaTeX.com") and ![1.0](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-2b23fc34a5447966d71dba505f0d8e9c_l3.svg "Rendered by QuickLaTeX.com").
    > 
    > The `softmax_weights` are the weights between the middle layer and the output layer of our network. You can also think of them as the matrix ![U](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-2b60fc262803f27ba3717d8ec4eb656d_l3.svg "Rendered by QuickLaTeX.com") from our previous post. On its rows, it has an "output vector" of length `EMBEDDING_SIZE` for each word in the vocabulary. When we run the model in our session, we will initialize these weights with (truncated) normally distributed random variables with mean zero and a standard deviation that is inversely proportional to `EMBEDDING_SIZE`. Why are these variables initialized using a normal distribution, instead of with a uniform distribution like we used for the embeddings? The short answer is: because this way of initialisation has apparently worked well in the past. You can try different initialisation schemes yourself, and see what it does to your end-to-end performance. The long answer; well, perhaps that's food for another blog post.
    > 
    > The `softmax_biases` are initialised here with zeroes. In our previous post, we mentioned that softmax biases are often used, but omitted them in our final loss function. Here, we used them, because the word2vec implementation we based this notebook on used them. And the function we use for negative sampling wants them, too.
    > 
    > The activation in the middle layer, or, alternatively, the estimated document vector for a document id is given by `embed`:
    > 
    > ```python=
    > embed = tf.nn.embedding_lookup(embeddings, dataset)
    > ```
    > 
    > `tf.nn.embedding_lookup` will provide us with fast lookup of a document vector for a given document id.
    > 
    > Finally, we are ready to compute the loss function that we'll minimise:
    > 
    > ```python=
    > 
    > loss = tf.reduce_mean(
    >         tf.nn.sampled_softmax_loss(
    >                 softmax_weights, softmax_biases, embed,
    >                 labels, NUM_SAMPLED, vocab_size))
    > 
    > ```
    > 
    > Here, `tf.nn.sampled_softmax_loss` takes care of negative sampling for us. `tf.reduce_mean` will compute the average loss over all the training examples in our batch.
    > 
    > As an aside, if you take a look at the complete source code in the notebook, you'll notice that we also have a function `test_loss`. That function does not use negative sampling. It should not, because negative sampling underestimates the true loss of the network. It is only used because it is faster to compute than the real loss. When you run the notebook, you will see that the training losses it prints are always lower than the test losses. One other remark about the test loss is the following: the test examples are taken from the same set of documents as the training examples! This is because in our network, we have no input to represent a document that we have never seen before. The text windows that have to be predicted for a given document id are different though, in the test set.
    > 
    > ---
    > 
    > Q: i want to change your code for my dataset which is a numpy lists of a list of documents. Am i suppose to give them each a file id or divide them by training and test label?
    > 
    > ---
    > 
    > A: There are a bunch of things you can do. One of them is include your test set in the training set while fitting this doc2vec model. That would not be cheating if you are aware of the locations of the points in your test set at the time you want to classify them (as you do not use the class labels while training doc2vec). This kind of learning task is sometimes called semi-supervised learning, or transductive learning. The doc2vec embeddings are then your feature vectors. Once you have them, for the document classification task (I assume you are doing), you divide your data in training and test sets as usual.
    > 


## Doc2Vec with tags

- [A gentle introduction to Doc2Vec – ScaleAbout – Medium](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)

    > if we have tags for our documents (as we actually have), we can add them, and get their representation as vectors.
    > 
    > Additionally, they don't have to be unique. This way, we can add to the unique document tag one of our 17 tags, and create a doc2vec representation for them as well! see below:
    > 
    > ![](https://cdn-images-1.medium.com/max/960/1*YfOv1_8tmTiahgbpEt5LCw.png)
    > 
    > fig 5 --- doc2vec model with tag vector
    > 
    > we will use **gensim** [**implementation**](https://rare-technologies.com/doc2vec-tutorial/)of **doc2vec.** here is how the gensim TaggedDocument object looks like:
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*As22mK8YKolvVFCGmHvGxw.png)
    > 
    > gensim TaggedDocument object. SENT_3 is the unique document id, remodeling and renovating is the tag
    > 
    > Using **gensim** doc2vec is very straight-forward. As always, model should be initialized, trained for a few epochs:
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*dDwfUMiLfNFUju19ruUyyw.png)
    > 
    > and then we can check the similarity of every unique **document** to every **tag**, this way:
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*T9swFeb7vqTOWKA9NNnIDA.png)
    > 
    > The tag with highest similarity to document will be predicted.



# RNN-based model

## TopicRNN

- [TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency阅读笔记](https://zhuanlan.zhihu.com/p/27151433)

    - [[1611.01702] TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency](https://arxiv.org/abs/1611.01702)

    > ![](https://pic1.zhimg.com/v2-8876dcef7d93048672d0b47f94cea481_r.jpg)
    > 
    > ![](https://pic1.zhimg.com/v2-994789251b56dad24d19e292b89de36c_r.jpg)




## TreeRNN(TreeLSTM)

- [[1503.00075] Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](https://arxiv.org/abs/1503.00075)


- [基于TreeLSTM的情感分析](https://zhuanlan.zhihu.com/p/35252733)


- [machine learning - How can a tree be encoded as input to a neural network? - Stack Overflow](https://stackoverflow.com/questions/26022866/how-can-a-tree-be-encoded-as-input-to-a-neural-network)

    > You need a **recursive neural network**. Please see this repository for an example implementation: <https://github.com/erickrf/treernn>
    > 
    > The principle of a recursive (not recurrent) neural network is shown in this picture.
    > 
    > It learns representation of each leaf, and then goes up through the parents to finally construct the representation of the whole structure. [![enter image description here](https://i.stack.imgur.com/1BW1F.png)](https://i.stack.imgur.com/1BW1F.png)
    > 
    > ---
    > 
    > Encode each leaf node using (i) the sequence of nodes that connects it to the root node and (ii) the encoding of the leaf node that comes before it.
    > 
    > For (i), use a recurrent network whose input is tags. Feed this RNN the root tag, the second level tag, ..., and finally the parent tag (or their embeddings). Combine this with the leaf itself (the word or its embedding). Now, you have a feature that describes the leaf and its ancestors.
    > 
    > For (ii), also use a recurrent network! Simply start by computing the feature described above for the left most leaf and feed it to a second RNN. Keep doing this for each leaf moving from left to right. At each step, the second RNN will give you a vector that represents the current leaf with its ancestors, the leaves that come before it and their ancestors.
    > 
    > Optionally, do (ii) bi-directionally and you will get a leaf feature that incorporates the whole tree!


## Recursive RNN 與 Recurrent RNN 比較

- [在NLP中深度学习模型何时需要树形结构？ - 人工智能 - 掘金](https://juejin.im/entry/5ad838805188252ea02c0bb5)

    - [[1503.00185] When Are Tree Structures Necessary for Deep Learning of Representations?](https://arxiv.org/abs/1503.00185)

    > 该文在NLP领域中4种类型5个任务进行了实验，具体的实验数据大家可以从论文中查阅，这里我主要分析一下每个任务的特点，以及最后实验的结果：
    > 
    > - Sentiment Classification on the Stanford Sentiment Treebank
    > 
    >     这是一个细粒度的情感分类问题，根据Stanford的句法树库，在每一个节点上都标注了情感类型，所以实验分为了句子级别和短语级别，从结果来看，树形结构对于句子级别有点帮助，对于短语级别并没什么作用。
    > 
    > - Binary Sentiment Classification
    > 
    >     这同样是一个情感分类问题，与上面不同的是，它只有二元分类，并且只有在句子级别上进行了标注，且每个句子都比较长。实验结果是树形结构并没有起到什么作用，可能原因是句子较长，而且并没有丰富的短语级别标注，导致在长距离的学习中丢失了学习到的情感信息。
    > 
    > - Question-Answer Matching
    > 
    >     这个任务是机智问答，就是给出一段描述一般由4~6句组成，然后根据描述给出一个短语级别的答案，例如地名，人名等。在这个任务上，树形结构也没有发挥作用。
    >     
    > - Semantic Relation Classification
    > 
    >     这个任务是给出两个句子中的名词，然后判断这两个名词是什么语义关系。树形结构的方法在这个任务上有明显的提升。
    >     
    > - Discourse Parsing
    > 
    >     是一个分类任务，特点是其输入的单元很短，树形结构也没有什么效果。
    > 
    > ## 结论
    > 
    > 通过上面的实验，作者总结出下面的结论。
    > 
    > 需要树形结构：
    > 
    > 1. 需要长距离的语义依存信息的任务（例如上面的语义关系分类任务）Semantic relation extraction
    > 2. 输入为长序列，即复杂任务，且在片段有足够的标注信息的任务（例如句子级别的Stanford情感树库分类任务），此外，实验中作者还将这个任务先通过标点符号进行了切分，每个子片段使用一个双向的序列模型，然后总的再使用一个单向的序列模型得到的结果比树形结构的效果更好一些。
    > 
    > 不需要树形结构：
    > 
    > 1. 长序列并且没有足够的片段标注任务（例如上面的二元情感分类，Q-A Matching任务）
    > 2. 简单任务（例如短语级别的情感分类和Discourse分析任务），每个输入片段都很短，句法分析可能没有改变输入的顺序。
    > 
    > 此外，哈工大的车万翔在哈工大的微信公众号也发表了《自然语言处理中的深度学习模型是否依赖于树结构？》[3]，其中提到了"即使面对的是复杂问题，只要我们能够获得足够的训练数据"也可以无需树形结构。
    > 
    > 通过这篇论文和车老师的博文以及一些相关资料，句法树形结构是否需要值得我们关注，我们应该根据自己做的任务以及句法分析的优缺点进行判断，我自己总结如下：
    > 
    > 句法分析能够带给我们什么？
    > 
    > - 长距离的语义依赖关系
    > - 包含语言学知识的序列片段
    > - 简化复杂句子提取核心
    > 
    > 句法分析的缺点
    > 
    > - 自身分析存在错误，引入噪声
    > - 简单任务复杂化
    > - 句法分析时间长
    > 

- [cs224n-2018-lecture14-TreeRNNs - lecture14.pdf](http://web.stanford.edu/class/cs224n/lectures/lecture14.pdf)

    > ![](https://screenshotscdn.firefoxusercontent.com/images/3cae62c7-0187-430c-ace7-3d06d7916c32.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/a4c3699b-8329-41be-bdd5-435672db5d05.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/40460542-2e9f-43c1-aa64-cd2cb9c405f4.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/09f57e08-4475-4a29-8444-f47472718a88.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/612baa6f-7b85-49f5-ae4e-1b3d2ea6364f.png)
    > 




