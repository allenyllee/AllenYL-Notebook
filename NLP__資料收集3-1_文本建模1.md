# NLP__è³‡æ–™æ”¶é›†3-1_æ–‡æœ¬å»ºæ¨¡1

[toc]
<!-- toc --> 

## Secquence Modeling

### GTMM
- [DeepMindä¸¨æ·±åº¦å­¸ç¿’æœ€æ–°ç”Ÿæˆè¨˜æ†¶æ¨¡åž‹ï¼Œé è¶…RNNçš„GTMM](http://www.bigdatafinance.tw/index.php/tech/557-deepmind-rnn-gtmm)

### RNN+CNN

- [Representing Language with Recurrent and Convolutional Layers: An Authorship Attribution Example](https://hergott.github.io/language-representation-rnn-cnn/)

### [1803.01271] An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (TCN)

- [[1803.01271] An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)

### [1812.10464] Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond

- [[1812.10464] Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond](https://arxiv.org/abs/1812.10464)



## æ–‡ç« å‘é‡

### æ¦‚è§€

- [ç•¶å‰æœ€å¥½çš„è©žå¥åµŒå…¥æŠ€è¡“æ¦‚è¦½ï¼šå¾žç„¡ç›£ç£å­¸ç¿’åˆ°ç›£ç£ã€å¤šä»»å‹™å­¸ç¿’ - å¹«è¶£](http://bangqu.com/59R436.html#utm_source=Facebook_PicSee&utm_medium=Social)

    - [ðŸ“šThe Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)

    > FastText ç›¸å°æ–¼åŽŸå§‹çš„ word2vec å‘é‡æœ€ä¸»è¦çš„æå‡æ˜¯å®ƒå¼•å…¥äº† n å…ƒå­—ç¬¦ï¼ˆn-gramï¼‰ï¼Œé€™ä½¿å¾—å°æ²’æœ‰åœ¨è¨“ç·´æ•¸æ“šä¸­å‡ºç¾çš„å–®è©žï¼ˆè©žå½™è¡¨å¤–çš„å–®è©žï¼‰è¨ˆç®—å–®è©žçš„è¡¨å¾µæˆçˆ²äº†å¯èƒ½ã€‚
    >     
    > ---
    >     
    > æ·±åº¦ä¸Šä¸‹æ–‡å–®è©žè¡¨å¾µï¼ˆELMoï¼‰åœ¨å¾ˆå¤§çš„ç¨‹åº¦ä¸Šæé«˜äº†ç›®å‰æœ€å…ˆé€²çš„è©žåµŒå…¥æ¨¡åž‹çš„æ€§èƒ½ã€‚å®ƒå€‘ç”± Allen äººå·¥æ™ºèƒ½ç ”ç©¶æ‰€ç ”ç™¼ï¼Œä¸¦å°‡åœ¨ 6 æœˆåˆçš„ NAACL 2018 ï¼ˆ https://arxiv.org/abs/1802.05365 ï¼‰ ä¸­å±•ç¤ºã€‚
    > 
    > ELMo æ¨¡åž‹æœƒçˆ²æ¯ä¸€å€‹å–®è©žåˆ†é…ä¸€å€‹è¡¨å¾µï¼Œè©²è¡¨å¾µæ˜¯å®ƒå€‘æ‰€å±¬çš„æ•´å€‹èªžæ–™åº«ä¸­çš„å¥å­çš„ä¸€å€‹å‡½æ•¸ã€‚è©žåµŒå…¥å°‡å¾žä¸€å€‹å…©å±¤çš„é›™å‘èªžè¨€æ¨¡åž‹ï¼ˆLMï¼‰çš„å…§éƒ¨ç‹€æ…‹ä¸­è¨ˆç®—å‡ºä¾†ï¼Œå› æ­¤è©²æ¨¡åž‹è¢«å‘½åçˆ²ã€ŒELMoã€ï¼š Embeddings from Language Modelsï¼ˆE ä»£è¡¨ã€ŒåµŒå…¥ã€ï¼ŒLM ä»£è¡¨ã€Œèªžè¨€æ¨¡åž‹ã€ï¼‰ã€‚
    > 
    > ELMo æ¨¡åž‹çš„ç‰¹é»žï¼š
    > 
    > - ELMo æ¨¡åž‹çš„è¼¸å…¥æ˜¯å­—ç¬¦è€Œä¸æ˜¯å–®è©žã€‚å› æ­¤ï¼Œå®ƒå€‘å¯ä»¥åˆ©ç”¨å­è©žå–®å…ƒçš„å„ªå‹¢ä¾†è¨ˆç®—æœ‰æ„ç¾©çš„å–®è©žè¡¨ç¤ºï¼Œå³ä½¿é€™äº›å–®è©žå¯èƒ½åœ¨è©žå½™è¡¨ä¹‹å¤–ï¼ˆå°±åƒ FastText ä¸€æ¨£ï¼‰ã€‚
    > 
    > - ELMo æ˜¯åœ¨é›™å‘èªžè¨€æ¨¡åž‹ä¸­çš„ä¸€äº›å±¤ä¸Šçš„æ¿€å‹µå‡½æ•¸çš„ä¸²æŽ¥ã€‚ä¸€å€‹èªžè¨€æ¨¡åž‹çš„ä¸åŒå±¤æœƒå°ä¸€å€‹å–®è©žçš„ä¸åŒé¡žåž‹çš„ä¿¡æ¯é€²è¡Œç·¨ç¢¼ï¼ˆä¾‹å¦‚ï¼Œè©žæ€§æ¨™è¨»ï¼ˆPart-Of-Speech taggingï¼‰ç”±é›™å‘ LSTMï¼ˆbiLSTMï¼‰çš„è¼ƒä½Žå±¤å¾ˆå¥½åœ°é æ¸¬ï¼Œè€Œè©žç¾©æŽ’æ­§å‰‡ç”±è¼ƒé«˜å±¤æ›´å¥½åœ°é€²è¡Œç·¨ç¢¼ï¼‰ã€‚å°‡æ‰€æœ‰çš„å±¤ä¸²æŽ¥èµ·ä¾†ä½¿å¾—è‡ªç”±çµ„åˆå„ç¨®ä¸åŒçš„å–®è©žè¡¨å¾µæˆçˆ²äº†å¯èƒ½ï¼Œå¾žè€Œåœ¨ä¸‹æ¸¸ä»»å‹™ä¸­å¾—åˆ°æ›´å¥½çš„æ¨¡åž‹æ€§èƒ½ã€‚
    > 
    > ---
    > 
    > åœ¨é€™å€‹é ˜åŸŸæœ‰ä¸€å€‹å»£æ³›çš„å…±è­˜ï¼ˆ http://arxiv.org/abs/1805.01070 ï¼‰ ï¼Œé‚£å°±æ˜¯ï¼šç›´æŽ¥å°å¥å­çš„è©žåµŒå…¥å–å¹³å‡ï¼ˆæ‰€è¬‚çš„è©žè¢‹æ¨¡åž‹ï¼ˆBag-of-Wordï¼ŒBoWï¼‰ï¼‰é€™æ¨£ç°¡å–®çš„æ–¹æ³•å¯ä»¥çˆ²è¨±å¤šä¸‹æ¸¸ä»»å‹™æä¾›ä¸€å€‹å¾ˆå¼·å¤§çš„å°æ¯”åŸºç·šã€‚
    > 
    > Arora ç­‰äººåœ¨ ICLR 2017 ä¸Šæå‡ºäº†ã€ŒA Simple but Tough-to-Beat Baseline for Sentence Embeddingsã€ ï¼ˆ https://openreview.net/forum?id=SyK00v5xx ï¼‰ ï¼Œé€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„èƒ½å¤ è¢«ç”¨æ–¼è¨ˆç®—é€™å€‹åŸºç·šï¼ˆBoWï¼‰çš„ç®—æ³•ï¼Œç®—æ³•çš„å¤§è‡´æè¿°å¦‚ä¸‹ï¼šé¸æ“‡ä¸€å€‹æµè¡Œçš„è©žåµŒå…¥æ–¹æ³•ï¼Œé€šéŽè©žå‘é‡çš„ç·šæ€§çš„åŠ æ¬Šçµ„åˆå°ä¸€å€‹å¥å­é€²è¡Œç·¨ç¢¼ï¼Œä¸¦ä¸”åˆªé™¤å…±æœ‰çš„éƒ¨åˆ†ï¼ˆåˆªé™¤å®ƒå€‘çš„ç¬¬ä¸€å€‹ä¸»æˆåˆ†ä¸Šçš„æŠ•å½±ï¼‰ã€‚
    > 
    > ---
    > 
    > é™¤äº†ç°¡å–®çš„è©žå‘é‡å¹³å‡ï¼Œç¬¬ä¸€å€‹ä¸»è¦çš„æè­°æ˜¯ä½¿ç”¨ç„¡ç›£ç£å­¸ç¿’è¨“ç·´ç›®æ¨™ï¼Œé€™é …å·¥ä½œæ˜¯èµ·å§‹æ–¼ Jamie Kiros å’Œä»–çš„åŒäº‹å€‘åœ¨ 2015 å¹´æå‡ºçš„ã€ŒSkip-thought vectorsã€ï¼ˆ https://arxiv.org/abs/1506.06726 ï¼‰ ã€‚
    > 
    > ã€ŒSkip-thoughts vectorã€æ˜¯ä¸€å€‹å…¸åž‹çš„å­¸ç¿’ç„¡ç›£ç£å¥å­åµŒå…¥çš„æ¡ˆä¾‹ã€‚å®ƒå¯ä»¥è¢«èªçˆ²ç›¸ç•¶æ–¼çˆ²è©žåµŒå…¥è€Œé–‹ç™¼çš„ã€Œskip-gramã€æ¨¡åž‹çš„å¥å­å‘é‡ï¼Œæˆ‘å€‘åœ¨é€™è£è©¦åœ–é æ¸¬ä¸€å€‹çµ¦å®šçš„å¥å­å‘¨åœçš„å¥å­ï¼Œè€Œä¸æ˜¯é æ¸¬ä¸€å€‹å–®è©žå‘¨åœçš„å…¶ä»–å–®è©žã€‚è©²æ¨¡åž‹ç”±ä¸€å€‹åŸºæ–¼å¾ªç’°ç¥žç¶“ç¶²çµ¡çš„ç·¨ç¢¼å™¨â€”è§£ç¢¼å™¨çµæ§‹çµ„æˆï¼Œç ”ç©¶è€…é€šéŽè¨“ç·´é€™å€‹æ¨¡åž‹å¾žç•¶å‰å¥å­ä¸­é‡æ§‹å‘¨åœçš„å¥å­ã€‚
    > 
    > Skip-Thoughts çš„è«–æ–‡ä¸­æœ€ä»¤äººæ„Ÿèˆˆè¶£çš„è§€é»žæ˜¯ä¸€ç¨®è©žå½™è¡¨æ“´å±•æ–¹æ¡ˆï¼šKiros ç­‰äººé€šéŽåœ¨ä»–å€‘çš„å¾ªç’°ç¥žç¶“ç¶²çµ¡è©žåµŒå…¥ç©ºé–“å’Œä¸€å€‹æ›´å¤§çš„è©žåµŒå…¥ç©ºé–“ï¼ˆä¾‹å¦‚ï¼Œword2vecï¼‰ä¹‹é–“å­¸ç¿’ä¸€ç¨®ç·šæ€§è®Šæ›ä¾†è™•ç†è¨“ç·´éŽç¨‹ä¸­æ²’æœ‰å‡ºç¾çš„å–®è©žã€‚
    > 
    > ã€ŒQuick-thoughts vectorsã€ï¼ˆ https://openreview.net/forum?id=rJvJXZb0W ï¼‰ æ˜¯ç ”ç©¶äººå“¡æœ€è¿‘å°ã€ŒSkip-thoughts vectorsã€çš„ä¸€å€‹æ”¹é€²ï¼Œå®ƒåœ¨ä»Šå¹´çš„ ICLR ä¸Šè¢«æå‡ºã€‚åœ¨é€™é …å·¥ä½œä¸­ï¼Œåœ¨çµ¦å®šå‰ä¸€å€‹å¥å­çš„æ¢ä»¶ä¸‹é æ¸¬ä¸‹ä¸€å€‹å¥å­çš„ä»»å‹™è¢«é‡æ–°å®šç¾©çˆ²äº†ä¸€å€‹åˆ†é¡žå•é¡Œï¼šç ”ç©¶äººå“¡å°‡ä¸€å€‹ç”¨æ–¼åœ¨è¡†å¤šå€™é¸è€…ä¸­é¸å‡ºä¸‹ä¸€å€‹å¥å­çš„åˆ†é¡žå™¨ä»£æ›¿çž­è§£ç¢¼å™¨ã€‚å®ƒå¯ä»¥è¢«è§£é‡‹çˆ²å°ç”Ÿæˆå•é¡Œçš„ä¸€å€‹åˆ¤åˆ¥åŒ–çš„è¿‘ä¼¼ã€‚
    > 
    > è©²æ¨¡åž‹çš„é‹è¡Œé€Ÿåº¦æ˜¯å®ƒçš„å„ªé»žä¹‹ä¸€ï¼ˆèˆ‡ Skip-thoughts æ¨¡åž‹å±¬æ–¼åŒä¸€å€‹æ•¸é‡ç´šï¼‰ï¼Œä½¿å…¶æˆçˆ²åˆ©ç”¨æµ·é‡æ•¸æ“šé›†çš„ä¸€å€‹å…·æœ‰ç«¶çˆ­åŠ›çš„è§£æ±ºæ–¹æ¡ˆã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180606/59R4361528261210081r28S2.png)*ã€ŒQuick-thoughtsã€åˆ†é¡žä»»å‹™ç¤ºæ„åœ–ã€‚åˆ†é¡žå™¨éœ€è¦å¾žä¸€çµ„å¥å­åµŒå…¥ä¸­é¸å‡ºä¸‹ä¸€å€‹å¥å­ã€‚åœ–ç‰‡ä¾†è‡ª Logeswaran ç­‰äººæ‰€è‘—çš„ã€ŒAn efficient framework for learning sentence representationsã€ã€‚*
    > 
    > ---
    > 
    > åœ¨å¾ˆé•·ä¸€æ®µæ™‚é–“å…§ï¼Œäººå€‘èªçˆ²ç›£ç£å­¸ç¿’æŠ€è¡“æ¯”ç„¡ç›£ç£å­¸ç¿’æŠ€è¡“å¾—åˆ°çš„å¥å­åµŒå…¥çš„è³ªé‡è¦ä½Žä¸€äº›ã€‚ç„¶è€Œï¼Œé€™ç¨®å‡èªªæœ€è¿‘è¢«æŽ¨ç¿»äº†ï¼Œé€™è¦éƒ¨åˆ†æ­¸åŠŸæ–¼ã€ŒInferSentã€ï¼ˆ https://arxiv.org/abs/1705.02364 ï¼‰ çš„æå‡ºã€‚
    > 
    > InferSent å…·æœ‰éžå¸¸ç°¡å–®çš„æž¶æ§‹ï¼Œé€™ä½¿å¾—å®ƒæˆçˆ²äº†ä¸€ç¨®éžå¸¸æœ‰è¶£çš„æ¨¡åž‹ã€‚å®ƒä½¿ç”¨ Sentence Natural Language Inferenceï¼ˆNLIï¼‰æ•¸æ“šé›†ï¼ˆè©²æ•¸æ“šé›†åŒ…å« 570,000 å°å¸¶æ¨™ç±¤çš„å¥å­ï¼Œå®ƒå€‘è¢«åˆ†æˆäº†ä¸‰é¡žï¼šä¸­ç«‹ã€çŸ›ç›¾ä»¥åŠè˜Šå«ï¼‰è¨“ç·´ä¸€å€‹ä½æ–¼å¥å­ç·¨ç¢¼å™¨é ‚å±¤çš„åˆ†é¡žå™¨ã€‚å…©å€‹å¥å­ä½¿ç”¨åŒä¸€å€‹ç·¨ç¢¼å™¨é€²è¡Œç·¨ç¢¼ï¼Œè€Œåˆ†é¡žå™¨å‰‡æ˜¯ä½¿ç”¨é€šéŽå…©å€‹å¥å­åµŒå…¥æ§‹å»ºçš„ä¸€å°å¥å­è¡¨å¾µè¨“ç·´çš„ã€‚Conneau ç­‰äººæŽ¡ç”¨äº†ä¸€å€‹é€šéŽæœ€å¤§æ± åŒ–æ“ä½œå¯¦ç¾çš„é›™å‘ LSTM ä½œçˆ²ç·¨ç¢¼å™¨ã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180606/59R4361528261211217433N2.png)
    > 
    > 
    > ---
    >
    > åœ¨ ICLR 2018 ä¸Šç™¼è¡¨çš„æè¿° MILA å’Œå¾®è»Ÿè’™ç‰¹åˆ©çˆ¾ç ”ç©¶é™¢çš„å·¥ä½œçš„è«–æ–‡ã€ŠLearning General Purpose Distributed Sentence Representation via Large Scale Multi-Task Learningã€‹ï¼ˆ https://arxiv.org/abs/1804.00079 ï¼‰ä¸­ï¼ŒSubramanian ç­‰äººè§€å¯Ÿåˆ°ï¼Œçˆ²äº†èƒ½å¤ åœ¨å„ç¨®å„æ¨£çš„ä»»å‹™ä¸­æ³›åŒ–å¥å­è¡¨å¾µï¼Œå¾ˆæœ‰å¿…è¦å°‡ä¸€å€‹å¥å­çš„å¤šå€‹å±¤é¢çš„ä¿¡æ¯é€²è¡Œç·¨ç¢¼ã€‚
    > 
    > å› æ­¤ï¼Œé€™ç¯‡æ–‡ç« çš„ä½œè€…åˆ©ç”¨äº†ä¸€å€‹ä¸€å°å¤šçš„å¤šä»»å‹™å­¸ç¿’æ¡†æž¶ï¼Œé€šéŽåœ¨ä¸åŒçš„ä»»å‹™ä¹‹é–“é€²è¡Œåˆ‡æ›åŽ»å­¸ç¿’ä¸€å€‹é€šç”¨çš„å¥å­åµŒå…¥ã€‚è¢«é¸ä¸­çš„ 6 å€‹ä»»å‹™ï¼ˆå°æ–¼ä¸‹ä¸€å€‹/ä¸Šä¸€å€‹å¥å­çš„ Skip-thoughts é æ¸¬ã€ç¥žç¶“æ©Ÿå™¨ç¿»è­¯ã€çµ„åˆ¥è§£æžï¼ˆconstituency parsingï¼‰ï¼Œä»¥åŠç¥žç¶“èªžè¨€æŽ¨ç†ï¼‰å…±äº«ç›¸åŒçš„ç”±ä¸€å€‹é›™å‘é–€æŽ§å¾ªç’°å–®å…ƒå¾—åˆ°çš„å¥å­åµŒå…¥ã€‚å¯¦é©—è¡¨æ˜Žï¼Œåœ¨å¢žæ·»äº†ä¸€å€‹å¤šèªžè¨€ç¥žç¶“æ©Ÿå™¨ç¿»è­¯ä»»å‹™æ™‚ï¼Œå¥æ³•å±¬æ€§èƒ½å¤ è¢«æ›´å¥½åœ°å­¸ç¿’åˆ°ï¼Œå¥å­é•·åº¦å’Œè©žåºèƒ½å¤ é€šéŽä¸€å€‹å¥æ³•åˆ†æžä»»å‹™å­¸ç¿’åˆ°ï¼Œä¸¦ä¸”è¨“ç·´ä¸€å€‹ç¥žç¶“èªžè¨€æŽ¨ç†èƒ½å¤ ç·¨ç¢¼èªžæ³•ä¿¡æ¯ã€‚
    > 
    > è°·æ­Œåœ¨ 2018 å¹´åˆç™¼å¸ƒçš„çš„é€šç”¨å¥å­ç·¨ç¢¼å™¨ï¼ˆ https://arxiv.org/abs/1803.11175 ï¼‰ä¹Ÿä½¿ç”¨äº†åŒæ¨£çš„æ–¹æ³•ã€‚ä»–å€‘çš„ç·¨ç¢¼å™¨ä½¿ç”¨ä¸€å€‹åœ¨å„ç¨®å„æ¨£çš„æ•¸æ“šæºå’Œå„ç¨®å„æ¨£çš„ä»»å‹™ä¸Šè¨“ç·´çš„è½‰æ›ç¶²çµ¡ï¼Œæ—¨åœ¨å‹•æ…‹åœ°é©æ‡‰å„é¡žè‡ªç„¶èªžè¨€ç†è§£ä»»å‹™ã€‚è©²æ¨¡åž‹çš„ä¸€å€‹é è¨“ç·´å¥½çš„ç‰ˆæœ¬å¯ä»¥åœ¨ TensorFlow ç²å¾—ã€‚









### DSSM

#### åŽŸç†

- [A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval - Microsoft Research](https://www.microsoft.com/en-us/research/publication/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval/)


- [DSSM - Microsoft Research](https://www.microsoft.com/en-us/research/project/dssm/)


- [ç‚¼ä¸¹å¸ˆè¯»æºç ä¹‹ç»†ç©¶DSSM Embeddingå®žçŽ°](https://zhuanlan.zhihu.com/p/37082976)


- [ç”¨äºŽSentence Embeddingçš„DSSMä¸ŽLSTMï¼šç®¡ä¸­çª¥è±¹ â€“ D&C](https://boweihe.me/2016/08/26/dssm%E4%B8%8Elstm/)


- [å­¦ä¹ è®°å½•ä¸€ä¸‹æ·±åº¦è¯­ä¹‰åŒ¹é…æ¨¡åž‹-DSSM | Kubi Code'Blog](http://kubicode.me/2017/04/21/Deep%20Learning/Study-With-Deep-Structured-Semantic-Model/)

- [Deep Learning for Web Search and Natural Language Processing - wsdm2015.v3.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/wsdm2015.v3.pdf)



#### å¯¦ä½œ

- [airalcorn2/Deep-Semantic-Similarity-Model: My Keras implementation of the Deep Semantic Similarity Model (DSSM)/Convolutional Latent Semantic Model (CLSM) described here: http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf.](https://github.com/airalcorn2/Deep-Semantic-Similarity-Model)

- [Model DSSM on Tensorflow](https://liaha.github.io/models/2016/06/21/dssm-on-tensorflow.html)
    - [liaha/dssm](https://github.com/liaha/dssm)






### Neural Bag-of-Ngrams

- [Neural Bag-of-Ngrams - Association for the Advancement of Artificial ...](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14513/14079)

- [libofang/Neural-BoN: code for AAAI-17 paper "Neural Bag-of-Ngrams"](https://github.com/libofang/Neural-BoN)

- [Neural Bag-of-Ngrams - CSDNåšå®¢](https://blog.csdn.net/qq_22548549/article/details/78213004)

    > è¯¥æ–‡ä¸€å…±æå‡ºäº†ä¸‰ç§ngramå‘é‡åˆ›å»ºçš„æ–¹æ³•ã€‚åˆ†åˆ«æ˜¯Context guided N-gram representation, Text Guided N-gram representationä»¥åŠLabel guided N-gram representation. å…¶ä¸­å‰ä¸¤ä¸ªæ¨¡åž‹éƒ½æ˜¯æ— ç›‘ç£çš„ï¼Œæœ€åŽä¸€ä¸ªæ¨¡åž‹æ˜¯ç›‘ç£ç®—æ³•ï¼Œéœ€è¦å¯¹æ–‡æœ¬æ‰“æ ‡ç­¾ã€‚\
    > ![](https://img-blog.csdn.net/20171012123832474?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    > 
    > Context guided N-gram representation
    > ------------------------------------
    > 
    > Context guided N-gram representationç®€ç§°CGNRï¼Œå€Ÿé‰´çš„æ˜¯Skip-gramæ¨¡åž‹ç”Ÿæˆè¯å‘é‡çš„æ–¹æ³•ã€‚CGNRç”¨å½“å‰ngramå‘é‡é¢„æµ‹å…¶ä¸Šä¸‹æ–‡ä¸­çš„ngramå‘é‡ã€‚å…¶å½¢å¼åŒ–è¡¨ç¤ºå¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012124622467?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­gè¡¨ç¤ºå…¶æ˜¯ç¬¬iç¯‡æ–‡ç« ç¬¬jä¸ªngramï¼Œvgæ˜¯å…¶å¯¹åº”çš„å‘é‡ï¼Œcè¡¨ç¤ºgçš„ç¬¬qä¸ªä¸Šä¸‹æ–‡ä¸­çš„ngramã€‚å…¶ä¸­ä¸Šä¸‹æ–‡é›†åˆè¡¨ç¤ºå¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012124917166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­winæ˜¯ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°ï¼Œmæ˜¯æœ€å¤§çš„gramå€¼ã€‚ä¸Šä¸‹æ–‡é›†åˆåˆ›å»ºçš„ä¾‹å­å¦‚ä¸‹å›¾æ‰€ç¤º\
    > ![](https://img-blog.csdn.net/20171012125347703?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­ç»¿è‰²è¡¨ç¤ºç›®æ ‡gramï¼Œè“è‰²è¡¨ç¤ºè¯¥gramçš„ä¸Šä¸‹æ–‡é›†åˆï¼Œgramæœ€å¤§å€¼æ˜¯3ï¼Œå³åŒ…å«1gram, 2gramå’Œ3gramã€‚\
    > CGNRä¾ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå› ä¸ºæœ‰ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„è¯å®ƒä»¬çš„è¯ä¹‰ä¸ä¸€å®šç›¸ä¼¼ï¼Œå¯èƒ½åˆšå¥½ç›¸åï¼Œå¦‚è¯"good"å’Œ"bad"ã€‚æ‰€ä»¥ä½œè€…åˆæå‡ºäº†Text Guided N-gram representationä»¥åŠLabel guided N-gram representation.
    > 
    > Text Guided N-gram representation
    > ---------------------------------
    > 
    > ä¸€èˆ¬è€Œè¨€ï¼ŒåŒä¸€ç¯‡æ–‡ç« é‡Œå‡ºçŽ°çš„è¯ï¼Œä»–ä»¬çš„è¯ä¹‰ä¹Ÿæ˜¯ç›¸ä¼¼çš„ï¼Œå°¤å…¶æ˜¯æœ‰å…³è¯„è®ºçš„æ–‡ç« é‡Œï¼Œç»å¸¸ä¼šå‡ºçŽ°"good", "not bad"ç­‰è¯ã€‚ä»–ä»¬æ‰€è¡¨è¾¾çš„è¯ä¹‰æ˜¯ä¸€æ ·çš„ã€‚å› æ­¤ï¼Œè¯¥æ–‡æå‡ºè¦ç”¨ngramå‘é‡é¢„æµ‹è¯¥ngramæ‰€å±žçš„æ–‡æœ¬ï¼Œå½¢å¼åŒ–è¡¨è¿°å¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012130421982?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­tiè¡¨ç¤ºç¬¬iç¯‡æ–‡æ¡£ã€‚TGNRæ¨¡åž‹é€‚åˆäºŽé•¿æ–‡æœ¬ï¼Œæ¯”å¦‚åœ¨ä¸€ç¯‡æ¶ˆæžçš„ç”µå½±å½±è¯„ä¸­ï¼Œå¯èƒ½ä¼šåŒæ—¶å‡ºçŽ°"terrible", "waste of time"ç­‰è¯ï¼ŒTGNRæ¨¡åž‹èƒ½å°†è¿™äº›è¯èšé›†åœ¨ä¸€èµ·ï¼Œè€Œå¯¹äºŽçŸ­æ–‡æœ¬ï¼Œå¯èƒ½å¹¶ä¸ä¼šå‡ºçŽ°è¿™ä¹ˆå¤šè¯ã€‚ç”±æ­¤ï¼Œè¯¥æ–‡è¿›ä¸€æ­¥æå‡ºäº†Label guided N-gram representationæ¨¡åž‹ã€‚
    > 
    > Label guided N-gram representation
    > ----------------------------------
    > 
    > å½“è¯"good"å’Œ"perfect"å‡ºçŽ°åœ¨ä¸åŒçš„æ–‡æ¡£é‡Œæ˜¯ï¼ŒCGNRå’ŒTGNRå¯èƒ½éƒ½ä¸èƒ½è¾¾åˆ°è¾ƒå¥½çš„æ•ˆæžœã€‚ä½†æ˜¯å½“ä¸¤ç¯‡æ–‡æ¡£å½’å±žäºŽåŒä¸€ä¸ªæ ‡ç­¾æ—¶ï¼Œå…¶ä¸­çš„è¯å¯èƒ½å…·æœ‰ç›¸ä¼¼çš„è¯­ä¹‰ã€‚ç”±æ­¤è¯¥æ–‡æå‡ºäº†Label guided N-gram representationæ–¹æ³•ï¼Œç”¨å½“å‰ngramé¢„æµ‹å…¶æ‰€å±žçš„æ–‡æ¡£çš„æ ‡ç­¾ï¼Œå½¢å¼åŒ–è¡¨ç¤ºå¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012131351485?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > ç›¸å¯¹è€Œè¨€ï¼ŒLGNRæ›´é€‚åˆçŸ­æ–‡æœ¬ã€‚


### Neural Network è™•ç†å‹•æ…‹é•·åº¦çš„æ–¹æ³•

- [machine learning - How can neural networks deal with varying input sizes? - Artificial Intelligence Stack Exchange](https://ai.stackexchange.com/questions/2008/how-can-neural-networks-deal-with-varying-input-sizes)

    > Others already mentioned:
    > 
    > - zero padding
    > - RNN
    > - recursive NN
    > - using convolutions different number of times depending on the size of input. 



### Universal Sentence Encoder

- [é€šç”¨å¥å­èªžç¾©ç·¨ç¢¼å™¨ï¼Œè°·æ­Œåœ¨èªžç¾©æ–‡æœ¬ç›¸ä¼¼æ€§ä¸Šçš„æŽ¢ç´¢ - å¹«è¶£](http://bangqu.com/gY7194.html)

    > **èªžç¾©æ–‡æœ¬ç›¸ä¼¼åº¦**
    > 
    > åœ¨ã€ŒLearning Semantic Textual Similarity from Conversationsã€é€™ç¯‡è«–æ–‡ä¸­ï¼Œæˆ‘å€‘å¼•å…¥ä¸€ç¨®æ–°çš„æ–¹å¼ä¾†å­¸ç¿’èªžç¾©æ–‡æœ¬ç›¸ä¼¼çš„å¥å­è¡¨ç¤ºã€‚ç›´è§€çš„èªªï¼Œå¦‚æžœå¥å­çš„å›žç­”åˆ†ä½ˆç›¸ä¼¼ï¼Œå‰‡å®ƒå€‘åœ¨èªžç¾©ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚ä¾‹å¦‚ï¼Œã€Œä½ å¤šå¤§äº†ï¼Ÿã€ä»¥åŠã€Œä½ çš„å¹´é½¡æ˜¯å¤šå°‘ï¼Ÿã€éƒ½æ˜¯é—œæ–¼å¹´é½¡çš„å•é¡Œï¼Œå¯ä»¥é€šéŽé¡žä¼¼çš„å›žç­”ï¼Œä¾‹å¦‚ã€Œæˆ‘ 20 æ­²ã€ä¾†å›žç­”ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé›–ç„¶ã€Œä½ å¥½å—Žï¼Ÿã€å’Œã€Œä½ å¤šå¤§äº†ï¼Ÿã€åŒ…å«çš„å–®è©žå¹¾ä¹Žç›¸åŒï¼Œä½†å®ƒå€‘çš„å«ç¾©å»å¤§ç›¸å¾‘åº­ï¼Œæ‰€ä»¥å°æ‡‰çš„å›žç­”ä¹Ÿç›¸åŽ»ç”šé ã€‚
    > 
    > è«–æ–‡åœ°å€ï¼šhttps://arxiv.org/abs/1804.07754
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY719415273972350175UWgr.png)
    > 
    > *å¦‚æžœå¥å­å¯ä»¥é€šéŽç›¸åŒçš„ç­”æ¡ˆä¾†å›žç­”ï¼Œé‚£éº¼å¥å­åœ¨èªžç¾©ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚å¦å‰‡ï¼Œå®ƒå€‘åœ¨èªžç¾©ä¸Šæ˜¯ä¸åŒçš„ã€‚*
    > 
    > é€™é …å·¥ä½œä¸­ï¼Œæˆ‘å€‘å¸Œæœ›é€šéŽçµ¦å›žç­”åˆ†é¡žçš„æ–¹å¼å­¸ç¿’èªžç¾©ç›¸ä¼¼æ€§ï¼šçµ¦å®šä¸€å€‹å°è©±è¼¸å…¥ï¼Œæˆ‘å€‘å¸Œæœ›å¾žä¸€æ‰¹éš¨æ©Ÿé¸æ“‡çš„å›žè¦†ä¸­åˆ†é¡žå¾—åˆ°æ­£ç¢ºçš„ç­”æ¡ˆã€‚ä½†æ˜¯ï¼Œä»»å‹™çš„æœ€çµ‚ç›®æ¨™æ˜¯å­¸ç¿’ä¸€å€‹å¯ä»¥è¿”å›žè¡¨ç¤ºå„ç¨®è‡ªç„¶èªžè¨€é—œä¿‚ï¼ˆåŒ…æ‹¬ç›¸ä¼¼æ€§å’Œç›¸é—œæ€§ï¼‰çš„ç·¨ç¢¼æ¨¡åž‹ã€‚æˆ‘å€‘æå‡ºäº†å¦ä¸€é æ¸¬ä»»å‹™ï¼ˆæ­¤è™•æ˜¯æŒ‡ SNLI è˜Šå«æ•¸æ“šé›†ï¼‰ï¼Œä¸¦é€šéŽå…±äº«çš„ç·¨ç¢¼å±¤åŒæ™‚æŽ¨é€²å…©é …ä»»å‹™ã€‚åˆ©ç”¨é€™ç¨®æ–¹å¼ï¼Œæˆ‘å€‘åœ¨ STSBenchmark å’Œ CQA task B ç­‰ç›¸ä¼¼åº¦åº¦é‡æ¨™æº–ä¸Šå–å¾—äº†æ›´å¥½çš„è¡¨ç¾ï¼Œç©¶å…¶åŽŸå› ï¼Œæ˜¯ç°¡å–®ç­‰åƒ¹é—œä¿‚èˆ‡é‚è¼¯è˜Šå«ä¹‹é–“å­˜åœ¨å·¨å¤§ä¸åŒï¼Œå¾Œè€…çˆ²å­¸ç¿’è¤‡é›œèªžç¾©è¡¨ç¤ºæä¾›äº†æ›´å¤šå¯ä¾›ä½¿ç”¨çš„ä¿¡æ¯ã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397235681nLm72.png)
    > 
    > *å°æ–¼çµ¦å®šçš„è¼¸å…¥ï¼Œåˆ†é¡žå¯ä»¥èªçˆ²æ˜¯ä¸€ç¨®å°æ‰€æœ‰å¯èƒ½å€™é¸ç­”æ¡ˆçš„æŽ’åºå•é¡Œã€‚*
    > 
    > **é€šç”¨å¥å­ç·¨ç¢¼å™¨**
    > 
    > ã€ŒUniversal Sentence Encoderã€é€™ç¯‡è«–æ–‡ä»‹ç´¹äº†ä¸€ç¨®æ¨¡åž‹ï¼Œå®ƒé€šéŽå¢žåŠ æ›´å¤šä»»å‹™ä¾†æ“´å±•ä¸Šè¿°çš„å¤šä»»å‹™è¨“ç·´ï¼Œä¸¦èˆ‡ä¸€å€‹é¡žä¼¼ skip-thought çš„æ¨¡åž‹è¯åˆè¨“ç·´ï¼Œå¾žè€Œåœ¨çµ¦å®šæ–‡æœ¬ç‰‡æ®µä¸‹é æ¸¬å¥å­ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œæˆ‘å€‘ä¸ä½¿ç”¨åŽŸ skip-thought æ¨¡åž‹ä¸­çš„ç·¨ç¢¼å™¨ - è§£ç¢¼å™¨æž¶æ§‹ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç¨®åªæœ‰ç·¨ç¢¼å™¨çš„æ¨¡åž‹ï¼Œä¸¦é€šéŽå…±äº«ç·¨ç¢¼å™¨ä¾†æŽ¨é€²é æ¸¬ä»»å‹™ã€‚åˆ©ç”¨é€™ç¨®æ–¹å¼ï¼Œæ¨¡åž‹è¨“ç·´æ™‚é–“å¤§å¤§æ¸›å°‘ï¼ŒåŒæ™‚é‚„èƒ½ä¿è­‰å„é¡žé·ç§»å­¸ç¿’ä»»å‹™ï¼ˆåŒ…æ‹¬æƒ…æ„Ÿå’Œèªžç¾©ç›¸ä¼¼åº¦åˆ†é¡žï¼‰çš„æ€§èƒ½ã€‚é€™ç¨®æ¨¡åž‹çš„ç›®çš„æ˜¯çˆ²å„˜å¯èƒ½å¤šçš„æ‡‰ç”¨ï¼ˆé‡‹ç¾©æª¢æ¸¬ã€ç›¸é—œæ€§ã€èšé¡žå’Œè‡ªå®šç¾©æ–‡æœ¬åˆ†é¡žï¼‰æä¾›ä¸€ç¨®é€šç”¨çš„ç·¨ç¢¼å™¨ã€‚
    > 
    > è«–æ–‡åœ°å€ï¼šhttps://arxiv.org/abs/1803.11175
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397237211r96N9.png)
    > 
    > *æˆå°èªžç¾©ç›¸ä¼¼æ€§æ¯”è¼ƒï¼Œçµæžœçˆ² TensorFlow Hub é€šç”¨å¥å­ç·¨ç¢¼å™¨æ¨¡åž‹çš„è¼¸å‡ºã€‚*
    > 
    > æ­£å¦‚æ–‡ä¸­æ‰€èªªï¼Œé€šç”¨å¥å­ç·¨ç¢¼å™¨æ¨¡åž‹çš„ä¸€å€‹è®Šé«”ä½¿ç”¨äº†æ·±åº¦å¹³å‡ç¶²çµ¡ï¼ˆDANï¼‰ç·¨ç¢¼å™¨ï¼Œè€Œå¦ä¸€å€‹è®Šé«”ä½¿ç”¨äº†æ›´åŠ è¤‡é›œçš„è‡ªæ³¨æ„åŠ›ç¶²çµ¡æž¶æ§‹ Transformerã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397238079233wo.png)
    > 
    > *ã€ŒUniversal Sentence Encoderã€ä¸€æ–‡ä¸­æåˆ°çš„å¤šä»»å‹™è¨“ç·´ã€‚å„é¡žä»»å‹™åŠçµæ§‹é€šéŽå…±äº«çš„ç·¨ç¢¼å±¤/åƒæ•¸ï¼ˆç°è‰²æ¡†ï¼‰é€²è¡Œé€£æŽ¥ã€‚*
    > 
    > éš¨ç€å…¶é«”ç³»çµæ§‹çš„è¤‡é›œåŒ–ï¼ŒTransformer æ¨¡åž‹åœ¨å„ç¨®æƒ…æ„Ÿå’Œç›¸ä¼¼åº¦åˆ†é¡žä»»å‹™ä¸Šçš„è¡¨ç¾éƒ½å„ªæ–¼ç°¡å–®çš„ DAN æ¨¡åž‹ï¼Œä¸”åœ¨è™•ç†çŸ­å¥å­æ™‚åªç¨æ…¢ä¸€äº›ã€‚ç„¶è€Œï¼Œéš¨ç€å¥å­é•·åº¦çš„å¢žåŠ ï¼Œä½¿ç”¨ Transformer çš„è¨ˆç®—æ™‚é–“æ˜Žé¡¯å¢žåŠ ï¼Œä½†æ˜¯ DAN æ¨¡åž‹çš„è¨ˆç®—è€—æ™‚å»å¹¾ä¹Žä¿æŒä¸è®Šã€‚
    > 
    > **æ–°æ¨¡åž‹**
    > 
    > é™¤äº†ä¸Šè¿°çš„é€šç”¨å¥å­ç·¨ç¢¼å™¨æ¨¡åž‹ä¹‹å¤–ï¼Œæˆ‘å€‘é‚„åœ¨ TensorFlow Hub ä¸Šå…±äº«äº†å…©å€‹æ–°æ¨¡åž‹ï¼šå¤§åž‹é€šç”¨å¥åž‹ç·¨ç¢¼å™¨é€šå’Œç²¾ç°¡ç‰ˆé€šç”¨å¥åž‹ç·¨ç¢¼å™¨ã€‚
    > 
    > -   å¤§åž‹ï¼šhttps://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-large/1
    > 
    > -   ç²¾ç°¡ï¼šhttps://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-lite/1
    > 
    >  é€™äº›éƒ½æ˜¯é è¨“ç·´å¥½çš„ Tensorflow æ¨¡åž‹ï¼Œçµ¦å®šé•·åº¦ä¸å®šçš„æ–‡æœ¬è¼¸å…¥ï¼Œè¿”å›žä¸€å€‹èªžç¾©ç·¨ç¢¼ã€‚é€™äº›ç·¨ç¢¼å¯ç”¨æ–¼èªžç¾©ç›¸ä¼¼æ€§åº¦é‡ã€ç›¸é—œæ€§åº¦é‡ã€åˆ†é¡žæˆ–è‡ªç„¶èªžè¨€æ–‡æœ¬çš„èšé¡žã€‚
    > 
    > å¤§åž‹é€šç”¨å¥åž‹ç·¨ç¢¼å™¨æ¨¡åž‹æ˜¯ç”¨æˆ‘å€‘ä»‹ç´¹çš„ç¬¬äºŒç¯‡æ–‡ç« ä¸­æåˆ°çš„ Transformer ç·¨ç¢¼å™¨è¨“ç·´çš„ã€‚å®ƒé‡å°éœ€è¦é«˜ç²¾åº¦èªžç¾©è¡¨ç¤ºçš„å ´æ™¯ï¼ŒçŠ§ç‰²äº†é€Ÿåº¦å’Œé«”ç©ä¾†ç²å¾—æœ€ä½³çš„æ€§èƒ½ã€‚
    > 
    > ç²¾ç°¡ç‰ˆæ¨¡åž‹ä½¿ç”¨ Sentence Piece è©žå½™åº«è€Œéžå–®è©žé€²è¡Œè¨“ç·´ï¼Œé€™ä½¿å¾—æ¨¡åž‹å¤§å°é¡¯è‘—æ¸›å°ã€‚å®ƒé‡å°å…§å­˜å’Œ CPU ç­‰è³‡æºæœ‰é™çš„å ´æ™¯ï¼ˆå¦‚å°åž‹è¨­å‚™æˆ–ç€è¦½å™¨ï¼‰ã€‚
    > 
    > æˆ‘å€‘å¾ˆé«˜èˆˆèˆ‡å¤§å®¶åˆ†äº«é€™é …ç ”ç©¶ä»¥åŠé€™äº›æ¨¡åž‹ã€‚é€™åªæ˜¯ä¸€å€‹é–‹å§‹ï¼Œä¸¦ä¸”ä»ç„¶é‚„æœ‰å¾ˆå¤šå•é¡ŒäºŸå¾…è§£æ±ºï¼Œå¦‚å°‡æŠ€è¡“æ“´å±•åˆ°æ›´å¤šèªžè¨€ä¸Šï¼ˆä¸Šè¿°æ¨¡åž‹ç›®å‰åƒ…æ”¯æŒè‹±èªžï¼‰ã€‚æˆ‘å€‘ä¹Ÿå¸Œæœ›é€²ä¸€æ­¥åœ°é–‹ç™¼é€™ç¨®æŠ€è¡“ï¼Œä½¿å…¶èƒ½å¤ ç†è§£æ®µè½ç”šè‡³æ•´å€‹æ–‡æª”ã€‚åœ¨å¯¦ç¾é€™äº›ç›®æ¨™çš„éŽç¨‹ä¸­ï¼Œå¾ˆæœ‰å¯èƒ½æœƒç”¢ç”Ÿå‡ºçœŸæ­£çš„ã€Œé€šç”¨ã€ç·¨ç¢¼å™¨ã€‚
    > 
    > *åŽŸæ–‡éˆæŽ¥ï¼šhttps://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html*


- [ã€ŠUniversal Sentence Encoderã€‹è®ºæ–‡åˆ†äº«](https://zhuanlan.zhihu.com/p/35174235)

    > è®ºæ–‡ä¸»è¦æ˜¯æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¥å­ç¼–ç æ¡†æž¶ï¼Œå¥å­çº§åˆ«çš„encodeæ¯”Word2vecä½¿ç”¨èµ·æ¥ä¼šæ›´åŠ çš„æ–¹ä¾¿ï¼Œå› ä¸ºå¯ä»¥ç›´æŽ¥æ‹¿æ¥åšå¥å­åˆ†ç±»ç­‰ä»»åŠ¡ã€‚
    > 
    > æœ¬æ–‡ä¸»è¦æå‡ºäº†ä¸¤ä¸ªå¥å­encodeçš„æ¡†æž¶ï¼Œä¸€ä¸ªæ˜¯ä¹‹å‰ã€Šattention is all you needã€‹é‡Œé¢çš„ä¸€ä¸ªencodeæ¡†æž¶ï¼Œå¦ä¸€ä¸ªæ˜¯DANï¼ˆdeep average networkï¼‰çš„encodeæ–¹å¼ã€‚ä¸¤ä¸ªçš„è®­ç»ƒæ–¹å¼è¾ƒä¸ºç±»ä¼¼ï¼Œéƒ½æ˜¯é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ï¼Œå°†encodeç”¨åœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬åˆ†ç±»ï¼Œç”Ÿæˆç­‰ç­‰ï¼Œä»¥ä¸åŒçš„ä»»åŠ¡æ¥è®­ç»ƒä¸€ä¸ªç¼–ç å™¨ï¼Œä»Žè€Œå®žçŽ°æ³›åŒ–çš„ç›®çš„ã€‚
    > 
    > Transformerï¼š
    > 
    > ç¬¬ä¸€ä¸ªå¥å­ç¼–ç æ¨¡åž‹ä½¿ç”¨äº†transformeré‡Œé¢çš„ä¸€ä¸ªå­å›¾ï¼Œè¿™ä¸ªå­å›¾é€šè¿‡attentionæ¥ä¸€ä¸ªå¥å­ä¸­è¯è¯­çš„contextè¡¨ç¤ºï¼ŒåŒæ—¶è€ƒè™‘æ‰€æœ‰çš„è¯å¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªfixed lengthå‘é‡ã€‚å…·ä½“äº†è§£å¯ä»¥åŽ»çœ‹é‚£ç¯‡è®ºæ–‡ï¼Œè¿™ç¯‡è®ºæ–‡æœ¬èº«ä¹Ÿæ²¡æœ‰ç»™å‡ºå…·ä½“çš„å›¾ã€‚
    > 
    > ![](https://pic4.zhimg.com/80/v2-540dc62cdb5fe8939c5185a0eeb14c7b_hd.jpg)
    > 
    > DANï¼š
    > 
    > ç¬¬äºŒä¸ªå¥å­ç¼–ç æ¨¡åž‹ä½¿ç”¨äº†è¾ƒä¸ºç®€å•çš„ä¸€ä¸ªDANæ¨¡åž‹ï¼ˆ[Mohit Iyyer](https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/P/P15/P15-1162.pdf)ï¼‰ï¼Œé€šè¿‡å¯¹embeddingæ±‚å¹³å‡åŽè¾“å…¥åˆ°feed-forward networkï¼Œç„¶åŽä¸åŠ softmaxå³å¯å¾—åˆ°éšå±‚è¡¨ç¤ºï¼Œç„¶åŽæŠŠè¿™ä¸ªå‘é‡ç±»ä¼¼äºŽä¸Šé¢çš„æ–¹æ³•ï¼Œç”¨å¤šä»»åŠ¡æ¥è¿›è¡Œè®­ç»ƒæ¨¡åž‹ã€‚
    > 
    > ![](https://pic3.zhimg.com/80/v2-bb79f8465693d9834e45a616b6a53631_hd.jpg)
    > 
    > è®ºæ–‡ä¹‹æ‰€ä»¥æå‡ºä¸¤ä¸ªæ˜¯è€ƒè™‘åˆ°æ¨¡åž‹çš„å‡†ç¡®åº¦å’Œå¤æ‚ç¨‹åº¦ï¼Œä»¥åŠè®­ç»ƒçš„èµ„æºæ¶ˆè€—ã€‚
    > 
    > attentioné‚£ä¸ªencodeæ¨¡åž‹å‡†ç¡®çŽ‡é«˜ï¼Œä½†æ˜¯æ¨¡åž‹å¤æ‚åº¦é«˜ï¼Œè®­ç»ƒæ—¶é—´é•¿ï¼Œæ¶ˆè€—gpuèµ„æºå¤š
    > 
    > è€ŒDANæ¨¡åž‹è™½ç„¶å‡†ç¡®åº¦ä½Žï¼Œä½†æ˜¯è®­ç»ƒå¿«ï¼Œæ¨¡åž‹å¤æ‚åº¦ä½Žã€‚
    > 
    > æ–‡ä¸­ä¹Ÿç»™å‡ºäº†æ ·ä¾‹ï¼Œæœ‰äº†ä¸€ä¸ªåˆå§‹è®­ç»ƒå¥½çš„æ¨¡åž‹ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚å†åŠ å…¥ä¸€äº›æ–‡æœ¬è¿›è¡Œå†æ¬¡è®­ç»ƒã€‚ï¼ˆ[ä½¿ç”¨åœ°å€](https://link.zhihu.com/?target=https%3A//www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1)ï¼‰
    > 
    > ![](https://pic4.zhimg.com/80/v2-51aac31df9e31c1d907171f20e96fc0f_hd.jpg)
    > 
    > æ–‡ä¸­è¿˜æå‡ºäº†æ¯”è¾ƒä¸¤ä¸ªå¥å­å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦æ—¶å€™ä¸ç”¨åŽŸç”Ÿçš„cosè·ç¦»ï¼Œè€Œæ˜¯è½¬åŒ–ä¸ºå¼§åº¦ä¹‹åŽè®¡ç®—ï¼Œæ•ˆæžœè¦å¥½äºŽåŽŸç”Ÿçš„ï¼š
    > 
    > ![](https://pic3.zhimg.com/80/v2-cb699c1869f91223cffe3d77cc524f79_hd.jpg)
    > 


# Bag-of-Word based

## TF-IDF

- [tfâ€“idf - Wikiwand](https://www.wikiwand.com/en/Tf%E2%80%93idf)



- [[æ–‡ä»¶æŽ¢å‹˜] TF-IDF æ¼”ç®—æ³•ï¼šå¿«é€Ÿè¨ˆç®—å–®å­—èˆ‡æ–‡ç« çš„é—œè¯ â€“ David's Perspective](https://taweihuang.hpd.io/2017/03/01/tfidf/)

    > ### BoW (Bag of Words) èˆ‡è©žå½™æ•¸é‡-æ–‡ä»¶çŸ©é™£
    > 
    > å‡è¨­ç¾åœ¨æœ‰ ![D](https://s0.wp.com/latex.php?latex=D&bg=ffffff&fg=000&s=0 "D") ç¯‡æ–‡ä»¶ (document)ï¼Œè€Œæ‰€æœ‰æ–‡ä»¶ä¸­ç¸½å…±ä½¿ç”¨äº† ![~T~](https://s0.wp.com/latex.php?latex=%7ET%7E&bg=ffffff&fg=000&s=0 "~T~") å€‹è©žå½™ (term)ï¼Œæˆ‘å€‘å°±å¯ä»¥å°‡æ–‡ç« è½‰æ›æˆä»¥ä¸‹é¡žåž‹çš„çŸ©é™£ï¼Œå…¶ä¸­ç¬¬ä¸€æ¬„ç¬¬ä¸€åˆ—çš„ã€Œ12ã€ä»£è¡¨çš„æ˜¯ã€Œæ–‡ä»¶ 1ã€ ä¸­å‡ºç¾äº†12å€‹ã€Œæ–‡å­— 1ã€ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å¯ä»¥ç”¨ ![[12,~0,~3,~\cdots,~2]](https://s0.wp.com/latex.php?latex=%5B12%2C%7E0%2C%7E3%2C%7E%5Ccdots%2C%7E2%5D&bg=ffffff&fg=000&s=0 "[12,~0,~3,~\cdots,~2]") é€™å€‹å‘é‡ä¾†ä»£è¡¨ã€Œæ–‡ä»¶ 1ã€ï¼ŒåŒç†ä¹Ÿå¯ç”¨ã€Œæ–‡ä»¶ Dã€ä¹Ÿå¯ä»¥ç”¨ ![[0,~2,~8,~\cdots,~0]](https://s0.wp.com/latex.php?latex=%5B0%2C%7E2%2C%7E8%2C%7E%5Ccdots%2C%7E0%5D&bg=ffffff&fg=000&s=0 "[0,~2,~8,~\cdots,~0]") ä¾†è¡¨ç¤ºã€‚
    > 
    > ![åœ–ç‰‡1](https://taweihuang.hpd.io/wp-content/uploads/2017/03/åœ–ç‰‡1.png)
    > 
    > é€™æ¨£çš„æ–¹æ³•å°±æ˜¯ã€ŒBoW (Bag of Word)æ¼”ç®—æ³•ã€ï¼Œé€™ç¨®æ–¹æ³•é›–ç„¶å¾ˆç°¡å–®ï¼Œä½†æœ‰2å€‹ä¸»è¦çš„å•é¡Œ â”€ ä¸€æ˜¯æ¯ç¯‡æ–‡ç« çš„ç¸½å­—æ•¸ä¸ä¸€æ¨£ï¼Œæ¯”å¦‚èªªæ–‡å­— 2åœ¨æ–‡ä»¶ 2ä¸­å‡ºç¾9æ¬¡ï¼Œåœ¨æ–‡ä»¶ Dä¸­å»åªå‡ºç¾2æ¬¡ï¼Œé€™æ¨£æ˜¯å¦ä»£è¡¨æ–‡å­— 2 å°æ–‡ä»¶ 2 æ¯”è¼ƒé‡è¦ï¼Œå°æ–‡ä»¶ D æ¯”è¼ƒä¸é‡è¦å‘¢ï¼Ÿç­”æ¡ˆæ˜¯å¦å®šçš„ï¼Œèªªä¸å®šæ–‡ä»¶2æœ‰10000å€‹å­—ï¼Œè€Œæ–‡ä»¶Dåªæœ‰50å€‹å­—ï¼Œå¦‚æ­¤ä¸€ä¾†æ–‡å­—2æ‡‰è©²å°æ–‡ä»¶Dæ¯”è¼ƒé‡è¦æ‰å°ã€‚
    > 
    > å¦ä¸€å€‹å•é¡Œæ˜¯ï¼Œæ™‚å¸¸é‡è¤‡å‡ºç¾çš„æ…£ç”¨è©žå½™å°ä¸€å€‹æ–‡ä»¶çš„å½±éŸ¿å¾ˆå¤§ã€‚æ¯”å¦‚èªªï¼Œä¸Šåœ–ä¸­çš„æ–‡å­— 3åœ¨æ¯å€‹æ–‡ä»¶ä¸­éƒ½å‡ºç¾å¥½å¤šæ¬¡ï¼Œå¯èƒ½æ˜¯ã€Œtheã€ä¹‹é¡žçš„å¸¸ç”¨å­—ï¼Œå¦‚æ­¤ä¸€ä¾†ã€Œæ–‡ä»¶ Dã€çš„å‘é‡å°±æœƒè¢«  the é€™å€‹å­—æ‰€ä¸»å°Žï¼Œä½† the é€™å€‹å­—å…¶å¯¦æ²’ä»€éº¼ç‰¹åˆ¥çš„æ„ç¾©ã€‚
    > 
    > ç‚ºäº†è™•ç†ä»¥ä¸Šå…©å€‹å•é¡Œï¼Œæ­·å²æ‚ ä¹…ä½†éžå¸¸å¥½ç”¨çš„ TF-IDFæ¼”ç®—æ³•å°±è¢«ç™¼æ˜Žå‡ºä¾†äº†ã€‚
    > 
    > ### TF-IDF æ¼”ç®—æ³•
    > 
    > TF-IDF æ¼”ç®—æ³•åŒ…å«äº†å…©å€‹éƒ¨åˆ†ï¼š**è©žé »**ï¼ˆterm frequencyï¼ŒTFï¼‰è·Ÿ**é€†å‘æ–‡ä»¶é »çŽ‡**ï¼ˆinverse document frequencyï¼ŒIDFï¼‰ã€‚è©žé »æŒ‡çš„æ˜¯æŸä¸€å€‹çµ¦å®šçš„è©žèªžåœ¨è©²æ–‡ä»¶ä¸­å‡ºç¾çš„é »çŽ‡ï¼Œç¬¬ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")å€‹è©žå‡ºç¾åœ¨ç¬¬ ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") ç¯‡æ–‡ä»¶çš„é »çŽ‡è¨˜åš ![~tf_{t,d}~](https://s0.wp.com/latex.php?latex=%7Etf_%7Bt%2Cd%7D%7E&bg=ffffff&fg=000&s=0 "~tf_{t,d}~")ï¼Œèˆ‰ä¾‹ä¾†èªªï¼Œå¦‚æžœæ–‡ä»¶ 1 ç¸½å…±æœ‰100å€‹å­—ï¼Œè€Œç¬¬ 1 å€‹å­—åœ¨æ–‡ä»¶ 1 å‡ºç¾çš„æ¬¡æ•¸æ˜¯12æ¬¡ï¼Œå› æ­¤![~tf_{1,1}=12/100~](https://s0.wp.com/latex.php?latex=%7Etf_%7B1%2C1%7D%3D12%2F100%7E&bg=ffffff&fg=000&s=0 "~tf_{1,1}=12/100~")ï¼Œå¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å°±å¯ä»¥é‡å°ä¸Šè¿°çš„ç¬¬ä¸€å€‹å•é¡Œé€²è¡Œä¿®æ­£ï¼Œä»¥é »çŽ‡è€Œä¸æ˜¯æ¬¡æ•¸ä¾†çœ‹å¾…æ–‡å­—çš„é‡è¦æ€§ï¼Œè®“æ–‡ç« èˆ‡æ–‡ç« ä¹‹é–“æ¯”è¼ƒæœ‰å¯æ¯”è¼ƒæ€§ã€‚
    > 
    > è€Œé€†å‘æ–‡ä»¶é »çŽ‡å‰‡æ˜¯ç”¨ä¾†è™•ç†å¸¸ç”¨å­—çš„å•é¡Œã€‚å‡è¨­è©žå½™ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t") ç¸½å…±åœ¨ ![d_t](https://s0.wp.com/latex.php?latex=d_t&bg=ffffff&fg=000&s=0 "d_t") ç¯‡æ–‡ç« ä¸­å‡ºç¾éŽï¼Œå‰‡è©žå½™ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t") çš„ IDF å®šç¾©æˆ ![~idf_t = \log\left(\frac{D}{d_t}\right)~](https://s0.wp.com/latex.php?latex=%7Eidf_t+%3D+%5Clog%5Cleft%28%5Cfrac%7BD%7D%7Bd_t%7D%5Cright%29%7E&bg=ffffff&fg=000&s=0 "~idf_t = \log\left(\frac{D}{d_t}\right)~")ã€‚æ¯”å¦‚èªªï¼Œå‡è¨­æ–‡å­— 1 ç¸½å…±å‡ºç¾åœ¨ 25 ç¯‡ä¸åŒçš„æ–‡ä»¶ï¼Œå‰‡ ![~~idf_1 = \log\left(\frac{D}{25}\right)~~](https://s0.wp.com/latex.php?latex=%7E%7Eidf_1+%3D+%5Clog%5Cleft%28%5Cfrac%7BD%7D%7B25%7D%5Cright%29%7E%7E&bg=ffffff&fg=000&s=0 "~~idf_1 = \log\left(\frac{D}{25}\right)~~")ã€‚å¦‚æžœè©žå½™ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")åœ¨éžå¸¸å¤šç¯‡æ–‡ç« ä¸­éƒ½å‡ºç¾éŽï¼Œå°±ä»£è¡¨ ![~d_t~](https://s0.wp.com/latex.php?latex=%7Ed_t%7E&bg=ffffff&fg=000&s=0 "~d_t~") å¾ˆå¤§ï¼Œæ­¤æ™‚![~idf_t~](https://s0.wp.com/latex.php?latex=%7Eidf_t%7E&bg=ffffff&fg=000&s=0 "~idf_t~") å°±æœƒæ¯”è¼ƒå°ã€‚
    > 
    > è€Œä¸€å€‹å­—å°æ–¼ä¸€ç¯‡æ–‡ä»¶é‡è¦æ€§çš„åˆ†æ•¸ (score) å°±å¯ä»¥é€éŽTFèˆ‡IDFå…©å€‹æŒ‡æ¨™è¨ˆç®—å‡ºä¾†ï¼Œæˆ‘å€‘å°‡ç¬¬ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")å€‹è©žå½™å°æ–¼ç¬¬ ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") ç¯‡æ–‡ä»¶çš„TF-IDFæ¬Šé‡å®šç¾©ç‚º ![~w_{t,d} =  tf_{t,d} \times  idf_t ~](https://s0.wp.com/latex.php?latex=%7Ew_%7Bt%2Cd%7D+%3D%C2%A0+tf_%7Bt%2Cd%7D+%5Ctimes+%C2%A0idf_t+%7E&bg=ffffff&fg=000&s=0 "~w_{t,d} =  tf_{t,d} \times  idf_t ~")ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œç•¶è©žå½™ ![~t~](https://s0.wp.com/latex.php?latex=%7Et%7E&bg=ffffff&fg=000&s=0 "~t~") å¾ˆå¸¸å‡ºç¾åœ¨æ–‡ä»¶  ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") æ™‚ï¼Œä»–çš„ ![~tf_{t,d} ~](https://s0.wp.com/latex.php?latex=%7Etf_%7Bt%2Cd%7D+%7E&bg=ffffff&fg=000&s=0 "~tf_{t,d} ~") å°±æœƒæ¯”è¼ƒå¤§ï¼Œè€Œå¦‚æžœè©žå½™![~~ t](https://s0.wp.com/latex.php?latex=%7E%7Et&bg=ffffff&fg=000&s=0 "~~t") ä¹Ÿå¾ˆå°‘å‡ºç¾åœ¨å…¶ä»–ç¯‡æ–‡ç« ï¼Œå‰‡ ![~idf_t~](https://s0.wp.com/latex.php?latex=%7Eidf_t%7E&bg=ffffff&fg=000&s=0 "~idf_t~") ä¹Ÿæœƒæ¯”è¼ƒå¤§ï¼Œä½¿ ![~w_{t,d}~](https://s0.wp.com/latex.php?latex=%7Ew_%7Bt%2Cd%7D%7E&bg=ffffff&fg=000&s=0 "~w_{t,d}~")æ•´é«”ä¾†èªªæ¯”è¼ƒå¤§ï¼Œä¹Ÿå°±æ˜¯èªªè©žå½™![~t~](https://s0.wp.com/latex.php?latex=%7Et%7E&bg=ffffff&fg=000&s=0 "~t~") å°æ–¼æ–‡ä»¶  ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") ä¾†èªªæ˜¯å¾ˆé‡è¦çš„ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å°±å¯ä»¥è¨ˆç®—å‡º TF-IDF çŸ©é™£ï¼Œå¦‚ä¸‹åœ–æ‰€ç¤ºã€‚
    > 
    > ![åœ–ç‰‡1](https://taweihuang.hpd.io/wp-content/uploads/2017/03/åœ–ç‰‡1-1.png)
    > 
    > å¦ä¸€æ–¹é¢ï¼Œ TF-IDF æ™‚å¸¸è¢«ç”¨ä¾†ä½œè³‡è¨Šæª¢ç´¢ (information retrieval)ï¼Œæ¯”å¦‚èªªï¼Œçµ¦å®šä¸€ä¸²æŒ‡ä»¤ (query)ã€Œæ–‡å­— 1 + æ–‡å­— 3 + ä¸åœ¨ç¾æœ‰è©žå½™è£¡é¢çš„æ–‡å­—ã€ï¼Œå‰‡é€™ä¸²æŒ‡ä»¤è·Ÿç¬¬ ![d](https://s0.wp.com/latex.php?latex=d&bg=ffffff&fg=000&s=0 "d") çš„é—œè¯åˆ†æ•¸å°±å¯ä»¥å®šç¾©æ–‡ ![tf_{1,d} + tf_{3,d} ](https://s0.wp.com/latex.php?latex=tf_%7B1%2Cd%7D+%2B+tf_%7B3%2Cd%7D%C2%A0&bg=ffffff&fg=000&s=0 "tf_{1,d} + tf_{3,d} ")ã€‚
    > 
    > ### å¤§é¼»æ˜¯æ€Žéº¼ç”¨ TF-IDFï¼Ÿ
    > 
    > TF-IDF å¸¸è¢«æˆ‘ç”¨åœ¨3å€‹åœ°æ–¹ï¼Œä¸€å€‹æ˜¯ä½œç‚º baseline modelçš„ç‰¹å¾µ (feature)ï¼Œæ¯”å¦‚èªªä½œæ–‡ä»¶åˆ†é¡ž (text classification) æ™‚ï¼Œæˆ‘å°±æœƒæŠŠ tf è·Ÿ idf éƒ½ç•¶ä½œæ–‡ä»¶çš„ç‰¹å¾µ(æ‰€ä»¥ä¸€ç¯‡æ–‡ç« ç¸½å¤ æœƒæœ‰ ![2T](https://s0.wp.com/latex.php?latex=2T&bg=ffffff&fg=000&s=0 "2T")å€‹ç‰¹å¾µ)ï¼ŒåŽ»è·‘åˆ†é¡žæ¨¡åž‹ï¼Œä½œç‚º baselineã€‚æœ‰æ™‚å€™æˆ‘æœƒæŠŠä¸€å€‹è©žå½™å°æ–¼æ¯ç¯‡çš„æ–‡ç« çš„ tf-idf å€¼ç•¶ä½œè©²è©žå½™çš„ç‰¹å¾µï¼ŒåŽ»è·‘æ–‡å­—çš„åˆ†ç¾¤ã€‚é‚„æœ‰ä¸€å€‹æ˜¯å¤§é¼»å¥½å‹ã„ã„“å‘Šè¨´æˆ‘çš„å¦™æ‹›ï¼Œå°±æ˜¯å¦‚æžœæˆ‘å€‘æƒ³è¦ç”¨word2vecå¾—å‡ºä¾†çš„è©žå‘é‡åŽ»è¡¨ç¤ºä¸€å€‹æ–‡ä»¶çš„è©±ï¼Œå¯ä»¥ç”¨tf-idfçš„å€¼ç•¶ä½œæ¬Šé‡ï¼Œå†æŠŠè©²æ–‡ä»¶ä¸­æ¯å€‹è©žå‘é‡ç”¨tf-idfç•¶ä½œæ¬Šé‡åŠ èµ·ä¾†ï¼Œæ•ˆæžœä¹Ÿå¾ˆä¸éŒ¯å–”ï¼

### å¯¦ä½œ

- [[AI] The fastest way to identify keywords in news articles â€” TFIDF with Wikipedia (Python version)](https://hackernoon.com/the-fastest-way-to-identify-keywords-in-news-articles-tfidf-with-wikipedia-python-version-baf874d7eb16)



### the advantages and disadvantages of TF-IDF

- [What are the advantages and disadvantages of TF-IDF? - Quora](https://www.quora.com/What-are-the-advantages-and-disadvantages-of-TF-IDF)

    > Advantages:
    > - Easy to compute
    > - You have some basic metric to extract the most descriptive terms in a document
    > - You can easily compute the similarity between 2 documents using it
    > 
    > Disadvantages:
    > - TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc.
    > - For this reason, TF-IDF is only useful as a lexical level feature
    > - Cannot capture semantics (e.g. as compared to topic models, word embeddings)
    > 
    > So it depends a lot for what you want to use TF-IDF.











# Topic model

## course

### Topic Models - David Blei

- [Topic Models - VideoLectures.NET](http://videolectures.net/mlss09uk_blei_tm/)

## metrics

### Palmetto Online Demo

- [Palmetto Online Demo](http://palmetto.aksw.org/palmetto-webapp/?coherence=umass)

    > CV is based on a sliding window, a one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosinus similarity.
    > 
    > This coherence measure retrieves cooccurrence counts for the given words using a sliding window and the window size 110. The counts are used to calculated the NPMI of every top word to every other top word, thus, resulting in a set of vectorsâ€”one for every top word. The one-set segmentation of the top words leads to the calculation of the similarity between every top word vector and the sum of all top word vectors. As similarity measure the cosinus is used. The coherence is the arithmetic mean of these similarities. (Note that this was the best coherence measure in our evalution.)
    > 
    > ---
    > 
    > CUMass is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure.
    > 
    > The main idea of this coherence is that the occurrence of every top word should be supported by every top preceding top word. Thus, the probability of a top word to occur should be higher if a document already contains a higher order top word of the same topic. Therefore, for every word the logarithm of its conditional probability is calculated using every other top word that has a higher order in the ranking of top words as condition. The probabilities are derived using document cooccurrence counts. The single conditional probabilities are summarized using the arithmetic mean. (Note that in the original publication only the sum of these values is calculated)


## LSA (Latent Semantic Analysis)

- [Latent Semantic Analysis(LSA/ LSI)ç®—æ³•ç®€ä»‹ - æ½˜çš„åšå®¢ - åšå®¢å›­](https://www.cnblogs.com/kemaswill/archive/2013/04/17/3022100.html)

    > æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç§æ¨¡åž‹ï¼Œèƒ½å¤Ÿæ•èŽ·åˆ°å•è¯ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å¦‚æžœä¸¤ä¸ªå•è¯ä¹‹é—´æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œé‚£ä¹ˆå½“ä¸€ä¸ªå•è¯å‡ºçŽ°æ—¶ï¼Œå¾€å¾€æ„å‘³ç€å¦ä¸€ä¸ªå•è¯ä¹Ÿåº”è¯¥å‡ºçŽ°(åŒä¹‰è¯)ï¼›åä¹‹ï¼Œå¦‚æžœæŸ¥è¯¢è¯­å¥æˆ–è€…æ–‡æ¡£ä¸­çš„æŸä¸ªå•è¯å’Œå…¶ä»–å•è¯çš„ç›¸å…³æ€§éƒ½ä¸å¤§ï¼Œé‚£ä¹ˆè¿™ä¸ªè¯å¾ˆå¯èƒ½è¡¨ç¤ºçš„æ˜¯å¦å¤–ä¸€ä¸ªæ„æ€(æ¯”å¦‚åœ¨è®¨è®ºäº’è”ç½‘çš„æ–‡ç« ä¸­ï¼ŒAppleæ›´å¯èƒ½æŒ‡çš„æ˜¯Appleå…¬å¸ï¼Œè€Œä¸æ˜¯æ°´æžœ)  ã€‚
    > 
    >   LSA(LSI)ä½¿ç”¨SVDæ¥å¯¹å•è¯-æ–‡æ¡£çŸ©é˜µè¿›è¡Œåˆ†è§£ã€‚SVDå¯ä»¥çœ‹ä½œæ˜¯ä»Žå•è¯-æ–‡æ¡£çŸ©é˜µä¸­å‘çŽ°ä¸ç›¸å…³çš„ç´¢å¼•å˜é‡(å› å­)ï¼Œå°†åŽŸæ¥çš„æ•°æ®æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´å†…ã€‚åœ¨å•è¯-æ–‡æ¡£çŸ©é˜µä¸­ä¸ç›¸ä¼¼çš„ä¸¤ä¸ªæ–‡æ¡£ï¼Œå¯èƒ½åœ¨è¯­ä¹‰ç©ºé—´å†…æ¯”è¾ƒç›¸ä¼¼ã€‚
    > 
    >   SVDï¼Œäº¦å³å¥‡å¼‚å€¼åˆ†è§£ï¼Œæ˜¯å¯¹çŸ©é˜µè¿›è¡Œåˆ†è§£çš„ä¸€ç§æ–¹æ³•ï¼Œä¸€ä¸ªt*dç»´çš„çŸ©é˜µ(å•è¯-æ–‡æ¡£çŸ©é˜µ)Xï¼Œå¯ä»¥åˆ†è§£ä¸ºT*S*D^T^ï¼Œå…¶ä¸­Tä¸ºt*mç»´çŸ©é˜µï¼ŒTä¸­çš„æ¯ä¸€åˆ—ç§°ä¸ºå·¦å¥‡å¼‚å‘é‡(left singular bector)ï¼ŒSä¸ºm*mç»´å¯¹è§’çŸ©é˜µï¼Œæ¯ä¸ªå€¼ç§°ä¸ºå¥‡å¼‚å€¼(singular value)ï¼ŒDä¸ºd*mç»´çŸ©é˜µ,Dä¸­çš„æ¯ä¸€åˆ—ç§°ä¸ºå³å¥‡å¼‚å‘é‡ã€‚åœ¨å¯¹å•è¯æ–‡æ¡£çŸ©é˜µXåšSVDåˆ†è§£ä¹‹åŽï¼Œæˆ‘ä»¬åªä¿å­˜Sä¸­æœ€å¤§çš„Kä¸ªå¥‡å¼‚å€¼ï¼Œä»¥åŠTå’ŒDä¸­å¯¹åº”çš„Kä¸ªå¥‡å¼‚å‘é‡ï¼ŒKä¸ªå¥‡å¼‚å€¼æž„æˆæ–°çš„å¯¹è§’çŸ©é˜µS'ï¼ŒKä¸ªå·¦å¥‡å¼‚å‘é‡å’Œå³å¥‡å¼‚å‘é‡æž„æˆæ–°çš„çŸ©é˜µT'å’ŒD'ï¼šX'=T'*S'*D'^T^å½¢æˆäº†ä¸€ä¸ªæ–°çš„t*dçŸ©é˜µã€‚
    > 
    > ---
    > 
    > åœ¨ **æŸ¥è¯¢** æ—¶ï¼Œå¯¹ä¸Žæ¯ä¸ªç»™å®šçš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬æ ¹æ®è¿™ä¸ªæŸ¥è¯¢ä¸­åŒ…å«çš„å•è¯(X~q~)æž„é€ ä¸€ä¸ªä¼ªæ–‡æ¡£ï¼šD~q~=X~q~TS^-1^ï¼Œç„¶åŽè¯¥ä¼ªæ–‡æ¡£å’ŒD'ä¸­çš„æ¯ä¸€è¡Œè®¡ç®—ç›¸ä¼¼åº¦(ä½™å¼¦ç›¸ä¼¼åº¦)æ¥å¾—åˆ°å’Œç»™å®šæŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚
    > 




## LDA

### åŽŸç†è§£èªª


> - [LDAåŽŸç†è§£èªª-ppt](https://docs.google.com/presentation/d/1xGGetms4CyirQAWbKM4i9I1sVPty8S9j_8n_GR6cFqg/edit?usp=sharing) [name=Ya-Lun Li]



- [Latent Dirichlet Allocation](http://www.jmlr.org/papers/v3/blei03a.html)

- [LDA-math-æ±‡æ€» LDAæ•°å­¦å…«å¦ | æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†](http://www.52nlp.cn/lda-math-%e6%b1%87%e6%80%bb-lda%e6%95%b0%e5%ad%a6%e5%85%ab%e5%8d%a6)


- [LDA(Latent Dirichlet Allocation)ä¸»é¢˜æ¨¡åž‹ - CSDNåšå®¢](https://blog.csdn.net/aws3217150/article/details/53840029)

    > LDAäºŽ2003å¹´ç”± David Blei, Andrew Ngå’Œ Michael I. Jordanæå‡ºï¼Œå› ä¸ºæ¨¡åž‹çš„ç®€å•å’Œæœ‰æ•ˆï¼ŒæŽ€èµ·äº†ä¸»é¢˜æ¨¡åž‹ç ”ç©¶çš„æ³¢æµªã€‚è™½ç„¶è¯´LDAæ¨¡åž‹ç®€å•ï¼Œä½†æ˜¯å®ƒçš„æ•°å­¦æŽ¨å¯¼å´ä¸æ˜¯é‚£ä¹ˆå¹³æ˜“è¿‘äººï¼Œä¸€èˆ¬åˆå­¦è€…ä¼šæ·±é™·æ•°å­¦ç»†èŠ‚æŽ¨å¯¼ä¸­ä¸èƒ½è‡ªæ‹”ã€‚äºŽæ˜¯ç‰›äººä»¬çœ‹ä¸ä¸‹åŽ»äº†ï¼Œçº·çº·ç«™å‡ºæ¥å‘è¡¨äº†å„ç§æ•™ç¨‹ã€‚å›½å†…æ–¹é¢rickjinæœ‰è‘—åçš„ã€Š[LDAæ•°å­¦å…«å¦](http://www.52nlp.cn/author/rickjin)ã€‹ï¼Œå›½å¤–çš„Gregor Heinrichæœ‰è‘—åçš„ã€Š[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)ã€‹ã€‚å…¶å®žæœ‰äº†è¿™ä¸¤ç¯‡äº’è¡¥çš„é€šä¿—æ•™ç¨‹ï¼Œå¤§å®¶æ²‰ä½å¿ƒçœ‹ä¸ª4ã€5éï¼ŒåŸºæœ¬å°±å¯ä»¥æ˜Žç™½LDAä¸ºä»€ä¹ˆæ˜¯ç®€å•çš„äº†ã€‚é‚£ä¹ˆå…¶å®žä¹Ÿæ²¡æˆ‘ä»€ä¹ˆäº‹äº†ï¼Œç„¶è€Œå¿ƒä¸­æ€»æœ‰ä¸€ç§è¢«å¤§ç‰›ç‚¹æ’­çš„è±ç„¶å¼€æœ—çš„å¿«æ„Ÿï¼Œå®žåœ¨æ˜¯ä¸åä¸å¿«å•Šã€‚
    > 
    > ä»€ä¹ˆæ˜¯ä¸»é¢˜
    > =====
    > 
    > å› ä¸ºLDAæ˜¯ä¸€ç§ä¸»é¢˜æ¨¡åž‹ï¼Œé‚£ä¹ˆé¦–å…ˆå¿…é¡»æ˜Žç¡®çŸ¥é“LDAæ˜¯æ€Žä¹ˆçœ‹å¾…ä¸»é¢˜çš„ã€‚å¯¹äºŽä¸€ç¯‡æ–°é—»æŠ¥é“ï¼Œæˆ‘ä»¬çœ‹åˆ°é‡Œé¢è®²äº†æ˜¨å¤©NBAç¯®çƒæ¯”èµ›ï¼Œé‚£ä¹ˆç”¨å¤§è…¿æƒ³éƒ½çŸ¥é“å®ƒçš„ä¸»é¢˜æ˜¯å…³äºŽä½“è‚²çš„ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬å¤§è…¿ä¼šé‚£ä¹ˆèªæ˜Žå‘¢ï¼Ÿè¿™æ—¶å¤§è…¿ä¼šå›žç­”å› ä¸ºé‡Œé¢å‡ºçŽ°äº†"ç§‘æ¯”"ã€"æ¹–äºº"ç­‰ç­‰å…³é”®è¯ã€‚é‚£ä¹ˆå¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸»é¢˜æ˜¯ä¸€ç§å…³é”®è¯é›†åˆï¼Œå¦‚æžœå¦å¤–ä¸€ç¯‡æ–‡ç« å‡ºçŽ°è¿™äº›å…³é”®è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æŽ¥åˆ¤æ–­ä»–å±žäºŽæŸç§ä¸»é¢˜ã€‚ä½†æ˜¯ï¼Œäº²çˆ±çš„è¯»è€…è¯·ä½ æƒ³æƒ³ï¼Œè¿™æ ·å®šä¹‰ä¸»é¢˜æœ‰ä»€ä¹ˆå¼Šç«¯å‘¢ï¼ŸæŒ‰ç…§è¿™ç§å®šä¹‰ï¼Œæˆ‘ä»¬ä¼šå¾ˆå®¹æ˜“ç»™å‡ºè¿™æ ·çš„æ¡ä»¶ï¼šä¸€æ—¦æ–‡ç« å‡ºçŽ°äº†ä¸€ä¸ªçƒæ˜Ÿçš„åå­—ï¼Œé‚£ä¹ˆé‚£ç¯‡æ–‡ç« çš„ä¸»é¢˜å°±æ˜¯ä½“è‚²ã€‚å¯èƒ½ä½ é©¬ä¸Šä¼šéª‚æˆ‘åœ¨çžŽè¯´ï¼Œç„¶åŽåé©³è¯´ä¸ä¸€å®šï¼Œæ–‡ç« ç¡®å®žæœ‰çƒæ˜Ÿçš„åå­—ï¼Œä½†æ˜¯é‡Œé¢å…¨éƒ¨åœ¨è®²çƒæ˜Ÿçš„æ€§ä¸‘é—»ï¼Œå’Œç¯®çƒæ²¡åŠæ¯›é’±å…³ç³»ï¼Œæ­¤æ—¶ä¸»é¢˜æ˜¯å¨±ä¹è¿˜å·®ä¸å¤šã€‚æ‰€ä»¥ä¸€ä¸ªè¯ä¸èƒ½ç¡¬æ€§åœ°æ‰£ä¸€ä¸ªä¸»é¢˜çš„å¸½å­ï¼Œå¦‚æžœè¯´ä¸€ç¯‡æ–‡ç« å‡ºçŽ°äº†æŸä¸ªçƒæ˜Ÿçš„åå­—ï¼Œæˆ‘ä»¬åªèƒ½è¯´æœ‰å¾ˆå¤§æ¦‚çŽ‡ä»–å±žäºŽä½“è‚²çš„ä¸»é¢˜ï¼Œä½†ä¹Ÿæœ‰å°æ¦‚çŽ‡å±žäºŽå¨±ä¹çš„ä¸»é¢˜ã€‚äºŽæ˜¯å°±ä¼šæœ‰è¿™ç§çŽ°è±¡ï¼š**åŒä¸€ä¸ªè¯ï¼Œåœ¨ä¸åŒçš„ä¸»é¢˜èƒŒæ™¯ä¸‹ï¼Œå®ƒå‡ºçŽ°çš„æ¦‚çŽ‡æ˜¯ä¸åŒçš„**ã€‚å¹¶ä¸”æˆ‘ä»¬éƒ½å¯ä»¥åŸºæœ¬ç¡®å®šï¼Œä¸€ä¸ªè¯ä¸èƒ½ä»£è¡¨ä¸€ç§ä¸»é¢˜ï¼Œé‚£ä¹ˆåˆ°åº•ä»€ä¹ˆæ‰æ˜¯ä¸»é¢˜å‘¢ï¼Ÿè€ä¸ä½æ€§å­çš„åŒå­¦ä¼šè¯´ï¼Œæ—¢ç„¶ä¸€ä¸ªè¯ä»£è¡¨ä¸äº†ä¸€ç§ä¸»é¢˜ï¼Œé‚£æˆ‘å°±æŠŠæ‰€æœ‰è¯éƒ½ç”¨æ¥ä»£è¡¨ä¸€ç§ä¸»é¢˜ï¼Œç„¶åŽä½ è‡ªå·±åŽ»æ…¢æ…¢æ„ä¼šã€‚æ²¡é”™ï¼Œè¿™æ ·ç¡®å®žæ˜¯ä¸€ç§å®Œå…¨çš„æ–¹å¼ï¼Œä¸»é¢˜æœ¬æ¥å°±è•´å«åœ¨æ‰€æœ‰è¯ä¹‹ä¸­ï¼Œè¿™æ ·ç¡®å®žæ˜¯æœ€ä¿é™©çš„åšæ³•ï¼Œä½†æ˜¯ä½ ä¼šå‘çŽ°è¿™æ ·ç­‰äºŽä»€ä¹ˆéƒ½æ²¡åšã€‚è€å¥¸å·¨çŒ¾çš„LDAä¹Ÿæ˜¯è¿™ä¹ˆæƒ³çš„ï¼Œä½†ä»–ç‹¡çŒ¾ä¹‹å¤„åœ¨äºŽå®ƒç”¨éžå¸¸åœ†æ»‘æ‰‹æ®µåœ°å°†ä¸»é¢˜ç”¨æ‰€æœ‰è¯æ±‡è¡¨è¾¾å‡ºæ¥ã€‚æ€Žä¹ˆä¸ªåœ†æ»‘æ³•å‘¢ï¼Ÿæ‰‹æ®µä¾¿æ˜¯æ¦‚çŽ‡ã€‚LDAè®¤ä¸ºå¤©ä¸‹æ‰€æœ‰æ–‡ç« éƒ½æ˜¯ç”¨åŸºæœ¬çš„è¯æ±‡ç»„åˆè€Œæˆï¼Œæ­¤æ—¶å‡è®¾æœ‰è¯åº“V={v1,v2,....,vn}
    > 
    > ï¼Œé‚£ä¹ˆå¦‚ä½•è¡¨è¾¾ä¸»é¢˜k
    > 
    > å‘¢ï¼ŸLDAè¯´é€šè¿‡è¯æ±‡çš„æ¦‚çŽ‡åˆ†å¸ƒæ¥åæ˜ ä¸»é¢˜ï¼å¤šä¹ˆç‹¡çŒ¾çš„å®¶ä¼™ã€‚æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­æ¥è¯´æ˜ŽLDAçš„è§‚ç‚¹ã€‚å‡è®¾æœ‰è¯åº“
    > 
    > {ç§‘æ¯”ï¼Œç¯®çƒï¼Œè¶³çƒï¼Œå¥¥å·´é©¬ï¼Œå¸Œæ‹‰é‡Œï¼Œå…‹æž—é¡¿}
    > 
    > å‡è®¾æœ‰ä¸¤ä¸ªä¸»é¢˜
    > 
    > {ä½“è‚²ï¼Œæ”¿æ²»}
    > 
    > LDAè¯´ä½“è‚²è¿™ä¸ªä¸»é¢˜å°±æ˜¯ï¼š
    > 
    > {ç§‘æ¯”:0.3ï¼Œç¯®çƒ:0.3ï¼Œè¶³çƒ:0.3ï¼Œå¥¥å·´é©¬:0.03ï¼Œå¸Œæ‹‰é‡Œ:0.03ï¼Œå…‹æž—é¡¿:0.04}
    > 
    > (æ•°å­—ä»£è¡¨æŸä¸ªè¯çš„å‡ºçŽ°æ¦‚çŽ‡)ï¼Œè€Œæ”¿æ²»è¿™ä¸ªä¸»é¢˜å°±æ˜¯ï¼š
    > 
    > {ç§‘æ¯”:0.03ï¼Œç¯®çƒ:0.03ï¼Œè¶³çƒ:0.04ï¼Œå¥¥å·´é©¬:0.3ï¼Œå¸Œæ‹‰é‡Œ:0.3ï¼Œå…‹æž—é¡¿:0.3}
    > 
    > LDAå°±æ˜¯è¿™æ ·è¯´æ˜Žä»€ä¹ˆæ˜¯ä¸»é¢˜çš„ï¼Œç«Ÿè¯´å¾—æˆ‘æ— è¨€ä»¥å¯¹ï¼Œç»†æ€ä¹‹ä¸‹ä¹Ÿæ˜¯éžå¸¸åˆç†ã€‚
    > 
    > æ–‡ç« åœ¨è®²ä»€ä¹ˆ
    > ======
    > 
    > ç»™ä½ ä¸€ç¯‡æ–‡ç« è¯»ï¼Œç„¶åŽè¯·ä½ ç®€è¦æ¦‚æ‹¬æ–‡ç« åœ¨è®²ä»€ä¹ˆï¼Œä½ å¯èƒ½ä¼šè¿™æ ·å›žç­”ï¼š80%åœ¨è®²æ”¿æ²»çš„è¯é¢˜ï¼Œå‰©ä¸‹15%åœ¨è®²å¨±ä¹ï¼Œå…¶ä½™éƒ½æ˜¯åºŸè¯ã€‚è¿™é‡Œå¤§æ¦‚å¯ä»¥æç‚¼å‡ºä¸‰ç§ä¸»é¢˜ï¼šæ”¿æ²»ï¼Œå¨±ä¹ï¼ŒåºŸè¯ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºŽæŸä¸€ç¯‡æ–‡ç« ï¼Œå¾ˆæœ‰å¯èƒ½é‡Œé¢ä¸æ­¢åœ¨è®²ä¸€ç§ä¸»é¢˜ï¼Œè€Œæ˜¯å‡ ç§ä¸»é¢˜æ··åœ¨ä¸€èµ·çš„ã€‚è¯»è€…å¯èƒ½ä¼šé—®ï¼ŒLDAæ˜¯ä¸€ç§å¯ä»¥ä»Žæ–‡æ¡£ä¸­æç‚¼ä¸»é¢˜æ¨¡åž‹ï¼Œé‚£ä»–åœ¨å»ºæ¨¡çš„æ—¶å€™æœ‰æ²¡æœ‰è€ƒè™‘è¿™ç§æƒ…å½¢å•Šï¼Œä»–ä¼šä¸ä¼šå¿˜è®°è€ƒè™‘äº†ã€‚é‚£æ‚¨å°±å¤§å¯æ”¾å¿ƒäº†ï¼Œæ·±è°‹è¿œè™‘çš„LDAæ—©å°±æ³¨æ„åˆ°è¿™äº›äº†ã€‚LDAè®¤ä¸ºï¼Œæ–‡ç« å’Œä¸»é¢˜ä¹‹é—´å¹¶ä¸ä¸€å®šæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ–‡ç« å¯ä»¥æœ‰å¤šä¸ªä¸»é¢˜ï¼Œä¸€ä¸ªä¸»é¢˜å¯ä»¥åœ¨å¤šç¯‡æ–‡ç« ä¹‹ä¸­ã€‚è¿™ç§è¯´æ³•ï¼Œç›¸ä¿¡è¯»è€…åªèƒ½ç‚¹å¤´ç§°æ˜¯ã€‚å‡è®¾çŽ°åœ¨æœ‰K
    > 
    > ä¸ªä¸»é¢˜ï¼Œæœ‰M
    > 
    > ç¯‡æ–‡ç« ï¼Œé‚£ä¹ˆæ¯ç¯‡æ–‡ç« é‡Œé¢ä¸åŒä¸»é¢˜çš„ç»„æˆæ¯”ä¾‹åº”è¯¥æ˜¯æ€Žæ ·çš„å‘¢ï¼Ÿç”±äºŽä¸Šä¸€å°èŠ‚æˆ‘ä»¬çŸ¥é“ä¸èƒ½ç¡¬æ€§çš„å°†æŸä¸ªè¯å¥—ä¸ŠæŸä¸ªä¸»é¢˜ï¼Œé‚£ä¹ˆè¿™é‡Œæˆ‘ä»¬å½“ç„¶ä¸èƒ½è®²æŸä¸ªä¸»é¢˜å¥—åœ¨æŸä¸ªæ–‡ç« ä¸­ï¼Œä¹Ÿå°±æ˜¯æœ‰è¿™æ ·çš„çŽ°è±¡ï¼š**åŒä¸€ä¸ªä¸»é¢˜ï¼Œåœ¨ä¸åŒçš„æ–‡ç« ä¸­ï¼Œä»–å‡ºçŽ°çš„æ¯”ä¾‹(æ¦‚çŽ‡)æ˜¯ä¸åŒçš„**ï¼Œçœ‹åˆ°è¿™é‡Œï¼Œè¯»è€…å¯èƒ½å·²ç»å‘çŽ°ï¼Œæ–‡æ¡£å’Œä¸»é¢˜ä¹‹é—´çš„å…³ç³»å’Œä¸»é¢˜å’Œè¯æ±‡çš„å…³ç³»æ˜¯å¤šä¹ˆæƒŠäººçš„ç±»ä¼¼ï¼LDAå…ˆäººä¸€æ­¥åœ°å°†è¿™ä¸€å‘çŽ°è¯´å‡ºæ¥ï¼Œå®ƒè¯´ï¼Œä¸Šä¸€èŠ‚æˆ‘ä»¬å·§å¦™åœ°ç”¨è¯æ±‡çš„åˆ†å¸ƒæ¥è¡¨è¾¾ä¸»é¢˜ï¼Œé‚£ä¹ˆè¿™ä¸€æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œæˆ‘ä»¬å·§å¦™åœ°ç”¨ä¸»é¢˜çš„åˆ†å¸ƒæ¥è¡¨è¾¾æ–‡ç« ï¼åŒæ ·ï¼Œæˆ‘ä»¬ä¸¾ä¸ªä¾‹å­æ¥è¯´æ˜Žä¸€ä¸‹ï¼Œå‡è®¾çŽ°åœ¨æœ‰ä¸¤ç¯‡æ–‡ç« ï¼š
    > 
    > ã€Šä½“è‚²å¿«è®¯ã€‹ï¼Œã€Šå¨±ä¹å‘¨æŠ¥ã€‹
    > 
    > æœ‰ä¸‰ä¸ªä¸»é¢˜
    > 
    > ä½“è‚²ï¼Œå¨±ä¹ï¼ŒåºŸè¯
    > 
    > é‚£ä¹ˆ
    > 
    > ã€Šä½“è‚²å¿«è®¯ã€‹æ˜¯è¿™æ ·çš„:[åºŸè¯,ä½“è‚²,ä½“è‚²,ä½“è‚²,ä½“è‚²,....,å¨±ä¹,å¨±ä¹]
    > 
    > è€Œ
    > 
    > ã€Šå¨±ä¹å‘¨æŠ¥ã€‹æ˜¯è¿™æ ·çš„:[åºŸè¯,åºŸè¯,å¨±ä¹,å¨±ä¹,å¨±ä¹,....,å¨±ä¹,ä½“è‚²]
    > 
    > ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€ç¯‡æ–‡ç« åœ¨è®²ä»€ä¹ˆï¼Œé€šè¿‡ä¸åŒçš„ä¸»é¢˜æ¯”ä¾‹å°±å¯ä»¥æ¦‚æ‹¬å¾—å‡ºã€‚
    > 
    > æ–‡ç« æ˜¯å¦‚ä½•ç”Ÿæˆçš„
    > ========
    > 
    > åœ¨å‰é¢ä¸¤å°èŠ‚ä¸­ï¼ŒLDAè®¤ä¸ºï¼Œæ¯ä¸ªä¸»é¢˜ä¼šå¯¹åº”ä¸€ä¸ªè¯æ±‡åˆ†å¸ƒï¼Œè€Œæ¯ä¸ªæ–‡æ¡£ä¼šå¯¹åº”ä¸€ä¸ªä¸»é¢˜åˆ†å¸ƒï¼Œé‚£ä¹ˆä¸€ç¯‡æ–‡ç« æ˜¯å¦‚ä½•è¢«å†™å‡ºæ¥çš„å‘¢ï¼Ÿè¯»è€…å¯èƒ½ä¼šè¯´é çµæ„Ÿ+è¯æ±‡ã€‚LDAè„¸ä¸€æ²‰ï¼Œæ„Ÿè§‰å®Œè›‹ï¼Œçµæ„Ÿæ˜¯ä»€ä¹ˆçŽ©æ„ï¼ŸLDAè‹¦æ€å†¥æƒ³ï¼Œæœ€åŽæ²¡åŠžæ³•ï¼Œæƒ³äº†ä¸ªé¦Šä¸»æ„ï¼Œå®ƒè¯´
    > 
    > çµæ„Ÿ=éšæœº
    > 
    > è¿™ä¹Ÿæ˜¯æ²¡åŠžæ³•ä¸­çš„åŠžæ³•ã€‚å› æ­¤å¯¹äºŽæŸä¸€ç¯‡æ–‡ç« çš„ç”Ÿäº§è¿‡ç¨‹æ˜¯è¿™æ ·çš„ï¼š
    > 
    > 1.  ç¡®å®šä¸»é¢˜å’Œè¯æ±‡çš„åˆ†å¸ƒ
    > 2.  ç¡®å®šæ–‡ç« å’Œä¸»é¢˜çš„åˆ†å¸ƒ
    > 3.  éšæœºç¡®å®šè¯¥æ–‡ç« çš„è¯æ±‡ä¸ªæ•°N
    > 
    > -\
    >     -   å¦‚æžœå½“å‰ç”Ÿæˆçš„è¯æ±‡ä¸ªæ•°å°äºŽN
    > 
    > 1.  æ‰§è¡Œç¬¬5æ­¥ï¼Œå¦åˆ™æ‰§è¡Œç¬¬6æ­¥
    > 2.  ç”±æ–‡æ¡£å’Œä¸»é¢˜åˆ†å¸ƒéšæœºç”Ÿæˆä¸€ä¸ªä¸»é¢˜ï¼Œé€šè¿‡è¯¥ä¸»é¢˜ç”±ä¸»é¢˜å’Œè¯æ±‡åˆ†å¸ƒéšæœºç”Ÿæˆä¸€ä¸ªè¯ï¼Œç»§ç»­æ‰§è¡Œç¬¬4æ­¥
    > 3.  æ–‡ç« ç”Ÿæˆç»“æŸ
    > 
    > åªè¦ç¡®å®šå¥½ä¸¤ä¸ªåˆ†å¸ƒ(ä¸»é¢˜ä¸Žè¯æ±‡åˆ†å¸ƒï¼Œæ–‡ç« ä¸Žä¸»é¢˜åˆ†å¸ƒ)ï¼Œç„¶åŽéšæœºç”Ÿæˆæ–‡ç« å„ä¸ªä¸»é¢˜æ¯”ä¾‹ï¼Œå†æ ¹æ®å„ä¸ªä¸»é¢˜éšæœºç”Ÿæˆè¯ï¼Œè¯ä¸Žè¯ä¹‹é—´çš„é¡ºåºå…³ç³»è¢«å½»åº•å¿½ç•¥äº†ï¼Œè¿™å°±æ˜¯LDAçœ¼ä¸­ä¸–é—´æ‰€æœ‰æ–‡ç« çš„ç”Ÿæˆè¿‡ç¨‹ï¼èªæ˜Žçš„è¯»è€…è‚¯å®šè§‰å¾—LDAå®Œå…¨å°±æ˜¯ä¸€ä¸ªéª—å­ï¼Œè¿™æ ·è®¤ä¸ºæ–‡ç« çš„ç”Ÿæˆæœªå…ä¹Ÿå¤ªè¿‡å¤©çœŸäº†å§ã€‚ç„¶è€Œäº‹å®žå°±æ˜¯å¦‚æ­¤ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆè¯´LDAæ˜¯ä¸€ä¸ªå¾ˆç®€å•çš„æ¨¡åž‹ã€‚å¹¸å¥½æˆ‘ä»¬è¿™æ˜¯ç”¨LDAæ¥åšä¸»é¢˜åˆ†æžï¼Œè€Œä¸æ˜¯ç”¨æ¥ç”Ÿæˆæ–‡ç« ï¼Œè€Œä»Žä¸Šä¸ŠèŠ‚çš„åˆ†æžæˆ‘ä»¬çŸ¥é“ï¼Œä¸»é¢˜å…¶å®žå°±æ˜¯ä¸€ç§è¯æ±‡åˆ†å¸ƒï¼Œè¿™é‡Œå¹¶ä¸æ¶‰åŠåˆ°è¯ä¸Žè¯çš„é¡ºåºå…³ç³»ï¼Œæ‰€ä»¥LDAè¿™ç§BOW(bag of words)çš„æ¨¡åž‹ä¹Ÿæ˜¯æœ‰å®ƒçš„ç®€ä¾¿å’Œå®žç”¨ä¹‹å¤„çš„ã€‚
    > 
    > LDAæ•°å­¦åˆ†æž
    > =======
    > 
    > ä¸Šä¸€å°èŠ‚ï¼Œæˆ‘ä»¬å¿½ç•¥äº†ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯å¦‚ä½•ç¡®å®šä¸»é¢˜å’Œè¯æ±‡åˆ†å¸ƒï¼Œè¿˜æœ‰æ–‡æ¡£ä¸Žè¯æ±‡çš„åˆ†å¸ƒï¼Œä½†æ˜¯è¦æžæ˜Žç™½è¿™ä¸ªé—®é¢˜ï¼Œå°±é¿å…ä¸äº†ä¸€äº›æ•°å­¦åˆ†æžäº†ã€‚å†æ¬¡å¼ºçƒˆæŽ¨èrickjinçš„ã€Š[LDAæ•°å­¦å…«å¦](http://www.52nlp.cn/author/rickjin)ã€‹è¿˜æœ‰Gregor Heinrichæœ‰è‘—åçš„ã€Š[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)ã€‹ã€‚æ­¤å¤„æˆ‘åªåšä¸€äº›å…³é”®æ‰¼è¦çš„ç†è®ºæŽ¨å¯¼ :-)
    > 
    > å¤šé¡¹å¼åˆ†å¸ƒ
    > -----
    > 
    > å›žå¿†ä¸€ä¸‹æ¦‚çŽ‡è®ºå­¦çš„ä¸œè¥¿ï¼Œå‡è®¾ä¸€ä¸ªç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚çŽ‡æ˜¯p
    > 
    > ï¼Œå¦‚æžœé‡å¤æŠ›næ¬¡ç¡¬å¸ï¼Œæ­£é¢æœä¸Šçš„æ¬¡æ•°ä¸ºk
    > 
    > çš„æ¦‚çŽ‡åˆ†å¸ƒå°±æ˜¯äºŒé¡¹åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯
    > 
    > Cknpk(1-p)n-k
    > 
    > å¦‚æžœæŠ›næ¬¡kä¸ªä¸åŒçš„ç¡¬å¸ï¼Œå‡è®¾æ¯ä¸ªç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚çŽ‡åˆ†åˆ«æ˜¯p1,p2,....,pk,é‚£ä¹ˆæŠ›næ¬¡ä¹‹åŽï¼Œå„ä¸ªç¡¬å¸æ­£é¢æœä¸Šçš„æ¬¡æ•°n1,n2,....,nkçš„æ¦‚çŽ‡åˆ†å¸ƒå°±æ˜¯å¤šé¡¹å¼åˆ†å¸ƒäº†ï¼Œè®°ä¸º
    > 
    > n!n1!n2!,....,nk!p1n1p2n2....pknk
    > 
    > LDAå‡ºæ¥è¯´è¯äº†ï¼š**ä¸»é¢˜å’Œè¯æ±‡çš„åˆ†å¸ƒå°±æ˜¯å¤šé¡¹å¼åˆ†å¸ƒ**ï¼å› ä¸ºä¸€ä¸ªä¸»é¢˜é‡Œé¢ä¸åŒè¯æ±‡å‡ºçŽ°çš„æ¦‚çŽ‡ä¸åŒï¼Œå¦‚æžœåªæœ‰1ä¸ªè¯æ±‡ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯äºŒé¡¹åˆ†å¸ƒï¼Œä½†æ˜¯è¯æ±‡ä¸å¯èƒ½åªæœ‰1ä¸ªï¼Œæ‰€ä»¥å®ƒç†æ‰€å½“ç„¶å°±æ˜¯ç¬¦åˆå¤šé¡¹å¼åˆ†å¸ƒã€‚è¿™æ˜¯æŒºåˆç†çš„å‡è®¾ï¼Œåæ­£æˆ‘ä»¬çŽ°åœ¨ç ”ç©¶çš„å†…å®¹æ˜¯ä¸ºè¯æ±‡æŒ‰ä¸»é¢˜ç»™å‡ºä¸åŒçš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œå…¶å®žä¹Ÿå°±æ˜¯ç»™å‡ºä¸åŒè¯æ±‡åœ¨ä¸åŒä¸»é¢˜ä¸‹çš„å‡ºçŽ°æ¯”ä¾‹ï¼Œæˆ‘ä»¬å¹¶ä¸å…³ç³»è¯æ±‡ä¹‹é—´çš„é¡ºåºå…³ç³»ï¼Œæ‰€ä»¥å¤šé¡¹å¼åˆ†å¸ƒå·²ç»å¾ˆå¥½åœ°åˆ»ç”»å‡ºè¿™ç§å…³ç³»äº†ã€‚LDAåˆè¯´ï¼š**æ–‡ç« å’Œä¸»é¢˜ä¹‹é—´çš„åˆ†å¸ƒä¹Ÿæ˜¯ç¬¦åˆå¤šé¡¹å¼åˆ†å¸ƒ**ï¼å› ä¸ºä¸€ç¯‡æ–‡ç« è¦ç¡®å®šä¸åŒä¸»é¢˜çš„å‡ºçŽ°æ¦‚çŽ‡ï¼Œå’Œä¸»é¢˜è¦ç¡®å®šæ¯ä¸ªè¯æ±‡çš„å‡ºçŽ°æ¦‚çŽ‡æ˜¯å®Œå…¨å¯ä»¥ç±»æ¯”çš„ï¼æ€è€ƒä¸€ä¸‹æˆ‘ä»¬ä¹ŸæŽ¥å—äº†LDAçš„è¯´æ³•ï¼ŒæŽ¥ä¸‹æ¥æˆ‘ä»¬ä»‹ç»ä¸€äº›è®°å·ï¼š
    > 
    > -   V
    > 
    > -   ä»£è¡¨æˆ‘ä»¬å­—å…¸çš„è¯æ±‡ä¸ªæ•°-   K-   ä»£è¡¨ä¸»é¢˜çš„ä¸ªæ•°-   M-   ä»£è¡¨æ–‡ç« çš„ä¸ªæ•°-   Ï•kâ†’ä»£è¡¨ç¬¬kä¸ªä¸»é¢˜çš„å¤šé¡¹å¼åˆ†å¸ƒå‚æ•°ï¼Œé•¿åº¦ä¸ºVï¼Œå› æ­¤Î¦æ˜¯ä¸€ä¸ªKâˆ—V-   çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªä¸»é¢˜çš„å¤šé¡¹å¼åˆ†å¸ƒå‚æ•°-   Î¸mâ†’ä»£è¡¨ç¬¬mç¯‡æ–‡ç« çš„å¤šé¡¹å¼åˆ†å¸ƒå‚æ•°ï¼Œé•¿åº¦ä¸ºKï¼Œå› æ­¤Î˜æ˜¯ä¸€ä¸ªMâˆ—K-   çš„çŸ©é˜µï¼Œæ²¡ä¸€è¡Œä»£è¡¨ä¸€ç¯‡æ–‡ç« çš„å¤šé¡¹å¼åˆ†å¸ƒå‚æ•°-   Nmä»£è¡¨ç¬¬m-   ç¯‡æ–‡ç« çš„é•¿åº¦-   zm,nä»£è¡¨ç¬¬mç¯‡æ–‡ç« ç¬¬n-   ä¸ªè¯ç”±å“ªä¸ªä¸»é¢˜äº§ç”Ÿçš„-   wm,nä»£è¡¨ç¬¬mç¯‡æ–‡ç« ç¬¬n
    > 
    > -   ä¸ªè¯
    > 
    > å¯¹äºŽç¬¬m
    > 
    > ç¯‡æ–‡ç« ï¼Œä»¤z
    > 
    > ä»£è¡¨ä¸€æ¬¡å®žéªŒäº§ç”Ÿçš„ä¸»é¢˜éšæœºå˜é‡ï¼Œé‚£ä¹ˆå®ƒå°±æœä»Ž:
    > 
    > zâˆ¼Multi(z|Î¸mâ†’)\
    > é‚£ä¹ˆå¯¹äºŽç¬¬kä¸ªä¸»é¢˜ï¼Œä»¤wä»£è¡¨ä¸€æ¬¡å®žéªŒäº§ç”Ÿçš„è¯éšæœºå˜é‡ï¼Œé‚£ä¹ˆå®ƒå°±æœä»Žï¼š
    > 
    > wâˆ¼Multi(w|Ï•kâ†’)
    > 
    > ç„¶åŽä¸ºäº†äº§ç”Ÿç¬¬mç¯‡æ–‡ç« ï¼Œåªè¦ç®€å•çš„æŒ‰é¡ºåºåˆ©ç”¨Multi(z|Î¸mâ†’)éšæœºç”Ÿæˆzm,1,zm,2,zm,3,...,zm,Nmï¼Œç„¶åŽå¯¹å·å…¥åº§ï¼Œå†åˆ©ç”¨Multi(w|Ï•zm,nâ†’)ï¼Œç”Ÿæˆwm,1,wm,2,wm,3,...,wm,Nmå³å¯ã€‚
    > 
    > Dirichletåˆ†å¸ƒ
    > -----------
    > 
    > å¿˜è®°å‘Šè¯‰å¤§å®¶ï¼ŒLDAå±žäºŽæ±Ÿæ¹–ä¸­çš„è´å¶æ–¯å­¦æ´¾ï¼Œåœ¨è´å¶æ–¯å­¦æ´¾çœ¼ä¸­ï¼Œåƒä¸Šé¢æåˆ°çš„Ï•kâ†’
    > 
    > å’ŒÎ¸mâ†’
    > 
    > éƒ½æ˜¯éšæœºå˜é‡ï¼Œéšæœºå˜é‡æ€Žä¹ˆå¯ä»¥æ²¡æœ‰å…ˆéªŒæ¦‚çŽ‡åˆ†å¸ƒå‘¢ï¼Ÿè¿™å²‚ä¸æ˜¯è´»ç¬‘å¤§æ–¹å—ï¼Ÿæ‰€ä»¥LDAæ•´ç†äº†ä¸€ä¸‹è¡£é¢†ï¼Œé©¬ä¸Šæå‡ºäº†ä»–ä»¬çš„å…ˆéªŒæ¦‚çŽ‡åˆ†å¸ƒï¼š
    > 
    > Ï•âƒ— âˆ¼Dirichlet(Î±âƒ— )
    > 
    > è€Œ
    > 
    > Î¸âƒ— âˆ¼Dirichlet(Î²âƒ— )
    > 
    > ä¸ºä»€ä¹ˆLDAè¦è¯´å®ƒçš„å…ˆéªŒåˆ†å¸ƒæ˜¯Dirichletåˆ†å¸ƒå‘¢ï¼Ÿå…¶ä¸­æœ€å¤§çš„åŽŸå› æ˜¯å› ä¸ºå¤šé¡¹å¼åˆ†å¸ƒå’ŒDirichletåˆ†å¸ƒæ˜¯ä¸€å¯¹å…±è½­åˆ†å¸ƒï¼Œå…±è½­åˆ†å¸ƒæœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿå¥½å¤„åœ¨äºŽè®¡ç®—åŽéªŒæ¦‚çŽ‡æœ‰æžå¤§çš„ä¾¿åˆ©ï¼è¯´åˆ°åº•æ˜¯LDAçœ‹ä¸­å®ƒè®¡ç®—æ–¹ä¾¿ã€‚å¢žåŠ äº†å…ˆéªŒæ¦‚çŽ‡åˆ†å¸ƒï¼Œé‚£ä¹ˆåœ¨ç¡®å®šæ–‡ç« ä¸Žä¸»é¢˜åˆ†å¸ƒè¿˜æœ‰ä¸»é¢˜ä¸Žè¯æ±‡åˆ†å¸ƒçš„æ—¶å€™ï¼Œå°±ç”±å…ˆéªŒæ¦‚çŽ‡åˆ†å¸ƒå…ˆéšæœºç”Ÿæˆç¡®å®šå¤šé¡¹å¼åˆ†å¸ƒçš„å‚æ•°ã€‚ç”¨ä¸€ä¸ªè”åˆæ¦‚çŽ‡åˆ†å¸ƒæ¥æè¿°ç¬¬mç¯‡æ–‡ç« ç”Ÿæˆè¿‡ç¨‹ï¼š
    > 
    > p(zmâ†’,wmâ†’,Î¸mâ†’,Î¦|Î±âƒ— ,Î²âƒ— )=âˆnNmp(wm,n|Ï•zm,nâ†’)p(zm,n|Î¸mâ†’)p(Î¸mâ†’|Î±âƒ— )p(Î¦|Î²âƒ— )\
    > å¯¹äºŽä¹ æƒ¯äº†ä½¿ç”¨æžå¤§ä¼¼ç„¶æ³•çš„åŒå­¦ï¼Œä¸ºäº†ä½¿ç”¨æžå¤§ä¼¼ç„¶æ³•ï¼Œæˆ‘ä»¬å¿…é¡»å°†éšå«å˜é‡æ¶ˆé™¤ï¼Œé‚£ä¹ˆå¯¹äºŽç¬¬mç¯‡æ–‡ç« å…¶ç”Ÿæˆçš„è¾¹ç¼˜æ¦‚çŽ‡ä¸ºï¼š
    > 
    > p(wmâ†’|Î±âƒ— ,Î²âƒ— )=âˆ«Î¸mâˆ«Î¦âˆ«zmâ†’âˆnNmp(wm,n|Ï•zm,nâ†’)p(zm,n|Î¸mâ†’)p(Î¸mâ†’|Î±âƒ— )p(Î¦|Î²âƒ— )=âˆ«Î¸mâˆ«Î¦âˆ‘zmâ†’âˆnNmp(wm,n|Ï•zm,nâ†’)p(zm,n|Î¸mâ†’)p(Î¸mâ†’|Î±âƒ— )p(Î¦|Î²âƒ— )=âˆ«Î¸mâˆ«Î¦âˆ‘zm1âˆ‘zm2...âˆ‘zmNmâˆnNmp(wm,n|Ï•zm,nâ†’)p(zm,n|Î¸mâ†’)p(Î¸mâ†’|Î±âƒ— )p(Î¦|Î²âƒ— )=âˆ«Î¸mâˆ«Î¦âˆnNmâˆ‘zmnp(wm,n|Ï•zm,nâ†’)p(zm,n|Î¸mâ†’)p(Î¸mâ†’|Î±âƒ— )p(Î¦|Î²âƒ— )\
    > ä»¥ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œè¾¹ç¼˜æ¦‚çŽ‡åˆ†å¸ƒå®žåœ¨æ˜¯å¤ªå¤æ‚äº†ï¼Œé æ™®é€šæžå¤§ä¼¼ç„¶æ³•æ¥æ±‚è§£åŸºæœ¬æ— æœ›ã€‚ä¸è¿‡è¿™å¹¶ä¸èƒ½éš¾å€’æˆ‘ä»¬èªæ˜Žçš„è®¡ç®—æœºç§‘å­¦å®¶ï¼Œä¸‹é¢æˆ‘ä»¬æ¥ä»‹ç»ä¸€ç§è¿‘ä¼¼æ±‚è§£æ³•ã€‚
    > 
    > Gibbs é‡‡æ ·ç®—æ³•
    > ----------
    > 
    > ### é‡‡æ ·ç®—æ³•çš„æ€æƒ³
    > 
    > å¯¹äºŽä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒp(xâƒ— )
    > 
    > ï¼Œæˆ‘ä»¬æƒ³å¾—åˆ°å®ƒå¾—æ¦‚çŽ‡åˆ†å¸ƒï¼Œæ— å¥ˆæœ‰äº›æ¦‚çŽ‡åˆ†å¸ƒå½¢å¼å®žåœ¨å¤æ‚ï¼Œæˆ‘ä»¬æ— æ³•ç›´æŽ¥æ±‚è§£ï¼Œé‚£ä¹ˆæ€Žä¹ˆåŠžå‘¢ï¼Ÿå‡è®¾æˆ‘ä»¬çš„éšæœºå˜é‡ä¸ºï¼š
    > 
    > xâƒ— =[x1,x2,x3,...,xi,...,xn]\
    > å…¶ä¸­xiâˆˆ{0,1}ï¼Œå‡è®¾æˆ‘ä»¬çŸ¥é“p(xâƒ— )çš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡é‡‡æ ·çš„æ–¹å¼å¾—åˆ°æ¯ä¸€æ¬¡çš„æ ·æœ¬å€¼ï¼š
    > 
    > xâƒ— (1)=[0,0,0,0,....0]xâƒ— (2)=[1,1,0,0,....0]xâƒ— (3)=[1,0,0,0,....0]xâƒ— (4)=[1,1,0,0,....0].......xâƒ— (N)=[1,1,0,0,....0]\
    > é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬å‡ºçŽ°çš„æ¬¡æ•°ï¼Œç„¶åŽé™¤ä»¥æ€»çš„é‡‡æ ·æ¬¡æ•°Næ¥å¾—åˆ°ç›¸åº”æ¦‚çŽ‡åˆ†å¸ƒã€‚æ¯”å¦‚æˆ‘ä»¬è§‚å¯Ÿæ ·æœ¬å€¼[1,1,0,0,....]å‡ºçŽ°äº†Kæ¬¡ï¼Œé‚£ä¹ˆæ ·æœ¬å‡ºçŽ°çš„æ¦‚çŽ‡å°±å¯ä»¥å¦‚ä¸‹æ±‚å¾—ï¼š
    > 
    > p(x1=1,x2=1,x3=0,....,xn=0)=KN\
    > è¿™æ˜¯é‡‡æ ·çš„åŸºæœ¬æ€æƒ³ï¼Œä½†æ˜¯è¿™é‡Œæœ‰ä»¤äººåŒªå¤·æ‰€æ€çš„åœ°æ–¹ï¼Œé¦–å…ˆæˆ‘ä»¬ä¸çŸ¥é“æ¦‚çŽ‡åˆ†å¸ƒæƒ³å¾—åˆ°æ¦‚çŽ‡åˆ†å¸ƒæˆ‘ä»¬å¿…é¡»é€šè¿‡é‡‡æ ·æ–¹æ³•æ¥æ±‚å¾—ï¼Œä½†æ˜¯é‡‡æ ·æ–¹æ³•åˆä¾èµ–äºŽæˆ‘ä»¬çŸ¥é“å¯¹åº”çš„æ¦‚çŽ‡åˆ†å¸ƒæ‰èƒ½å¾—åˆ°ç›¸åº”çš„é‡‡æ ·å€¼ï¼Œè¿™æ˜¯ä¸€ä¸ª"é¸¡ç”Ÿè›‹ï¼Œè›‹ç”Ÿé¸¡"çš„çŸ›ç›¾ï¼Œè¶³ä»¥ä»¤äººç™¾æ€ä¸å¾—å…¶è§£ï¼Œæ­¤æ—¶ç¥žå¥‡çš„Gibbsé‡‡æ ·æ›´åŠ ä»¤äººå……æ»¡æ•¬ç•ã€‚
    > 
    > ### ç¥žå¥‡çš„Gibbsé‡‡æ ·
    > 
    > å®žé™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æ˜¯æœ‰ä¸€å †æ–‡ç« ï¼Œç„¶åŽè¦æˆ‘ä»¬åŽ»è‡ªåŠ¨æå–ä¸»é¢˜ï¼Œæˆ–è€…é€šè¿‡çŽ°æœ‰çš„æ–‡ç« è®­ç»ƒå‡ºLDAæ¨¡åž‹ï¼Œç„¶åŽé¢„æµ‹æ–°çš„æ–‡ç« æ‰€å±žä¸»é¢˜åˆ†ç±»ã€‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„å…´è¶£ç‚¹åœ¨äºŽæ±‚å‡ºÎ˜
    > 
    > å’ŒÎ¦
    > 
    > çš„åŽéªŒæ¦‚çŽ‡åˆ†å¸ƒã€‚è™½ç„¶LDAçš„æ¨¡åž‹æ€æƒ³å¾ˆç®€å•ï¼Œä½†æ˜¯è¦ç²¾ç¡®æ±‚å‡ºå‚æ•°çš„åŽéªŒæ¦‚çŽ‡åˆ†å¸ƒå´æ˜¯ä¸å¯è¡Œçš„ï¼Œåªèƒ½é€šè¿‡è¿‘ä¼¼çš„æ–¹å¼æ±‚è§£ã€‚å¹¸å¥½å‰äººå·²ç»å‘çŽ°äº†å¾ˆå¤šè¿‘ä¼¼æ±‚è§£æ–¹æ³•ï¼Œå…¶ä¸­æ¯”è¾ƒç®€å•çš„å°±æ˜¯Gibbsé‡‡æ ·ï¼ŒGibbsçš„é‡‡æ ·ç²¾ç¥žå¾ˆç®€å•ï¼Œå¯¹äºŽä¸€ä¸ªå¾ˆå¤æ‚çš„é«˜ç»´æ¦‚çŽ‡åˆ†å¸ƒ:
    > 
    > p(xâƒ— )\
    > æˆ‘ä»¬æƒ³èŽ·å¾—p(xâƒ— )çš„æ ·æœ¬ï¼Œä»Žè€Œç¡®å®šp(xâƒ— )çš„å‚æ•°å€¼ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æ— æ³•ç›´æŽ¥æ±‚å–æ¦‚çŽ‡åˆ†å¸ƒçš„å‚æ•°ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥é€šè¿‡ç¥žå¥‡çš„Gibbsé‡‡æ ·ï¼ŒèŽ·å¾—éœ€è¦æ±‚è§£çš„æ¦‚çŽ‡åˆ†å¸ƒçš„æ ·æœ¬å€¼ï¼Œä»Žè€Œåè¿‡æ¥ç¡®å®šæ¦‚çŽ‡åˆ†å¸ƒã€‚Gibbsé‡‡æ ·å¾ˆç®€å•ï¼Œå®ƒè¯´å¦‚æžœä½ èƒ½å¾ˆå®¹æ˜“æ±‚è§£ï¼ˆé€šå¸¸éƒ½æ˜¯å¾ˆå®¹æ˜“æ±‚è§£ï¼Œå› ä¸ºæ­¤æ—¶çš„åˆ†å¸ƒæ˜¯ä¸€ä¸ªä¸€ç»´çš„æ¡ä»¶åˆ†å¸ƒï¼‰
    > 
    > p(xi|xÂ¬iâ†’)\
    > è¿™æ ·çš„æ¡ä»¶åˆ†å¸ƒï¼Œé‚£ä¹ˆæƒ³èŽ·å¾—è”åˆåˆ†å¸ƒçš„æ ·æœ¬ï¼Œåªéœ€æ‰§è¡Œå¦‚ä¸‹è¿‡ç¨‹ï¼š
    > 
    > -   éšæœºåˆå§‹åŒ–xâƒ— 0={x01,x02,...,x0N}
    > 
    > -\
    >     -   å¯¹äºŽ t=1,2,3,4,....T:
    > 
    >     -   xt1âˆ¼p(x1|xt-1Â¬1â†’)-\
    >     -   xt2âˆ¼p(x2|xt-1Â¬2â†’)-\
    >     -   xt3âˆ¼p(x3|xt-1Â¬3â†’)-\
    >     -   ....-   xtNâˆ¼p(xN|xt-1Â¬Nâ†’)-\
    >     -   å¾—åˆ°ä¸€ä¸ªæ ·æœ¬å€¼xâƒ— (t)=[xt1,xt2,xt3,....,xtN]
    > 
    > -   -
    > 
    > å½“é‡‡æ ·è¿‡ç¨‹æ”¶æ•›ä¹‹åŽï¼Œé€šè¿‡ä»¥ä¸Šé‡‡æ ·å¾—åˆ°çš„æ ·æœ¬å°±æ˜¯çœŸå®žçš„ p(xâƒ— )
    > 
    > æ ·æœ¬ã€‚ä¸ºä»€ä¹ˆå¯ä»¥å¦‚æ­¤ç¥žå¥‡åœ°æ“ä½œï¼Œå†æ¬¡æŽ¨èrickjinçš„ã€ŠLDAæ•°å­¦å…«å¦ã€‹ï¼
    > 
    > Collapsed Gibbs Sampler
    > -----------------------
    > 
    > å¯¹äºŽLDAçš„Inferenceé—®é¢˜ï¼Œæœ‰äº†ä¸‡èƒ½çš„Gibbsé‡‡æ ·ç®—æ³•ï¼Œé—®é¢˜æ±‚è§£å˜å¾—ç®€å•äº†ã€‚ä¸Šé¢æˆ‘ä»¬å·²ç»çŸ¥é“LDAæ¨¡åž‹çš„æ–‡æ¡£å»ºæ¨¡çš„è”åˆåˆ†å¸ƒï¼š
    > 
    > p(zmâ†’,wmâ†’,Î¸mâ†’,Î¦|Î±âƒ— ,Î²âƒ— )\
    > ä¸ºäº†é‡‡æ ·æ–¹ä¾¿ï¼Œå¯¹äºŽå‚æ•°Î¸mâ†’ï¼ŒÎ¦ï¼Œå®ƒä»¬æœ¬èº«å°±æ˜¯å…³è”zâƒ— å’Œwâƒ— çš„ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¦‚æžœå¾—åˆ°zâƒ— å’Œwâƒ— çš„é‡‡æ ·ï¼Œå‚æ•°Î¸mâ†’ï¼ŒÎ¦è‡ªç„¶å¯ä»¥å¾—çŸ¥ã€‚é‚£ä¹ˆå¯ä»¥ç§¯åˆ†åŒ–ç®€ï¼Œå¾—åˆ°æœ€ç»ˆè¦é‡‡æ ·çš„æ¨¡åž‹ï¼š
    > 
    > p(zâƒ— ,wâƒ— |Î±âƒ— ,Î²âƒ— )\
    > æœ‰äº†è”åˆåˆ†å¸ƒï¼ŒGibbsä¸‡èƒ½ç®—æ³•å°±å¯ä»¥å¥—ç”¨äº†ï¼Œé¦–å…ˆå®ƒå¿…é¡»å…ˆè§£å†³ä¸€ä¸ªé—®é¢˜(ä¸ºäº†è¡¨è¾¾æ–¹ä¾¿ï¼Œè¶…å‚æ•°Î±âƒ— ,Î²âƒ— çœç•¥)ï¼š
    > 
    > p(zm,i|zm,Â¬i,wmâ†’)
    > 
    > å…¶ä¸­zmâ†’={zm,iï¼k,zm,Â¬i}ï¼Œzm,Â¬iä»£è¡¨ç¬¬mç¯‡æ–‡ç« é‡Œé¢åŽ»é™¤ä¸»é¢˜zm,içš„å…¶ä»–æ‰€æœ‰ä¸»é¢˜ã€‚å…¶å…·ä½“æŽ¨å¯¼ä¸€å¼€å§‹æŽ¨èçš„æ–‡ç« éƒ½æœ‰è¯¦å°½ä¸¥æ ¼çš„è¿‡ç¨‹ï¼Œè¿™é‡Œå®žåœ¨æ²¡æœ‰å¿…è¦åœ¨èµ˜è¿°ï¼Œç›´æŽ¥ç»™å‡ºç»“æžœ(ä¸ºäº†è¡¨è¾¾æ–¹ä¾¿ï¼Œè¿™é‡ŒåŽ»é™¤ä¸‹æ ‡m)ï¼š
    > 
    > p(zi=k|zÂ¬i,wâƒ— )=p(wâƒ— ,zâƒ— )p(wâƒ— ,zÂ¬iâ†’)=p(wâƒ— |zâƒ— )p(wÂ¬i|zÂ¬iâ†’)p(wi)p(zâƒ— )p(zÂ¬i)â†’âˆp(wâƒ— |zâƒ— )p(wÂ¬i|zÂ¬iâ†’)p(zâƒ— )p(zÂ¬i)â†’âˆn(t)k,Â¬i+Î²tâˆ‘Vt=1n(t)k,Â¬i+Î²tn(k)m,Â¬i+Î±kâˆ‘Kk=1n(k)m,Â¬i+Î±kâˆn(t)k,Â¬i+Î²tâˆ‘Vt=1n(t)k,Â¬i+Î²t(n(k)m,Â¬i+Î±k)
    > 
    > å…¶ä¸­n(t)kä»£è¡¨ç¬¬mç¯‡æ–‡ç« ä¸­è¯æ±‡tå±žäºŽä¸»é¢˜kå‡ºçŽ°çš„æ¬¡æ•°ï¼Œn(t)k,Â¬iä»£è¡¨é™¤åŽ»å…¶ä¸­ziçš„å‰©ä½™ä¸ªæ•°ï¼Œä¹Ÿå°±æ˜¯è¯´
    > 
    > n(t)k,Â¬i=n(t)k-1
    > 
    > ç±»ä¼¼çš„ï¼Œn(k)mä»£è¡¨ç¬¬mç¯‡æ–‡ç« ä¸­ä¸»é¢˜kå‡ºçŽ°çš„æ¬¡æ•°ï¼Œn(k)m,Â¬iä»£è¡¨é™¤åŽ»å…¶ä¸­ziçš„å‰©ä½™ä¸ªæ•°ï¼Œä¹Ÿå°±æ˜¯è¯´
    > 
    > n(k)m,Â¬i=n(k)m-1
    > 
    > æ‰€ä»¥è®¡ç®—p(zi|zÂ¬i,wâƒ— )å˜æˆç®€å•åœ°åœ¨è®¡æ•°çš„é—®é¢˜ã€‚é€šè¿‡Gibbsé‡‡æ ·ç®—æ³•åŽï¼Œæœ€åŽçš„æ¨¡åž‹å‚æ•°å¯ä»¥è¿™æ ·å¾—å‡ºï¼š
    > 
    > Ï•k,t=n(t)k+Î²tâˆ‘Vt=1n(t)k+Î²t
    > 
    > Î¸m,k=n(k)m+Î±kâˆ‘Kk=1n(k)m+Î±k
    > 
    > åˆ¤æ–­æ–°æ–‡ç« çš„ä¸»é¢˜åˆ†å¸ƒ
    > ----------
    > 
    > ç”±ä¸Šä¸€å°èŠ‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¤§é‡æ–‡ç« æ±‚è§£å‡ºLDAè¿™ä¸ªæ¨¡åž‹ï¼Œé‚£ä¹ˆå¯¹äºŽä¸€ç¯‡æ–°çš„æ–‡ç« ï¼Œå¦‚ä½•è®¡ç®—å®ƒçš„ä¸»é¢˜åˆ†å¸ƒå‘¢ï¼Ÿä¸€ä¸ªæ–¹å¼æ˜¯å°†æ–‡ç« åŠ å…¥åˆ°åŽŸæ¥çš„è®­ç»ƒé›†åˆé‡Œé¢{z~âƒ— ,zâƒ— ;w~âƒ— ,wâƒ— }
    > 
    > ï¼Œç„¶åŽå¾—åˆ°å®ƒçš„é‡‡æ ·æ¡ä»¶æ¦‚çŽ‡ä¸ºï¼š
    > 
    > p(zi~=k|z~âƒ— Â¬i,zâƒ— ,wâƒ— ,w~âƒ— )âˆn(t)k+n~(t)k,Â¬i+Î²tâˆ‘Vt=1n(t)k+n~(t)k,Â¬i+Î²tn(k)m~,Â¬i+Î±kâˆ‘Kk=1n(k)m~,Â¬i+Î±k
    > 
    > å†é‡æ–°è·‘ä¸€æ¬¡Gibbsé‡‡æ ·ï¼Œç„¶åŽå¾—åˆ°å®ƒçš„åˆ†å¸ƒã€‚å®žé™…ä¸Šè¿™ç§åšæ³•ç¡®å®žæ˜¯æœ€ä¼˜çš„ï¼Œä½†æ˜¯å¤ªæ…¢äº†ï¼Œæ€Žä¹ˆåŠžå‘¢ï¼Ÿå› ä¸ºæ–°æ¥çš„æ–‡ç« ï¼Œå®ƒçš„è¯æ±‡æ˜¯å›ºå®šçš„ï¼Œæˆ‘ä»¬ä¸ŠèŠ‚å·²ç»æ±‚å‡ºÏ•k,tï¼Œä¹Ÿå°±æ˜¯è¯æ±‡å’Œä¸»é¢˜çš„åˆ†å¸ƒå·²ç»æ˜¯ç¡®å®šçš„ï¼Œæˆ‘ä»¬æ²¡å¿…è¦å†æµªè´¹è®¡ç®—åŠ›äº†ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è®¤ä¸ºå¯¹ä¸€ç¯‡æ–°æ–‡ç« ï¼Œå®ƒå·²ç»éš¾ä»¥æ’¼åŠ¨ä¹‹å‰æˆåƒä¸Šä¸‡æ–‡ç« çš„ç»Ÿè®¡ç»“æžœäº†ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š
    > 
    > n(t)k+n~(t)k,Â¬i+Î²tâˆ‘Vt=1n(t)k+n~(t)k,Â¬i+Î²tâ‰ˆn(t)k+Î²tâˆ‘Vt=1n(t)k+Î²t
    > 
    > æ‰€ä»¥æˆ‘ä»¬æ–°çš„æ¡ä»¶é‡‡æ ·æ¦‚çŽ‡å˜æˆäº†ï¼š
    > 
    > p(zi~=k|w~i=t,z~âƒ— Â¬i,zâƒ— ,wâƒ— ,w~Â¬iâ†’)âˆÏ•k,tâˆ—(n(k)m~,Â¬i+Î±k)
    > 
    > ç„¶åŽæˆ‘ä»¬åˆå¯ä»¥å¼€åŠ¨Gibbsé‡‡æ ·å‘åŠ¨æœºï¼Œå¾—åˆ°æœ€ç»ˆçš„åˆ†å¸ƒç»“æžœï¼š
    > 
    > Î¸m~,k=n(k)m~+Î±kâˆ‘Kk=1n(k)m~+Î±k
    > 
    > LDAçš„Gibbsé‡‡æ ·å®žçŽ°
    > =============
    > 
    > å¯¹äºŽç¨‹åºå‘˜æ¥è¯´ï¼Œçœ‹æƒ¯ä»£ç ï¼Œæ²¡æœ‰ä»£ç æœ‰ç‚¹æ— æ‰€é€‚ä»Žï¼Œæœ‰æ²¡æœ‰ç®€å•LDAçš„å®žçŽ°æ¼‚äº®ä»£ç å‘¢ï¼Ÿç­”æ¡ˆæ˜¯æœ‰çš„ï¼ŒLingPipeé‡Œé¢çš„LatentDirichletAllocationè¿™ä¸ªç±»ï¼Œå®Œæ•´åœ°æŒ‰ç…§Gregor Heinrichæœ‰è‘—åçš„ã€Š[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)ã€‹ä»‹ç»çš„ç®—æ³•å®žçŽ°äº†ï¼Œä»£ç éžå¸¸ç®€å•ï¼Œå¹¶ä¸”å¯è¯»æ€§æžé«˜ï¼Œå»ºè®®æŠ“æ¥ä¸€çœ‹ï¼Œå¿…ç„¶å¤§æœ‰æ¯—ç›Šã€‚æ­¤å¤„æˆ‘ä»¬è´´å‡ºGregorä¸­æä¾›çš„ä¼ªä»£ç ï¼Œä»¥ä¾›æŸ¥çœ‹ï¼š
    > 
    > -   åˆå§‹åŒ–é˜¶æ®µï¼Œå‡ºä¹Žæ„æ–™çš„ç®€å•ï¼Œåªè¦åˆå§‹åŒ–4ä¸ªç»Ÿè®¡é‡ï¼Œåˆ†åˆ«æ˜¯:
    > 
    >     -   æ–‡æ¡£m
    > 
    > å¯¹åº”ä¸»é¢˜kçš„è®¡æ•°ï¼š nkm-\
    >     -   æ–‡æ¡£mçš„è¯æ±‡æ•°ï¼šnm-\
    >     -   ä¸»é¢˜kå¯¹åº”çš„è¯æ±‡ä¸ºtçš„è®¡æ•°ï¼šntk-\
    >     -   ä¸»é¢˜kçš„è¯æ±‡æ•°ï¼šnk-   -\
    >         -   å°†nkm,nm,ntk,nkå†…å­˜æ¸…0ï¼Œç„¶åŽæ ¹æ®ä»¥ä¸‹ç¨‹åºéšæœºåˆå§‹åŒ–å€¼ï¼š
    > 
    > -   éåŽ†æ¯ä¸€ä¸ªæ–‡æ¡£mâˆˆ[1,M]
    > 
    > -   éåŽ†æ¯ä¸ªè¯æ±‡nâˆˆ[1,Nm]
    > 
    > -   ä»Žå¤šé¡¹å¼åˆ†å¸ƒMult(1/K)
    > 
    > å¾—åˆ°ä¸€ä¸ªé‡‡æ ·å€¼ï¼šzmn=k-\
    >     -   nkm=nkm+1-\
    >     -   nm=nm+1-\
    >     -   ntk=ntk+1-\
    >     -   nk=nk+1-   -   -   -\
    >                 -   Gibbsé‡‡æ ·è¿‡ç¨‹ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªé‡‡æ ·å‘¨æœŸçš„æ‰§è¡Œè¿‡ç¨‹ï¼š
    > 
    >     -   éåŽ†æ¯ä¸€ä¸ªæ–‡æ¡£mâˆˆ[1,M]
    > 
    > -   éåŽ†æ¯ä¸ªè¯æ±‡nâˆˆ[1,Nm]
    > 
    > -   å¯¹äºŽå½“å‰çš„wm,n
    > 
    > çš„ä¸»é¢˜kå¯¹åº”çš„è¯æ±‡tæ‰§è¡Œï¼š
    > 
    > -   nkm=nkm-1
    > 
    > -\
    >     -   nm=nm-1-\
    >     -   ntk=ntk-1-\
    >     -   nk=nk-1-\
    >     -   æ ¹æ®p(zi=k|zÂ¬i,wâƒ— )èŽ·å¾—ä¸€ä¸ªé‡‡æ ·å€¼ï¼šk^=zm,nå¹¶æ‰§è¡Œï¼š
    > 
    > -   nk^m=nk^m+1
    > 
    > -\
    >     -   nm=nm+1-\
    >     -   ntk^=ntk^+1-\
    >     -   nk^=nk^+1
    > 
    > -   -   -   -   -   -
    > 
    > éš¾ä»¥ç½®ä¿¡çš„ç®€å•ï¼Œä¸€èˆ¬åœ¨æ”¶æ•›ä¹‹å‰ï¼Œéœ€è¦è·‘ä¸€å®šæ•°é‡çš„é‡‡æ ·æ¬¡æ•°è®©é‡‡æ ·ç¨‹åºå¤§è‡´æ”¶æ•›ï¼Œè¿™ä¸ªæ¬¡æ•°ä¸€èˆ¬ç§°ä¸ºï¼šburnin periodã€‚æˆ‘ä»¬å¸Œæœ›ä»Žé‡‡æ ·ç¨‹åºä¸­èŽ·å¾—ç‹¬ç«‹çš„æ ·æœ¬ï¼Œä½†æ˜¯æ˜¾ç„¶ä»¥ä¸Šçš„é‡‡æ ·è¿‡ç¨‹æ˜Žæ˜¾ä¾èµ–ä¸Šä¸€ä¸ªé‡‡æ ·ç»“æžœï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨ä¸Šä¸€æ¬¡é‡‡æ ·åŽï¼Œå…ˆæŠ›å¼ƒä¸€å®šæ•°é‡çš„é‡‡æ ·ç»“æžœï¼Œå†èŽ·å¾—ä¸€ä¸ªé‡‡æ ·ç»“æžœï¼Œè¿™æ ·å¯ä»¥å¤§è‡´åšåˆ°æ ·æœ¬ç‹¬ç«‹ï¼Œè¿™æ ·çœŸæ­£é‡‡æ ·çš„æ—¶å€™ï¼Œæ€»æœ‰ä¸€å®šçš„æ»žåŽæ¬¡æ•°ï¼Œè¿™æ ·çš„æ ·æœ¬ä¸Žæ ·æœ¬çš„é—´éš”ç§°ä¸ºï¼šSampleLagã€‚
    > 
    > LDAä¸ºä»€ä¹ˆèƒ½work
    > ===========
    > 
    > å¦‚æžœæ‚¨åå¤è¯»äº†å‰é¢åå¤å¼ºè°ƒçš„ä¸¤ç¯‡LDAç§‘æ™®å¤§ä½œï¼Œå¹¶æ¸…æ¥šäº†è§£å®ƒçš„å®žçŽ°ç»†èŠ‚ï¼Œæœ‰ä¸€äº›é—®é¢˜å¯èƒ½ä¼šæ…¢æ…¢è¦ç»•åœ¨å¿ƒä¸­ï¼ŒæŒ¥ä¹‹ä¸åŽ»------ä¸ºä»€ä¹ˆLDAèƒ½å¤Ÿworkï¼Ÿä¸ºä»€ä¹ˆLDAèƒ½äº§ç”Ÿå¦‚ä¸‹ç»“æžœï¼š
    > ![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://img-blog.csdn.net/20170831143634112?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXdzMzIxNzE1MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    > ä¸ºä»€ä¹ˆå®ƒçš„ç»“æžœä¼šè¿™ä¹ˆç¥žå¥‡ï¼Œæ¯ä¸ªä¸»é¢˜ä¸‹é¢çš„è¯åˆ†å¸ƒåŸºæœ¬ç¬¦åˆæˆ‘ä»¬çš„ç›´è§‰ã€‚æ¯”å¦‚computersè¿™ä¸ªä¸»é¢˜ä¸‹é¢çš„è¯æ±‡åˆ†å¸ƒæ˜¯"computerï¼Œmodelï¼Œinformationï¼Œdataï¼Œ......"ã€‚
    > å‰é¢æˆ‘ä»¬æåˆ°ï¼ŒLDAæ¨¡åž‹ç”Ÿæˆè¿‡ç¨‹å¾ˆç®€å•ï¼ŒåŸºæœ¬ä¸Šæ˜¯æžå…¶naiveçš„ï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆå®ƒå°±èƒ½äº§ç”Ÿè¿™æ ·ç¬¦åˆç›´è§‰çš„ç»“æžœå‘¢ï¼Ÿï¼ˆè™½ç„¶è¯´ä»Žç›´è§‰ä¸Šï¼Œåˆ¤æ–­ä¸€ç¯‡æ–‡ç« çš„ä¸»é¢˜ï¼Œå³ä½¿æ–‡ç« è¯çš„é¡ºåºæ‰“ä¹±äº†ï¼Œæˆ‘ä»¬è¿˜æ˜¯èƒ½å¤§è‡´åˆ¤æ–­ä¸»é¢˜çš„ï¼Œä¹Ÿå°±æ˜¯LDAè¿™ç§BOWæ¨¡åž‹æœ‰å®ƒçš„åˆç†æ€§ï¼Œä½†æ˜¯è¿™å¹¶ä¸èƒ½è§£é‡Šå®ƒä¸ºä»€ä¹ˆèƒ½äº§ç”Ÿå¦‚ä¸Šç»“æžœï¼‰
    > 
    > çŽ°åœ¨æˆ‘ä»¬å†å›žé¡¾ä¸€ä¸‹ï¼Œå¯¹äºŽLDAæ¨¡åž‹ï¼Œæˆ‘ä»¬æŽ¨æ–­çš„ç›®æ ‡æ˜¯å®ƒèƒŒåŽçš„éšç»“æž„ï¼Œä¹Ÿå°±æ˜¯"æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ"å’Œ"ä¸»é¢˜-è¯æ±‡åˆ†å¸ƒ"ï¼Œé‚£ä¹ˆæˆ‘ä»¬å†æ¥ä»”ç»†è§‚å¯Ÿå®ƒçš„åŽéªŒæ¦‚çŽ‡åˆ†å¸ƒï¼ˆåŽŸè°…æˆ‘ä¸ºäº†è¡¨è¾¾æ–¹ä¾¿ï¼Œæ”¹å˜äº†ä¸€äº›è®°å·ï¼‰ï¼š
    > 
    > p(z|w,Î¸)âˆp(w,z,Î¸|Î±)=âˆmp(Î¸m|Î±)âˆnp(zmn|Î¸m)p(wmn|zmn,Î¦)
    > é¦–å…ˆéœ€è¦æ˜Žç¡®çš„æ˜¯**éšå˜é‡çš„åŽéªŒæ¦‚çŽ‡åˆ†å¸ƒæ˜¯æ­£æ¯”äºŽè”åˆæ¦‚çŽ‡åˆ†å¸ƒ**çš„ã€‚åœ¨æžå¤§ä¼¼ç„¶æ³•ä¸­ï¼Œ**æˆ‘ä»¬ä¼šå¸Œæœ›æ¨¡åž‹è¶ŠæŽ¥è¿‘å®žé™…è¶Šå¥½ï¼Œä¹Ÿå°±æ˜¯æ¨¡åž‹åœ¨æ•°æ®ä¸Šçš„æ¦‚çŽ‡è¶Šå¤§è¶Šå¥½**ã€‚æˆ‘ä»¬å…ˆåªè€ƒè™‘ä¸Šé¢è¡¨è¾¾å¼çš„æ•°æ®ä¼¼ç„¶é¡¹p(wmn|zmn,Î¦)ï¼Œä¸ºäº†ä½¿è¿™ä¸€ä¼¼ç„¶é¡¹è¶Šå¤§è¶Šå¥½ï¼Œæˆ‘ä»¬ä¼šå¸Œæœ›æŸä¸ª**ä¸»é¢˜ä¸‹å¯¹åº”çš„è¯çš„æ¦‚çŽ‡è¶Šå¤§è¶Šå¥½**ï¼Œç”±äºŽä¸»é¢˜-è¯æ±‡åˆ†å¸ƒç”±Î¦å†³å®šï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›Î¦çŸ©é˜µåº”è¯¥æ˜¯é•¿ä¸‹é¢è¿™ä¸ªæ ·å­ï¼š
    > ![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://img-blog.csdn.net/20170831181756008?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXdzMzIxNzE1MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    > æ¨ªå‘ä»£è¡¨ä¸»é¢˜ï¼Œæ€»å…±æœ‰Kè¡Œï¼Œåˆ—å‘ä»£è¡¨è¯æ±‡ï¼Œæ€»å…±æœ‰Våˆ—ã€‚
    > 
    > æˆ‘ä»¬å¸Œæœ›æ¯ä¸ªä¸»é¢˜çš„è¯æ±‡æ¦‚çŽ‡è´¨é‡å¦‚ä¸Šå›¾è“è‰²æ–¹å—æ‰€ç¤ºï¼ˆæ–¹å—çš„æ¦‚çŽ‡å’Œä¸º1ï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯æ±‡æŒ‰ç…§ä¸»é¢˜ä¸ªæ•°ä¸é‡å åœ°åˆ‡å‰²ï¼Œè¿™æ ·å¯ä»¥ä¿è¯è¯æ±‡åœ¨æ‰€å±žä¸»é¢˜ä¸‹æ¦‚çŽ‡å€¼æœ€å¤§ï¼Œå› ä¸ºæ¯ä¸€è¡Œæ¦‚çŽ‡å’Œä¸º1ï¼Œæˆ‘ä»¬å¸Œæœ›æ¦‚çŽ‡å¯†åº¦é›†ä¸­åœ¨æŸäº›è¯ä¸Šé¢ï¼Œè€Œä¸æ˜¯åˆ†æ•£åœ°è½åœ¨æ¯ä¸ªè¯ä¸Šé¢ï¼ˆè¯·è¯»è€…æ€è€ƒä¸€ä¸‹ä¸ºä»€ä¹ˆè¿™æ ·ä¼šä½¿æœ€ç»ˆçš„æ¦‚çŽ‡å€¼å¢žå¤§ï¼Œè¿™é‡Œæ¯”è¾ƒéš¾ä»¥ç”¨æ–‡å­—è¡¨è¾¾æ¸…æ¥šï¼Œå¯ä»¥è®¾æƒ³ä¸€ä¸‹ï¼Œå‡å¦‚æ¯ä¸ªä¸»é¢˜çš„æ¦‚çŽ‡å¯†åº¦å‡åŒ€åˆ†é…åˆ°æ¯ä¸ªè¯ä¸Šé¢ï¼Œé‚£ä¹ˆæœ€ç»ˆæ¯ä¸€é¡¹p(wmn|zmn,Î¦)
    > 
    > ä¼šå¾ˆå°ï¼Œè¿™ä¸Žæˆ‘ä»¬çš„æ„¿æœ›æ˜¯è¿èƒŒçš„ï¼‰ã€‚ä¸ºäº†ä½¿æ¨¡åž‹åœ¨æ•°æ®ä¸Šçš„æ¦‚çŽ‡æœ€å¤§ï¼Œç®—æ³•ä¼šå€¾å‘äºŽå°†ä¸»é¢˜è¯æ±‡åˆ†å¸ƒæŒ‰ç…§ä¸Šå›¾çš„å½¢å¼æ‹Ÿåˆï¼Œä¹Ÿå°±æ˜¯"ä¸»é¢˜-è¯æ±‡"åˆ†å¸ƒä¼šå€¾å‘äºŽæˆä¸ºä¸€ä¸ªç¨€ç–çš„åˆ†å¸ƒã€‚
    > 
    > æœ‰äº†ä¸Šé¢çš„è®¨è®ºï¼Œæˆ‘ä»¬å†æ¥æƒ³æƒ³ï¼Œæ¯ä¸ªä¸»é¢˜ä¼šé€‰å“ªäº›è¯æ±‡ä½œä¸ºè‡ªå·±çš„ä¸»é¢˜è¯å‘¢ï¼Ÿ ä¹Ÿå°±æ˜¯ä¸»é¢˜åº”è¯¥é€‰å“ªäº›è¯æ¥å°†è‡ªå·±çš„æ¦‚çŽ‡è´¨é‡æ•£è½åœ¨ä»–ä»¬èº«ä¸Šã€‚ç­”æ¡ˆæ˜¯é‚£äº›ç»å¸¸å‡ºçŽ°åœ¨ä¸€èµ·çš„è¯ã€‚å‡è®¾ä¸»é¢˜Aä¸‹é¢æœ‰ä¸»é¢˜è¯ï¼ša1,a2,...,an
    > 
    > ï¼Œä¸ºäº†ä½¿æ¦‚çŽ‡å€¼å˜å¤§ï¼Œé‚£ä¹ˆè¿™äº›è¯ä¸€å®šæ˜¯åŒæ—¶å‡ºçŽ°åœ¨å¾ˆå¤šä¸ªæ–‡æ¡£é‡Œé¢ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªæ–‡æ¡£ä¸­ï¼Œè¿™äº›è¯å¤§éƒ¨åˆ†éƒ½åŒæ—¶å‡ºçŽ°ã€‚å¯ä»¥æƒ³æƒ³ä¸ºä»€ä¹ˆè¦è¿™æ ·ï¼Œå‡è®¾ä¸æ˜¯è¿™æ ·ï¼Œæ¯”å¦‚åœ¨æ–‡æ¡£1ç§ä¸»é¢˜Açš„è¯æ±‡æ˜¯a1,a2,a3,a4ï¼Œåœ¨æ–‡æ¡£2ä¸­ä¸»é¢˜Açš„è¯æ±‡æ˜¯a5,a6,a7,a8ï¼Œåœ¨æ–‡æ¡£3ä¸­ä¸»é¢˜Açš„è¯æ±‡æ˜¯a9,a10,a11,a12
    > 
    > ï¼Œä»¥æ­¤ç±»æŽ¨ï¼Œå¦‚æžœä¸»é¢˜Açš„è¯æ±‡çœŸçš„æ˜¯è¿™ä¸ªæ ·å­------æ–‡æ¡£ä¸­åŒä¸€ä¸»é¢˜éƒ½æ²¡æœ‰åœ¨å¦ä¸€ä¸ªæ–‡æ¡£ä¸­å‡ºçŽ°ï¼Œé‚£ä¹ˆä¸»é¢˜Aä¸‹é¢çš„è¯ä¼šå¾ˆå¤šï¼Œä½†æ˜¯æˆ‘ä»¬ä¸Šé¢åˆ†æžäº†ï¼Œä¸ºäº†ä½¿æ¨¡åž‹æ¦‚çŽ‡å€¼å¤§ï¼Œæ¯ä¸ªä¸»é¢˜ä¸‹é¢çš„è¯å¿…é¡»è¶Šå°‘è¶Šå¥½ï¼Œæ‰€ä»¥è¿™ä¹Ÿæ˜¯æœ‰è¿èƒŒæ„¿æœ›çš„ã€‚å› æ­¤**ä¸»é¢˜ä¸‹é¢çš„è¯éƒ½ä¼šå€¾å‘äºŽåŒæ—¶å‡ºçŽ°åœ¨å¤šä¸ªæ–‡æ¡£ä¸­**ï¼Œåˆ°è¿™é‡Œä¸ºæ­¢ï¼Œè¯»è€…åº”è¯¥å¯ä»¥å¤§æ¦‚æ˜Žç™½ä¸ºä»€ä¹ˆLDAå¯ä»¥äº§ç”Ÿé‚£ä¹ˆç¬¦åˆç›´è§‰çš„è¯æ±‡åˆ†å¸ƒäº†ï¼Œå› ä¸ºåœ¨æˆ‘ä»¬äººç±»è‡ªå·±çš„æ¦‚å¿µä¸­ï¼Œä¸»é¢˜è¯æ±‡å°±æ˜¯è¿™æ ·çš„ä¸œè¥¿ï¼Œä»–ä»¬ä¼šç»å¸¸ä¸€èµ·å‡ºçŽ°ï¼Œæ¯”å¦‚"é“¶è¡Œ"ï¼Œ"å­˜æ¬¾"è¿™äº›é‡‘èžä¸»é¢˜è¯æ±‡ä¼šå¾ˆé¢‘ç¹åŒæ—¶å‡ºçŽ°åœ¨å¤šä¸ªåœºåˆä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šå°†ä»–ä»¬å½’ä¸ºä¸€ä¸ªä¸»é¢˜ï¼Œè€ŒLDAæ°æ°èƒ½æ•èŽ·è¿™ç§ç‰¹æ€§ã€‚
    > 
    > Edwin Chençš„åšå®¢ã€Š[Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)ã€‹ä¹Ÿæ¯”è¾ƒç›´è§‚åœ°ä»‹ç»äº†LDAçš„ç›´è§‰ç‰¹æ€§ã€‚è‡³äºŽæ€Žä¹ˆä»Žç†è®ºä¸Šæ¥è¯´æ˜Žä¸ºä»€ä¹ˆå…·æœ‰ç¨€ç–æ€§è´¨ï¼Œ [Quoraä¸Šé¢æœ‰ä¸€ä¸ªç›¸å¯¹ç›´è§‚çš„è§£é‡Š](https://www.quora.com/Why-does-LDA-work)ï¼Œæˆ‘å¤§æ¦‚æ€»ç»“ä¸€ä¸‹ï¼Œç”±äºŽLDAç”¨Dirichletä½œä¸ºPrioråˆ†å¸ƒï¼Œè€Œä½œä¸ºPriorçš„Dirichletåœ¨å…¶åˆ†å¸ƒå‚æ•°Î±âƒ— 
    > 
    > å–å¾ˆå°çš„æ—¶å€™ï¼ˆä¸€èˆ¬Î±å–K/50, Î²
    > 
    > å–å€¼0.01ï¼‰ï¼Œå¯ä»¥ä½¿å¾—å…¶é‡‡æ ·çš„å¤šé¡¹å¼å‚æ•°å˜å¾—ç¨€ç–ã€‚LDAæœ‰ä¸¤å¯¹ç¨€ç–å¯¹æŠ—ï¼Œä¸»é¢˜ä¸Žæ–‡æ¡£çš„ç¨€ç–æ€§å’Œä¸»é¢˜ä¸Žè¯æ±‡çš„ç¨€ç–æ€§ä¹‹é—´çš„å¯¹æŠ—ï¼Œè€ŒLDAä¼šä»Žæ•°æ®ä¸­å­¦ä¹ åˆ°ä¸€ä¸ªæƒè¡¡ç»“æžœã€‚ä¸ºä»€ä¹ˆDirichletä¼šæœ‰ç¨€ç–æ€§è´¨å‘¢ï¼Ÿå¯ä»¥å‚ç…§ä»¥è¿™ç¯‡ï¼š
    > ã€Š[Notes on Multinomial sparsity and the Dirichlet concentration parameter Î±](http://www.cs.cmu.edu/~dbamman/notes/dirichletConcentration.pdf)ã€‹
    > è¿™ç¯‡noteæåˆ°çš„Dirichletå…¶å®žå¯ä»¥çœ‹æˆå‡ ä¸ªGammaåˆ†å¸ƒå˜æ¢è€Œæ¥ï¼Œå…·ä½“å˜æ¢è¯æ˜Žå¯ä»¥å‚ç…§Quoraå¦ä¸€ä¸ªè§£ç­”ï¼š[Construction of Dirichlet distribution with Gamma distribution](http://stats.stackexchange.com/questions/36093/construction-of-dirichlet-distribution-with-gamma-distribution)ã€‚å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œå¤šäº†å…ˆéªŒåˆ†å¸ƒçš„æ¨¡åž‹æ¯”pLSAæ›´åŠ å¥å£®ï¼Œä¸å®¹æ˜“å¯¼è‡´overfitingï¼Œå¦‚æžœçœ‹å›žä¸Šé¢æŽ¨å¯¼Gibbs Samplingçš„å…¬å¼ï¼ŒDirichletå…¶å®žèµ·åˆ°ä¸€ç§Smoothçš„æ•ˆæžœã€‚å¯ä»¥å†å‚è€ƒKevin Gimpelå†™çš„ã€Š[Modeling Topics](http://www.cs.cmu.edu/~nasmith/LS2/gimpel.06.pdf)ã€‹ï¼Œå¯¹äºŽå‡ ç§å¸¸è§åŸºç¡€çš„ä¸»é¢˜æ¨¡åž‹çš„å¯¹æ¯”ï¼Œä¹Ÿå¯ä»¥è§£é™¤ä¸å°‘å›°æƒ‘ã€‚
    > 
    > å‚è€ƒæ–‡çŒ®
    > ====
    > 
    > -   ã€Š[LDAæ•°å­¦å…«å¦](http://www.52nlp.cn/author/rickjin)ã€‹
    > -   ã€Š[Parameter estimation for text analysis](https://users.soe.ucsc.edu/~amichelo/docs/text-est2.pdf)ã€‹
    > -   ã€Š[gibbs sampling for the uninitiated](http://wwwold.cs.umd.edu/~hardisty/papers/gsfu.pdf)ã€‹
    > -   ã€Š[text mining and topic models](http://cseweb.ucsd.edu/~elkan/250B/topicmodels.pdf)ã€‹
    > -   ã€Š[A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling](http://u.cs.biu.ac.il/~89-680/darling-lda.pdf)ã€‹
    > -   ã€Š[Probabilistic Latent Semantic Analysis](http://dl.acm.org/citation.cfm?id=2073829)ã€‹
    > -   [LingPipe](http://alias-i.com/lingpipe/)
    > -   ã€Š[Modeling Topics](http://www.cs.cmu.edu/~nasmith/LS2/gimpel.06.pdf)ã€‹
    > -   ã€Š[Notes on Multinomial sparsity and the Dirichlet concentration parameter Î±](http://www.cs.cmu.edu/~dbamman/notes/dirichletConcentration.pdf)ã€‹
    > -   [Why does LDA worksï¼Ÿ](https://www.quora.com/Why-does-LDA-work)
    > -   [Construction of Dirichlet distribution with Gamma distribution](http://stats.stackexchange.com/questions/36093/construction-of-dirichlet-distribution-with-gamma-distribution)
    > -   Dave Blei's video lecture on topic models: < http://videolectures.net/mlss09uk_blei_tm>

- [Conjugate prior - Wikiwand](https://www.wikiwand.com/en/Conjugate_prior)

    > Example
    > -------
    > 
    > The form of the conjugate prior can generally be determined by inspection of the [probability density](https://www.wikiwand.com/en/Probability_density_function) or [probability mass function](https://www.wikiwand.com/en/Probability_mass_function) of a distribution. For example, consider a [random variable](https://www.wikiwand.com/en/Random_variable) which consists of the number of successes ![s](https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632) in ![n](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) [Bernoulli trials](https://www.wikiwand.com/en/Bernoulli_trial) with unknown probability of success ![q](https://wikimedia.org/api/rest_v1/media/math/render/svg/06809d64fa7c817ffc7e323f85997f783dbdf71d) in [0,1]. This random variable will follow the [binomial distribution](https://www.wikiwand.com/en/Binomial_distribution), with a probability mass function of the form
    > 
    > ![{\displaystyle p(s)={n \choose s}q^{s}(1-q)^{n-s))](https://wikimedia.org/api/rest_v1/media/math/render/svg/db06457d8f50347a044b7672740227e722331976)
    > 
    > The usual conjugate prior is the [beta distribution](https://www.wikiwand.com/en/Beta_distribution) with parameters (![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3), ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8)):
    > 
    > ![p(q)={q^{\alpha -1}(1-q)^{\beta -1} \over \mathrm {B} (\alpha ,\beta )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c924d010fed83e219416a1ffb41331aa429ada9c)
    > 
    > where ![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) and ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) are chosen to reflect any existing belief or information (![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) = 1 and ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) = 1 would give a [uniform distribution](https://www.wikiwand.com/en/Uniform_distribution_(continuous) "Uniform distribution (continuous)")) and *Î’*(![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3), ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8)) is the [Beta function](https://www.wikiwand.com/en/Beta_function) acting as a [normalising constant](https://www.wikiwand.com/en/Normalising_constant).
    > 
    > In this context, ![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) and ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) are called *[hyperparameters](https://www.wikiwand.com/en/Hyperparameter)* (parameters of the prior), to distinguish them from parameters of the underlying model (here *q*). It is a typical characteristic of conjugate priors that the dimensionality of the hyperparameters is one greater than that of the parameters of the original distribution. If all parameters are scalar values, then this means that there will be one more hyperparameter than parameter; but this also applies to vector-valued and matrix-valued parameters. (See the general article on the [exponential family](https://www.wikiwand.com/en/Exponential_family), and consider also the [Wishart distribution](https://www.wikiwand.com/en/Wishart_distribution "Wishart distribution"), conjugate prior of the [covariance matrix](https://www.wikiwand.com/en/Covariance_matrix) of a [multivariate normal distribution](https://www.wikiwand.com/en/Multivariate_normal_distribution), for an example where a large dimensionality is involved.)
    > 
    > If we then sample this random variable and get *s* successes and *f* failures, we have
    > 
    > ![{\displaystyle {\begin{aligned}P(s,f\mid q=x)&={s+f \choose s}x^{s}(1-x)^{f},\\P(x)&={x^{\alpha -1}(1-x)^{\beta -1} \over \mathrm {B} (\alpha ,\beta )},\\P(q=x\mid s,f)&={\frac {P(s,f\mid x)P(x)}{\int P(s,f\mid x)P(x)dx))\\&=(({s+f \choose s}x^{s+\alpha -1}(1-x)^{f+\beta -1}/\mathrm {B} (\alpha ,\beta )} \over \int _{y=0}^{1}\left({s+f \choose s}y^{s+\alpha -1}(1-y)^{f+\beta -1}/\mathrm {B} (\alpha ,\beta )\right)dy}\\&={x^{s+\alpha -1}(1-x)^{f+\beta -1} \over \mathrm {B} (s+\alpha ,f+\beta )},\\\end{aligned))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cb14cab68a14e4ae9b49c0d3acea79ecf00c97ca)
    > 
    > which is another Beta distribution with parameters (![\alpha ](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3) + *s*, ![\beta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/7ed48a5e36207156fb792fa79d29925d2f7901e8) + *f*). This posterior distribution could then be used as the prior for more samples, with the hyperparameters simply adding each extra piece of information as it comes.

### LDA æ•¸å­¸æŽ¨å°Ž

$$
\begin{split}
&éŠæˆ²ä¸­çš„Kå€‹topic-wordéª°å­ï¼Œå¯è¨˜ç‚º\vec{\varphi_{1}},...,\vec{\varphi_{K}}ï¼Œ\\
&å°æ–¼åŒ…å«Mç¯‡æ–‡æª”çš„èªžæ–™åº«C=(d_1,d_2,...,d_M)\\
&ä¸­çš„æ¯ç¯‡æ–‡æª”d_mï¼Œéƒ½æœ‰ä¸€å€‹ç‰¹å®šçš„doc-topicéª°å­\vec{\theta_{m}}ï¼Œ\\
&æ‰€æœ‰å°æ‡‰çš„éª°å­è¨˜ç‚º\vec{\theta_{1}},...,\vec{\theta_{M}}\\
&\\
&ç”±æ¢ä»¶æ©ŸçŽ‡ï¼Œç¬¬mç¯‡æ–‡æª”d_mä¸­çš„æ¯å€‹è©žçš„ç”Ÿæˆæ©ŸçŽ‡å¯å±•é–‹ç‚º\\
&p(w|d_m)=\displaystyle\sum_{z=1}^{K}p(w|z)p(z|d_m)=\sum_{z=1}^{K}\varphi_{zw}\theta_{mz}\\
&æ‰€ä»¥æ•´ç¯‡æ–‡æª”çš„ç”Ÿæˆæ©ŸçŽ‡ç‚º\\
&p(\vec{w}|d_m)=\displaystyle\prod_{i=1}^{n}\sum_{z=1}^{K}p(w_i|z)p(z|d_m)=\prod_{i=1}^{n}\sum_{z=1}^{K}\varphi_{zw_i}\theta_{dz}
\end{split}
$$

---

$$
\begin{split}
&\vec{\theta_m} \sim Dir(\vec{\alpha})\\
&\vec{\varphi_k} \sim Dir(\vec{\beta})
\end{split}
$$

---

$$
\begin{split}
&\vec{\alpha} \rightarrow \vec{\theta_m} \rightarrow z_{m,n} : é€™å€‹éŽç¨‹è¡¨ç¤ºå¦‚ä½•ç”Ÿæˆç¬¬mç¯‡æ–‡æª”\\
&ç¬¬nå€‹è©žçš„topicç·¨è™Ÿã€‚å¾žç¬¬ä¸€å€‹ç½ˆå­æŠ½å‡ºMå€‹éª°å­ï¼Œ\\
&å…¶ä¸­çš„ç¬¬må€‹éª°å­ä½œç‚ºé€™ç¯‡æ–‡æª”çš„doc-topicéª°å­\vec{\theta_m}ï¼Œ\\
&ç„¶å¾ŒæŠ•æ“²é€™å€‹éª°å­ç”Ÿæˆæ–‡æª”ä¸­ç¬¬nå€‹è©žçš„topicç·¨è™Ÿz_{m,n}\\
\end{split}
$$

---

$$
\begin{split}
&\vec{\beta} \rightarrow \vec{\varphi_k} \rightarrow w_{m,n}|k=z_{m,n} : é€™å€‹éŽç¨‹è¡¨ç¤ºå¦‚ä½•\\
&ç”Ÿæˆç¬¬mç¯‡æ–‡æª”çš„ç¬¬nå€‹è©žã€‚å¾žç¬¬äºŒå€‹ç½ˆå­æŠ½å‡ºKå€‹éª°å­ï¼Œ\\
&å…¶ä¸­çš„ç¬¬k=z_{m,n}å€‹éª°å­ä½œç‚ºé€™å€‹è©žçš„topic-word\\
&éª°å­\vec{\varphi_k}ï¼Œç„¶å¾ŒæŠ•æ“²é€™å€‹éª°å­ç”Ÿæˆæ–‡æª”ä¸­ç¬¬nå€‹è©žw_{m,n}
\end{split}
$$

---
$$
\vec{\alpha} \underbrace{\longrightarrow}_{Dirichlet} \vec{\theta_m} \underbrace\longrightarrow_{Multinomial} \vec{z_m}
$$

---


$$
\begin{split}
&p(\vec{z_m}|\vec{\alpha})=\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta\vec{\alpha}}\\
&å…¶ä¸­\vec{n_m}=(n_m^{(1)},...,n_m^{(K)})ï¼Œn_m^{(k)}è¡¨ç¤ºç¬¬mç¯‡æ–‡æª”ä¸­\\
&ç¬¬kå€‹topicç”¢ç”Ÿçš„è©žçš„å€‹æ•¸
\end{split}
$$

---

$$
\begin{split}
p(\vec{z}|\vec{\alpha})&=\prod_{m=1}^{M}p(\vec{z_m}|\vec{\alpha})\\
&=\prod_{m=1}^{M}\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta\vec{\alpha}}\\
\end{split}
$$

---

$$
\vec{\beta} \underbrace{\longrightarrow}_{Dirichlet} \vec{\varphi_k} \underbrace\longrightarrow_{Multinomial} \vec{w_{(k)}}
$$

---

$$
\begin{split}
&p(\vec{w_{(k)}}|\vec{\beta})=\frac{\Delta(\vec{n_k}+\vec{\beta})}{\Delta\vec{\beta}}\\
&å…¶ä¸­\vec{n_k}=(n_k^{(1)},...,n_k^{(K)})ï¼Œn_k^{(t)}è¡¨ç¤ºç¬¬kå€‹topicç”¢ç”Ÿçš„è©žä¸­\\
&word\ t çš„å€‹æ•¸
\end{split}
$$

---

$$
\begin{split}
p(\vec{w}|\vec{z},\vec{\beta})&=\prod_{k=1}^{K}p(\vec{w_{(k)}}|\vec{z_{(k)},\beta})\\
&=\prod_{k=1}^{K}\frac{\Delta(\vec{n_k}+\vec{\beta})}{\Delta\vec{\beta}}\\
\end{split}
$$

---

$$
\begin{split}
p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})&=p(\vec{w}|\vec{z},\vec{\beta})p(\vec{z}|\vec{\alpha})\\
&=\prod_{k=1}^{K}\frac{\Delta(\vec{n_k}+\vec{\beta})}{\Delta\vec{\beta}}\prod_{m=1}^{M}\frac{\Delta(\vec{n_m}+\vec{\alpha})}{\Delta\vec{\alpha}}\\
\end{split}
$$

---

$$
P(X_{t+1}=x|X_t,X_t-1,...)=P(X_{t+1}=x|X_t)
$$

---

$$
Ï€_0P^n=Ï€P=Ï€
$$

---

$$
\begin{split}
&ç”±æ–¼\vec{w}æ˜¯è§€æ¸¬åˆ°çš„å·²çŸ¥æ•¸æ“šï¼Œåªæœ‰\vec{z}æ˜¯éš±å«çš„è®Šé‡ï¼Œ\\
&æ‰€ä»¥æˆ‘å€‘çœŸæ­£éœ€è¦æŽ¡æ¨£çš„æ˜¯åˆ†å¸ƒp(\vec{z}|\vec{w})ã€‚èªžæ–™åº«\vec{z}ä¸­\\
&çš„ç¬¬iå€‹è©žå°æ‡‰çš„topic æˆ‘å€‘è¨˜ç‚ºz_iï¼Œå…¶ä¸­i=(m,n)ï¼Œ\\
&è¡¨ç¤ºç¬¬mç¯‡æ–‡æª”çš„ç¬¬nå€‹è©žï¼Œæˆ‘å€‘ç”¨\neg i è¡¨ç¤ºåŽ»é™¤ä¸‹æ¨™ç‚º\\
&içš„è©žã€‚é‚£éº¼ï¼ŒæŒ‰ç…§Gibbs Samplingç®—æ³•çš„è¦æ±‚ï¼Œ\\
&æˆ‘å€‘è¦æ±‚å¾—ä»»æ„åº§æ¨™è»¸iå°æ‡‰çš„æ¢ä»¶åˆ†å¸ƒ\\
&p(z_i=k|\vec{z}_{\neg i},\vec{w})\\
&\\
&å‡è¨­å·²è§€æ¸¬åˆ°çš„è©žw_i=tï¼Œå‰‡ç”±è²è‘‰æ–¯æ³•å‰‡ï¼Œ\\
&å¯ä»¥å¾—åˆ°\\
\end{split}
$$

---

$$
\begin{split}
p(z_i=k|\vec{z}_{\neg i},\vec{w}) &\propto p(z_i=k,w_i=t|\vec{z}_{\neg i},\vec{w}_{\neg i})\\
&=\int p(z_i=k,w_i=t,\vec{\theta}_m,\vec{\varphi}_k|\vec{z}_{\neg i},\vec{w}_{\neg i})d\vec{\theta}_md\vec{\varphi}_k\\
&...\\
&=\int \theta_{mk} Dir(\vec{\theta}_m|\vec{n}_{m,\neg i}+\vec{\alpha}) d \vec{\theta}_m 
\cdot \int \varphi_{kt} Dir(\vec{\varphi}_k|\vec{n}_{k,\neg i}+\vec{\beta})  d \vec{\varphi}_k\\
&=E(\theta_{mk}) \cdot E(\varphi_{kt})\\
&=\hat{\theta}_{mk} \cdot \hat{\varphi}_{kt}\\
\end{split}
$$

---

$$
\begin{split}
\hat{\theta}_{mk}&=\frac{n_{m,\neg i}^{(k)}+\alpha_k}{\sum_{k=1}^{K}(n_{m,\neg i}^{(k)}+\alpha_k)}\\
\hat{\varphi}_{kt}&=\frac{n_{k,\neg i}^{(t)}+\beta_t}{\sum_{t=1}^{V}(n_{k,\neg i}^{(t)}+\beta_t)}\\
\end{split}
$$

---

$$
\begin{split}
p(z_i=k|\vec{z}_{\neg i},\vec{w}) &\propto \underbrace{\frac{n_{m,\neg i}^{(k)}+\alpha_k}{\sum_{k=1}^{K}(n_{m,\neg i}^{(k)}+\alpha_k)}}_{p(topic|doc)} \cdot \underbrace{\frac{n_{k,\neg i}^{(t)}+\beta_t}{\sum_{t=1}^{V}(n_{k,\neg i}^{(t)}+\beta_t)}}_{p(word|topic)}\\
\end{split}
$$

### topic coherence 

- [topic_coherence_tutorial](https://markroxor.github.io/gensim/static/notebooks/topic_coherence_tutorial.html)

- [Coherence Score u_mass : learnmachinelearning](https://www.reddit.com/r/learnmachinelearning/comments/9bcr77/coherence_score_u_mass/)

    > [Palmetto (link)](http://palmetto.aksw.org/palmetto-webapp/?coherence=umass) states that in u_mass, "for every word the logarithm of its conditional probability is calculated using every other top word that has a higher order in the ranking of top words as condition" - so it is not tested against every other word, but only high ranking words.
    > 
    > Also, in [this tutorial (link)](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda) the optimal topic number is being calculated using coherence score, so I feel like it is a valid approach.


- [Gensim Topic Modeling - A Guide to Building Best LDA models](https://web.archive.org/web/20180524091929/https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda)

    > 17\. How to find the optimal number of topics for LDA?
    > ------------------------------------------------------
    > 
    > My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.
    > 
    > Choosing a 'k' that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.
    > 
    > If you see the same keywords being repeated in multiple topics, it's probably a sign that the 'k' is too large.
    > 
    > The `compute_coherence_values()` (see below) trains multiple LDA models and provides the models and their corresponding coherence scores.
    > 
    > ```python
    > def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    >     """
    >     Compute c_v coherence for various number of topics
    > 
    >     Parameters:
    >     ----------
    >     dictionary : Gensim dictionary
    >     corpus : Gensim corpus
    >     texts : List of input texts
    >     limit : Max num of topics
    > 
    >     Returns:
    >     -------
    >     model_list : List of LDA topic models
    >     coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    >     """
    >     coherence_values = []
    >     model_list = []
    >     for num_topics in range(start, limit, step):
    >         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
    >         model_list.append(model)
    >         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
    >         coherence_values.append(coherencemodel.get_coherence())
    > 
    >     return model_list, coherence_values
    > ```
    > 
    > ```python
    > # Can take a long time to run.
    > model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)
    > ```
    > 
    > ```python
    > # Show graph
    > limit=40; start=2; step=6;
    > x = range(start, limit, step)
    > plt.plot(x, coherence_values)
    > plt.xlabel("Num Topics")
    > plt.ylabel("Coherence score")
    > plt.legend(("coherence_values"), loc='best')
    > plt.show()
    > ```
    > 
    > [![Choosing the optimal number of LDA topics](https://i.imgur.com/QVCloCa.jpg)](https://i.imgur.com/QVCloCa.jpg)
    > 
    > Choosing the optimal number of LDA topics
    > 
    > ```python
    > # Print the coherence scores
    > for m, cv in zip(x, coherence_values):
    >     print("Num Topics =", m, " has Coherence Value of", round(cv, 4))
    > ```
    > 
    > ```python
    > Num Topics = 2  has Coherence Value of 0.4451
    > Num Topics = 8  has Coherence Value of 0.5943
    > Num Topics = 14  has Coherence Value of 0.6208
    > Num Topics = 20  has Coherence Value of 0.6438
    > Num Topics = 26  has Coherence Value of 0.643
    > Num Topics = 32  has Coherence Value of 0.6478
    > Num Topics = 38  has Coherence Value of 0.6525
    > ```
    > 
    > If the coherence score seems to keep increasing, it may make better sense to pick the model that gave the highest CV before flattening out. This is exactly the case here.
    > 

    > ---
    > 
    > 18\. Finding the dominant topic in each sentence
    > ------------------------------------------------
    > 
    > One of the practical application of topic modeling is to determine what topic a given document is about.
    > 
    > To find that, we find the topic number that has the highest percentage contribution in that document.
    > 
    > The `format_topics_sentences()` function below nicely aggregates this information in a presentable table.
    > 
    > ```python
    > def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):
    >     # Init output
    >     sent_topics_df = pd.DataFrame()
    > 
    >     # Get main topic in each document
    >     for i, row in enumerate(ldamodel[corpus]):
    >         row = sorted(row, key=lambda x: (x[1]), reverse=True)
    >         # Get the Dominant topic, Perc Contribution and Keywords for each document
    >         for j, (topic_num, prop_topic) in enumerate(row):
    >             if j == 0:  # => dominant topic
    >                 wp = ldamodel.show_topic(topic_num)
    >                 topic_keywords = ", ".join([word for word, prop in wp])
    >                 sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
    >             else:
    >                 break
    >     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
    > 
    >     # Add original text to the end of the output
    >     contents = pd.Series(texts)
    >     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    >     return(sent_topics_df)
    > 
    > df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)
    > 
    > # Format
    > df_dominant_topic = df_topic_sents_keywords.reset_index()
    > df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
    > 
    > # Show
    > df_dominant_topic.head(10)
    > ```
    > 
    > [![Dominant Topic For Each Document](https://i.imgur.com/DZTDFlx.jpg)](https://i.imgur.com/DZTDFlx.jpg)
    > 
    > Dominant Topic For Each Document
    > 
    > 19\. Find the most representative document for each topic
    > ---------------------------------------------------------
    > 
    > Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!
    > 
    > ```python
    > # Group top 5 sentences under each topic
    > sent_topics_sorteddf_mallet = pd.DataFrame()
    > 
    > sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')
    > 
    > for i, grp in sent_topics_outdf_grpd:
    >     sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,
    >                                              grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],
    >                                             axis=0)
    > 
    > # Reset Index
    > sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)
    > 
    > # Format
    > sent_topics_sorteddf_mallet.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", "Text"]
    > 
    > # Show
    > sent_topics_sorteddf_mallet.head()
    > ```
    > 
    > [![Most Representative Topic For Each Document](https://i.imgur.com/D8D09c9.jpg)](https://i.imgur.com/D8D09c9.jpg)
    > 
    > Most Representative Topic For Each Document
    > 
    > The tabular output above actually has 20 rows, one each for a topic. It has the topic number, the keywords, and the most representative document. The `Perc_Contribution` column is nothing but the percentage contribution of the topic in the given document.
    > 
    > 20\. Topic distribution across documents
    > ----------------------------------------
    > 
    > Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information.
    > 
    > ```python
    > # Number of Documents for Each Topic
    > topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()
    > 
    > # Percentage of Documents for Each Topic
    > topic_contribution = round(topic_counts/topic_counts.sum(), 4)
    > 
    > # Topic Number and Keywords
    > topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]
    > 
    > # Concatenate Column wise
    > df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)
    > 
    > # Change Column names
    > df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']
    > 
    > # Show
    > df_dominant_topics
    > ```
    > 
    > [![Topic Volume Distribution](https://i.imgur.com/dm8F3sf.jpg)](https://i.imgur.com/dm8F3sf.jpg)
    > 
    > Topic Volume Distribution
    > 
    > 



### LDA and TFIDF

- [How do you combine LDA and tf-idf? - Quora](https://www.quora.com/How-do-you-combine-LDA-and-tf-idf)

    > LDA requires data in the form of integer counts. So modifying feature values using TF-IDF and then using with LDA doesn't really fit in. You might instead want to try some of the NMF algorithms, which aren't MCMC usually, but they work with general non-negative data. I've seen nice results in CCA this way. However, you can use TF-IDF as a way of screening features/words to then use in the LDA.
    > 
    > Also, in most IR experiments I've done, LDA weights for a document are usually dominated (in value for retrieval) by TF-IDF weights, so not much good.
    > 
    > Having said that, check out:
    > [Why is the performance improved by using TFIDF instead of bag-of-words in LDA clustering?](https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering)
    > where they talk about doing it and [Tanmoy Mukherjee](https://www.quora.com/profile/Tanmoy-Mukherjee-1) gives some insights.


- [Why is the performance improved by using TFIDF instead of bag-of-words in LDA clustering? - Quora](https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering)

    > tf part of tf-idf is the bag of word assumption. Let me add some points where one might use tf-idf to get better performance
    > LDA is similar to matrix factorization i.e it takes a term document matrix and gives two matrices topic by word and document by topic matrix. According to standard approach it is assumed all words are *equally*
    > important for calculating the conditional probability. However certain terms comes with a certain frequency i.e a stop word would have more frequency than certain keywords. So if one were to weight in  the importance of a term
    > you would replace the sampling eqn 2.26 of [1]
    > where you would replace all the counts of the term N with a new count N' which gives you the importance of a term. The remaining steps should remain similar.
    > 
    > [1][http://cxwangyi.files.wordpress....](http://cxwangyi.files.wordpress.com/2012/01/llt.pdf)
    > Distributed Gibbs Sampling of Latent Topic
    > Models: The Gritty Details
    > 
    > P.S I have been lazy to skip details but a good place to initiate this discussion should be in the topic models mailing list.

- [Necessary to apply TF-IDF to new documents in gensim LDA model? - Stack Overflow](https://stackoverflow.com/questions/44781047/necessary-to-apply-tf-idf-to-new-documents-in-gensim-lda-model)

    > tf-idf is used in the latent dirichlet allocation to some extent. As can be read in the paper [Topic Models by Blei and Lafferty](http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf) (e.g. p.6 - Visualizing Topics and p.12), the tf-idf score can be very useful for LDA. It can be used to visualize topics or to chose the vocabulary. "It is often computationally expensive to use the entire vocabulary. Choosing the top V words by TFIDF is an effective way to prune the vocabulary".
    > 
    > This said, LDA does not need tf-idf to infer topics, but it can be useful and it can improve your results.
    > 


## interactive topic modeling/Incorporating Domain Knowledge into Topic Modeling

- [How do I remove non-sense topics generated using LDA? - Quora](https://www.quora.com/How-do-I-remove-non-sense-topics-generated-using-LDA)

    > While the paper on Interactive topic modeling would be something you should definitely look into [Page on nih.gov](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2943854/) where the authors use the concept of must links and cannot links and creating an appropriate Dirichet tree prior.
    > Have a look at these slides where the authors also use first order logic for the same
    > [http://pages.cs.wisc.edu/~andrze...](http://pages.cs.wisc.edu/~andrzeje/publications/ijcai-2011-slides.pdf)
    > Chck some related work by Dan Roth on constrained models although they are not Mixture models
    > Hope that helps
    > 
    > ---
    > 
    > At the risk of self-promotion, you may want to consider interactive topic modeling:
    > [Page on colorado.edu](http://www.cs.colorado.edu/~jbg/docs/mlj_2013_itm.pdf)
    > 
    > More generally, here's a book chapter on care and feeding of topic models:
    > [Page on colorado.edu](http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf)


## nonnegative matrix factorization (NMF)/probabilistic latent semantic analysis (PLSA)

- [What is the difference between NMF and LDA? Why are the priors of LDA sparse-induced? - Quora](https://www.quora.com/What-is-the-difference-between-NMF-and-LDA-Why-are-the-priors-of-LDA-sparse-induced)

    > The first thing to note is that nonnegative matrix factorization (NMF) can be shown to be equivalent to optimizing the same objective function as the one from probabilistic latent semantic analysis (PLSA) ([Ding et al., 2008](http://core.ac.uk/download/pdf/22118.pdf)).  The two approaches only differ in how inference proceeds, but the underlying model is the same.
    > 
    > This makes it easier to compare NMF to latent Dirichlet allocation (LDA), as PLSA is simply the special case of LDA where we assume the Dirichlet prior in the data generating process for LDA is uniform.
    > 
    > On sparseness: This follows directly from the properties of the Dirichlet distribution. David Blei's [Topic Models](http://videolectures.net/mlss09uk_blei_tm/) lecture at 54:30 provides a good intuition for this.


## LFM vs LSA(LSI) vs pLSA(pLSI) vs LDA

- [LSA vs pLSA vs LDA : MachineLearning](https://www.reddit.com/r/MachineLearning/comments/10mdtf/lsa_vs_plsa_vs_lda/)

    > each of them is used to describe incoming set of bag-of-word distribution data into a lower dimensional bag-of-topic data.
    > 
    > LSA -> uses SVD, and as a result the topics are assumed to be orthogonal.
    > 
    > pLSA -> Treats topics as word distributions, uses probabilistic methods, and topics are allowed to be non-orthogonal.
    > 
    > LDA -> similar to pLSA, but with dirichlet priors for the document-topic and topic-word distributions. This prevents over-fitting, and gives better results.
    > 


- [topic model (LSAã€PLSAã€LDA) - lmm6895071çš„ä¸“æ  - CSDNåšå®¢](https://blog.csdn.net/lmm6895071/article/details/74999129)

    > # Topicæ¨¡åž‹
    > 
    > **æ¦‚è¦ï¼š**
    > 
    > > [LFM](https://www.baidu.com/s?wd=LFM&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)ï¼ˆä¾èµ–äºŽçŸ©é˜µåˆ†è§£ï¼‰  
    > > LSA(LSI)ï¼ˆ[SVD](https://www.baidu.com/s?wd=SVD&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)åˆ†è§£ï¼‰  
    > > PLSIï¼ˆEMç®—æ³•ä¼˜åŒ–ï¼Œé¢‘çŽ‡å­¦æ´¾ï¼Œå‚æ•°æœªçŸ¥ä½†å›ºå®šï¼‰  
    > > LDAï¼ˆåœ¨PLSAåŸºç¡€ä¸ŠåŠ ä¸Šè´å¶æ–¯æ¡†æž¶ï¼Œ$\alpha$, $\beta$ ~dirichletåˆ†å¸ƒ,åˆ†åˆ«ä½œä¸ºä¸»é¢˜-æ–‡æ¡£å’Œè¯-ä¸»é¢˜çš„å…ˆéªŒåˆ†å¸ƒï¼›è´å¶æ–¯å­¦æ´¾çš„ç‰¹ç‚¹æ˜¯å‚æ•°æ˜¯éšæœºå˜åŒ–çš„ï¼Œä½†æ˜¯æœä»ŽæŸä¸ªåˆ†å¸ƒï¼Œä¸æ–­çš„å­¦ä¹ æ–°çš„çŸ¥è¯†ï¼Œå½¢æˆåŽéªŒï¼‰
    > 
    > **ä»‹ç»ï¼š**
    > 
    > > LFMã€LSIã€PLSIã€LDAéƒ½æ˜¯éšå«è¯­ä¹‰åˆ†æžæŠ€æœ¯ï¼Œæ˜¯åŒä¸€ç±»æ¦‚å¿µï¼›åœ¨æœ¬è´¨ä¸Šæ˜¯ç›¸é€šçš„ï¼Œéƒ½æ˜¯æ‰¾å‡ºæ½œåœ¨çš„ä¸»é¢˜æˆ–ç‰¹å¾ã€‚è¿™äº›æŠ€æœ¯é¦–å…ˆåœ¨æ–‡æœ¬æŒ–æŽ˜é¢†åŸŸä¸­è¢«æå‡ºæ¥ï¼Œè¿‘äº›å¹´ä¹Ÿè¢«ä¸æ–­åº”ç”¨åˆ°å…¶ä»–é¢†åŸŸä¸­ï¼Œå¹¶å¾—åˆ°äº†ä¸é”™çš„åº”ç”¨æ•ˆæžœã€‚  
    > > åœ¨æŽ¨èç³»ç»Ÿä¸­å®ƒèƒ½å¤ŸåŸºäºŽç”¨æˆ·çš„è¡Œä¸ºå¯¹itemè¿›è¡Œè‡ªåŠ¨èšç±»ï¼Œä¹Ÿå°±æ˜¯æŠŠitemåˆ’åˆ†åˆ°ä¸åŒç±»åˆ«/ä¸»é¢˜ï¼Œè¿™äº›ä¸»é¢˜/ç±»åˆ«å¯ä»¥ç†è§£ä¸ºç”¨æˆ·çš„å…´è¶£ã€‚å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œéšå«ä¸»é¢˜å‘æŽ˜ä»¥æå–å¿…è¦ç‰¹å¾ï¼Œè­¬å¦‚LDAèŽ·å¾—ä¸»é¢˜åˆ†å¸ƒä¹‹åŽï¼Œå¯ä»¥å®žçŽ°å¯¹æ–‡æ¡£çš„é™ç»´ã€‚åœ¨è®ºæ–‡æŽ¨èé¢†åŸŸï¼Œæ¬¡LDA+PMFæ¨¡åž‹å®žçŽ°ååŒä¸»é¢˜å›žå½’æ¨¡åž‹ï¼ˆCTR)ã€‚
    > 
    > * * *
    > 
    > ## LFM ï¼ˆéšè¯­ä¹‰æ¨¡åž‹ï¼‰
    > 
    > ä¾‹å­ï¼š  
    > å°†ç”¨æˆ·è¯„åˆ†çŸ©é˜µï¼ˆæ··æ·†çŸ©é˜µï¼‰åˆ†è§£R=P* Q  
    > PçŸ©é˜µä»£è¡¨äº† user-class  
    > QçŸ©é˜µä»£è¡¨äº†class-item  
    > **class:æ ¹æ®è‡ªåŠ¨èšç±»ç®—æ³•èŽ·å¾—å‡ ä¸ªç±»æ ‡ç­¾**ï¼›  
    > Pã€Qä¸­çš„å‚æ•°é€šè¿‡æ¨¡åž‹å­¦ä¹ å¾—åˆ°ï¼š  
    > æœ€åŽè®¡ç®—å¹³æ–¹æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œä½¿å¾—æŸå¤±å€¼æœ€å°ï¼›  
    > ![çŸ©é˜µåˆ†è§£](https://i.imgur.com/3lJui0i.jpg)  
    > ![æŸå¤±å‡½æ•°](https://i.imgur.com/zCMSkta.jpg)
    > 
    > [å‚è€ƒæ–‡çŒ®](http://blog.csdn.net/harryhuang1990/article/details/9924377)
    > 
    > * * *
    > 
    > ## LSAæ¨¡åž‹
    > 
    > Latent Semantic Analysis (Latent Semantic Indexing)
    > 
    > > èƒŒæ™¯  
    > > ä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢ä¸­ï¼šå°†å•è¯ä½œä¸ºç‰¹å¾ï¼Œæž„é€ ç‰¹å¾å‘é‡ï¼›è®¡ç®—æŸ¥è¯¢å•è¯ä¸Žæ–‡æ¡£é—´çš„ç›¸ä¼¼åº¦ï¼›ä½†æ˜¯æ²¡æœ‰è€ƒè™‘åˆ°è¯­ä¹‰ã€åŒä¹‰è¯ç­‰ç›¸å…³ä¿¡æ¯ï¼›åœ¨åŸºäºŽå•è¯çš„æ£€ç´¢æ–¹æ³•ä¸­ï¼ŒåŒä¹‰è¯ä¼šé™ä½Žæ£€ç´¢ç®—æ³•çš„å¬å›žçŽ‡(Recall)ï¼Œè€Œå¤šä¹‰è¯çš„å­˜åœ¨ä¼šé™ä½Žæ£€ç´¢ç³»ç»Ÿçš„å‡†ç¡®çŽ‡(Precision)ã€‚  
    > > æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç§æ¨¡åž‹ï¼Œèƒ½å¤Ÿæ•èŽ·åˆ°å•è¯ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å¦‚æžœä¸¤ä¸ªå•è¯ä¹‹é—´æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œé‚£ä¹ˆå½“ä¸€ä¸ªå•è¯å‡ºçŽ°æ—¶ï¼Œå¾€å¾€æ„å‘³ç€å¦ä¸€ä¸ªå•è¯ä¹Ÿåº”è¯¥å‡ºçŽ°(åŒä¹‰ è¯)ï¼›åä¹‹ï¼Œå¦‚æžœæŸ¥è¯¢è¯­å¥æˆ–è€…æ–‡æ¡£ä¸­çš„æŸä¸ªå•è¯å’Œå…¶ä»–å•è¯çš„ç›¸å…³æ€§éƒ½ä¸å¤§ï¼Œé‚£ä¹ˆè¿™ä¸ªè¯å¾ˆå¯èƒ½è¡¨ç¤ºçš„æ˜¯å¦å¤–ä¸€ä¸ªæ„æ€(æ¯”å¦‚åœ¨è®¨è®ºäº’è”ç½‘çš„æ–‡ç« ä¸­ï¼Œ[Apple](https://www.baidu.com/s?wd=Apple&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd) æ›´å¯èƒ½æŒ‡çš„æ˜¯Appleå…¬å¸ï¼Œè€Œä¸æ˜¯æ°´æžœ) ã€‚  
    > > $LSA(LSI)ä½¿ç”¨SVD$æ¥å¯¹**å•è¯-æ–‡æ¡£çŸ©é˜µ**è¿›è¡Œåˆ†è§£ã€‚$SVD$å¯ä»¥çœ‹ä½œæ˜¯ä»Žå•è¯-æ–‡æ¡£çŸ©é˜µä¸­å‘çŽ°ä¸ç›¸å…³çš„ç´¢å¼•å˜é‡(å› å­)ï¼Œå°†åŽŸæ¥çš„æ•°æ®æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´å†…ã€‚åœ¨å•è¯-æ–‡æ¡£çŸ©é˜µä¸­ä¸ç›¸ä¼¼çš„ä¸¤ä¸ªæ–‡æ¡£ï¼Œå¯èƒ½åœ¨è¯­ä¹‰ç©ºé—´å†…æ¯”è¾ƒç›¸ä¼¼ã€‚
    > 
    > $SVD$ï¼Œäº¦å³å¥‡å¼‚å€¼åˆ†è§£ ï¼Œä¸€ä¸ª$t*d$ç»´çš„(å•è¯-æ–‡æ¡£çŸ©é˜µ)$X$ï¼Œå¯ä»¥åˆ†è§£ä¸º:
    > 
    > $$X=T*S*D^T$$
    > 
    > å…¶ä¸­$T$ä¸º$t*m$ç»´çŸ©é˜µï¼Œ$T$ä¸­çš„æ¯ä¸€åˆ—ç§°ä¸ºå·¦å¥‡å¼‚å‘é‡(left singular vector)ï¼Œ$S$ä¸º$m*m$ç»´å¯¹è§’çŸ©é˜µï¼Œæ¯ä¸ªå€¼ç§°ä¸ºå¥‡å¼‚å€¼(singular value)ï¼Œ$D$ä¸º$d*m$ç»´çŸ©é˜µ,$D$ä¸­çš„æ¯ä¸€åˆ—ç§°ä¸ºå³å¥‡å¼‚å‘é‡ã€‚åœ¨å¯¹å•è¯æ–‡æ¡£çŸ©é˜µ$X$åš$SVD$åˆ†è§£ä¹‹åŽï¼Œæˆ‘ä»¬åªä¿å­˜$S$ä¸­æœ€å¤§çš„$K$ä¸ªå¥‡å¼‚å€¼ï¼Œå¾—åˆ°$D^{'}$ã€$T'$ã€$S'$ï¼›åˆ™å½¢æˆäº†ä¸€ä¸ªæ–°çš„$t*d$çŸ©é˜µï¼š
    > 
    > $$X'=T^{'}*S'*D^{'T}$$
    > 
    > è¿˜åŽŸåŽçš„Xâ€™ä¸ŽXå·®åˆ«å¾ˆå¤§ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬è®¤ä¸ºä¹‹å‰Xå­˜åœ¨å¾ˆå¤§çš„å™ªéŸ³ï¼ŒXâ€™æ˜¯å¯¹Xå¤„ç†è¿‡åŒä¹‰è¯å’Œå¤šä¹‰è¯åŽçš„ç»“æžœã€‚
    > 
    > åœ¨æŸ¥è¯¢æ—¶ï¼Œå¯¹ä¸Žæ¯ä¸ªç»™å®šçš„æŸ¥è¯¢qï¼Œæˆ‘ä»¬æ ¹æ®è¿™ä¸ªæŸ¥è¯¢ä¸­åŒ…å«çš„å•è¯($X_q$)æž„é€ ä¸€ä¸ªä¼ªæ–‡æ¡£ï¼š$D_q=X_qTS^{-1}$ï¼Œç„¶åŽè¯¥ä¼ªæ–‡æ¡£å’Œ$D'$ä¸­çš„æ¯ä¸€è¡Œè®¡ç®—ç›¸ä¼¼åº¦(ä½™å¼¦ç›¸ä¼¼åº¦)æ¥å¾—åˆ°å’Œç»™å®šæŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚
    > 
    > [å‚è€ƒæ–‡çŒ®](http://www.cnblogs.com/kemaswill/archive/2013/04/17/3022100.html)
    > 
    > * * *
    > 
    > > ä¸‹é¢ä»‹ç»ä¸»é¢˜æ¨¡åž‹ï¼ŒPLSAï¼ŒLDAï¼›  
    > > è¿™é‡Œéœ€è¦ä»‹ç»ä¸€éƒ¨åˆ†åŸºç¡€çŸ¥è¯†ï¼šå…±è½­åˆ†å¸ƒï¼Œé¢‘çŽ‡å­¦æ´¾ï¼Œè´å¶æ–¯å­¦æ´¾ï¼›  
    > > **é¢‘çŽ‡å­¦æ´¾æ€æƒ³ï¼š** å‚æ•°æœªçŸ¥ï¼Œä½†æ˜¯å›ºå®šï¼Œå¯ä»¥é€šè¿‡æ ·æœ¬ï¼Œè®¡ç®—æœ€å¤§ä¼¼ç„¶ä¼°è®¡èŽ·å¾—ï¼›  
    > > **è´å¶æ–¯å­¦æ´¾æ€æƒ³ï¼š** å‚æ•°æœªçŸ¥ï¼Œæ˜¯ä¸ªéšæœºå˜é‡ï¼Œä½†æ˜¯æœä»ŽæŸä¸ªåˆ†å¸ƒï¼›å‚æ•°æœä»ŽæŸä¸ªå…ˆéªŒåˆ†å¸ƒï¼Œç„¶åŽæˆ‘ä»¬é€šè¿‡çŽ°æœ‰æ•°æ®ä¿®æ­£æ¨¡åž‹ï¼ŒèŽ·å¾—åŽéªŒåˆ†å¸ƒï¼›  
    > > å…ˆéªŒçŸ¥è¯†+æ•°æ®çŸ¥è¯† â€”â€”â€”>åŽéªŒåˆ†å¸ƒï¼›  
    > > **å…±è½­åˆ†å¸ƒ**ï¼šå…ˆéªŒåˆ†å¸ƒçš„å½¢å¼å’ŒåŽéªŒåˆ†å¸ƒçš„å½¢å¼ä¸€æ ·ï¼›  
    > > æ¯”å¦‚ï¼šå…ˆéªŒæ˜¯Betaåˆ†å¸ƒï¼Œæ•°æ®åˆ†å¸ƒæ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆ0-1åˆ†å¸ƒï¼‰ï¼Œé‚£ä¹ˆåŽéªŒåˆ†å¸ƒä»ç„¶æ˜¯Betaåˆ†å¸ƒï¼›  
    > > Dirichletåˆ†å¸ƒ+å¤šé¡¹å¼åˆ†å¸ƒ=Dirichletåˆ†å¸ƒ  
    > > ![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/Y7AugpF.jpg)
    > 
    > ## PLSAæ¨¡åž‹
    > 
    > > é¦–å…ˆï¼Œå›žé¡¾ä¸€å…ƒæ¨¡åž‹ï¼Œç„¶åŽå¼•å‡ºè´å¶æ–¯å­¦æ´¾çš„ä¸€å…ƒæ¨¡åž‹ï¼›  
    > > ![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/5k5FVsB.jpg)  
    > > å¦‚å›¾ç¤ºï¼š  
    > > ä¸€å…ƒæ¨¡åž‹ä¸­ï¼Œä¸å­˜åœ¨æ½œåœ¨ä¸»é¢˜ï¼Œæˆ‘ä»¬äº§ç”Ÿwordçš„è¿‡ç¨‹ï¼Œç›¸å½“äºŽæŠ•éª°å­ï¼ˆVé¢ï¼‰ï¼›é‚£ä¹ˆæ•´ä¸ªæ–‡æ¡£é›†çš„åˆ†å¸ƒæ˜¯ï¼š(æ–‡æ¡£ç›´æŽ¥ç‹¬ç«‹ï¼Œwordä¹‹é—´ç‹¬ç«‹)  
    > > 
    > > $$p(W)=\prod_d^D \prod_i^N p(w_i)=\prod_d^D \prod_v^V p(w_v)^{c_v}$$
    > > 
    > >   
    > > ç„¶åŽé€šè¿‡æœ€å¤§ä¼¼ç„¶æ–¹æ³•èŽ·å¾—å‚æ•°ï¼Œ$\hat{p(w_i)}=\frac{c_i}{C}$,$C$æ˜¯æ€»çš„é »æ•°ï¼›
    > > 
    > > * * *
    > > 
    > > æ··åˆä¸€å…ƒæ¨¡åž‹ï¼š  
    > > è¿™é‡Œï¼Œæˆ‘ä»¬å‡å®šï¼Œä¸€ç¯‡æ–‡æ¡£æœ‰ä¸€ä¸ªä¸»é¢˜zï¼Œå› æ­¤ï¼Œ  
    > > 
    > > $$p(W,z|d)=p(z|d)\prod_i^Np(w_i|z)\\ p(W|d)=\sum_z p(z|d)\prod_i^Np(w_i| z)$$
    > > 
    > > * * *
    > > 
    > > ä»¥ä¸Šé¢‘çŽ‡å­¦æ´¾æ€æƒ³ï¼ŒçŽ°åœ¨ï¼Œåˆ©ç”¨è´å¶æ–¯å­¦æ´¾æ€æƒ³ï¼Œé‡æ–°æ€è€ƒæ¨¡åž‹ï¼š  
    > > çŽ°åœ¨æœ‰ä¸€ä¸ªå›å­ï¼Œé‡Œé¢æœ‰æ— ç©·å¤šä¸ªéª°å­ï¼ˆVé¢ï¼‰;çŽ°åœ¨ï¼Œæˆ‘ä»¬é¦–å…ˆå¾—æŠ½å–ä¸€ä¸ªéª°å­ï¼Œç„¶åŽæ‰èƒ½è¿›è¡Œè®¡ç®—ï¼›æˆ‘ä»¬å‡å®šé€‰å–è¿‡ç¨‹æ˜¯æœä»ŽDirichletåˆ†å¸ƒçš„ï¼ˆå…ˆéªŒ)ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“ï¼ŒæŠ•éª°å­æ—¶ï¼ŒèŽ·å¾—wordçš„é »æ•°æ˜¯æœä»Žå¤šé¡¹å¼åˆ†å¸ƒçš„ï¼›è¿™æ ·åŽéªŒæ¦‚çŽ‡ä¹Ÿæ˜¯Dirichletåˆ†å¸ƒï¼›  
    > > è¿™é‡Œå…ˆéªŒå‚æ•°æ˜¯$\theta$,é‚£ä¹ˆ  
    > > 
    > > $$p(W,\theta)= p(\theta)p(W|\theta) \\ p(W)=\int p(\theta)p(W|\theta)d\theta = \int p(\theta)\prod p(w_i|\theta)d\theta$$
    > 
    > æˆ‘ä»¬å›žé¡¾äº†åŸºç¡€çŸ¥è¯†ï¼›çŽ°åœ¨æˆ‘ä»¬æ¥åˆ†æžä¸€ä¸‹PLSAæ¨¡åž‹ï¼Œæ¦‚çŽ‡å›¾æ¨¡åž‹å¦‚å›¾Cæ‰€ç¤ºï¼›å¯ä»¥çœ‹åˆ°ï¼Œ**æ¯ä¸€ç¯‡æ–‡æ¡£å«æœ‰å¤šä¸ªä¸»é¢˜ï¼›**ï¼›  
    > çŽ°åœ¨ï¼Œæˆ‘ä»¬ç”Ÿæˆæ–‡æ¡£çš„è¿‡ç¨‹æ˜¯ï¼šæˆ‘ä»¬æŠ•éª°å­ï¼ˆKé¢ï¼Œä»£è¡¨æ–‡æ¡£-ä¸»é¢˜æ¦‚çŽ‡ï¼‰èŽ·å¾—ä¸»é¢˜zï¼Œç„¶åŽå¯»æ‰¾åˆ°ä¸»é¢˜ä¸ºzçš„é‚£ä¸ªä¸»é¢˜-wordéª°å­ï¼Œç„¶åŽæŠ•éª°å­èŽ·å¾—word;  
    > å³ï¼š  
    > 
    > $$p(w_i|d_m)=\sum_z p(w_i|z)p(z|d_m) \\ p(W|d_m)=\prod_i^N \sum_z p(w_i|z)p(z|d_m)=\prod_i^N\sum_z \theta_{w_i,z}\phi_{d_m}$$
    > 
    > è¿™é‡Œå¯ä»¥ä½¿ç”¨EMç®—æ³•ï¼Œæœ€å¤§ä¼¼ç„¶æ–¹æ³•è¿›è¡Œæ¨¡åž‹ä¼°è®¡ï¼›
    > 
    > * * *
    > 
    > ## LDAæ¨¡åž‹
    > 
    > > PLSA æ¨¡åž‹æœ¬è´¨ä¸Šæ˜¯é¢‘çŽ‡å­¦æ´¾æ€æƒ³ï¼Œæˆ‘ä»¬çŽ°åœ¨åˆ©ç”¨è´å¶æ–¯æ€æƒ³è¿›è¡Œè€ƒè™‘ï¼›  
    > > å¼•å…¥Dirichletå…ˆéªŒï¼Œ$\alpha,\beta$æ˜¯Dirichletåˆ†å¸ƒçš„å‚æ•°ï¼›  
    > > è¿™æ ·ï¼Œå…ˆæ ¹æ®å…ˆéªŒèŽ·å¾—ä¸€ä¸ªä¸»é¢˜-æ–‡æ¡£åˆ†å¸ƒçš„å‚æ•°ï¼Œç„¶åŽä»Žå¤šé¡¹å¼åˆ†å¸ƒå¾—åˆ°ä¸€ä¸ªä¸»é¢˜ï¼Œå³$\alpha \thicksim\theta_m \thicksim z_{m,n}$ï¼›  
    > > åŒæ—¶ä»Ž$\beta$å…ˆéªŒä¸­ï¼ŒèŽ·å¾—å¤šé¡¹å¼åˆ†å¸ƒï¼Œç„¶åŽæ ¹æ®å…·ä½“ä¸»é¢˜èŽ·å¾—word,å³ï¼š$\beta \thicksim \phi_k \thicksim w_{m,n}|k=z_{m,n}$  
    > > ![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/vbhhBoO.jpg)  
    > > æ•°æ®çŸ¥è¯†ä»ç„¶æ˜¯å¤šé¡¹åˆ†å¸ƒï¼ˆè¯é¢‘ï¼‰ï¼›  
    > > è¿™æ ·çš„è¯ï¼Œå¯ä»¥å¾—åˆ°å‚æ•°çš„åŽéªŒæ¦‚çŽ‡ï¼š$Dir(\theta_m|n_m+\alpha)$  
    > > æ‰€ä»¥topicçš„åŽéªŒæ¦‚çŽ‡æ˜¯ï¼š  
    > > 
    > > $$p(Z|\alpha)=\prod_m^M \int p(z_m|\theta_m)p(\theta_m|\alpha)d_{\theta_m}\\ =\prod_m^M\int \prod_n^Np(\theta_{m,n})^{n_{z_n}} Dir(\theta_m|\alpha)d_{\theta_m} \\ =\prod_m^M \int \prod_n^Np(\theta_{m,n})^{n_{z_n}}\frac{1}{\bigtriangleup(\alpha) }\prod_n^Np(\theta_{m,n})^{\alpha-1}d_{\theta_m}\\ =\prod_m^M\frac{1}{\bigtriangleup(\alpha)}\int \prod_n^N p(\theta_{{m,n}})^{n_{n}+\alpha-1}d_{\theta_m}= \prod_m^M\frac{\bigtriangleup(n_m+\alpha)}{\bigtriangleup(\alpha)}$$
    > 
    > æ³¨æ„ï¼šn_mæ˜¯å‘é‡è¡¨ç¤ºï¼Œä»£è¡¨ä¼ªè®¡æ•°ï¼›  
    > åŒç†ï¼Œå¯ä»¥èŽ·å¾—word-topic çš„åˆ†å¸ƒçš„åŽéªŒæ¦‚çŽ‡æ˜¯$Dir(\phi_k|n_k+\beta)$,  
    > 
    > $$p(W|Z,\beta)=\prod_k^Kp(W_{(k)}|Z_{(k)},\beta)\\ =\prod_k^K \frac{\bigtriangleup(n_k+\beta)}{\bigtriangleup(\beta)}$$
    > 
    > **ç„¶åŽè®¡ç®—è”åˆæ¦‚çŽ‡ï¼š**  
    > 
    > $$p(W,Z|\alpha,\beta)=p(Z|\alpha)p(W|Z,\beta)\\ =\prod_k^K \frac{\bigtriangleup(n_k+\beta)}{\bigtriangleup(\beta)}\prod_m^M\frac{\bigtriangleup(n_m+\alpha)}{\bigtriangleup{(\alpha)}}$$
    > 
    >   
    > **ç”±äºŽWæ˜¯è§‚æµ‹å˜é‡ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥èŽ·å¾—éšå˜é‡Zçš„æ¡ä»¶æ¦‚çŽ‡ï¼›**  
    > ï¼ˆæ³¨ï¼šè¿™é‡Œå¯ä»¥ä½¿ç”¨å˜åˆ†EMæ¨¡åž‹è§£è€¦ï¼Œç„¶åŽä¼°è®¡éšå˜é‡Zçš„åˆ†å¸ƒï¼›å¦ä¸€ç§æ˜¯ä½¿ç”¨gibbs é‡‡æ ·è¿›è¡Œä¼°è®¡ï¼‰ï¼›
    > 
    > > ![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/2w2qO3B.jpg)
    > 
    > gibbs é‡‡æ ·éœ€è¦å·²çŸ¥æ¡ä»¶æ¦‚çŽ‡ï¼Œæ‰€ä»¥æˆ‘ä»¬ç»§ç»­æŽ¨å¯¼å¦‚ä¸‹ï¼š  
    > ![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/cYl8Ad4.jpg)![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/UtFeiXM.jpg)![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/3vWJVsm.jpg)![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/B7btwFh.jpg)![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://i.imgur.com/zGOR9dk.jpg)
    > 
    > å‚è€ƒæ–‡çŒ®ï¼š  
    > [http://blog.csdn.net/baimafujinji/article/details/53946367](http://blog.csdn.net/baimafujinji/article/details/53946367)  
    > [http://blog.csdn.net/pipisorry/article/details/51525308](http://blog.csdn.net/pipisorry/article/details/51525308)  
    > [http://www.cnblogs.com/pinard/p/6873703.html](http://www.cnblogs.com/pinard/p/6873703.html)  
    > LDAæ•°å­¦å…«å¦



## ç”¨ neural network åŠ é€Ÿ LDA æŽ¨è«–

- [[1508.01011] Learning from LDA using Deep Neural Networks](https://arxiv.org/abs/1508.01011)


## LDA2vec

- [[1605.02019] Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec](https://arxiv.org/abs/1605.02019)

- [Introducing our Hybrid lda2vec Algorithm | Stitch Fix Technology â€“ Multithreaded](https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&lambda=1&term=)

    ![](https://multithreaded.stitchfix.com/assets/posts/2016-05-27-lda2vec/lda2vec_network_publish_text_header.gif)

    > The final ingredient is to encourage the weights to look like a sparse Dirichlet distribution. Sampling from the [Dirichlet can get involved](http://stiglerdiet.com/blog/2015/Jul/28/dirichlet-distribution-and-dirichlet-process/), but conveniently [measuring and optimizing the likelihood](http://www.msr-waypoint.com/en-us/um/people/minka/papers/dirichlet/minka-dirichlet.pdf) is extremely simple:
    > 
    > $$\begin{equation} \Sigma_k (\alpha_k - 1) log p_k \end{equation}$$
    > 
    > (Note that weâ€™ve thrown out the terms independent of document weights. Furthermore,  $\alpha_k$ is usually a constant set to $\frac{1}{number\ of\ documents}$, and so the only variable to optimize is the document-to-topic proportion, $p_k$.)
    > 
    > This simple likelihood makes projections onto our latent topic basis sparse. Without this sparsity-inducing term the document weights tend to have evenly spread out mass which makes reading the document vectors as difficult as word vectors. Furthermore, the topic vectors that the document weights couple to are also junk when not imposing a Dirichlet likelihood. Curiously, without this term the topic vectors are poorly defined and seem to produce incoherent groups of words.

- [How to easily do Topic Modeling with LSA, PSLA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)

- [ã€ŠMixing Dirichlet Topic Models and Word Embeddings to Make lda2vecã€‹é˜…è¯»ç¬”è®°](https://zhuanlan.zhihu.com/p/23767128)

- [LDA2vec: Word Embeddings in Topic Models (article) - DataCamp](https://www.datacamp.com/community/tutorials/lda2vec-topic-model)


# Word2Vec based

## Doc2Vec

### åŽŸç†



- [æ·±åº¦å­¦ä¹ ç¬”è®°â€”â€”Word2vecå’ŒDoc2vecåŽŸç†ç†è§£å¹¶ç»“åˆä»£ç åˆ†æž - CSDNåšå®¢](https://blog.csdn.net/mpk_no1/article/details/72458003)


- [How does doc2vec represent feature vector of a document? Can anyone explain mathematically how the process is done? - Quora](https://www.quora.com/How-does-doc2vec-represent-feature-vector-of-a-document-Can-anyone-explain-mathematically-how-the-process-is-done)

    - [[1405.4053] Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)

    > Doc2Vec explores the above observation by adding additional input nodes representing documents as additional context. Each additional node can be thought of just as an id for each input document.
    > 
    > ![](https://i.imgur.com/gXTwNOU.jpg)
    > 
    > D represent the features representing the document context and W
    > 
    > represent the word context in a window surrounding the target word. Training is similar to word2vec, with additional document context. The objective of doc2vec learning is
    > 
    > maxâˆ‘âˆ€(tar,con,doc)logP
    > 
    > (target word|context words, document context)
    > 
    > At the end of the training process, you will have word embeddings, W
    > 
    > and document embedding D
    > 
    > for documents in the training corpus.
    > 
    > Great! We got document embeddings for input corpus but what about new unseen documents that appear in the test set. The idea is to learn their representation at test time by solving an optimization problem for inference.
    > 
    > ![](https://i.imgur.com/WUCU8W5.jpg)
    > 
    > The optimization problem is not any different from the training problem. maxâˆ‘âˆ€(tar,con)logP
    > 
    > (target word|context words, document = test doc). One may however, choose to keep W and Wâ€² fixed and learn variable D as document embedding.
    > 
    > The other resource is my (unfinished and unedited) notes on W2V and P2V derivations: "[Understanding Word2Vec and Paragraph2Vec](http://piyushbhardwaj.github.io/documents/w2v_p2vupdates.pdf)".
    > 

- [Distributed representations of sentences and documents | the morning paper](https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/)

    > the word vectors are asked to contribute to a prediction task about the next word in the sentence.
    > 
    > The paragraph vectors are also asked to contribute to the prediction task of the next word given many contexts sampled from the paragraph. 
    > 
    > ![](https://adriancolyer.files.wordpress.com/2016/05/paragraph-vectors-fig-2.png?w=566&zoom=2)
    > 
    > The only change compared to word vector learning is that the paragraph vector is concatenated with the word vectors to predict the next word in a context. Contexts are fixed length and sampling from a sliding window over a paragraph. Paragraph vectors are shared for all windows generated from the same paragraph, but not across paragraphs.
    > 
    > It acts as a memory that remembers what is missing from the current context â€“ or the topic of the paragraph. For this reason, we often call this model the Distributed Memory Model of Paragraph Vectors (PV-DM).
    > 
    > In summary, the algorithm itself has two key stages: 1) training to get word vectors W, softmax weights U, b and paragraph vectors D on already seen paragraphs; and 2) â€œthe inference stageâ€ to get paragraph vectors D for new paragraphs (never seen before) by adding more columns in D and gradient descending on D while holding W, U, b fixed. We use D to make a prediction about some particular labels using a standard classifier, e.g., logistic regression. 
    > 
    > A variation on the above scheme is ignore context words in the input (i.e., do away with the sliding window), and instead force the model to predict words randomly sampled from the paragraph in the output.
    > 
    > we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vectorâ€¦ We name this version the Distributed Bag of Words version of Paragraph Vector (PV-DBOW)
    > 
    > ![](https://adriancolyer.files.wordpress.com/2016/05/paragraph-vectors-fig-3.png?w=566&zoom=2)
    > 
    > PV-DM performs better than PV-DBOW, but in tests combining both PV-DM and PV-DBOW gives the best results of all

- [Word2vec å¥å‘é‡æ¨¡åž‹PV-DMä¸ŽPV-DBOWåŽŸè®ºæ–‡ç¿»è¯‘ - å¾®é›¨è½»å¯’çš„åšå®¢ - CSDNåšå®¢](https://blog.csdn.net/liaocyintl/article/details/50369158)


    > 2.2 å¥å‘é‡ï¼šä¸€ä¸ªåˆ†å¸ƒè®°å¿†æ¨¡åž‹
    > ----------------
    > 
    > åœ¨æˆ‘ä»¬çš„å¥ï¼ˆParagraphï¼‰å‘é‡æ¨¡åž‹ä¸­ï¼Œæ¯ä¸€ä¸ªå¥å­éƒ½è¢«æ˜ å°„æˆä¸€ä¸ªç‹¬ç«‹çš„å‘é‡ï¼Œè¿™ä¸ªå¥å‘é‡ä½œä¸ºçŸ©é˜µ D çš„ä¸€åˆ—ï¼›åŒæ—¶ï¼Œæ¯ä¸€ä¸ªè¯ä¹Ÿè¢«æ˜ å°„æˆä¸€ä¸ªç‹¬ç«‹çš„å‘é‡ï¼Œè¿™ä¸ªè¯å‘é‡ä½œä¸ºçŸ©é˜µ W çš„ä¸€åˆ—ã€‚å¯¹è¿™ä¸ªå¥å‘é‡å’Œè¿™äº›è¯å‘é‡æ±‚å¹³å‡æˆ–è€…é¦–å°¾ç›¸è¿žï¼Œç”¨æ¥é¢„æµ‹æ–‡æœ¬ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚åœ¨æœ¬ç ”ç©¶çš„è¯•éªŒä¸­ï¼Œæˆ‘ä»¬é€‰ç”¨é¦–å°¾ç›¸è¿žæ¥ç»„åˆè¿™äº›çŸ©é˜µã€‚
    > ä¸¥æ ¼çš„è¯´ï¼Œä¸Žå…¬å¼1ï¼ˆWord2vecçš„å…¬å¼ï¼‰ç›¸æ¯”ï¼Œå”¯ä¸€çš„ä¸åŒç‚¹åœ¨äºŽè¿™é‡Œä»Ž W å’ŒD ä¸¤ä¸ªçŸ©é˜µä¸­æž„é€  h ã€‚
    > å¥å­çš„æ ‡è¯†ï¼ˆTokenï¼‰è¢«å½“åšå¦å¤–ä¸€ä¸ª"è¯"çœ‹å¾…ã€‚å®ƒæ‰®æ¼”ä¸€ä¸ª"Memory"çš„è§’è‰²ï¼Œç”¨æ¥è®°å¿†å½“å‰æ–‡æœ¬æˆ–æ–‡ç« ä¸»é¢˜ä¸­æ¼æŽ‰äº†ä»€ä¹ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªæ¨¡åž‹ç§°ä¸º"å¥å‘é‡çš„åˆ†å¸ƒè®°å¿†æ¨¡åž‹"(PV-DM: Distributed Memory Model of Paragraph Vectors)ã€‚
    > ä¸Šä¸‹æ–‡æ˜¯å›ºå®šé•¿åº¦çš„ï¼Œä»Žå¥å­çš„ä¸€ä¸ªæ»‘åŠ¨çª—å£ä¸­å–æ ·ã€‚å¥å‘é‡è¢«é™åˆ¶åœ¨ä¸€ä¸ªå¥å­çš„æ‰€æœ‰ä¸Šä¸‹æ–‡é‡Œé¢ï¼Œä½†ä¸è¶…è¶Šå¥å­ã€‚ä½†æ˜¯è¯å‘é‡çŸ©é˜µ W æ˜¯è¶…è¶Šå¥å­çš„ã€‚æ¯”å¦‚è¯´ï¼Œ"powerful"çš„è¯å‘é‡ä¹Ÿå¯¹æ‰€æœ‰çš„å¥å­æœ‰æ•ˆã€‚
    > æˆ‘ä»¬é€šè¿‡ **éšæœºæ¢¯åº¦ä¸‹é™æ³•** æ¥è®­ç»ƒè¿™äº›å¥å‘é‡å’Œè¯å‘é‡ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­é€šè¿‡åå‘ä¼ æ’­èŽ·å¾—æ¢¯åº¦ã€‚åœ¨éšæœºæ¢¯åº¦ä¸‹é™çš„æ¯ä¸€æ­¥ï¼Œéƒ½å¯ä»¥ä»Žä¸€ä¸ªéšæœºçš„å¥å­ä¸­æŠ½å–ä¸€ä¸ªå®šé•¿çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚å›¾2ä»Žç½‘ç»œä¸­è®¡ç®—å‡ºæ¢¯åº¦è¯¯å·®ï¼Œç„¶åŽæ›´æ–°æ¨¡åž‹çš„å‚æ•°ã€‚
    > åœ¨é¢„æµ‹é˜¶æ®µï¼Œéœ€è¦æ‰§è¡Œä¸€ä¸ª"æŽ¨æ–­ï¼ˆinferenceï¼‰"æ­¥éª¤è®¡ç®—æ–°å¥å­çš„å¥å‘é‡ã€‚ä»–ä¹Ÿæ˜¯é€šè¿‡æ¢¯åº¦ä¸Šå‡æ¥èŽ·å–ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œå…¶ä½™çš„æ¨¡åž‹å‚æ•°ã€è¯å‘é‡çŸ©é˜µ W å’Œ softmax æƒé‡æ˜¯å›ºå®šçš„ã€‚
    > å‡è®¾è¯­æ–™åº“ä¸­æœ‰ N ä¸ªå¥å­ï¼Œå­—å…¸é‡Œæœ‰ M ä¸ªè¯æ±‡ï¼›æˆ‘ä»¬è¯•å›¾å°†æ¯ä¸€ä¸ªå¥å­æ˜ å°„åˆ° p ç»´ç©ºé—´ï¼Œæ¯ä¸€ä¸ªè¯æ˜ å°„åˆ° q ç»´ç©ºé—´ï¼ŒäºŽæ˜¯è¿™ä¸ªæ¨¡åž‹å°±æœ‰æ€»å…± NÃ—p+MÃ—q ä¸ªå‚æ•°ï¼ˆåŒ…æ‹¬softmaxå‚æ•°ï¼‰ã€‚å³ä½¿å¥å­çš„æ•°é‡ä¼šéšç€ N çš„å¢žå¤§è€Œå¢žå¤§ï¼Œè®­ç»ƒä¸­çš„æ›´æ–°è¿˜æ˜¯ç¨€ç–ä¸”é«˜æ•ˆã€‚
    > 
    > ![](https://img-blog.csdn.net/20151221160314787)
    > å›¾2ï¼šè¿™æ˜¯ä¸€ä¸ªå¥å‘é‡æ¡†æž¶ã€‚è¿™ä¸ªæ¡†æž¶ç±»ä¼¼äºŽå›¾1ä¸­çš„æ¡†æž¶ã€‚å”¯ä¸€ä¸åŒç‚¹åœ¨äºŽå¢žåŠ äº†ä¸€ä¸ªå¥å­æ ‡è¯†ï¼ˆTokenï¼‰ï¼Œè¿™ä¸ªæ ‡è¯†è¢«æ˜ å°„åˆ°çŸ©é˜µ D çš„ä¸€ä¸ªå‘é‡é‡Œã€‚åœ¨è¿™ä¸ªæ¨¡åž‹é‡Œï¼Œé€šè¿‡å¯¹ä¸Šä¸‹æ–‡ä¸‰ä¸ªè¯å‘é‡çš„é¦–å°¾ç›¸æŽ¥æˆ–æ±‚å‡å€¼ï¼Œæ¥é¢„æµ‹ç¬¬å››ä¸ªè¯ã€‚è¿™ä¸ªå¥å‘é‡è¡¨ç¤ºä»Žå½“å‰ä¸Šä¸‹æ–‡è€Œæ¥çš„ç¼ºå¤±çš„ä¿¡æ¯ï¼Œè¢«å½“åšä¸€ä¸ªå…³äºŽå¥å­ä¸»é¢˜çš„å­˜å‚¨å™¨ã€‚
    > 
    > ç»è¿‡è®­ç»ƒï¼Œè¿™äº›å¥å‘é‡å°±å¯ä»¥å½“åšå¥å­çš„ç‰¹å¾ä½¿ç”¨ã€‚æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›ç‰¹å¾ç›´æŽ¥ç”¨äºŽä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæ¯”å¦‚é€»è¾‘å›žå½’ã€æ”¯æŒå‘é‡æœºæˆ–è€…K-meansèšç±»ã€‚
    > æ€»è€Œè¨€ä¹‹ï¼Œè¿™ä¸ªç®—æ³•æœ‰ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼š1ï¼‰é€šè¿‡è®­ç»ƒèŽ·å¾—è¯å‘é‡çŸ©é˜µ W, softmaxæƒé‡ U, b ä»¥åŠ å¥å‘é‡ D ä»Žå·²çŸ¥çš„å¥å­é‡Œï¼›2ï¼‰ç¬¬äºŒä¸ªé˜¶æ®µæ˜¯æŽ¨æ–­é˜¶æ®µï¼Œç”¨äºŽå–å¾—ä¸€ä¸ªæ–°å¥å­ï¼ˆæ²¡æœ‰å‡ºçŽ°è¿‡ï¼‰çš„å¥å‘é‡ Dï¼Œé€šè¿‡å¢žåŠ æ›´å¤šçš„åˆ—åœ¨çŸ©é˜µ D é‡Œï¼Œå¹¶ä¿æŒ W, U, b ä¸å˜çš„æƒ…å†µä¸‹åœ¨çŸ©é˜µ D ä¸Šè¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬ä½¿ç”¨ D é€šè¿‡ä¸€ä¸ªåŸºç¡€çš„åˆ†ç±»å™¨ç»™å¥å­åŠ ä¸Šæ ‡ç­¾ã€‚
    > **å¥å‘é‡çš„ä¼˜ç‚¹ï¼š** å¥å‘é‡çš„ä¸€ä¸ªé‡è¦çš„ä¼˜ç‚¹åœ¨äºŽï¼Œå®ƒçš„è®­ç»ƒé›†æ˜¯æ²¡æœ‰è¢«åŠ ä¸Šæ ‡ç­¾çš„æ•°æ®ï¼Œå› æ­¤å®ƒå¯ä»¥è¢«ç”¨äºŽä¸€äº›è®­ç»ƒæ ·æœ¬æ ‡ç­¾ä¸è¶³çš„ä»»åŠ¡ã€‚
    > å¥å‘é‡ä¹Ÿè§£å†³äº†è¯è¢‹æ¨¡åž‹çš„ä¸€äº›å…³é”®çš„å¼±ç‚¹ã€‚ç¬¬ä¸€ï¼Œå®ƒä¼ æ‰¿äº†è¯å‘é‡çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§------è¯å’Œè¯ä¹‹é—´çš„è¯­ä¹‰ã€‚åœ¨è¯­ä¹‰é‡Œï¼Œ"å¼ºæœ‰åŠ›"æ¯”èµ·"å·´é»Ž"æ¥è¯´ï¼Œå’Œ"å¼ºå£®"æ›´æŽ¥è¿‘ã€‚å¥å‘é‡çš„ç¬¬äºŒä¸ªä¼˜ç‚¹åœ¨äºŽå®ƒè€ƒè™‘åˆ°äº†"è¯åºï¼ˆword orderï¼‰"ï¼Œn-gramæ¨¡åž‹åˆ™éœ€è¦è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„næ‰èƒ½åšåˆ°ã€‚è¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå› ä¸ºn-gramæ¨¡åž‹ä¿å­˜äº†å¥å­ä¸­å¤§é‡çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯åºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡åž‹ä¼˜äºŽè¯è¢‹n-gramæ¨¡åž‹å› ä¸ºåŽè€…ä¼šè¡¨çŽ°å‡ºä¸€ä¸ªæžé«˜çš„ç»´åº¦ï¼Œè¿™ä¼šå½±å“æ•ˆçŽ‡ã€‚
    > 
    > 2.3 æ— è¯åºå¥å‘é‡ï¼šåˆ†å¸ƒè¯è¢‹æ¨¡åž‹
    > -----------------
    > 
    > ä¸Šé¢çš„æ–¹æ³•è®¨è®ºäº†åœ¨ä¸€ä¸ªæ–‡æœ¬çª—å£å†…ï¼Œé€šè¿‡å¥å‘é‡å’Œè¯å‘é‡çš„é¦–å°¾ç›¸æŽ¥æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚å¦ä¸€ç§æ–¹æ³•ä¸æŠŠä¸Šä¸‹æ–‡ä¸­çš„è¯ä½œä¸ºè¾“å…¥ï¼Œè€Œæ˜¯å¼ºåˆ¶è¿™ä¸ªæ¨¡åž‹åœ¨è¾“å‡ºä¸­ä»Žå¥å­ä¸­éšæœºæŠ½å–è¯æ±‡æ¥è¿›è¡Œé¢„æµ‹ã€‚å®žé™…ä¸Šï¼Œå…¶æ„ä¹‰åœ¨äºŽåœ¨æ¯ä¸€ä¸ªéšæœºæ¢¯åº¦ä¸‹é™çš„å¾ªçŽ¯ä¸­ï¼Œæˆ‘ä»¬æŠ½å–ä¸€ä¸ªæ–‡æœ¬çª—å£ï¼Œç„¶åŽä»Žè¿™ä¸ªæ–‡æœ¬çª—å£ä¸­æŠ½å–ä¸€ä¸ªè¯ï¼Œç„¶åŽé€šè¿‡ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡å¾—åˆ°å¥å‘é‡ã€‚è¿™é¡¹æŠ€æœ¯å¦‚å›¾3æ‰€ç¤ºã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªç‰ˆæœ¬ç§°ä¸ºå¥å‘é‡çš„åˆ†å¸ƒè¯è¢‹ï¼ˆPV-DBOW: Distributed Bag of Words version of Paragraph Vectorï¼‰ç‰ˆæœ¬ï¼Œç›¸æ¯”äºŽä¸Šä¸€èŠ‚æåˆ°çš„PV-DMç‰ˆæœ¬ã€‚
    > 
    > ![](https://img-blog.csdn.net/20151221174010394)\
    > å›¾3ï¼šå¥å‘é‡çš„åˆ†å¸ƒè¯è¢‹ç‰ˆæœ¬ã€‚åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œå¥å‘é‡è¢«è®­ç»ƒå‡ºæ¥ï¼Œç”¨æ¥é¢„æµ‹åœ¨ä¸€ä¸ªå°çª—å£ä¸­çš„è¯æ±‡ã€‚
    > 
    > é™¤äº†åœ¨æ¦‚å¿µä¸Šç®€å•ä»¥å¤–ï¼Œè¿™ä¸ªæ¨¡åž‹åªéœ€è¦å­˜å‚¨å°‘é‡çš„æ•°æ®ã€‚ç›¸æ¯”äºŽä¸Šä¸€ä¸ªæ¨¡åž‹éœ€è¦å­˜å‚¨softmaxæƒé‡å’Œè¯å‘é‡ï¼Œè¿™ä¸ªæ¨¡åž‹åªéœ€è¦å­˜å‚¨softmaxæƒé‡ã€‚åŒæ ·çš„ï¼Œè¿™ä¸ªæ¨¡åž‹ä¹Ÿè¿‘ä¼¼äºŽSkip-gramæ¨¡åž‹ã€‚\
    > åœ¨æˆ‘ä»¬çš„è¯•éªŒä¸­ï¼Œæ¯ä¸€ä¸ªå¥å‘é‡éƒ½æ˜¯ä¸¤ä¸ªå‘é‡çš„ç»„åˆï¼šä¸€ä¸ªé€šè¿‡PV-DMè®­ç»ƒï¼Œå¦ä¸€ä¸ªé€šè¿‡PV-DBOWè®­ç»ƒã€‚PV-DMèƒ½å¤Ÿå¾ˆå¥½åœ°æ‰§è¡Œå¤šç§ä»»åŠ¡ï¼Œä½†æ˜¯å®ƒç»“åˆPV-DBOWåŽï¼Œå¸¸å¸¸èƒ½å¤Ÿæ›´åŠ å‡ºè‰²å®Œæˆä»»åŠ¡ï¼Œæ­¤æˆ‘ä»¬å¼ºçƒˆæŽ¨èè¿™ç§åšæ³•ã€‚


### å¯¦ä½œ

- [åŸºäºŽgensimçš„Doc2Vecç®€æž - CSDNåšå®¢](https://blog.csdn.net/lenbow/article/details/52120230)

- [åŸºäºŽjiebaå’Œdoc2vecçš„ä¸­æ–‡æƒ…æ„Ÿè¯­æ–™åˆ†ç±» - ä¸ªäººæ–‡ç«  - SegmentFault æ€å¦](https://segmentfault.com/a/1190000012203525)

- [ç”¨ Doc2Vec å¾—åˆ°æ–‡æ¡£ï¼æ®µè½ï¼å¥å­çš„å‘é‡è¡¨è¾¾ - ç®€ä¹¦](https://www.jianshu.com/p/854a59b93e09)



- [jhlau/doc2vec: Python scripts for training/testing paragraph vectors](https://github.com/jhlau/doc2vec)

- [RaRe-Technologies/gensim: Topic Modelling for Humans](https://github.com/RaRe-Technologies/gensim)
    - [gensim/doc2vec-lee.ipynb at develop Â· RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)
    - [gensim/doc2vec-IMDB.ipynb at develop Â· RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)
    - [gensim/doc2vec-wikipedia.ipynb at develop Â· RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb)

- [abtpst/Doc2Vec: Doc2Vec algorithm for solving moview review sentiment analysis](https://github.com/abtpst/Doc2Vec)

- [hiyijian/doc2vec: C++ implement of Tomas Mikolov's word/document embedding](https://github.com/hiyijian/doc2vec)

- [fbkarsdorp/doc2vec: Tutorial and review of word2vec / doc2vec](https://github.com/fbkarsdorp/doc2vec)

- [ibrahimsharaf/doc2vec: Text classification using Doc2Vec & Random Forest](https://github.com/ibrahimsharaf/doc2vec)

- [Doc2vec tutorial | RARE Technologies](https://rare-technologies.com/doc2vec-tutorial/)




- [æƒ…æ„Ÿåˆ†æžçš„æ–°æ–¹æ³•â€”â€”åŸºäºŽWord2Vec/Doc2Vec/Python - OPEN å¼€å‘ç»éªŒåº“](http://www.open-open.com/lib/view/open1444351655682.html)

- [doc2vecè®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦ - ç¨‹åºå›­](http://www.voidcn.com/article/p-fiwnobtj-dx.html)

- [Doc2Vec tutorial using Gensim â€“ Andreas Klintberg â€“ Medium](https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1)


- [jhlau/doc2vec: Python scripts for training/testing paragraph vectors](https://github.com/jhlau/doc2vec)


- [TensorFlow-Machine-Learning-Cookbook/doc2vec.py at master Â· PacktPublishing/TensorFlow-Machine-Learning-Cookbook](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py#L112:1)


- [Coding doc2vec - Luminis Amsterdam](https://amsterdam.luminis.eu/2017/02/21/coding-doc2vec/)

    - [blog-doc2vec/pvdbow.ipynb at master Â· luminis-ams/blog-doc2vec](https://github.com/luminis-ams/blog-doc2vec/blob/master/pvdbow.ipynb)

    > Training the network
    > --------------------
    > 
    > In Tensorflow, training a network is done in two steps. First, you define the model. You can think of the model as a graph. Second, you run the model. We'll take a look at the first step, how our model is defined. First: the input.
    > 
    > 
    > ```python=
    > # Input data
    > 
    > dataset = tf.placeholder(tf.int32, shape=[BATCH_SIZE])
    > 
    > labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])
    > ```
    >  
    > 
    > The `dataset` is defined as a placeholder with the shape of a simple array that contains ints. When we run the model, it will contain document ids. Nothing more, nothing less. It will contain as many document ids as we want to feed the stochastic gradient descent algorithm in a single iteration. The `labels` placeholder is also a vector. It will contain integers that represent words from the vocabulary. So, basically, for each document id we want to predict a word that occurs in it. In our implementation, we make sure that a batch contains one or more text windows. So if we use a text window of size 8, a batch will contain one or more sequences of eight consecutive words. Next, we take a look at the weights in our neural network:
    > 
    > 
    > ```python=
    > # Weights
    > 
    > embeddings = tf.Variable(
    > 
    >     tf.random_uniform([len(doclens), EMBEDDING_SIZE],
    >                     -1.0, 1.0))
    > 
    > softmax_weights = tf.Variable(
    >     tf.truncated_normal(
    >             [vocab_size, EMBEDDING_SIZE],
    >             stddev=1.0 / np.sqrt(EMBEDDING_SIZE)))
    > 
    > softmax_biases = tf.Variable(tf.zeros([vocab_size]))
    > ```
    > 
    > 
    > You can think of `embeddings` as the transpose of the matrix ![D](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-4b9ef1bbd23fd1b198de883813285620_l3.svg "Rendered by QuickLaTeX.com") from our [previous post](https://amsterdam.luminis.eu/2017/01/30/implementing-doc2vec/) [2]. In its rows, it has a document vector of length `EMBEDDING_SIZE` for each document id. This document vector is also called an "input vector". You can also think of `embeddings` as the weights between the input layer and the middle layer of our small doc2vec network. When we run the session, we will initialize the `embeddings` variable with random weights between ![-1.0](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-ee8fa04fc2bf1dbbff787fc33f469ffe_l3.svg "Rendered by QuickLaTeX.com") and ![1.0](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-2b23fc34a5447966d71dba505f0d8e9c_l3.svg "Rendered by QuickLaTeX.com").
    > 
    > The `softmax_weights` are the weights between the middle layer and the output layer of our network. You can also think of them as the matrix ![U](https://amsterdam.luminis.eu/wp-content/ql-cache/quicklatex.com-2b60fc262803f27ba3717d8ec4eb656d_l3.svg "Rendered by QuickLaTeX.com") from our previous post. On its rows, it has an "output vector" of length `EMBEDDING_SIZE` for each word in the vocabulary. When we run the model in our session, we will initialize these weights with (truncated) normally distributed random variables with mean zero and a standard deviation that is inversely proportional to `EMBEDDING_SIZE`. Why are these variables initialized using a normal distribution, instead of with a uniform distribution like we used for the embeddings? The short answer is: because this way of initialisation has apparently worked well in the past. You can try different initialisation schemes yourself, and see what it does to your end-to-end performance. The long answer; well, perhaps that's food for another blog post.
    > 
    > The `softmax_biases` are initialised here with zeroes. In our previous post, we mentioned that softmax biases are often used, but omitted them in our final loss function. Here, we used them, because the word2vec implementation we based this notebook on used them. And the function we use for negative sampling wants them, too.
    > 
    > The activation in the middle layer, or, alternatively, the estimated document vector for a document id is given by `embed`:
    > 
    > ```python=
    > embed = tf.nn.embedding_lookup(embeddings, dataset)
    > ```
    > 
    > `tf.nn.embedding_lookup` will provide us with fast lookup of a document vector for a given document id.
    > 
    > Finally, we are ready to compute the loss function that we'll minimise:
    > 
    > ```python=
    > 
    > loss = tf.reduce_mean(
    >         tf.nn.sampled_softmax_loss(
    >                 softmax_weights, softmax_biases, embed,
    >                 labels, NUM_SAMPLED, vocab_size))
    > 
    > ```
    > 
    > Here, `tf.nn.sampled_softmax_loss` takes care of negative sampling for us. `tf.reduce_mean` will compute the average loss over all the training examples in our batch.
    > 
    > As an aside, if you take a look at the complete source code in the notebook, you'll notice that we also have a function `test_loss`. That function does not use negative sampling. It should not, because negative sampling underestimates the true loss of the network. It is only used because it is faster to compute than the real loss. When you run the notebook, you will see that the training losses it prints are always lower than the test losses. One other remark about the test loss is the following: the test examples are taken from the same set of documents as the training examples! This is because in our network, we have no input to represent a document that we have never seen before. The text windows that have to be predicted for a given document id are different though, in the test set.
    > 
    > ---
    > 
    > Q: i want to change your code for my dataset which is a numpy lists of a list of documents. Am i suppose to give them each a file id or divide them by training and test label?
    > 
    > ---
    > 
    > A: There are a bunch of things you can do. One of them is include your test set in the training set while fitting this doc2vec model. That would not be cheating if you are aware of the locations of the points in your test set at the time you want to classify them (as you do not use the class labels while training doc2vec). This kind of learning task is sometimes called semi-supervised learning, or transductive learning. The doc2vec embeddings are then your feature vectors. Once you have them, for the document classification task (I assume you are doing), you divide your data in training and test sets as usual.
    > 


## Doc2Vec with tags

- [A gentle introduction to Doc2Vec â€“ ScaleAbout â€“ Medium](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)

    > if we have tags for our documents (as we actually have), we can add them, and get their representation as vectors.
    > 
    > Additionally, they don't have to be unique. This way, we can add to the unique document tag one of our 17 tags, and create a doc2vec representation for them as well! see below:
    > 
    > ![](https://cdn-images-1.medium.com/max/960/1*YfOv1_8tmTiahgbpEt5LCw.png)
    > 
    > fig 5â€Š---â€Šdoc2vec model with tag vector
    > 
    > we will use **gensim** [**implementation**](https://rare-technologies.com/doc2vec-tutorial/)of **doc2vec.** here is how the gensim TaggedDocument object looks like:
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*As22mK8YKolvVFCGmHvGxw.png)
    > 
    > gensim TaggedDocument object. SENT_3 is the unique document id, remodeling and renovating is the tag
    > 
    > Using **gensim** doc2vec is very straight-forward. As always, model should be initialized, trained for a few epochs:
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*dDwfUMiLfNFUju19ruUyyw.png)
    > 
    > and then we can check the similarity of every unique **document** to every **tag**, this way:
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*T9swFeb7vqTOWKA9NNnIDA.png)
    > 
    > The tag with highest similarity to document will be predicted.



# RNN-based model

## TopicRNN

- [TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependencyé˜…è¯»ç¬”è®°](https://zhuanlan.zhihu.com/p/27151433)

    - [[1611.01702] TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency](https://arxiv.org/abs/1611.01702)

    > ![](https://pic1.zhimg.com/v2-8876dcef7d93048672d0b47f94cea481_r.jpg)
    > 
    > ![](https://pic1.zhimg.com/v2-994789251b56dad24d19e292b89de36c_r.jpg)




## TreeRNN(TreeLSTM)

- [[1503.00075] Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](https://arxiv.org/abs/1503.00075)


- [åŸºäºŽTreeLSTMçš„æƒ…æ„Ÿåˆ†æž](https://zhuanlan.zhihu.com/p/35252733)


- [machine learning - How can a tree be encoded as input to a neural network? - Stack Overflow](https://stackoverflow.com/questions/26022866/how-can-a-tree-be-encoded-as-input-to-a-neural-network)

    > You need a **recursive neural network**. Please see this repository for an example implementation: <https://github.com/erickrf/treernn>
    > 
    > The principle of a recursive (not recurrent) neural network is shown in this picture.
    > 
    > It learns representation of each leaf, and then goes up through the parents to finally construct the representation of the whole structure. [![enter image description here](https://i.stack.imgur.com/1BW1F.png)](https://i.stack.imgur.com/1BW1F.png)
    > 
    > ---
    > 
    > Encode each leaf node using (i) the sequence of nodes that connects it to the root node and (ii) the encoding of the leaf node that comes before it.
    > 
    > For (i), use a recurrent network whose input is tags. Feed this RNN the root tag, the second level tag, ..., and finally the parent tag (or their embeddings). Combine this with the leaf itself (the word or its embedding). Now, you have a feature that describes the leaf and its ancestors.
    > 
    > For (ii), also use a recurrent network! Simply start by computing the feature described above for the left most leaf and feed it to a second RNN. Keep doing this for each leaf moving from left to right. At each step, the second RNN will give you a vector that represents the current leaf with its ancestors, the leaves that come before it and their ancestors.
    > 
    > Optionally, do (ii) bi-directionally and you will get a leaf feature that incorporates the whole tree!


## Recursive RNN èˆ‡ Recurrent RNN æ¯”è¼ƒ

- [åœ¨NLPä¸­æ·±åº¦å­¦ä¹ æ¨¡åž‹ä½•æ—¶éœ€è¦æ ‘å½¢ç»“æž„ï¼Ÿ - äººå·¥æ™ºèƒ½ - æŽ˜é‡‘](https://juejin.im/entry/5ad838805188252ea02c0bb5)

    - [[1503.00185] When Are Tree Structures Necessary for Deep Learning of Representations?](https://arxiv.org/abs/1503.00185)

    > è¯¥æ–‡åœ¨NLPé¢†åŸŸä¸­4ç§ç±»åž‹5ä¸ªä»»åŠ¡è¿›è¡Œäº†å®žéªŒï¼Œå…·ä½“çš„å®žéªŒæ•°æ®å¤§å®¶å¯ä»¥ä»Žè®ºæ–‡ä¸­æŸ¥é˜…ï¼Œè¿™é‡Œæˆ‘ä¸»è¦åˆ†æžä¸€ä¸‹æ¯ä¸ªä»»åŠ¡çš„ç‰¹ç‚¹ï¼Œä»¥åŠæœ€åŽå®žéªŒçš„ç»“æžœï¼š
    > 
    > - Sentiment Classification on the Stanford Sentiment Treebank
    > 
    >     è¿™æ˜¯ä¸€ä¸ªç»†ç²’åº¦çš„æƒ…æ„Ÿåˆ†ç±»é—®é¢˜ï¼Œæ ¹æ®Stanfordçš„å¥æ³•æ ‘åº“ï¼Œåœ¨æ¯ä¸€ä¸ªèŠ‚ç‚¹ä¸Šéƒ½æ ‡æ³¨äº†æƒ…æ„Ÿç±»åž‹ï¼Œæ‰€ä»¥å®žéªŒåˆ†ä¸ºäº†å¥å­çº§åˆ«å’ŒçŸ­è¯­çº§åˆ«ï¼Œä»Žç»“æžœæ¥çœ‹ï¼Œæ ‘å½¢ç»“æž„å¯¹äºŽå¥å­çº§åˆ«æœ‰ç‚¹å¸®åŠ©ï¼Œå¯¹äºŽçŸ­è¯­çº§åˆ«å¹¶æ²¡ä»€ä¹ˆä½œç”¨ã€‚
    > 
    > - Binary Sentiment Classification
    > 
    >     è¿™åŒæ ·æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»é—®é¢˜ï¼Œä¸Žä¸Šé¢ä¸åŒçš„æ˜¯ï¼Œå®ƒåªæœ‰äºŒå…ƒåˆ†ç±»ï¼Œå¹¶ä¸”åªæœ‰åœ¨å¥å­çº§åˆ«ä¸Šè¿›è¡Œäº†æ ‡æ³¨ï¼Œä¸”æ¯ä¸ªå¥å­éƒ½æ¯”è¾ƒé•¿ã€‚å®žéªŒç»“æžœæ˜¯æ ‘å½¢ç»“æž„å¹¶æ²¡æœ‰èµ·åˆ°ä»€ä¹ˆä½œç”¨ï¼Œå¯èƒ½åŽŸå› æ˜¯å¥å­è¾ƒé•¿ï¼Œè€Œä¸”å¹¶æ²¡æœ‰ä¸°å¯Œçš„çŸ­è¯­çº§åˆ«æ ‡æ³¨ï¼Œå¯¼è‡´åœ¨é•¿è·ç¦»çš„å­¦ä¹ ä¸­ä¸¢å¤±äº†å­¦ä¹ åˆ°çš„æƒ…æ„Ÿä¿¡æ¯ã€‚
    > 
    > - Question-Answer Matching
    > 
    >     è¿™ä¸ªä»»åŠ¡æ˜¯æœºæ™ºé—®ç­”ï¼Œå°±æ˜¯ç»™å‡ºä¸€æ®µæè¿°ä¸€èˆ¬ç”±4~6å¥ç»„æˆï¼Œç„¶åŽæ ¹æ®æè¿°ç»™å‡ºä¸€ä¸ªçŸ­è¯­çº§åˆ«çš„ç­”æ¡ˆï¼Œä¾‹å¦‚åœ°åï¼Œäººåç­‰ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šï¼Œæ ‘å½¢ç»“æž„ä¹Ÿæ²¡æœ‰å‘æŒ¥ä½œç”¨ã€‚
    >     
    > - Semantic Relation Classification
    > 
    >     è¿™ä¸ªä»»åŠ¡æ˜¯ç»™å‡ºä¸¤ä¸ªå¥å­ä¸­çš„åè¯ï¼Œç„¶åŽåˆ¤æ–­è¿™ä¸¤ä¸ªåè¯æ˜¯ä»€ä¹ˆè¯­ä¹‰å…³ç³»ã€‚æ ‘å½¢ç»“æž„çš„æ–¹æ³•åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šæœ‰æ˜Žæ˜¾çš„æå‡ã€‚
    >     
    > - Discourse Parsing
    > 
    >     æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œç‰¹ç‚¹æ˜¯å…¶è¾“å…¥çš„å•å…ƒå¾ˆçŸ­ï¼Œæ ‘å½¢ç»“æž„ä¹Ÿæ²¡æœ‰ä»€ä¹ˆæ•ˆæžœã€‚
    > 
    > ## ç»“è®º
    > 
    > é€šè¿‡ä¸Šé¢çš„å®žéªŒï¼Œä½œè€…æ€»ç»“å‡ºä¸‹é¢çš„ç»“è®ºã€‚
    > 
    > éœ€è¦æ ‘å½¢ç»“æž„ï¼š
    > 
    > 1. éœ€è¦é•¿è·ç¦»çš„è¯­ä¹‰ä¾å­˜ä¿¡æ¯çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ä¸Šé¢çš„è¯­ä¹‰å…³ç³»åˆ†ç±»ä»»åŠ¡ï¼‰Semantic relation extraction
    > 2. è¾“å…¥ä¸ºé•¿åºåˆ—ï¼Œå³å¤æ‚ä»»åŠ¡ï¼Œä¸”åœ¨ç‰‡æ®µæœ‰è¶³å¤Ÿçš„æ ‡æ³¨ä¿¡æ¯çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚å¥å­çº§åˆ«çš„Stanfordæƒ…æ„Ÿæ ‘åº“åˆ†ç±»ä»»åŠ¡ï¼‰ï¼Œæ­¤å¤–ï¼Œå®žéªŒä¸­ä½œè€…è¿˜å°†è¿™ä¸ªä»»åŠ¡å…ˆé€šè¿‡æ ‡ç‚¹ç¬¦å·è¿›è¡Œäº†åˆ‡åˆ†ï¼Œæ¯ä¸ªå­ç‰‡æ®µä½¿ç”¨ä¸€ä¸ªåŒå‘çš„åºåˆ—æ¨¡åž‹ï¼Œç„¶åŽæ€»çš„å†ä½¿ç”¨ä¸€ä¸ªå•å‘çš„åºåˆ—æ¨¡åž‹å¾—åˆ°çš„ç»“æžœæ¯”æ ‘å½¢ç»“æž„çš„æ•ˆæžœæ›´å¥½ä¸€äº›ã€‚
    > 
    > ä¸éœ€è¦æ ‘å½¢ç»“æž„ï¼š
    > 
    > 1. é•¿åºåˆ—å¹¶ä¸”æ²¡æœ‰è¶³å¤Ÿçš„ç‰‡æ®µæ ‡æ³¨ä»»åŠ¡ï¼ˆä¾‹å¦‚ä¸Šé¢çš„äºŒå…ƒæƒ…æ„Ÿåˆ†ç±»ï¼ŒQ-A Matchingä»»åŠ¡ï¼‰
    > 2. ç®€å•ä»»åŠ¡ï¼ˆä¾‹å¦‚çŸ­è¯­çº§åˆ«çš„æƒ…æ„Ÿåˆ†ç±»å’ŒDiscourseåˆ†æžä»»åŠ¡ï¼‰ï¼Œæ¯ä¸ªè¾“å…¥ç‰‡æ®µéƒ½å¾ˆçŸ­ï¼Œå¥æ³•åˆ†æžå¯èƒ½æ²¡æœ‰æ”¹å˜è¾“å…¥çš„é¡ºåºã€‚
    > 
    > æ­¤å¤–ï¼Œå“ˆå·¥å¤§çš„è½¦ä¸‡ç¿”åœ¨å“ˆå·¥å¤§çš„å¾®ä¿¡å…¬ä¼—å·ä¹Ÿå‘è¡¨äº†ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡åž‹æ˜¯å¦ä¾èµ–äºŽæ ‘ç»“æž„ï¼Ÿã€‹[3]ï¼Œå…¶ä¸­æåˆ°äº†"å³ä½¿é¢å¯¹çš„æ˜¯å¤æ‚é—®é¢˜ï¼Œåªè¦æˆ‘ä»¬èƒ½å¤ŸèŽ·å¾—è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®"ä¹Ÿå¯ä»¥æ— éœ€æ ‘å½¢ç»“æž„ã€‚
    > 
    > é€šè¿‡è¿™ç¯‡è®ºæ–‡å’Œè½¦è€å¸ˆçš„åšæ–‡ä»¥åŠä¸€äº›ç›¸å…³èµ„æ–™ï¼Œå¥æ³•æ ‘å½¢ç»“æž„æ˜¯å¦éœ€è¦å€¼å¾—æˆ‘ä»¬å…³æ³¨ï¼Œæˆ‘ä»¬åº”è¯¥æ ¹æ®è‡ªå·±åšçš„ä»»åŠ¡ä»¥åŠå¥æ³•åˆ†æžçš„ä¼˜ç¼ºç‚¹è¿›è¡Œåˆ¤æ–­ï¼Œæˆ‘è‡ªå·±æ€»ç»“å¦‚ä¸‹ï¼š
    > 
    > å¥æ³•åˆ†æžèƒ½å¤Ÿå¸¦ç»™æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ
    > 
    > - é•¿è·ç¦»çš„è¯­ä¹‰ä¾èµ–å…³ç³»
    > - åŒ…å«è¯­è¨€å­¦çŸ¥è¯†çš„åºåˆ—ç‰‡æ®µ
    > - ç®€åŒ–å¤æ‚å¥å­æå–æ ¸å¿ƒ
    > 
    > å¥æ³•åˆ†æžçš„ç¼ºç‚¹
    > 
    > - è‡ªèº«åˆ†æžå­˜åœ¨é”™è¯¯ï¼Œå¼•å…¥å™ªå£°
    > - ç®€å•ä»»åŠ¡å¤æ‚åŒ–
    > - å¥æ³•åˆ†æžæ—¶é—´é•¿
    > 

- [cs224n-2018-lecture14-TreeRNNs - lecture14.pdf](http://web.stanford.edu/class/cs224n/lectures/lecture14.pdf)

    > ![](https://screenshotscdn.firefoxusercontent.com/images/3cae62c7-0187-430c-ace7-3d06d7916c32.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/a4c3699b-8329-41be-bdd5-435672db5d05.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/40460542-2e9f-43c1-aa64-cd2cb9c405f4.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/09f57e08-4475-4a29-8444-f47472718a88.png)
    > 
    > ![](https://screenshotscdn.firefoxusercontent.com/images/612baa6f-7b85-49f5-ae4e-1b3d2ea6364f.png)
    > 




