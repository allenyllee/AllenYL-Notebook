# 深度學習理論

<!-- toc --> 
[toc]

## Back Propergation

- [derivative - Backpropagation with Softmax / Cross Entropy - Cross Validated](https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy)

    The cross entropy error function is

    $$
    E=-\sum_jt_j\log o_j
    $$

    with $t$   and $o$   as the target and output at neuron $j$  , respectively. The sum is over each neuron in the output layer. $o_j$    itself is the result of the softmax function:

    $$
    o_j=softmax(z_j)=\frac{e^{z_j}}{\sum_j e^{z_j}}
    $$

    Again, the sum is over each neuron in the output layer and $z_j$ is the input to neuron $j$:

    $$
    \begin{aligned}
    z_j=\sum_i w_{ij} y_i+b_j
    \end{aligned}
    $$

    $$
    \begin{aligned}
    z_j=\sum_i w_{ij}\log o_j
    \end{aligned}
    $$

    That is the sum over all neurons in the previous layer with their corresponding output $o_i$ and weight $w_ij$ towards neuron $j$ plus a bias $b$.

    Finally, to get the gradient of $E$ with respect to the weight-matrix $w$, giving the final expression (assuming a one-hot $t$  , i.e. $τ=1$  )

    $$
    \frac{\partial E}{\partial w_{ij}}=y_i(o_j-t_j)
    $$

    where $y$ is the input on the lowest level (of your example).


- [machine learning - Cross entropy function (python) - Stack Overflow](https://stackoverflow.com/questions/47377222/cross-entropy-function-python)

    > ```python
    > def cross_entropy(predictions, targets, epsilon=1e-12):
    >     """
    >     Computes cross entropy between targets (encoded as one-hot vectors)
    >     and predictions. 
    >     Input: predictions (N, k) ndarray
    >            targets (N, k) ndarray        
    >     Returns: scalar
    >     """
    >     predictions = np.clip(predictions, epsilon, 1. - epsilon)
    >     N = predictions.shape[0]
    >     ce = -np.sum(np.sum(targets*np.log(predictions+1e-9)))/N
    >     return ce
    > 
    > predictions = np.array([[0.25,0.25,0.25,0.25],
    >                         [0.01,0.01,0.01,0.96]])
    > targets = np.array([[0,0,0,1],
    >                    [0,0,0,1]])
    > ans = 0.71355817782  #Correct answer
    > x = cross_entropy(predictions, targets)
    > print(np.isclose(x,ans))
    > ```
    > 
    > Here, I think it's a little clearer if you stick with np.sum(). Also, I added 1e-9 into the np.log() to avoid the possibility of having a log(0) in your computation. Hope this helps!



## CNN 

### State of the art model

- MNIST, CIFAR-10, CIFAR-100, STL-10, SVHN, ILSVRC2012 task 1 [Classification datasets results](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)


### Paper

- AlexNet [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
- VGGNet [[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
- GoogLeNet [[1409.4842] Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)
- ResNet [[1512.03385] Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)


### Course

#### CS231n
- Notes
    - [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)
    - [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/)
- Slides
    - [Syllabus | CS 231N](http://cs231n.stanford.edu/syllabus.html)
- Lecture Video
    - [Lecture Collection | Convolutional Neural Networks for Visual Recognition (Spring 2017) - YouTube - YouTube](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)


### Articles
- [Exploring Deep Learning & CNNs - RSIP Vision](https://www.rsipvision.com/exploring-deep-learning/)
- [A Beginner's Guide To Understanding Convolutional Neural Networks – Adit Deshpande – CS Undergrad at UCLA ('19)](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)


### Theory

#### Convolution

- 公式推導

    $$
    \begin{aligned}
    & L: Origin\ Width &\\
    & p: padding &\\
    & f: filter\ Width &\\
    & s: stride &\\
    & N: New\ Width &\\
    &\\
    &L+2p = f \times N -(f-s)(N-1) &\\
    & \Longrightarrow L = f + s(N-1) - 2p &\\
    & \Longrightarrow N = \frac{L-f+2p}{s}+1 &\\
    \end{aligned}
    $$

    ![](https://screenshotscdn.firefoxusercontent.com/images/4ca43278-1643-4e84-8aa4-b9f4bb8545b1.png)





### Dataset

- [CIFAR-10 and CIFAR-100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html)
- 

### Projects

#### 破解 capcha

- [使用深度学习来破解 captcha 验证码](https://zhuanlan.zhihu.com/p/26078299)
    - [ypwhs/captcha_break: 验证码识别](https://github.com/ypwhs/captcha_break)
- [rickyhan/SimGAN-Captcha: Solve captcha without manually labeling a training set](https://github.com/rickyhan/SimGAN-Captcha)
- [JasonLiTW/simple-railway-captcha-solver: 實作基於CNN的台鐵訂票驗證碼辨識以及驗證性高的訓練集產生器 (Simple captcha solver based on CNN and a training set generator by imitating the style of captcha)](https://github.com/JasonLiTW/simple-railway-captcha-solver)
- [裤҉裆҉里҉的҉霸҉气҉/verification-decoder - 碼雲 Gitee.com](https://gitee.com/kdldbq/verification-decoder?from=weekmail)


#### Image Caption

- [karpathy/neuraltalk2: Efficient Image Captioning code in Torch, runs on GPU](https://github.com/karpathy/neuraltalk2)

    <iframe src="https://player.vimeo.com/video/146492001" width="640" height="400" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
    <p><a href="https://vimeo.com/146492001">NeuralTalk and Walk</a> from <a href="https://vimeo.com/kylemcdonald">Kyle McDonald</a> on <a href="https://vimeo.com">Vimeo</a>.</p>

- [[1703.09137] Where to put the Image in an Image Caption Generator](https://arxiv.org/abs/1703.09137)


#### Ultrasound image with deep learning

- [[1710.10006] Deep Learning for Accelerated Ultrasound Imaging](https://arxiv.org/abs/1710.10006)
    - [[1710.06304] Towards CT-quality Ultrasound Imaging using Deep Learning](https://arxiv.org/abs/1710.06304)


- [Ultrasound DL](http://deepultrasound.ai/)
- [Automating Breast Cancer Detection with Deep Learning](https://blog.insightdatascience.com/automating-breast-cancer-detection-with-deep-learning-d8b49da17950)



## Data Domain Transfer

### image style transfer

- [NVIDIA/FastPhotoStyle: Style transfer, deep learning, feature transform](https://github.com/NVIDIA/FastPhotoStyle)


### 2D to 3D

- [Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression](http://aaronsplace.co.uk/papers/jackson2017recon/)
    - [AaronJackson/vrn: Code for "Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression"](https://github.com/AaronJackson/vrn)


## GAN

- [Gan的数学推导 | Sherlock Blog](https://sherlockliao.github.io/2017/06/20/gan_math/)

- [Timnit Gebru 在 Twitter："Does someone have a list like the 10 or even 20 GAN related papers I should read this year or something like this? I can't keep up. @goodfellow_ian ?"](https://twitter.com/timnitGebru/status/968242968007200769)

    > 1\. Progressive GANs: [ https://arxiv.org/abs/1710.10196  ](https://t.co/UEFhewds2M "https://arxiv.org/abs/1710.10196") (probably the highest quality images so far)
    > 
    > 2\. Spectral normalization: [ https://openreview.net/forum?id=B1QRgziT-&noteId=BkxnM1TrM …](https://t.co/tt2os9H1Py "https://openreview.net/forum?id=B1QRgziT-&noteId=BkxnM1TrM") (got GANs working on lots of classes, which has been hard)
    > 
    > 3\. Projection discriminator: [ https://openreview.net/forum?id=ByS1VpgRZ …](https://t.co/qG1xwu1PuX "https://openreview.net/forum?id=ByS1VpgRZ") (from the same lab as #2, both techniques work well together, overall give very good results with 1000 classes) Here's the video of putting the two methods together: [https://www.youtube.com/watch?time_continue=3&v=r6zZPn-6dPY](https://www.youtube.com/watch?time_continue=3&v=r6zZPn-6dPY)
    > 
    > 4\. pix2pixHD (GANs for 2-megapixel video) [https://arxiv.org/abs/1711.11585](https://arxiv.org/abs/1711.11585)
    > [https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be](https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be) 
    > 
    > 
    > 5\. Are GANs created equal? [ https://arxiv.org/abs/1711.10337  ](https://t.co/4dIIOjPBC3 "https://arxiv.org/abs/1711.10337") A big empirical study showing the importance of good rigorous empirical work and how a lot of the GAN variants don't seem to actually offer improvements in practice 
    > 
    > 
    > 6\. WGAN-GP [ https://arxiv.org/abs/1704.00028  ](https://t.co/zJ6ZDSdz7w "https://arxiv.org/abs/1704.00028") : probably the most popular GAN variant today and seems to be pretty good in my opinion. Caveat: the baseline GAN variants should not perform nearly as badly as this paper claims, especially the text one
    > 
    > 7\. StackGAN++: [ https://arxiv.org/abs/1710.10916  ](https://t.co/ccOlTNW43F "https://arxiv.org/abs/1710.10916") High quality text-to-image synthesis with GANs
    > 
    > 8\. Making all ML algorithms differentially private by training them on fake private data generated by GANs [https://www.biorxiv.org/content/early/2017/07/05/159756](https://www.biorxiv.org/content/early/2017/07/05/159756)
    > 
    > 9\. You should be a little bit aware of the "GANs with encoders" space, one of my favorites is [ https://arxiv.org/abs/1701.04722](https://t.co/2uWTwu6kes "https://arxiv.org/abs/1701.04722")
    > 
    > 10\. You should be a little bit aware of the "theory of GAN convergence" space, one of my favorites is [ https://arxiv.org/abs/1706.04156](https://t.co/JpCKaHy9im "https://arxiv.org/abs/1706.04156")
    > 
    > [name=Ian Goodfellow]


### Nash equilibrium

- [[1706.08500] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500)

- [[1705.02894] Geometric GAN](https://arxiv.org/abs/1705.02894)

- [[1708.08819] Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields](https://arxiv.org/abs/1708.08819)



## RNN


### Articles

- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)


### NLP

- [从文本生成看Seq2Seq模型](https://zhuanlan.zhihu.com/p/29967933)

- [中美两位 AI 大师的“巅峰对话”：为何 NLP 领域难以出现“独角兽”？ | 独家](https://zhuanlan.zhihu.com/p/33970936)

- [deep-learning-with-keras-notebooks/8.0-using-word-embeddings.ipynb at master · erhwenkuo/deep-learning-with-keras-notebooks](https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/8.0-using-word-embeddings.ipynb)

