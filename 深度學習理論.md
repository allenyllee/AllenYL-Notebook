# 深度學習理論

[toc]
<!-- toc --> 

## Some Ideas

- super resolution for text image




## Back Propergation

- [三十分钟理解计算图上的微积分：Backpropagation，反向微分 - CSDN博客](https://blog.csdn.net/xbinworld/article/details/56523063)

    > 神经网络的训练算法，目前基本上是以Backpropagation (BP) 反向传播为主（加上一些变化），NN的训练是在1986年被提出，但实际上，BP 已经在不同领域中被重复发明了数十次了（参见 Griewank (2010)[1]）。更加一般性且与应用场景独立的名称叫做：反向微分 (reverse-mode differentiation)。本文是看了资料[2]中的介绍，写的蛮好，自己记录一下，方便理解。
    > 
    > 从本质上看，BP 是一种快速求导的技术，可以作为一种不单单用在深度学习中并且可以胜任大量数值计算场景的基本的工具。
    > 
    > 计算图
    > ---
    > 
    > 必须先来讲一讲计算图的概念，计算图出现在Bengio 09年的《Learning Deep Architectures for AI》，\
    > Bengio使用了有向图结构来描述神经网络的计算:
    > 
    > ![这里写图片描述](https://i.imgur.com/xr10GF1.jpg)
    > 
    > 整张图可看成三部分：输入结点、输出结点、从输入到输出的计算函数。上图很容易理解，就是output=sin(a*x+b) * x
    > 
    > 计算图上的导数
    > -------
    > 
    > 有向无环图在计算机科学领域到处可见，特别是在函数式程序中。他们与依赖图（dependency graph）或者调用图（call graph）紧密相关。同样他们也是大部分非常流行的深度学习框架背后的核心抽象。
    > 
    > 下文以下面简单的例子来描述：
    > 
    > ![这里写图片描述](https://i.imgur.com/PKdkrti.jpg)
    > 
    > 假设 a = 2, b = 1，最终表达式的值就是 6。\
    > 为了计算在这幅图中的偏导数，我们需要 和式法则（sum rule ）和 乘式法则（product rule）：
    > 
    > ![这里写图片描述](https://i.imgur.com/EpXcBUr.jpg)
    > 
    > 下面，在图中每条边上都有对应的导数了：\
    > ![这里写图片描述](https://i.imgur.com/2PsrGvT.jpg)
    > 
    > 那如果我们想知道哪些没有直接相连的节点之间的影响关系呢？假设就看看 e 如何被 a 影响的。如果我们以 1 的速度改变 a，那么 c 也是以 1 的速度在改变，导致 e 发生了 2 的速度在改变。因此 e 是以 1 * 2 的关于 a 变化的速度在变化。\
    > 而一般的规则就是对一个点到另一个点的所有的可能的路径进行求和，每条路径对应于该路径中的所有边的导数之积。因此，为了获得 e 关于 b 的导数，就采用路径求和：
    > 
    > ![这里写图片描述](https://i.imgur.com/cuyVDGs.jpg)
    > 
    > 这个值就代表着 b 改变的速度通过 c 和 d 影响到 e 的速度。聪明的你应该可以想到，事情没有那么简单吧？是的，上面例子比较简单，在稍微复杂例子中，路径求和法很容易产生路径爆炸：
    > 
    > ![这里写图片描述](https://i.imgur.com/Bc8CbTc.jpg)
    > 
    > 在上面的图中，从 X 到 Y 有三条路径，从 Y 到 Z 也有三条。如果我们希望计算 dZ/dX，那么就要对 3 * 3 = 9 条路径进行求和了：
    > 
    > ![这里写图片描述](https://i.imgur.com/P8coBQj.jpg)
    > 
    > 该图有 9 条路径，但是在图更加复杂的时候，路径数量会指数级地增长。相比于粗暴地对所有的路径进行求和，更好的方式是进行因式分解：
    > 
    > ![这里写图片描述](https://i.imgur.com/hJzrVUn.jpg)
    > 
    > 有了这个因式分解，就出现了高效计算导数的可能------通过在每个节点上反向合并路径而非显式地对所有的路径求和来大幅提升计算的速度。实际上，两个算法对每条边的访问都只有一次！
    > 
    > 前向微分和反向微分
    > ---------
    > 
    > 前向微分**从图的输入开始**，一步一步到达终点。在每个节点处，对输入的路径进行求和。每个这样的路径都表示输入影响该节点的一个部分。通过将这些影响加起来，我们就得到了输入影响该节点的全部，也就是关于输入的导数。
    > 
    > ![这里写图片描述](https://i.imgur.com/ibZ33TE.jpg)
    > 
    > 相对的，反向微分是从图的输出开始，反向一步一步抵达最开始输入处。在每个节点处，会合了所有源于该节点的路径。
    > 
    > ![这里写图片描述](https://i.imgur.com/CM40UJP.jpg)
    > 
    > 前向微分 跟踪了输入如何改变每个节点的情况。反向微分 则跟踪了每个节点如何影响输出的情况。也就是说，前向微分应用操作 d/dX 到每个节点，而反向微分应用操作 dZ/d 到每个节点。
    > 
    > **让我们重新看看刚开始的例子：**\
    > ![这里写图片描述](https://i.imgur.com/DSSIWVg.jpg)
    > 
    > 我们可以从 b 往上使用前向微分。这样获得了每个节点关于 b 的导数。（写在边上的导数我们已经提前算高了，这些相对比较容易，只和一条边的输入输出关系有关）
    > 
    > ![这里写图片描述](https://i.imgur.com/uxeeoQk.jpg)
    > 
    > 我们已经计算得到了 de/db，**输出关于一个输入 b 的导数**。但是如果我们从 e 往回计算反向微分呢？这会得到 **e 关于每个节点的导数：**
    > 
    > ![这里写图片描述](https://i.imgur.com/tiReB9V.jpg)
    > 
    > 反向微分给出了 e 关于每个节点的导数，这里的确是每一个节点。我们得到了 de/da 和 de/db，e 关于输入 a 和 b 的导数。（当然中间节点都是包括的），**前向微分给了我们输出关于某一个输入的导数，而反向微分则给出了所有的导数。**
    > 
    > **想象一个拥有百万个输入和一个输出的函数。前向微分需要百万次遍历计算图才能得到最终的导数，而反向微分仅仅需要遍历**一次**就能得到所有的导数！速度极快！**
    > 
    > 训练神经网络时，我们将衡量神经网络表现的代价函数看做是神经网络参数的函数。我们希望计算出代价函数关于所有参数的偏导数，从而进行梯度下降（gradient descent）。现在，常常会遇到百万甚至千万级的参数的神经网络。所以，反向微分，也就是 BP，在神经网络中发挥了关键作用！所以，其实BP的本质就是链式法则。
    > 
    > （有使用前向微分更加合理的场景么？当然！因为反向微分得到一个输出关于所有输入的导数，前向微分得到了所有输出关于一个输入的导数。如果遇到了一个有多个输出的函数，前向微分肯定更加快速）
    > 
    > BP 也是一种理解导数在模型中如何流动的工具。在推断为何某些模型优化非常困难的过程中，BP 也是特别重要的。典型的例子就是在 Recurrent Neural Network 中理解 vanishing gradient 的原因。


- [Automatic differentiation - Wikiwand](https://www.wikiwand.com/en/Automatic_differentiation)


    > ![Figure 2: Example of forward accumulation with computational graph](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/ForwardAccumulationAutomaticDifferentiation.png/600px-ForwardAccumulationAutomaticDifferentiation.png) 
    > 
    > Figure 2: Example of forward accumulation with computational graph
    > 
    > ![Figure 3: Example of reverse accumulation with computational graph](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/ReverseaccumulationAD.png/600px-ReverseaccumulationAD.png) 
    > 
    > Figure 3: Example of reverse accumulation with computational graph
    > 


### softmax/corss entropy

- [derivative - Backpropagation with Softmax / Cross Entropy - Cross Validated](https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy)

    The cross entropy error function is

    $$
    E=-\sum_jt_j\log o_j
    $$

    with $t$   and $o$   as the target and output at neuron $j$  , respectively. The sum is over each neuron in the output layer. $o_j$    itself is the result of the softmax function:

    $$
    o_j=softmax(z_j)=\frac{e^{z_j}}{\sum_j e^{z_j}}
    $$

    Again, the sum is over each neuron in the output layer and $z_j$ is the input to neuron $j$:

    $$
    z_j=\sum_i w_{ij}y_i+b_j
    $$

    That is the sum over all neurons in the previous layer with their corresponding output $o_i$ and weight $w_ij$ towards neuron $j$ plus a bias $b$.

    Finally, to get the gradient of $E$ with respect to the weight-matrix $w$, giving the final expression (assuming a one-hot $t$  , i.e. $τ=1$  )

    $$
    \frac{\partial E}{\partial w_{ij}}=y_i(o_j-t_j)
    $$

    where $y$ is the input on the lowest level (of your example).


- [machine learning - Cross entropy function (python) - Stack Overflow](https://stackoverflow.com/questions/47377222/cross-entropy-function-python)

    > ```python
    > def cross_entropy(predictions, targets, epsilon=1e-12):
    >     """
    >     Computes cross entropy between targets (encoded as one-hot vectors)
    >     and predictions. 
    >     Input: predictions (N, k) ndarray
    >            targets (N, k) ndarray        
    >     Returns: scalar
    >     """
    >     predictions = np.clip(predictions, epsilon, 1. - epsilon)
    >     N = predictions.shape[0]
    >     ce = -np.sum(np.sum(targets*np.log(predictions+1e-9)))/N
    >     return ce
    > 
    > predictions = np.array([[0.25,0.25,0.25,0.25],
    >                         [0.01,0.01,0.01,0.96]])
    > targets = np.array([[0,0,0,1],
    >                    [0,0,0,1]])
    > ans = 0.71355817782  #Correct answer
    > x = cross_entropy(predictions, targets)
    > print(np.isclose(x,ans))
    > ```
    > 
    > Here, I think it's a little clearer if you stick with np.sum(). Also, I added 1e-9 into the np.log() to avoid the possibility of having a log(0) in your computation. Hope this helps!


## Activation Function

### sigmoid

### tanh

### relu

### selu

- [[1706.02515] Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)

- [最新激活神經元:Self-Normalization Neural Network (SELU) | Learning by Hacking](https://data-sci.info/2017/06/11/%E6%9C%80%E6%96%B0%E6%BF%80%E6%B4%BB%E7%A5%9E%E7%B6%93%E5%85%83self-normalization-neural-network-selu/)

    > 在訓練深度神經網路時會碰到gradient vanishing的問題，這個問題在Weight初始值或Activation  Neuron設定不當時會更嚴重，以下引用Andrej Karpathy在[Standford CS231n](http://cs231n.stanford.edu/)上的說明:
    > 
    > [![](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.17.22.png)](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.17.22.png)
    > 
    > 圖為個訓練10層神經網路，每層500個hidden unit實驗，Weight初始值是從Gaussian(0,0.01)隨機抽，Activation Neuron採用tanh。上圖由左到下分別為每層輸出平均、標準差和分佈。我們可以看到在第三層以後輸出幾乎都在0附近，這會造成最後面幾層的Gradient非常小，非常小的Gradient再往前傳會更小(可以想像成一個<0的數一直乘)，於是到最後gradient就幾乎等於0，我們稱這個叫做Gradient Vanishing。Gradient Vanishing後Weight就失去了更新的方向，於是model就train不動了。
    > 
    > 為了改善這個問題，前人做過很多的努力，以下簡單帶過如下(細節請參考[Standford CS231n 2017: Training Neural Network Part I](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf))
    > 
    > 1.  Xavier Initialization[1]: 這個方法設計了一個Initialization的方式，讓輸出層的值在線性的activation neuron會有長得像Gaussian(0,1)的分佈。但是遇到非線性的activation neuron像是relu就會失效(mean ~0, std ~ 0 -> Gradient Vanishing)。
    > 2.  He Initialization[2]: 也是一個特殊的Initialization方式，雖然改善了前者讓Gradient Vanishing情況變好，但不是Gaussian(0,1)。
    > 3.  Batch Normalization[3]:簡單來說就是既然想要Gaussian(0,1)，那就每層都做一個吧。直覺上解釋就是每層去normalize他，此外又加上了兩個model可以學出來的參數來決定normalize的程度。這是目前在訓練深度神經網路中最常見的做法。
    > 
    > 那麼本篇的主角Selu，到底是什麼呢？ 他從本質上去改進Activation Neuron，讓他在數學上具有自動讓輸出值收斂到 mean = 0, std =1，即便是在有noise的情況下。而就算std不收斂到1，作者們也給出了上下界。那這神奇的Selu長什麼樣子呢？
    > 
    > [![](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.57.28-300x63.png)](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午9.57.28.png)
    > 
    > Source: Paper
    > 
    > 其中的λ=1.0507009873554804934193349852946, α= 1.6732632423543772848170429916717
    > 
    > 讀者看到這可能以為小編在亂寫，但是這些數字真的是作者用各種數學證明算出來的(主要是Banach Fix Point Theorem)，程式碼也是這個數字。而他的的Performance上贏過了許多現有的baselines，小編在此取其一給讀者
    > 
    > [![](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午10.06.58.png)](https://data-sci.info/wp-content/uploads/2017/06/螢幕快照-2017-06-11-下午10.06.58.png)
    > 
    > 上圖是將SELU + Feed Forward Neuron(FNN) 和 BatchNorm + FNN分別在MNIST和CIFAR上的表現。我們可以看到還不到很深層SELU的表現已經勝過Batch Norm。更多細節請參考
    > 
    > -   原始論文([Arxiv](https://arxiv.org/pdf/1706.02515.pdf)) (題外話:共同作者之一Hochreiter也是LSTM的共同作者)
    > -   code([Github](https://github.com/bioinf-jku/SNNs))
    > -   作者教你怎麼算λ,α的Code([Github](https://github.com/bioinf-jku/SNNs/blob/master/getSELUparameters.ipynb))
    > 
    > 另外已經有強者用Tensorflow在CIFAR、SVHN、MNIST上做出SELU,RELU,leaky RELU的輸出值的比較，非常適合想要實際玩玩SELU的讀者:
    > 
    > -   Activation Visualization Histogram ([Github](https://github.com/shaohua0116/Activation-Visualization-Histogram))
    > 

## initializer

#### Xavier
- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)

#### He
#### Lecun


## optimizer

- Gradient Descent
    $$
    \begin{aligned}
    &\theta_t : \text{目前參數值所成向量}&\\
    &g_t = \nabla_\theta L(\theta_t)&\\
    &\\
    &\theta_{t+1}=\theta_t+\eta g_t&\\
    \end{aligned}
    $$
    
    
- adam

## Regularization

- dropout
- L2 Regularization

通常 dropout, L2 regularization 選⼀個就好


## Convolution

- 公式推導

    $$
    \begin{aligned}
    & L: Origin\ Width &\\
    & p: padding &\\
    & f: filter\ Width &\\
    & s: stride &\\
    & N: New\ Width &\\
    &\\
    &L+2p = f \times N -(f-s)(N-1) &\\
    & \Longrightarrow L = f + s(N-1) - 2p &\\
    & \Longrightarrow N = \frac{L-f+2p}{s}+1 &\\
    \end{aligned}
    $$

    ![](https://screenshotscdn.firefoxusercontent.com/images/4ca43278-1643-4e84-8aa4-b9f4bb8545b1.png)


- [[1603.07285] A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)



## 侷限性 Limitation 

- [深度学习的局限性](https://zhuanlan.zhihu.com/p/27955150)

    > 如果你开发了一个能控制人身体的神经网络，并希望它能够在不被任何车碰撞的情况下游览整个城市，那么神经网路控制的人在各种情况下都要死掉数千次，直到可以判断出车辆的情况以及各种危险，并制定执行回避行为。而再去一个新的城市，神经网络必须重新学习大部分的知识。反过来，人类能够学习安全的行为，而没有用死亡试错 的过程，这要归功于人类假设情境的抽象建模。
    > 
    > ![](https://pic3.zhimg.com/80/v2-01c43aec1debaa7f6b7fb5c18915c519_hd.jpg)
    > 
    > （图：同样的经验，左侧是机器学习的局部泛化，缺乏抽象能力。右侧是人类的极端泛华，可以通过假设而抽象建模，不必真的遍历）
    > 
    > 简而言之，尽管我们在机器感知上取得了进步，但我们仍然远离感性的人文 AI：我们的模型只能执行*局部泛化*，适应与过去数据非常接近的新情况，而人类认知能够*极端泛化*，迅速适应大胆新奇的情况，或为长远未来的情况进行规划。
    > 
    > ---
    > 
    > ## 结论
    > 
    > 这是你应该记住的：到目前为止，深度学习的唯一真正的成功是使用连续几何变换将空间 X 映射到空间 Y 的能力，但还要给出了大量的人为注释的数据。做好这一切，基本上能改变每一个行业的游戏规则，但是距离更人性化的 AI 还有很长一段路要走。
    > 
    > 为了让 AI 解决这些限制，并开始与人类大脑竞争，我们需要跳出「简单的输入到输出映射」，关注*推理*和*抽象*。
    > 
    > > 原文：[The limitations of deep learning](https://link.zhihu.com/?target=https%3A//blog.keras.io/the-limitations-of-deep-learning.html)
    > 


## Overfitting

- [機器學習5年大躍進，可能是個錯覺 - 幫趣](http://bangqu.com/Zwd41J.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > 這項研究，就是加州大學伯克利分校和MIT的幾名科學家在arXiv上公開的一篇論文：Do CIFAR-10 Classifiers Generalize to CIFAR-10?。
    > 
    > 解釋一下，這個看似詭異的問題——「CIFAR-10分類器能否泛化到CIFAR-10？」，針對的是當今深度學習研究的一個大缺陷：
    > 
    > 看起來成績不錯的深度學習模型，在現實世界中不見得管用。因爲很多模型和訓練方法取得的好成績，都來自對於那些著名基準驗證集的過擬合。

### Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質

- [Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質 - 幫趣](http://bangqu.com/qA6W5k.html)

    > 本文是 DeepMind 創始人 Demis Hassabis 和 Mobileye 創始人 Amnon Shashua 的導師、MIT 教授 Tomaso Poggio 的深度學習理論系列的第三部分，分析深度神經網絡的泛化能力。該系列前兩部分討論了深度神經網絡的表徵和優化問題，機器之心之前對整個理論系列進行了簡要總結。在深度網絡的實際應用中，通常會添加顯性（如權重衰減）或隱性（如早停）正則化來避免過擬合，但這並非必要，尤其是在分類任務中。在本文中，Poggio 討論了深度神經網絡的過擬合缺失問題，即在參數數量遠遠超過訓練樣本數的情況下模型也具備良好的泛化能力。其中特別強調了經驗損失和分類誤差之間的差別，證明深度網絡每一層的權重矩陣可收斂至極小范數解，並得出深度網絡的泛化能力取決於多種因素的互相影響，包括損失函數定義、任務類型、數據集類型等。
    > 
    > **1 引言**
    > 
    > 過去幾年來，深度學習在許多機器學習應用領域都取得了極大的成功。然而，我們對深度學習的理論理解以及開發原理的改進能力上都有所落後。如今對深度學習令人滿意的理論描述正在形成。這涵蓋以下問題：1）深度網絡的表徵能力；2）經驗風險的優化；3）泛化------當網絡過參數化（overparametrized）時，即使缺失顯性的正則化，爲什麼期望誤差沒有增加？
    > 
    > 本論文解決了第三個問題，也就是非過擬合難題，這在最近的多篇論文中都有提到。論文 [1] 和 [7] 展示了線性網絡的泛化特性可被擴展到 DNN 中從而解決該難題，這兩篇論文中的泛化即用梯度下降訓練的帶有特定指數損失的線性網絡收斂到最大間隔解，提供隱性的正則化。本論文還展示了同樣的理論可以預測經驗風險的不同零最小值（zero minimizer）的泛化。
    > 
    > **2 過擬合難題**
    > 
    > 經典的學習理論將學習系統的泛化行爲描述爲訓練樣本數 n 的函數。從這個角度看，DNN 的行爲和期望一致：更多訓練數據帶來更小的測試誤差，如圖 1a 所示。該學習曲線的其他方面似乎不夠直觀，但也很容易解釋。例如即使在訓練誤差爲零時，測試誤差也會隨着 n 的增加而減小（正如 [1] 中所指出的那樣，因爲被報告的是分類誤差，而不是訓練過程中被最小化的風險，如交叉熵）。看起來 DNN 展示出了泛化能力，從技術角度上可定義爲：隨着 n → ∞，訓練誤差收斂至期望誤差。圖 1 表明對於正常和隨機標籤，模型隨 n 的增加的泛化能力變化。這與之前研究的結果一致（如 [8]），與 [9] 的穩定性結果尤其一致。注意泛化的這一特性並不尋常：很多算法（如 K 最近鄰算法）並不具備該保證。
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k15318036171099JP52.png)
    > 
    > *圖 1：不同數量訓練樣本下的泛化。（a）在 CIFAR 數據集上的泛化誤差。（b）在隨機標籤的 CIFAR 數據集上的泛化誤差。該深度神經網絡是通過最小化交叉熵損失訓練的，並且是一個 5 層卷積網絡（即沒有池化），每個隱藏層有 16 個通道。ReLU 被用做層之間的非線性函數。最終的架構有大約 1 萬個參數。圖中每一個點使用批大小爲 100 的 SGD 並訓練 70 個 epoch 而得出，訓練過程沒有使用數據增強和正則化。*
    > 
    > 泛化的這一特性雖然重要，但在這裏也只是學術上很重要。現在深度網絡典型的過參數化真正難題（即本論文的重點）是在缺乏正則化的情況下出現明顯缺乏過擬合的現象。從隨機標註數據中獲得零訓練誤差的同樣網絡（圖 1b）很顯然展示出了大容量，但並未展示出在不改變多層架構的情況下，每一層神經元數量增加時期望誤差會有所增加（圖 2a）。具體來說，當參數數量增加並超過訓練集大小時，未經正則化的分類誤差在測試集上的結果並未變差。
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k15318036187892u56K.png)
    > 
    > *圖 2：在 CIFAR-10 中的期望誤差，橫軸爲神經元數量。該 DNN 與圖 1 中的 DNN 一樣。（a）期望誤差與參數數量增加之間的相關性。（b）交叉熵風險與參數數量增加之間的相關性。期望風險中出現部分「過擬合」，儘管該指數損失函數的特點略微有些誇大。該過擬合很小，因爲 SGD 收斂至每一層具備最小弗羅貝尼烏斯範數（Frobenius norm）的網絡。因此，當參數數量增加時，這裏的期望分類誤差不會增加，因爲分類誤差比損失具備更強的魯棒性（見附錄 9）。*
    > 
    > 我們應該明確參數數量只是過參數化的粗略表徵。實驗設置詳見第 6 章。
    > 
    > **5 深度網絡的非線性動態**
    > 
    > **5.3 主要問題**
    > 
    > 把所有引理合在一起，就得到了
    > 
    > **定理 3**：給定一個指數損失函數和非線性分割的訓練數據，即對於訓練集中的所有 x_n，∃f(W; x_n) 服從 y_n*f(W; x_n) > 0，獲得零分類誤差。以下特性展示了漸近平衡（asymptotic equilibrium）：
    > 
    > 1.  GD 引入的梯度流從拓撲學角度來看等於線性化流；
    > 
    > 2.  解是每一層權重矩陣的局部極小弗羅貝尼烏斯範數解。
    > 
    > 在平方損失的情況下分析結果相同，但是由於線性化動態只在零初始條件下收斂至極小范數，因此該定理的最終表述「解是局部極小範數解」僅適用於線性網絡，如核機器，而不適用於深度網絡。因此在非線性的情況下，平方損失和指數損失之間的差別變得非常顯著。對其原因的直觀理解見圖 3。對於全局零最小值附近的深度網絡，平方損失的「地形圖」通常有很多零特徵值，且在很多方向上是平坦的。但是，對於交叉熵和其他指數損失而言，經驗誤差山谷有一個很小的向下的坡度，在||w||無限大時趨近於零（詳見圖 3）。
    > 
    > 在補充材料中，研究者展示了通過懲罰項 λ 寫出 W_k = ρ_k*V_k，並使 ||V_k||^2 = 1，來考慮相關動態，從而展示初始條件的獨立性以及早停和正則化的等效性。
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k153180362114695b7b.png)
    > 
    > *圖 3：具備參數 w_1 和 w_2 的平方損失函數（左）。極小值具備一個退化 Hessian（特徵值爲零）。如文中所述，它表示在零最小值的小近鄰區域的「一般」情況，零最小值具備很多零特徵值，和針對非線性多層網絡的 Hessian 的一些正特徵值。收斂處的全局最小值附近的交叉熵風險圖示如右圖所示。隨着||w|| → ∞，山谷坡度稍微向下。在多層網絡中，損失函數可能表面是分形的，具備很多退化全局最小值，每個都類似於這裏展示的兩個最小值的多維度版本。*
    > 
    > **5.4 爲什麼分類比較不容易過擬合**
    > 
    > 由於這個解是線性化系統的極小範數解，因此我們期望，對於低噪聲數據集，與交叉熵最小化相關的分類誤差中幾乎很少或沒有過擬合。注意：交叉熵作爲損失函數的情況中，梯度下降在線性分離數據上可收斂至局部極大間隔解（local max-margin solution），起點可以是任意點（原因是非零斜率，如圖 3 所示）。因此，對於期望分類誤差，過擬合可能根本就不會發生，如圖 2 所示。通常相關損失中的過擬合很小，至少在幾乎無噪聲的數據情況下是這樣，因爲該解是局部極大間隔解，即圍繞極小值的線性化系統的僞逆。近期結果（Corollary 2.1 in [10]）證明具備 RELU 激活函數的深度網絡的 hinge-loss 的梯度最小值具備大的間隔，前提是數據是可分離的。這個結果與研究者將 [1] 中針對指數損失的結果擴展至非線性網絡的結果一致。注意：目前本論文研究者沒有對期望誤差的性質做出任何聲明。不同的零最小值可能具備不同的期望誤差，儘管通常這在 SGD 的類似初始化場景中很少出現。本文研究者在另一篇論文中討論了本文提出的方法或許可以預測與每個經驗最小值相關的期望誤差。
    > 
    > 總之，本研究結果表明多層深度網絡的行爲在分類中類似於線性模型。更準確來說，在分類任務中，通過最小化指數損失，可確保全局最小值具備局部極大間隔。因此動態系統理論爲非過擬合的核心問題提供了合理的解釋，如圖 2 所示。主要結果是：接近經驗損失的零極小值，非線性流的解繼承線性化流的極小範數特性，因爲這些流拓撲共軛。損失中的過擬合可以通過正則化來顯性（如通過權重衰減）或隱性（通過早停）地控制。分類誤差中的過擬合可以被避免，這要取決於數據集類型，其中漸近解是與特定極小值相關的極大間隔解（對於交叉熵損失來說）。
    > 
    > **6 實驗**
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k1531803622387653TL.png)
    > 
    > *圖 4：使用平方損失在特徵空間中對線性網絡進行訓練和測試（即 y = WΦ(X)），退化 Hessian 如圖 3 所示。目標函數是一個 sine 函數 f(x) = sin(2πfx)，在區間 [-1, 1] 上 frequency f = 4。訓練數據點有 9 個，而測試數據點的數量是 100。第一對圖中，特徵矩陣 φ(X) 是多項式的，degree 爲 39。第一對圖中的數據點根據 Chebyshev 節點機制進行採樣，以加速訓練，使訓練誤差達到零。訓練使用完整梯度下降進行，步長 0.2，進行了 10, 000, 000 次迭代。每 120, 000 次迭代後權重受到一定的擾動，每一次擾動後梯度下降被允許收斂至零訓練誤差（機器準確率的最高點）。通過使用均值 0 和標準差 0.45 增加高斯噪聲，進而擾動權重。在第 5, 000, 000 次迭代時擾動停止。第二張圖展示了權重的 L_2 範數。注意訓練重複了 29 次，圖中報告了平均訓練和測試誤差，以及權重的平均範數。第二對圖中，特徵矩陣 φ(X) 是多項式的，degree 爲 30。訓練使用完整梯度下降進行，步長 0.2，進行了 250, 000 次迭代。第四張圖展示了權重的 L_2 範數。注意：訓練重複了 30 次，圖中報告了平均訓練和測試誤差，以及權重的平均範數。該實驗中權重沒有遭到擾動。*
    > 
    > **7 解決過擬合難題**
    > 
    > 本研究的分析結果顯示深度網絡與線性模型類似，儘管它們可能過擬合期望風險，但不經常過擬合低噪聲數據集的分類誤差。這遵循線性網絡梯度下降的特性，即風險的隱性正則化和對應的分類間隔最大化。在深度網絡的實際應用中，通常會添加顯性正則化（如權重衰減）和其他正則化技術（如虛擬算例），而且這通常是有益的，雖然並非必要，尤其是在分類任務中。
    > 
    > 如前所述，平方損失與指數損失不同。在平方損失情況中，具備任意小的 λ 的正則化（沒有噪聲的情況下）保留梯度系統的雙曲率，以收斂至解。但是，解的範數依賴於軌跡，且無法確保一定會是線性化引入的參數中的局部極小範數解（在非線性網絡中）。在沒有正則化的情況下，可確保線性網絡（而不是深度非線性網絡）收斂至極小范數解。在指數損失線性網絡和非線性網絡的情況下，可獲得雙曲梯度流。因此可確保該解是不依賴初始條件的極大間隔解。對於線性網絡（包括核機器），存在一個極大間隔解。在深度非線性網絡中，存在多個極大間隔解，每個對應一個全局最小值。在某種程度上來說，本研究的分析結果顯示了正則化主要提供了動態系統的雙曲率。在條件良好的線性系統中，即使 λ → 0，這種結果也是對的，因此內插核機器的通用情況是在無噪聲數據情況下無需正則化（即條件數依賴於 x 數據的分割，因此 y 標籤與噪聲無關，詳見 [19]）。在深度網絡中，也會出現這種情況，不過只適用於指數損失，而非平方損失。
    > 
    > 結論就是深度學習沒什麼神奇，在泛化方面深度學習需要的理論與經典線性網絡沒什麼不同，泛化本身指收斂至期望誤差，尤其是在過參數化時出現了過擬合缺失的情況。本研究分析通過將線性網絡的特性（如 [1] 強調的那些）應用到深度網絡，解釋了深度網絡泛化方面的難題，即不會過擬合期望分類誤差。
    > 
    > **8 討論**
    > 
    > 當然，構建對深度網絡性能有用的量化邊界仍然是一個開放性問題，因爲它是非常常見的情形，即使是對於簡單的僅包含一個隱藏層的網絡，如 SVM。本論文研究者主要的成果是圖 2 所展示的令人費解的行爲可以通過經典理論得到定性解釋。
    > 
    > 該領域存在很多開放性問題。儘管本文解釋了過擬合的缺失，即期望誤差對參數數量增加的容錯，但是本文並未解釋爲什麼深度網絡泛化得這麼好。也就是說，本論文解釋了爲什麼在參數數量增加並超過訓練數據數量時，圖 2 中的測試分類誤差沒有變差，但沒有解釋爲什麼測試誤差這麼低。
    > 
    > 基於 [20]、[18]、[16]、[10]，研究者猜測該問題的答案包含在以下深度學習理論框架內：
    > 
    > -   不同於淺層網絡，深度網絡能逼近層級局部函數類，且不招致維數災難（[21, 20]）。
    > 
    > -   經由 SGD 選擇，過參數化的深度網絡有很大概率會產生很多全局退化，或者大部分退化，以及「平滑」極小值（[16]）。
    > 
    > -   過參數化，可能會產生預期風險的過擬合。因爲梯度下降方法獲得的間隔最大化，過參數化也能避免過擬合低噪聲數據集的分類誤差。
    > 
    > 根據這一框架，淺層網絡與深度網絡之間的主要區別在於，基於特定任務的組織結構，兩種網絡從數據中學習較好表徵的能力，或者說是逼近能力。不同於淺層網絡，深度局部網絡特別是卷積網絡，能夠避免逼近層級局部合成函數類時的維度災難（curse of dimensionality）。這意味着對於這類函數，深度局部網絡可以表徵一種適當的假設類，其允許可實現的設置，即以最小容量實現零逼近誤差。
    > 
    > **論文：Theory IIIb: Generalization in Deep Networks**
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k1531803623697vY5a7.png)
    > 
    > 論文鏈接：<https://arxiv.org/abs/1806.11379>
    > 
    > **摘要：**深度神經網絡（DNN）的主要問題圍繞着「過擬合」的明顯缺失，本論文將其定義如下：當神經元數量或梯度下降迭代次數增加時期望誤差卻沒有變差。鑑於 DNN 擬合隨機標註數據的大容量和顯性正則化的缺失，這實在令人驚訝。近期 Srebro 等人的研究結果爲二分類線性網絡中的該問題提供瞭解決方案。他們證明損失函數（如 logistic、交叉熵和指數損失）最小化可在線性分離數據集上漸進、「緩慢」地收斂到最大間隔解，而不管初始條件如何。本論文中我們證明了對於非線性多層 DNN 在經驗損失最小值接近零的情況下也有類似的結果。指數損失的結果也是如此，不過不適用於平方損失。具體來說，我們證明深度網絡每一層的權重矩陣可收斂至極小范數解，達到比例因子（在獨立案例下）。我們對動態系統的分析對應多層網絡的梯度下降，這展示了一種對經驗損失的不同零最小值泛化性能的簡單排序標準。




## Course

### CS231n
- Notes
    - [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)
    - [Convolutional Neural Networks (CNNs / ConvNets)](https://cs231n.github.io/)
- Slides
    - [Syllabus | CS 231N](http://cs231n.stanford.edu/syllabus.html)
- Lecture Video
    - [Lecture Collection | Convolutional Neural Networks for Visual Recognition (Spring 2017) - YouTube - YouTube](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

## Datasets

- [CIFAR-10 and CIFAR-100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html)
- [ImageNet](http://www.image-net.org/)
- [COCO - Common Objects in Context](http://cocodataset.org/#home)
- 

## CNN 

### State of the art

- MNIST, CIFAR-10, CIFAR-100, STL-10, SVHN, ILSVRC2012 task 1 [Classification datasets results](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)

### Papers

- AlexNet [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
- VGGNet [[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
- GoogLeNet [[1409.4842] Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)
- ResNet [[1512.03385] Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

### Articles

- [#Deep Learning回顾#之LeNet、AlexNet、GoogLeNet、VGG、ResNet - 我爱机器学习 - 博客园](https://www.cnblogs.com/52machinelearning/p/5821591.html)
- [深度学习方法（五）：卷积神经网络CNN经典模型整理Lenet，Alexnet，Googlenet，VGG，Deep Residual Learning - CSDN博客](http://blog.csdn.net/xbinworld/article/details/45619685)
- [残差网络ResNet笔记 - 简书](https://www.jianshu.com/p/e58437f39f65)
- [无需数学背景，读懂 ResNet、Inception 和 Xception 三大变革性架构 | 机器之心](https://www.jiqizhixin.com/articles/2017-08-19-4)
- [The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3) – Adit Deshpande – CS Undergrad at UCLA ('19)](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
- [Exploring Deep Learning & CNNs - RSIP Vision](https://www.rsipvision.com/exploring-deep-learning/)
- [A Beginner's Guide To Understanding Convolutional Neural Networks – Adit Deshpande – CS Undergrad at UCLA ('19)](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)

### Topics

#### 破解 capcha

- [使用深度学习来破解 captcha 验证码](https://zhuanlan.zhihu.com/p/26078299)
    - [ypwhs/captcha_break: 验证码识别](https://github.com/ypwhs/captcha_break)
- [rickyhan/SimGAN-Captcha: Solve captcha without manually labeling a training set](https://github.com/rickyhan/SimGAN-Captcha)
- [JasonLiTW/simple-railway-captcha-solver: 實作基於CNN的台鐵訂票驗證碼辨識以及驗證性高的訓練集產生器 (Simple captcha solver based on CNN and a training set generator by imitating the style of captcha)](https://github.com/JasonLiTW/simple-railway-captcha-solver)
- [裤҉裆҉里҉的҉霸҉气҉/verification-decoder - 碼雲 Gitee.com](https://gitee.com/kdldbq/verification-decoder?from=weekmail)


#### Image Caption

- [karpathy/neuraltalk2: Efficient Image Captioning code in Torch, runs on GPU](https://github.com/karpathy/neuraltalk2)

    <iframe src="https://player.vimeo.com/video/146492001" width="640" height="400" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
    <p><a href="https://vimeo.com/146492001">NeuralTalk and Walk</a> from <a href="https://vimeo.com/kylemcdonald">Kyle McDonald</a> on <a href="https://vimeo.com">Vimeo</a>.</p>

- [[1703.09137] Where to put the Image in an Image Caption Generator](https://arxiv.org/abs/1703.09137)


#### Ultrasound image with deep learning

- [[1710.10006] Deep Learning for Accelerated Ultrasound Imaging](https://arxiv.org/abs/1710.10006)
    - [[1710.06304] Towards CT-quality Ultrasound Imaging using Deep Learning](https://arxiv.org/abs/1710.06304)

- [Ultrasound DL](http://deepultrasound.ai/)
- [Automating Breast Cancer Detection with Deep Learning](https://blog.insightdatascience.com/automating-breast-cancer-detection-with-deep-learning-d8b49da17950)

## GAN

### State of the art

- [the-gan-zoo/gans.tsv at master · hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv)


#### 二次元妹子

- [MakeGirlsMoe - Create Anime Characters with A.I.!](https://make.girls.moe/)
- 


### Papers

- [Timnit Gebru 在 Twitter："Does someone have a list like the 10 or even 20 GAN related papers I should read this year or something like this? I can't keep up. @goodfellow_ian ?"](https://twitter.com/timnitGebru/status/968242968007200769)

    > 1\. Progressive GANs: [ https://arxiv.org/abs/1710.10196  ](https://t.co/UEFhewds2M "https://arxiv.org/abs/1710.10196") (probably the highest quality images so far)
    > 
    > 2\. Spectral normalization: [ https://openreview.net/forum?id=B1QRgziT-&noteId=BkxnM1TrM …](https://t.co/tt2os9H1Py "https://openreview.net/forum?id=B1QRgziT-&noteId=BkxnM1TrM") (got GANs working on lots of classes, which has been hard)
    > 
    > 3\. Projection discriminator: [ https://openreview.net/forum?id=ByS1VpgRZ …](https://t.co/qG1xwu1PuX "https://openreview.net/forum?id=ByS1VpgRZ") (from the same lab as #2, both techniques work well together, overall give very good results with 1000 classes) Here's the video of putting the two methods together: [https://www.youtube.com/watch?time_continue=3&v=r6zZPn-6dPY](https://www.youtube.com/watch?time_continue=3&v=r6zZPn-6dPY)
    > 
    > 4\. pix2pixHD (GANs for 2-megapixel video) [https://arxiv.org/abs/1711.11585](https://arxiv.org/abs/1711.11585)
    > [https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be](https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be) 
    > 
    > 
    > 5\. Are GANs created equal? [ https://arxiv.org/abs/1711.10337  ](https://t.co/4dIIOjPBC3 "https://arxiv.org/abs/1711.10337") A big empirical study showing the importance of good rigorous empirical work and how a lot of the GAN variants don't seem to actually offer improvements in practice 
    > 
    > 
    > 6\. WGAN-GP [ https://arxiv.org/abs/1704.00028  ](https://t.co/zJ6ZDSdz7w "https://arxiv.org/abs/1704.00028") : probably the most popular GAN variant today and seems to be pretty good in my opinion. Caveat: the baseline GAN variants should not perform nearly as badly as this paper claims, especially the text one
    > 
    > 7\. StackGAN++: [ https://arxiv.org/abs/1710.10916  ](https://t.co/ccOlTNW43F "https://arxiv.org/abs/1710.10916") High quality text-to-image synthesis with GANs
    > 
    > 8\. Making all ML algorithms differentially private by training them on fake private data generated by GANs [https://www.biorxiv.org/content/early/2017/07/05/159756](https://www.biorxiv.org/content/early/2017/07/05/159756)
    > 
    > 9\. You should be a little bit aware of the "GANs with encoders" space, one of my favorites is [ https://arxiv.org/abs/1701.04722](https://t.co/2uWTwu6kes "https://arxiv.org/abs/1701.04722")
    > 
    > 10\. You should be a little bit aware of the "theory of GAN convergence" space, one of my favorites is [ https://arxiv.org/abs/1706.04156](https://t.co/JpCKaHy9im "https://arxiv.org/abs/1706.04156")
    > 
    > [name=Ian Goodfellow]


#### 對抗樣本

- [[1412.6572] Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)

- [[1801.02610] Generating Adversarial Examples with Adversarial Networks](https://arxiv.org/abs/1801.02610)

#### 防禦對抗樣本

- [Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models | OpenReview](https://openreview.net/forum?id=BkJ3ibb0-&noteId=SJwPXJaHG)

#### Convergence and Stability

- [[1705.07215] On Convergence and Stability of GANs](https://arxiv.org/abs/1705.07215)

- [[1703.10717] BEGAN: Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/abs/1703.10717)

#### GAN + RL



- [Learning to write programs that generate images | DeepMind](https://deepmind.com/blog/learning-to-generate-images/)

    - [DeepMind提出SPIRAL：使用強化對抗學習，實現會用畫筆的智能體 - 幫趣](http://bangqu.com/Sr63zI.html)




### Articles

- [Gan的数学推导 | Sherlock Blog](https://sherlockliao.github.io/2017/06/20/gan_math/)

- [amazing_gans_pycon2017 - Google 簡報](https://docs.google.com/presentation/d/14jJL6MR2uf4CD7PmsgMtivX8WfK-QMdKdwrLEYH9qDA/edit#slide=id.g22c42edce6_0_644)

- [[1701.04862] Towards Principled Methods for Training Generative Adversarial Networks](https://arxiv.org/abs/1701.04862)



#### Training giude

- [soumith/ganhacks: starter from "How to Train a GAN?" at NIPS2016](https://github.com/soumith/ganhacks)

- [[1701.00160] NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)
- 

#### Mode colapse

<iframe width="560" height="315" src="https://www.youtube.com/embed/ktxhiKhWoEE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

- [Mode collapse in GANs - Aiden Nibali](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/)

    > Addressing mode collapse
    > ------------------------
    > 
    > Mode collapse is a well-recognised problem, and researchers have made a few attempts at addressing it. I have identified 4 broad approaches to tackling mode collapse, which are described below.
    > 
    > ### Directly encourage diversity
    > 
    > It is impossible to determine output diversity by considering individual samples in isolation. This leads to a very logical next step of using batches of samples to directly assess diversity. Minibatch discrimination and feature mapping [1](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/#fn:imprgan) are two techniques which fall into this category.
    > 
    > Minibatch discrimination gives the discriminator the power of comparing samples across a batch to help determine whether the batch is real or fake.
    > 
    > Feature matching modifies the generator cost function to factor in the diversity of generated batches. It does this by matching statistics of discriminator features for fake batches to those of real batches. I had some success combining feature matching with the traditional GAN generator loss function to form a hybrid objective.
    > 
    > ### Anticipate counterplay
    > 
    > One way to prevent the cat-and-mouse game of hopping between modes is to peek into the future and anticipate counterplay when updating parameters. This approach should be familiar to those who know a bit about game theory (eg [minimax](https://en.wikipedia.org/wiki/Minimax)). Intuitively, this prevents players of the GAN game from making moves which are easily countered.
    > 
    > Unrolled GANs [2](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/#fn:unrolled) take this kind of approach by allowing the generator to “unroll” updates of the discriminator in a fully differentiable way. Now instead of the generator learning to fool the current discriminator, it learns to maximally fool the discriminator _after it has a chance to respond_, thus taking counterplay into account. Downsides of this approach are increased training time (each generator update has to simulate multiple discriminator updates) and a more complicated gradient calculation (backprop through an optimiser update step can be difficult).
    > 
    > ### Use experience replay
    > 
    > Hopping back and forth between modes can be minimised by showing old fake samples to the discriminator every so often. This prevents the discriminator from becoming too exploitable, but only for modes that have already been explored by the generator in the past.
    > 
    > A similar kind of effect can be achieved by occasionally substituting in an old discriminator/generator for a few iterations.
    > 
    > ### Use multiple GANs
    > 
    > Rather than fight mode collapse we could simply accept that the GAN will cover only a subset of the modes in the dataset, and train multiple GANs for different modes. When combined, these GANs cover all of the modes. AdaGAN [3](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/#fn:adagan) takes this approach. The major downside here is that training multiple GANs takes a lot of time. Furthermore, using a combination of GANs is generally more unwieldy than working with just one.




### Topics

#### Variational Autoencoder

- [[1606.05908] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)


#### WGAN

- [[1701.07875] Wasserstein GAN](https://arxiv.org/abs/1701.07875)
- [Wasserstein GAN and the Kantorovich-Rubinstein Duality - Vincent Herrmann](https://vincentherrmann.github.io/blog/wasserstein/)
- [A Hitchhikers guide to Wasserstein.pdf](http://n.ethz.ch/~gbasso/download/A%20Hitchhikers%20guide%20to%20Wasserstein/A%20Hitchhikers%20guide%20to%20Wasserstein.pdf)
- [f-divergence - Wikiwand](https://www.wikiwand.com/en/F-divergence)

- [ComputationalOT.pdf](https://optimaltransport.github.io/pdf/ComputationalOT.pdf)

- [Duality (optimization) - Wikiwand](https://www.wikiwand.com/en/Duality_(optimization))

- [[1704.00028] Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)

#### LSGAN

- [[1611.04076] Least Squares Generative Adversarial Networks](https://arxiv.org/abs/1611.04076)




#### Nash equilibrium

- [[1706.08500] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500)

- [[1705.02894] Geometric GAN](https://arxiv.org/abs/1705.02894)

- [[1708.08819] Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields](https://arxiv.org/abs/1708.08819)

#### image style transfer

- [NVIDIA/FastPhotoStyle: Style transfer, deep learning, feature transform](https://github.com/NVIDIA/FastPhotoStyle)

#### 2D to 3D

- [Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression](http://aaronsplace.co.uk/papers/jackson2017recon/)
    - [AaronJackson/vrn: Code for "Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression"](https://github.com/AaronJackson/vrn)


#### sample code

- [tjwei/DIY_AI](https://github.com/tjwei/DIY_AI)

- [tjwei/GANotebooks: wgan, wgan2(improved, gp), infogan, and dcgan implementation in lasagne, keras, pytorch](https://github.com/tjwei/GANotebooks)


## RNN

### State of the art

### Papers

### Articles

- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    - [The unreasonable effectiveness of Character-level Language Models (and why RNNs are still cool) - Jupyter Notebook Viewer](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)
    - [The Unreasonable Effectiveness of Recurrent Neural Networks：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/36s673/the_unreasonable_effectiveness_of_recurrent/)

        > > In particular, setting temperature very near zero will give the most likely thing that Paul Graham might say:
        > > 
        > > "is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same"
        > > 
        > > looks like we've reached an infinite loop about startups.
        > 
        > Great article, but this part is fundamentally incorrect, and probably the reason the sample is so loopy.
        > 
        > It may be counter-intuitive, but if you pick the most likely next character at every step, you will not necessarily end up with the most likely sequence. In other words, the greedy solution is not necessarily optimal.
        > 
        > Consider:
        > 
        > ```
        > P(00) = 0.4
        > P(01) = 0.0
        > P(10) = 0.3
        > P(11) = 0.3
        > 
        > ```
        > 
        > `1` is the most likely first character, but `00` is the most likely sequence.
        > 
        > Back in college, my differential equations professor had this to say: If you eat as much as you can every single day, you probably won't maximize your total food consumption.


- [Understanding LSTM Networks -- colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

- [LSTM RNN 循环神经网络 (LSTM) - 有趣的机器学习 | 莫烦Python](https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-4-LSTM/)

- [(33 条消息)CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？ - 知乎](https://www.zhihu.com/question/34681168)

- [遞歸神經網路和長短期記憶模型 RNN & LSTM · 資料科學・機器・人](https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_rnns_lstm_work.html)

- [A Beginner's Guide to Recurrent Networks and LSTMs - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM](https://deeplearning4j.org/lstm.html)

#### Recursive NN

- [用 Recursive Neural Networks 得到分析树 - 简书](https://www.jianshu.com/p/403665b55cd4)

- [Recursive (not Recurrent!) Neural Networks in TensorFlow](https://www.kdnuggets.com/2016/06/recursive-neural-networks-tensorflow.html)

    > In RNNs, at each time step the network takes as input its previous state s(t-1) and its current input x(t) and produces an output y(t) and a new hidden state s(t). TreeNets, on the other hand, don’t have a simple linear structure like that. With RNNs, you can ‘unroll’ the net and think of it as a large feedforward net with inputs x(0), x(1), …, x(T), initial state s(0), and outputs y(0),y(1),…,y(T), with T varying depending on the input data stream, and the weights in each of the cells tied with each other. You can also think of TreeNets by unrolling them – the weights in each branch node are tied with each other, and the weights in each leaf node are tied with each other. The TreeNet illustrated above has different numbers of inputs in the branch nodes. Usually, we just restrict the TreeNet to be a binary tree – each node either has one or two input nodes. There may be different _types_ of branch nodes, but branch nodes of the same type have tied weights.
    > 
    > The advantage of TreeNets is that they can be very powerful in learning hierarchical, tree-like structure.
    > 
    > The disadvantages are, firstly, that the tree structure of every input sample must be known at training time. We will represent the tree structure like this (lisp-like notation):
    >
    > (S (NP that movie) (VP was) (ADJP cool))
    >
    > In each sub-expression, the _type_ of the sub-expression must be given – in this case, we are parsing a sentence, and the type of the sub-expression is simply the part-of-speech (POS) tag.
    >
    > The second disadvantage of TreeNets is that training is hard because the tree structure changes for each training sample and it’s not easy to map training to mini-batches and so on.
    > 
    - [Implementation in TensorFlow - subexpr.py](https://gist.github.com/anj1/504768e05fda49a6e3338e798ae1cddd)


- [machine learning - Recurrent vs Recursive Neural Networks: Which is better for NLP? - Cross Validated](https://stats.stackexchange.com/questions/153599/recurrent-vs-recursive-neural-networks-which-is-better-for-nlp)




### Topics

#### NLP

- [下一步研究目標：盤點NLP領域最具潛力的六大方向 - 幫趣](http://bangqu.com/TTN8wD.html)

- [从文本生成看Seq2Seq模型](https://zhuanlan.zhihu.com/p/29967933)

- [中美两位 AI 大师的“巅峰对话”：为何 NLP 领域难以出现“独角兽”？ | 独家](https://zhuanlan.zhihu.com/p/33970936)

- [deep-learning-with-keras-notebooks/8.0-using-word-embeddings.ipynb at master · erhwenkuo/deep-learning-with-keras-notebooks](https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/8.0-using-word-embeddings.ipynb)

- [Understanding Convolutional Neural Networks for NLP – WildML](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)

- [從零開始的 Sequence to Sequence | 雷德麥的藏書閣](https://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/)

#### Adversiral lstm

- [Recurrent Generative Adversarial Networks?：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/2ttq4g/recurrent_generative_adversarial_networks/)

- [對抗式神經翻譯機 Adversarial Neural Machine Translation | Learning by Hacking](https://data-sci.info/2017/04/30/adversarial-neural-machine-translation/)

#### 英文 word stem 

- [Word stem - Wikiwand](https://www.wikiwand.com/en/Word_stem)
    The stem of the [verb](https://www.wikiwand.com/en/Verb) **wait** is **wait**: it is the part that is common to all its inflected variants.

#### 有毒留言分析

- [rnn_tutorial/toxic_tutorial.ipynb at master · IKMLab/rnn_tutorial](https://github.com/IKMLab/rnn_tutorial/blob/master/toxic_tutorial.ipynb)

#### Word2Vec

- [理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)

- Skip-gram: 給定中間的字來預測上下文
    - [skipgram/Skip-Gram Practice.ipynb at master · IKMLab/skipgram](https://github.com/IKMLab/skipgram/blob/master/Skip-Gram%20Practice.ipynb)

- Con.nuous Bag of Words (CBOW): 給定前後文來預測中間的字


#### 中文斷詞

[中研院 - 中文斷詞系統](http://ckipsvr.iis.sinica.edu.tw/)

#### 情意分析

- [Deep-Learning-MOOC/04-1. 用RNN做情意分析.ipynb at master · yenlung/Deep-Learning-MOOC](https://github.com/yenlung/Deep-Learning-MOOC/blob/master/04-1.%20%E7%94%A8RNN%E5%81%9A%E6%83%85%E6%84%8F%E5%88%86%E6%9E%90.ipynb)

#### 自然語言推論

- [IKMLab/arct: Code for NLITrans at SemEval18 Task12](https://github.com/IKMLab/arct)


#### 無須平行文本之無監督翻譯

- [Notes on Unsupervised Neural Machine Translation](https://zhuanlan.zhihu.com/p/30649985)
    - [《UNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLY》阅读笔记](https://zhuanlan.zhihu.com/p/32375955)
    - [Machine Translation Without the Data – buZZrobot](https://buzzrobot.com/machine-translation-without-the-data-21846fecc4c0)
    - [【Science】無監督式機器翻譯，不需要人類干預和平行文本 - 壹讀](https://read01.com/Rnzx84N.html)
    - [[1711.00043] Unsupervised Machine Translation Using Monolingual Corpora Only](https://arxiv.org/abs/1711.00043)
    - [[1710.11041] Unsupervised Neural Machine Translation](https://arxiv.org/abs/1710.11041)


- [无平行文本照样破解密码，CipherGAN有望提升机器翻译水平](https://zhuanlan.zhihu.com/p/33672256)
    - [[1801.04883] Unsupervised Cipher Cracking Using Discrete GANs](https://arxiv.org/abs/1801.04883)
- [密码学家百年来无法辨认，500年前古怪手稿的加密希伯来语被AI算法破译](https://zhuanlan.zhihu.com/p/34063499)
    - [Decoding Anagrammed Texts Written in an Unknown Language and Script | Hauer | Transactions of the Association for Computational Linguistics](https://transacl.org/ojs/index.php/tacl/article/view/821)

#### 語音合成

##### WaveNet

- [[1712.05884] Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)
    - [谷歌新一代WaveNet ：深度學習怎麼生成語音？ | 2分鐘論文 - 幫趣](http://bangqu.com/3C6864.html)
        在原先Google的WaveNet論文中，我們爲了解決語音合成難題，創造了擴張卷積，這個網絡結構跳躍性地輸入數據，由此使我們我們有了更好的全局視野。這有點像增加我們眼睛的感受野，讓我們能夠感受整個景觀，而不是照片中只有樹的狹窄的視角。
    
        新框架利用梅爾聲譜作爲WaveNet的輸入，這種聲譜是一種基於人類感知的中間媒介，它不僅記錄了不同的單詞如何發音，而且還記錄了預期的音量和語調。

    - [r9y9/wavenet_vocoder: WaveNet vocoder](https://github.com/r9y9/wavenet_vocoder)

- [[1609.03499] WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)
    - [(1) WaveNet by Google DeepMind | Two Minute Papers #93 - YouTube](https://www.youtube.com/watch?v=CqFIVCD1WWo)
    - [WaveNet: A Generative Model for Raw Audio | DeepMind](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
    - [WaveNet launches in the Google Assistant | DeepMind](https://deepmind.com/blog/wavenet-launches-google-assistant/)
    - [DeepMind: WaveNet - A Generative Model for Raw Audio：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/51sr9t/deepmind_wavenet_a_generative_model_for_raw_audio/)
    - [Deepmind 打造語音生成模型 WaveNet，比傳統音檔生成速度快 1000 倍，聲音更擬真？ | TechOrange](https://buzzorange.com/techorange/2017/10/11/deepmind-wavenet-1000-times-faster/)

- [ibab/tensorflow-wavenet: A TensorFlow implementation of DeepMind's WaveNet paper](https://github.com/ibab/tensorflow-wavenet)
- [tomlepaine/fast-wavenet: Speedy Wavenet generation using dynamic programming](https://github.com/tomlepaine/fast-wavenet)
- [buriburisuri/speech-to-text-wavenet: Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition based on DeepMind's WaveNet and tensorflow](https://github.com/buriburisuri/speech-to-text-wavenet)

#### 音樂生成

- [821760408-sp/the-wavenet-pianist: A Wavenet generative model in TensorFlow, trained with Western Classical solo piano canon with global and local conditioning](https://github.com/821760408-sp/the-wavenet-pianist)

- [Can music be generated using generative adversarial networks? - Quora](https://www.quora.com/Can-music-be-generated-using-generative-adversarial-networks)
    > Yes. Just this AAAI, [Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473) combines GANs with Policy gradient (reinforcement learning) to generate music.
    > 
    > The remaining answer might be useful/useless for different audience:
    > 
    > However, unlike images where GANs clearly beat RNNs in generation quality IMO, [\[1612.04357\] Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.04357) vs [openai/pixel-cnn](https://github.com/openai/pixel-cnn) , it appears the other way round in music.
    > 
    > Check out these RNN modifications:
    > 
    > [An Unconditional End-to-End Neural Audio Generation Model](https://arxiv.org/abs/1612.07837)
    > 
    > [soroushmehr/sampleRNN_ICLR2017](https://github.com/soroushmehr/sampleRNN_ICLR2017)
    > 
    > [SampleRNN](https://soundcloud.com/samplernn)
    > 
    > and
    > 
    > [WaveNet: A Generative Model for Raw Audio | DeepMind](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)


- [CodePen - Neural Drum Machine](https://codepen.io/allenyllee/full/ZxGBXP/)
- [CodePen - Deep Roll](https://codepen.io/allenyllee/full/zWGRRg/)

- [Learning to generate lyrics and music with Recurrent Neural Networks](https://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/)

- [`pretty_midi` tutorial](https://nbviewer.jupyter.org/github/craffel/pretty-midi/blob/master/Tutorial.ipynb)

- [MusicVAE: Creating a palette for musical scores with machine learning.](https://magenta.tensorflow.org/music-vae)



#### 股價預測

- [lucko515/tesla-stocks-prediction: The implementation of LSTM in TensorFlow used for the stock prediction.](https://github.com/lucko515/tesla-stocks-prediction)


## Bayesian Neural Networks(BNN)

- [Engineering Uncertainty Estimation in Neural Networks](https://eng.uber.com/neural-networks-uncertainty-estimation/)

- [[1801.07710] Bayesian Neural Networks](https://arxiv.org/abs/1801.07710)

- [What is a Bayesian Neural Network? - Quora](https://www.quora.com/What-is-a-Bayesian-Neural-Network)

    > A Bayesian neural network (BNN) refers to extending standard networks with posterior inference. Standard NN training via optimization is (from a probabilistic perspective) equivalent to maximum likelihood estimation (MLE) for the weights.
    > 
    > For many reasons this is unsatisfactory. One reason is that it lacks proper theoretical justification from a probabilistic perspective: why maximum likelihood? Why just point estimates? Using MLE ignores any uncertainty that we may have in the proper weight values. From a practical standpoint, this type of training is often susceptible to overfitting, as NNs often do.
    > 
    > One partial fix for this is to introduce regularization. From a Bayesian perspective, this is equivalent to inducing priors on the weights (say Gaussian distributions if we are using L2 regularization). Optimization in this case is akin to searching for MAP estimators rather than MLE. Again from a probabilistic perspective, this is not the *right* thing to do, though it certainly works well in practice.
    > 
    > The correct (i.e., theoretically justifiable) thing to do is posterior inference, though this is very challenging both from a modelling and computational point of view. BNNs are neural networks that take this approach. In the past this was all but impossible, and we had to resort to poor approximations such as Laplace’s method (low complexity) or MCMC (long convergence, difficult to diagnose). However, lately there have been some super-interesting results on using variational inference to do this \[1\], and this has sparked a great deal of interest in the area.
    > 
    > BNNs are important in specific settings, especially when we care about uncertainty very much. Some examples of these cases are decision making systems, (relatively) smaller data settings, Bayesian Optimization, model-based reinforcement learning and others.
    > 
    > \[1\] -\[[1505.05424\] Weight Uncertainty in Neural Networks](https://arxiv.org/abs/1505.05424)
    > 
    > 


- [What are the advantages of using a Bayesian neural network - Cross Validated](https://stats.stackexchange.com/questions/141879/what-are-the-advantages-of-using-a-bayesian-neural-network)

    > Bayesian neural nets are useful for solving problems in domains where data is scarce, as a way to prevent overfitting.
    > 
    > They often beat all other methods in such situations. Example applications are molecular biology ([for example this paper](http://bioinformatics.oxfordjournals.org/content/early/2011/07/29/bioinformatics.btr444.full.pdf+html)) and medical diagnosis (areas where data often come from costly and difficult expiremental work).
    > 
    > ---
    > 
    > Let's define our BNN prediction as $\bar{f}(x′|x,t)=∫f(x′,ω)p(ω|x,t)dω$ , where $f$ is the NN function, $x′$ are your inputs, $ω$ are the NN parameters, and $x,t$ are the training inputs and targets. This should be compatible with the syntax used by Neal in the links provided by @forecaster. Then we can calculate a standard deviation of the posterior predictive distribution, which I would naively use as an accuracy on the prediction : 
    > $$
    > \sigma(x′)=\sqrt{∫[f(x′,ω)−\bar{f}(x′|x,t)]^2p(ω|x,t)dω}
    > $$
    > 
    > 


## Reinforcement Learning

### State of the art

### Papers

- [Actor-Critic Algorithms](https://papers.nips.cc/paper/1786-actor-critic-algorithms)
- 

### Articles

- [RL Course by David Silver - Lecture 2: Markov Decision Process - YouTube](https://www.youtube.com/watch?v=lfHX2hHRMVQ&index=2&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
- [强化学习 Reinforcement Learning 教程系列 | 莫烦Python](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/)
- [106 Fall - ADL x MLDS, NTU](https://www.csie.ntu.edu.tw/~yvchen/f106-adl/syllabus.html)

#### DQN

- [DQN从入门到放弃6 DQN的各种改进 - 知乎](https://zhuanlan.zhihu.com/p/21547911)
- 

### Topics

###### tags: `execrise`
- [Deep RL Bootcamp - Labs](https://sites.google.com/view/deep-rl-bootcamp/labs)

#### 下棋
- [Zeta36/chess-alpha-zero: Chess reinforcement learning by AlphaGo Zero methods.](https://github.com/Zeta36/chess-alpha-zero)

#### System ML

- [SysML Conference](http://www.sysml.cc/)



## Computer Vision

### State of the art

### Papers

#### Action and gesture recognition
- [Learning Adaptive Hidden Layers for Mobile Gesture Recognition
](http://cvlab.citi.sinica.edu.tw/ProjectWeb/AHL/#top)

### Articles

#### CNN 架構比較
- [LeNet、AlexNet、GoogLeNet、VGG、ResNetInception-ResNet-v2、FractalNet、DenseNet - 程序园](http://www.voidcn.com/article/p-rewgmeze-bcq.html)

- [VGGNet | 简说](http://simtalk.cn/2016/09/25/VGGNet/)

- [比ResNet更加有效的網路架構 DenseNet: Densely Connected Convolutional Networks | Learning by Hacking](https://data-sci.info/2017/07/26/%E6%AF%94resnet%E6%9B%B4%E5%8A%A0%E6%9C%89%E6%95%88%E7%9A%84%E7%B6%B2%E8%B7%AF%E6%9E%B6%E6%A7%8B-densenet-densely-connected-convolutional-networks/)

- [adl_1103 - 161103_ConvolutionalNN.pdf](https://www.csie.ntu.edu.tw/~yvchen/f105-adl/doc/161103_ConvolutionalNN.pdf)

### Topics

#### Image matching and alignment

###### tags `SIFT`
- [SIFT特征提取分析 - CSDN博客](http://blog.csdn.net/abcjennifer/article/details/7639681)

###### tags `complementary descriptor`

- [shamangary/DeepCD: [ICCV17] DeepCD: Learning Deep Complementary Descriptors for Patch Representations](https://github.com/shamangary/DeepCD)



#### Image Inpainting

- [基于CNN的图像修复（CNN-based Image Inpainting） - CSDN博客](http://blog.csdn.net/YhL_Leo/article/details/56674833)

#### Image style transfer

###### tags: `Gram matrices`

- [[1701.01036] Demystifying Neural Style Transfer](https://arxiv.org/abs/1701.01036)
    the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel


- [Neural Artistic Style Transfer: A Comprehensive Look](https://medium.com/artists-and-machine-intelligence/neural-artistic-style-transfer-a-comprehensive-look-f54d8649c199)


#### Object Detection

- [COCO - Common Objects in Context](http://cocodataset.org/#detections-leaderboard)

- [[1711.07240] MegDet: A Large Mini-Batch Object Detector](https://arxiv.org/abs/1711.07240)

- [政府在看你，2000萬個攝影機時刻監視，中國最大AI獨角獸商湯：這樣更安全｜產業｜科技｜2018-01-17｜即時｜天下雜誌](https://www.cw.com.tw/article/article.action?id=5087696)

###### tags: `RCNN`
- [Selective Search for Object Recognition](https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&bib=all.bib)
- [【目标检测】RCNN算法详解 - CSDN博客](http://blog.csdn.net/shenxiaolu1984/article/details/51066975)
- [RCNN- 将CNN引入目标检测的开山之作 - 知乎](https://zhuanlan.zhihu.com/p/23006190)
- [CS231n Lecture 8 - Localization and Detection - YouTube](https://www.youtube.com/watch?v=_GfPYLNQank&t=574s)

###### tags: `Fast RCNN`
- [【目标检测】Fast RCNN算法详解 - CSDN博客](http://blog.csdn.net/shenxiaolu1984/article/details/51036677)
- [fast-rcnn - fast-rcnn-slides.pdf](http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf)

###### tags: `Faster RCNN`
- [Tutorial: Deep Learning for Objects and Scenes - Part 1 - YouTube](https://www.youtube.com/watch?v=jHv37mKAhV4)
- [目标检测之RCNN，SPP-NET，Fast-RCNN，Faster-RCNN | 冰蓝记录思考的地方](http://lanbing510.info/2017/08/24/RCNN-FastRCNN-FasterRCNN.html)
- [keras版faster-rcnn算法详解（1.RPN计算） - 知乎](https://zhuanlan.zhihu.com/p/28585873)
- [How does the region proposal network (RPN) in Faster R-CNN work? - Quora](https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work)


###### tags: `YOLO`

- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/)

- [[1506.02640] You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
- [[1612.08242] YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242)

- [YOLO 升級到 v3 版，速度相比 RetinaNet 快 3.8 倍 - 幫趣](http://bangqu.com/zLN4cl.html)


#### Image segmentation

###### tags: `U-Net`

- [[1505.04597] U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
- [[1605.06211] Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)


## 新聞

- [深度學習專家很缺嗎？ - EE Times Taiwan 電子工程專輯網](https://www.eettaiwan.com/news/article/20180410NT01-Are-We-Short-of-Deep-Learning-Experts?utm_source=EETT%20Article%20Alert&utm_medium=Email&utm_campaign=2018-04-12)

    > 深度學習「可說是一門跨學科的領域，你不僅要有數學和演算法方面的專家，也需要知道如何在平台上建置軟體的電腦系統專家。然後，我們也會需要熟悉資料管線的專家來調整和管理資料組合。」
    > 





$$E(t,o)=-\sum_j t_j \log o_j$$
