# 深度學習算法1-3_其他

[toc]
<!-- toc --> 


# Motion detection

## QuaterNet

- [[1805.06485] QuaterNet: A Quaternion-based Recurrent Model for Human Motion](https://arxiv.org/abs/1805.06485)

    - [facebookresearch/QuaterNet: Proposes neural networks that can generate animation of virtual characters for different actions.](https://github.com/facebookresearch/QuaterNet)

# Time series

## 太陽黑子預測

- [Time Series Deep Learning, Part 1: Forecasting Sunspots With Keras Stateful LSTM In R](http://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html)
- [Time Series Deep Learning, Part 2: Predicting Sunspot Frequency with Keras LSTM In R](http://www.business-science.io/timeseries-analysis/2018/07/01/keras-lstm-sunspots-part2.html)

## CNN+RNN

- [Combining CNNs and RNNs – Crazy or Genius? - Data Science Central](https://www.datasciencecentral.com/profiles/blogs/combining-cnns-and-rnns-crazy-or-genius)

    > There are things that just don't seem to go together.  Take oil and water for instance.  Both valuable, but try putting them together? 
    > 
    > That was my reaction when I first came across the idea of combining CNNs (convolutional neural nets) and RNNs (recurrent neural nets).  After all they're optimized for completely different problem types.
    > 
    > -   **CNNs** are good with hierarchical or spatial data and extracting unlabeled features. Those could be images or written characters.  CNNs take fixed size inputs and generate fixed size outputs.
    > -   **RNNs** are good at temporal or otherwise sequential data. Could be letters or words in a body of text, stock market data, or speech recognition.  RNNs can input and output arbitrary lengths of data.  LSTMs are a variant of RNNs that allow for controlling how much of prior training data should be remembered, or more appropriately forgotten.
    > 
    > We all know to reach for the appropriate tool based on these very unique problem types.
    > 
    > **So are there problem types that need the capability of both these tools?**
    > 
    > [![](https://api.ning.com/files/2sIBZeVc-HK0kJHviuClGiSw2rukTzwuMt0i33WB69KvR6Xww9TrrLbguqPYL7HmHhwGMQ7FifHWrLjVj5F-8HQO9AjpV0xM/CRFCNNRNNFusion.png?width=250)](https://api.ning.com/files/2sIBZeVc-HK0kJHviuClGiSw2rukTzwuMt0i33WB69KvR6Xww9TrrLbguqPYL7HmHhwGMQ7FifHWrLjVj5F-8HQO9AjpV0xM/CRFCNNRNNFusion.png)
    > As it turns out, yes.  Most of these are readily identified as images that occur in a temporal sequence, in other words video.  But there are some other clever applications not directly related to video that may spark your imagination.  We'll describe several of those below.
    > 
    > There are also several emerging models of how to combine these tools.  In most cases CNNs and RNNs have been married as separate layers with the output of the CNN being used as input to the RNN.  But there are some researchers cleverly combining these two capabilities within a single deep neural net. 
    > 
    > **Video Scene Labeling**
    > 
    > The classical approach to scene labeling is to train a CNN to identify and classify the objects within a frame and perhaps to further classify the objects into a higher level logical group.  For example, the CNN identifies a stove, a refrigerator, a sink, etc. and also up-classifies them as a kitchen.
    > 
    > Clearly the element that's missing is the meaning of the motion over several frames (time).  For example, several frames of a game of pool might correctly say, the shooter sinks the eight ball in the side pocket.  Or several frames of a young person learning to ride a two-wheeler followed by the frame of the rider on the ground, might reasonably be summarized as 'boy falls off bike'.
    > 
    > Researchers have used layered CNN-RNN pairs where the output of the CNN is input to the RNN.  Logically the RNN has also been replaced with LSTMs to create a more 'in the moment' description of each video segment.  Finally there has been some experimentation done with combined RCNNs where the recurrent connection is directly in the kernels as in the diagram above.  See more [*here*](http://proceedings.mlr.press/v32/pinheiro14.pdf).
    > 
    > **Emotion Detection**
    > 
    > Judging the emotion of individuals or groups of individuals from video remains a challenge.  There is an annual competition around this held by the ACM International Conference on Multimodal Interaction known as the EmotiW Grand Challenge.
    > 
    > Each year the target data changes somewhat in nature and typically there are different tests for classifying groups of people versus individuals appearing in videos.
    > 
    > 2016: Group based happiness intensity.
    > 
    > 2017: Group based three class (positive/neutral/negative) emotion detection.
    > 
    > 2018 (scheduled for November) is even more complex.  The challenge will involve classification of eating conditions.  There are three sub-challenges:
    > 
    > 1.  Food-type Sub-Challenge: Perform seven-class food classification per utterance.
    > 2.  Food-likability Sub-Challenge: Recognize the subjects' food likability rating.
    > 3.  Chew and Speak Sub-Challenge: Recognize the level of difficulty to speak while eating.
    > 
    > The key to this challenge is not only the combination of CNNs and RNNs but also the inclusion of an audio track that can be separately modeled and integrated. 
    > 
    > In 2016 the winners created a hybrid network consisting of a RNN and 3D convolutional networks (C3D).  Data fusion and classification takes place late in the process as is traditional.  The RNN takes appearance features extracted by the CNN from individual frames as input and encodes motion later, while C3D models appearance and motion of video simultaneously, subsequently also merged with the audio module. 
    > 
    > Accuracy in this very difficult field still isn't great.  The 2016 winners scored 59.02% on the individual faces.  By 2017 the individual face scores were up to 60.34% and the group scores were up to 80.89% -- keeping in mind that the nature of the challenge changes each year so year-over-year comparison isn't possible.  More on this annual challenge [*here*](https://icmi.acm.org/2018/index.php?id=challenges).
    > 
    > **Video Based Person Re-identification / Gait Recognition**
    > 
    > The goal here is to identify a person when seen in video (from a database of existing labeled individuals) or simply to recognize if this person has been seen before (re-identification -- not labeled).  The dominant line of research has been in gait recognition and the evolving field uses full body motion recognition (arm swing, limps, carriage, etc.).
    > 
    > [![](https://api.ning.com/files/2sIBZeVc-HIN4Zq6aiFj2hMBNjX9p1WRwf*iIu76zefF1t7uVKzPlFe0mCvzccJfMFTiid6wDjUmS-8PpSPbwf9M1mSN59SV/fullbodyrecognition.png?width=250)](https://api.ning.com/files/2sIBZeVc-HIN4Zq6aiFj2hMBNjX9p1WRwf*iIu76zefF1t7uVKzPlFe0mCvzccJfMFTiid6wDjUmS-8PpSPbwf9M1mSN59SV/fullbodyrecognition.png)There are some evident non-technical challenges here the most obvious being change in clothing, shoes, partial obscurity from coats or packages, and the like.  The technical problems well known in CNN are multiple viewpoints (in fact multiple view points as a single person passes say from right to left offering first front, then side, then back views) and the classical image problems of lighting, albedo, and size.
    > 
    > Prior efforts were based on combining several frames of CNN derived data representing one full step (gait) into a type of heat map called a Gait Energy Image (GEI).
    > 
    > Addition of an LSTM allowed several 'steps' to be analyzed together and the time series capability of the LSTM works as a frame-to-frame view transformation models to adjust for perspective.
    > 
    > This study including the image can be found [*here*](https://vision.unipv.it/CV/materiale2016-17/3rd%20Choice/0213.pdf).  Not surprisingly given the application to surveillance, gait recognition has the highest number of cited research papers, almost all of which are being conducted in China.
    > 
    > Full human pose recognition, both for recognition and for labeling (the person is standing, jumping, sitting, etc.) is the next frontier with separate convolutional models for each body part.  Gesture recognition as part of a UI especially in augmented reality is becoming a hot topic.
    > 
    >  [![](https://api.ning.com/files/2sIBZeVc-HJLKZxxKFxF0FHTQNKo9okXYyjUAsPzCPmN15kI3sUgbioJgfdzyrcYKmYMxBBUY2t7oz6oFdPVOgwsJ0ydKVNt/gesturerecognition.png?width=500)](https://api.ning.com/files/2sIBZeVc-HJLKZxxKFxF0FHTQNKo9okXYyjUAsPzCPmN15kI3sUgbioJgfdzyrcYKmYMxBBUY2t7oz6oFdPVOgwsJ0ydKVNt/gesturerecognition.png)
    > 
    > **Weather Prediction**
    > 
    > The objective is to predict the intensity of rainfall over a localized region and over a fairly short time span.  This field is known as 'nowcasting'.
    > 
    > **Quantifying the Functions of DNA Sequences**
    > 
    > Some 98% of human DNA is non-coding, known as Introns.  Originally thought to be evolutionary leftovers with no value, geneticists now know that for example 93% of disease associated variants lie in these regions.  Modeling properties and functions of these regions is an ongoing challenge recently made somewhat easier by a combination CNN/LSTM called DanQ.
    > 
    > According to the developers "the convolution layer captures regulatory motifs, while the recurrent layer captures long-term dependencies between the motifs in order to learn a regulatory 'grammar' to improve predictions. DanQ improves considerably upon other models across several metrics. For some regulatory markers, DanQ can achieve over a 50% relative improvement in the area under the precision-recall curve metric compared to related models."  The study is [*here*](https://academic.oup.com/nar/article/44/11/e107/2468300).
    > 
    > **Creating Realistic Sound Tracks for Silent Videos**
    > 
    > MIT researchers created an extensive collection of labeled sound clips of drumsticks hitting pretty much everything they could think of.  Using a combined CNN/LSTM, the CNN identifies the visual context (what the drumstick is hitting in the silent video) but since the sound clip is temporal and extends over several frames, the LSTM layer is used to match the sound clip to the appropriate frames.
    > 
    > The developers report that humans were fooled by the predicted sound match more than 50% of the time.  See video [*here*](https://www.youtube.com/watch?t=0s&v=flOevlA9RyQ).
    > 
    > **Future Direction**
    > 
    > I was surprised to find such a wealth of examples in which researchers combine CNNs and RNNs to gain the advantages of both.  There are even some [*studies utilizing GANs*](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.pdf) in hybrid networks that are quite interesting.
    > 
    > However, while these mashups seem to provide additional capabilities there is another, newer and perhaps more prominent line of research that says that CNNs alone can do the job and that the days of RNNs/LSTMs are numbered.
    > 
    > One group of researchers used [*a novel architecture of deep forests*](https://www.datasciencecentral.com/profiles/blogs/off-the-beaten-path-using-deep-forests-to-outperform-cnns-and-rnn) embedded in a node structure to outperform CNNs and RNNs, with significant savings in compute and complexity.
    > 
    > We also reviewed the more mainstream movement by both Facebook and Google who recently abandoned their RNN/LSTM based tools for speech-to-speech translation in favor of [*Temporal Convolutional Nets (TCNs)*](https://www.datasciencecentral.com/profiles/blogs/temporal-convolutional-nets-tcns-take-over-from-rnns-for-nlp-pred). 
    > 
    > Especially in text problems, but more generally in any time series problem RNNs have an inherent design problem.  Because they read and interpret the input text one word (or character or image) at a time, the deep neural network must wait to process the next word until the current word processing is complete. 
    > 
    > This means that RNNs cannot take advantage of massive parallel processing (MPP) in the same way the CNNs can.  Especially true when the RNN/LSTMs are running both ways at once to better understand context. 
    > 
    > This is a barrier that won't go away and seems to place an absolute limit on the utility of RNN/LSTM architecture.  Temporal Convolutional Neural Nets get around this by using CNN architecture which can easily use MPP acceleration with the emerging concepts of attention and gate hopping.  For more detail [*please see our original article*](https://www.datasciencecentral.com/profiles/blogs/temporal-convolutional-nets-tcns-take-over-from-rnns-for-nlp-pred).
    > 
    > I'm certainly not one to write off an entire line of research, especially where the need for minimum latency is not as severe as speech-to-speech translation.  However, all of these problems described above do seem ripe for a repeated examination using this newer methodology of TCNs.
    > 
    > [*Other articles by Bill Vorhies.*](https://www.datasciencecentral.com/profiles/blog/list?user=0h5qapp2gbuf8)
    > 


## TCN

- [Temporal Convolutional Nets (TCNs) Take Over from RNNs for NLP Predictions - Data Science Central](https://www.datasciencecentral.com/profiles/blogs/temporal-convolutional-nets-tcns-take-over-from-rnns-for-nlp-pred)

    > It's only been since 2014 or 2015 when our DNN-powered applications passed the 95% accuracy point on text and speech recognition allowing for whole generations of chatbots, personal assistants, and instant translators.
    > 
    > Convolutional Neural Nets (CNNs) are the acknowledged workhorse of image and video recognition while Recurrent Neural Nets (RNNs) became the same for all things language.
    > 
    > One of the key differences is that CNNs can recognize features in static images (or video when considered one frame at a time) while RNNs exceled at text and speech which were recognized as sequence or time-dependent problems.  That is where the next predicted character or word or phrase depends on those that came before (left-to-right) introducing the concept of time and therefore sequence.
    > 
    > Actually RNNs are good at all types of sequence problems, including speech/text recognition, language-to-language translation, handwriting recognition, sequence data analysis (forecasting), and even automatic code generation in many different configurations.
    > 
    >  [![](https://api.ning.com/files/iIlSVBG0bp*u5kZnSvL5xoCLJw960*axGU6QL0FMjjbTKTe*khoLLvb12qFTL2zQgGK5VwFgNY8qnQEVk50A6CvZxmwBRsxt/rnndiagrams.jpeg?width=500)](https://api.ning.com/files/iIlSVBG0bp*u5kZnSvL5xoCLJw960*axGU6QL0FMjjbTKTe*khoLLvb12qFTL2zQgGK5VwFgNY8qnQEVk50A6CvZxmwBRsxt/rnndiagrams.jpeg)
    > 
    > In a very short period of time, major improvements to RNNs became dominant including LSTM (long short term memory) and GRU (gated recurring units) both of which improved the span over which RNNs could remember and incorporate data far from the immediate text into its meaning.
    > 
    > **Solving the 'Not' Joke Problem**
    > 
    > With RNNs reading characters or words from left-to-right in time, context became a problem.  For example, in trying to predict the sentiment of a review, the first few comments might be positive (e.g. good food, good atmosphere) but might end with several negative comments (e.g. terrible service, high price) that might ultimately mean the review was negative.  This is the logical equivalent of the 'Not' joke.  'That's a great looking tie. NOT!'.
    > 
    > The solution was to read the text in both directions at once by having two LSTM encoder working at once (bi-directional encoders).  This meant having information from the future (further down the text) leaking into the present but it largely solved the problem.  Accuracy improved.
    > 
    > **Facebook and Google Have a Problem**
    > 
    > Early on when Facebook and Google were launching their automatic language translators, they realized they had a problem.  The translations were taking too long.
    > 
    > RNNs have an inherent design problem.  Because they read and interpret the input text one word (or character) at a time, the deep neural network must wait to process the next word until the current word processing is complete. 
    > 
    > This means that RNNs cannot take advantage of massive parallel processing (MPP) in the same way the CNNs can.  Especially true when the RNN/LSTMs are running both ways at once to better understand context. 
    > 
    > It also means they are very compute intensive since all the intermediate results must be stored until the processing of the entire phrase is complete.
    > 
    > In early 2017 both Google and Facebook came to similar solutions, a way to use CNNs with their MPP capability to process text for language-to-language translation.  In CNNs the computation doesn't rely on information from the previous time step, freeing each computation to be conducted separately with massive parallelization.
    > 
    > Google's solution is called ByteNet, Facebook's is FairSeq (named after their internal Facebook Artificial Intelligence Research (FAIR) team.  FairSeq is available on GitHub.
    > 
    > Facebook says their FairSeq net runs 9X faster than their RNN benchmark.
    > 
    > **How Does It Work -- The Basics**
    > 
    > Thinking about CNNs and how they process images as a 2D 'patch' (height and width), making the jump to text requires only that you think of text as a 1D object (1 unit high and *n* units long). 
    > 
    > But while RNNs do not directly predefine object length, CNNs do so by definition.  So using CNNs requires us to add more layers until the entire receptive field is covered.  This can and does result in very deep CNNs, but with the advantage being, no matter how deep, each can be processed separately in parallel for the huge time savings.
    > 
    > **The Special Sauce: Gating + Hopping = Attention**
    > 
    > Of course it's not quite as simple as that and both the Google and Facebook techniques add something called an 'Attention' function.
    > 
    > The original Attention function appears to have been introduced last year by researchers at Google Brain and the University of Toronto under the name Transformer.  [*Read the original*](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) research paper here.
    > 
    > By the time the basically identical function was adopted by both Facebook and Google it had become the 'Attention' function which has two unique features.
    > 
    > The first is identified by Facebook as 'multi-hoping'.  Rather than the traditional RNN approach of looking at the sentence only once, multi-hoping allows the system to take multiple 'glimpses' of the sentence in a manner more like human translators. 
    > 
    > Each glimpse might focus on a noun or verb, not necessarily in sequence, which offers the most insight into meaning for that pass.  Glimpses might be independent or might be dependent on the previous look to then focus on a related adjective, adverb, or auxiliary verb.
    > 
    > [![](https://api.ning.com/files/iIlSVBG0bp9*sLbkNQbbly2DmvOsRMirzdj5nhO67J1SmbrWC-KBffhPyE87*Juq2RtNFZg7gdm1OdCxiwpYmaw-yBKaJ61n/Facebookhopping.png?width=350)](https://api.ning.com/files/iIlSVBG0bp9*sLbkNQbbly2DmvOsRMirzdj5nhO67J1SmbrWC-KBffhPyE87*Juq2RtNFZg7gdm1OdCxiwpYmaw-yBKaJ61n/Facebookhopping.png)Facebook offers this illustration to show the first pass of the French-to-English translation is a single pass to encode each French word, then the multi-hoping routine of the decoder to select the most appropriate English translation.
    > 
    > The second feature is gating which controls the information flow between the hidden layers.  Gating determines what information best produces the next word by essentially allowing the CNN to zoom in or out on the translation in process to achieve the best context for making the next decision.
    > 
    > **Beyond Language Translation -- Temporal Convolutional Nets (TCNs)**
    > 
    > By mid-2017 Facebook and Google had solved the problem of speed of translation by using CNNs combined with the attention function.  The larger question however was this technique good for more than just speeding up translation.  Should we be looking at all the problem types previously assumed to be solvable only with RNNs?  And the answer of course is yes.
    > 
    > There were a number of studies published in 2017; some coming out literally at the same time that Facebook and Google published.  One which does a good job of covering the broader question of what's beyond translation is "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling" by Shaojie Bai, J. Zico Kolter, and Vladlen Koltun ([*original here*](https://arxiv.org/pdf/1803.01271.pdf)). 
    > 
    > These are the folks who have labeled this new architecture **Temporal Convolutional Nets (TCNs)** though it's possible that may change as the industry continues to implement them.
    > 
    > Their study is a series of head-to-head benchmark competitions of TCNs versus RNNs, LSTMs, and GRUs across 11 different industry standard RNN problems well beyond language-to-language translation. 
    > 
    > Their conclusion: TCNs are not only faster but also produced greater accuracy in 9 cases and tied in one *(bold figures in the table below drawn from the original study).* [![](https://api.ning.com/files/iIlSVBG0bp-rJSpXg2IE30wWWGbot5GDeBw8WMEgz7zvzWDSQOuqYUHC83KhEvFNgB6mmDPwrN2DYesV4vdgSgX*NsbaXdzo/TCNbenchmarks.png?width=400)](https://api.ning.com/files/iIlSVBG0bp-rJSpXg2IE30wWWGbot5GDeBw8WMEgz7zvzWDSQOuqYUHC83KhEvFNgB6mmDPwrN2DYesV4vdgSgX*NsbaXdzo/TCNbenchmarks.png)
    > 
    > **TCN Advantages / Disadvantages**
    > 
    > Shaojie Bai, J. Zico Kolter, and Vladlen Koltun also provide this useful list of advantages and disadvantages of TCNs.
    > 
    > -   Speed is important. Faster networks shorten the feedback cycle.  The massive parallelism available with TCNs shortens both training and evaluation cycles.
    > -   TCNs offer more flexibility in changing its receptive field size, principally by stacking more convolutional layers, using larger dilation factors, or increasing filter size. This offers better control of the model's memory size.
    > -   TCNs have a backpropagation path different from the temporal direction of the sequence. This avoids the problem of exploding or vanishing gradients which are a major issue with RNNs.
    > -   Lower memory requirement for training, especially in the case of long input sequences.
    > 
    > However, the researchers note that TCNs may not be as easy to adapt to transfer learning as regular CNN applications because different domains can have different requirements on the amount of history the model needs in order to predict. Therefore, when transferring a model from a domain where only little memory is needed to a domain where much longer memory is required, the TCN may perform poorly for not having a sufficiently large receptive field.
    > 
    > **Going Forward**
    > 
    > TCNs have been implemented in major applications with significant benefits that appear to apply across all types of sequence problems.  We'll need to rethink our assumption that sequence problems are exclusively the domain of RNNs and begin to think of TCNs as the natural starting point for our future projects.


# Machine Learning Sorting

## 用機器學習構建O(N)複雜度的排序算法

- [用機器學習構建O(N)複雜度的排序算法，可在GPU和TPU上加速計算 - 幫趣](http://bangqu.com/6vV731.html)


# Neural ODE

## [1806.07366] Neural Ordinary Differential Equations

- [[1806.07366] Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)


- [JSeam2/Neural-Ordinary-Differential-Equations: Sample implementation of Neural Ordinary Differential Equations](https://github.com/JSeam2/Neural-Ordinary-Differential-Equations)

- [A radical new neural network design could overcome big challenges in AI - MIT Technology Review](https://www.technologyreview.com/s/612561/a-radical-new-neural-network-design-could-overcome-big-challenges-in-ai/?fbclid=IwAR3kSDP18XmWQhc5-eEuq0133DT4FIRrhdmOv3cjpcMXC9lKwLKNExqQZA0)




- [Neural Ordinary Differential Equation - 知乎](https://zhuanlan.zhihu.com/p/51514687)


    > 今天给大家介绍一下刚刚拿到NIPS2018 best paper的多伦多大学做的Neural ODE的想法
    > 
    > ***Chen, Tian Qi, et al. "Neural Ordinary Differential Equations."arXiv preprint arXiv:1806.07366(2018).***
    > 
    > ***新闻报道：*** [NeurIPS 2018最佳论文出炉：UT陈天琦、华为上榜](http://link.zhihu.com/?target=https%3A//baijiahao.baidu.com/s%3Fid%3D1618880815687732300%26wfr%3Dspider%26for%3Dpc)
    > 
    > 下面我整理一下这个方向------把ode和神经网络建立联系的一系列工作的的脉络
    > 
    > 这里我都放了arxiv的发表时间让大家自行justify contribution
    > 
    > 开始之前我先安利一下自己的观点
    > 
    > 图像处理有三类最成功的算法【nonlocal更多是一个idea我就不放进来了】
    > 
    > 微分方程
    > 
    > 压缩感知、稀疏表示
    > 
    > 深度学习
    > 
    > 每个的出发点不一样，但是有着联系，我们可以从一块中找到idea promote另一块
    > 
    > 这篇blog主要讲微分方程和深度学习的关系
    > 
    > 另外两种联系的参考文献可以看我主页的slide：[http://about.2prime.cn/slide/dynamicNN_slide.pdf](http://link.zhihu.com/?target=http%3A//about.2prime.cn/slide/dynamicNN_slide.pdf)
    > 
    > 在开始之前安利一下
    > 
    > 这方面【利用深度学习和数值微分方程结合】最早的文章我觉得应该是
    > 
    > Chen, Yunjin, and Thomas Pock. "Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration."*IEEE transactions on pattern analysis and machine intelligence*39.6 (2017): 1256-1272.
    > 
    > 这篇文章是CVPR2015 oral也是去燥的benchmark算法之一的TNRD
    > 
    > 从learn pde出发设计网络，独立于resnet 提出了residual结构【惨啊。。。做一个分类就能多多少引用【x】】
    > 
    > 【off topic一下下】
    > 
    > 在学习pde处理图像也有很多
    > 
    > 朱松纯老师97年就提出的Prior learning and Gibbs reaction-diffusion【learning一个variation pde
    > 
    > gilboa 2004年的Estimation of Optimal PDE-based Denoising in the SNR Sense【主要优化stopping time和正则项系数
    > 
    > 林宙辰老师eccv2010年的Learning PDEs for Image Restoration via Optimal Control【learn的更general
    > 
    > 2007年lipson 在pnas上也有通过学习自动发现pde的工作Automated reverse engineering of nonlinear dynamical systems.
    > 
    > 【我们回来】
    > 
    > 我们在深度学习下也对深度学习没有关注doffusion停止时间【翻译成深度学习的话就是深度】的问题进行了修正
    > 
    > Xiaoshuai Zhang*, Yiping Lu*, Jiaying Liu, Bin Dong. "Dynamically Unfolding Recurrent Restorer: A Moving Endpoint Control Method for Image Restoration" preprint(*equal contribution)
    > 
    > 我们最后学出来一个类似filter的东西 可以处理所有噪声级别的图片
    > 
    > 我们从简单 resnet我可以写成
    > 
    > ![X_{n+1}-X_n=f(X_n)](https://www.zhihu.com/equation?tex=X_%7Bn%2B1%7D-X_n%3Df%28X_n%29)
    > 
    > 这个左边项把n理解成artificial time的话可以看成求导
    > 
    > ![\dot X=f(X)](https://www.zhihu.com/equation?tex=%5Cdot+X%3Df%28X%29)
    > 
    > 所以我们把resnet理解成求解ODE
    > 
    > 这个想法最早在鄂老师的一个proposal里提到了
    > 
    > Weinan, E. "A proposal on machine learning via dynamical systems."*Communications in Mathematics and Statistics*5.1 (2017): 1-11.
    > 
    > 我们相近时间的一个工作
    > 
    > Lu, Yiping, et al. "Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations."*arXiv preprint arXiv:1710.10121*(2017).【发表icml2018】
    > 
    > 把这个观点推广到了更多的网络
    > 
    > 我们把网络都理解成对一个ODE的离散化，在我们的paper里对各种net都给了ode的解释
    > 
    > ResNet- ODE的前向欧拉格式
    > 
    > PolyNet- ODE的反向欧拉格式的逼近
    > 
    > FractalNet-ODE的Runge-Kutta 格式
    > 
    > 【格式的意思是用一个离散的序列逼近一个连续的动力系统】
    > 
    > 上面的理解的理论上也有保证，有人利用TL_p空间下对数值格式证明gamma收敛的手段给出了分析
    > 
    > Thorpe, Matthew, and Yves van Gennip. "Deep Limits of Residual Neural Networks."*arXiv preprint arXiv:1810.11741*(2018).
    > 
    > 这样ode方面的稳定性分析也可以对神经网络做了
    > 
    > Zhang, Linan, and Hayden Schaeffer. "Forward Stability of ResNet and Its Variants."*arXiv preprint arXiv:1811.09885*(2018).
    > 
    > 同时针对deep learning setting，鄂老师团队把optimal control 修改成了mean field version以期望从中解决deep learning generalization的问题
    > 
    > Han, Jiequn, and Qianxiao Li. "A mean-field optimal control formulation of deep learning."*arXiv preprint arXiv:1807.01083*(2018).
    > 
    > 我们文章的另一个贡献是把dropout的分析也放进了这个框架
    > 
    > 我们发现各种dropout都可以理解为一个随机微分方程的离散化
    > 
    > 我们的神经网络建模多了一个随机的布朗运动
    > 
    > ![\dot X=f(X_t)dt+g(X_t)dW_t](https://www.zhihu.com/equation?tex=%5Cdot+X%3Df%28X_t%29dt%2Bg%28X_t%29dW_t)
    > 
    > 更加神奇的是我们用数值格式收敛推荐推导出来了stochastic depth深度变深要概率趋于1/2和他们实验一致
    > 
    > 杜强老师把nonlocal PDE与nonlocal nn建立了联系后，提出了一种更加稳定的nonlocal block在分类任务上得到了更好的效果
    > 
    > //因为我们做数学的没有卡，所以如果有人试试看这种block在视频有没有boost也很有意思
    > 
    > Tao, Yunzhe, et al. "Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling."*arXiv preprint arXiv:1806.00681*(2018).
    > 
    > 【发表在今年nips】
    > 
    > 基于上面的观察
    > 
    > 我们发现没有人在深度学习运用多步法的格式，也就是说大家都在考虑对右端项的逼近
    > 
    > 所以我们文章采用了多步法，从另一个维度我们换了逼近 ![\dot X](https://www.zhihu.com/equation?tex=%5Cdot+X) ,用了线性多步法也就是我们的网络架构是
    > 
    > ![X_n = kX_{n-1}+(1-k)X_{n-2}+f(X_{n-1})](https://www.zhihu.com/equation?tex=X_n+%3D+kX_%7Bn-1%7D%2B%281-k%29X_%7Bn-2%7D%2Bf%28X_%7Bn-1%7D%29)
    > 
    > 就这么每层引入一个参数就提升了网络效果，我们56层网络效果和110层的resnet效果一样
    > 
    > 特别像强调的是请大家看下我们""Explanation on the performance boost via modified equations.""这一部分，这一部分的release了一个很high level的idea
    > 
    > 我之前也写过文章，优化也可以用ode来看
    > 
    > [优化算法新观点](https://zhuanlan.zhihu.com/p/33563623)
    > 
    > 我们想强调步长->0来看可能很多时候是无法理解加速效果的，因为在极限意义下逼近的动力系统和离散的会有gap，有时候是有好处的【比如我们的例子】，有时候是有坏处的【推荐一篇最近的这个方向文章我很喜欢，有时间我介绍下[Direct Runge-Kutta Discretization Achieves Acceleration](http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1805.00521)】
    > 
    > 我们的分析手段是所谓的修正方程
    > 
    > 同样还有在方程右端项下功夫设计的
    > 
    > Chang, Bo, et al. "Reversible architectures for arbitrarily deep residual neural networks."*arXiv preprint arXiv:1709.03698*(2017).
    > 
    > 我们还反过来用神经网络去发现数据underlying的物理规律
    > 
    > Long, Zichao, et al. "PDE-Net: Learning PDEs from Data."*arXiv preprint arXiv:1710.09668*(2017).
    > 
    > 我们工作里发现卷积核和逼近微分算子 ![u_x](https://www.zhihu.com/equation?tex=u_x) 数值格式很类似【这个也是传统cv里面边缘提取算子的想法】，给了卷积和数值格式的联系，我们的好处是
    > 
    > -   我们不希望是black box model我们可以知道我们的网络在simulate什么pde【有些应用肯定是不希望用black box的】
    > -   同时不想传统pde，又慢又没有精度
    > 
    > 而且做了data driven的格式有很多很惊人的地方，比如用很简单的差分格式就handle了highly nonlinear的equation，我觉得之前我都不敢想象
    > 
    > 同时我们的learning problem可以刻画成了一个控制问题
    > 
    > 控制可以理解成限制优化问题，就是约束条件是一个ode
    > 
    > 我们可以利用控制中的算法进行求解
    > 
    > Li, Qianxiao, et al. "Maximum principle based algorithms for deep learning."*The Journal of Machine Learning Research*18.1 (2017): 5998-6026.
    > 
    > 在他们文章中有趣的是bp算法是他们的特例【一个maximal principle改成梯度上升】
    > 
    > 但是bp算法没法解决的量化神经网络缺可以被他们的算法解决
    > 
    > Li, Qianxiao, and Shuji Hao. "An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks."*arXiv preprint arXiv:1803.01299*(2018).【发表icml2018】
    > 
    > 同样有用multigrid算法来解的
    > 
    > Chang, Bo, et al. "Multi-level residual networks from dynamical systems view."*arXiv preprint arXiv:1710.10348*(2017).
    > 
    > 最近火起来的就是这个观点在generative models里的应用
    > 
    > Chen, Tian Qi, et al. "Neural Ordinary Differential Equations."*arXiv preprint arXiv:1806.07366*(2018).
    > 
    > 【ps作者iclr有一个更新版也很有意思 安利一下】
    > 
    > 这篇文章第一次把这个观点用到了normalizing flow
    > 
    > 把以前O(n^3)的计算量降低到O(n)，代价是引入了一个ode求解器
    > 
    > Zhang, Linfeng, and Lei Wang. "Monge-Amp\ere Flow for Generative Modeling."*arXiv preprint arXiv:1809.10188*(2018).
    > 
    > 这篇则是从optimal transport角度出发
    > 
    > optimal transport也是一个control problem，这个就比较深刻。。。以后有机会写文章介绍下
    > 
    > 这篇文章很有意思，这篇文章我的理解其实更多是ising model的一个快速采样算法
    > 
    > 类似还有和 lddmm【一个医疗图像配准模型】结合的normalizing flow算法
    > 
    > Salman, Hadi, et al. "Deep Diffeomorphic Normalizing Flows."*arXiv preprint arXiv:1810.03256*(2018).
    > 


# Solve ODE/PDE

## 貌离神合的RNN与ODE：花式RNN简介
- [貌离神合的RNN与ODE：花式RNN简介 | 机器之心](https://www.jiqizhixin.com/articles/2018-06-29-3)

    > 本来笔者已经决心不玩 RNN 了，但是在上个星期思考时忽然意识到 **RNN 实际上对应了 ODE（常微分方程）的数值解法**，这为我一直以来想做的事情------用深度学习来解决一些纯数学问题------提供了思路。事实上这是一个颇为有趣和有用的结果，遂介绍一翻。顺便地，**本文也涉及到了自己动手编写 RNN 的内容，所以本文也可以作为编写自定义的 RNN 层的一个简单教程**。
    > 
    > 注：本文并非前段时间的热点"**神经 ODE** [1]"的介绍（但有一定的联系）。
    > 
    > ### **RNN基本**
    > 
    > **什么是RNN？** 
    > 
    > 众所周知，RNN 是"循环神经网络（Recurrent Neural Network）"，跟 CNN 不同，RNN 可以说是一类模型的总称，而并非单个模型。简单来讲，只要是输入向量序列 (x1,x2,...,xT)，输出另外一个向量序列 (y1,y2,...,yT)，并且满足如下递归关系的模型，都可以称为 RNN。
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/6276c75e-1c16-470c-b701-bbbf404997d8/1530250675445.png)
    > 
    > 也正因为如此，原始的朴素 RNN，还有改进的如 GRU、LSTM、SRU 等模型，我们都称为 RNN，因为它们都可以作为上式的一个特例。还有一些看上去与 RNN 没关的内容，比如前不久介绍的 [CRF 的分母的计算](http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247489378&idx=1&sn=0e0ed4424bb336022f36d8e2236f96cc&chksm=96e9c8e2a19e41f4d1fb67254ee3c057ce66a4eaa4084db89d53f314c833b73fb79b8ee3c0dd&scene=21#wechat_redirect)，实际上也是一个简单的 RNN。
    > 
    > 说白了，**RNN 其实就是递归计算**。
    > 
    > **自己编写RNN**
    > 
    > 这里我们先介绍如何用 Keras 简单快捷地编写自定义的 RNN。 
    > 
    > 事实上，不管在 Keras 还是纯 tensorflow 中，要自定义自己的 RNN 都不算复杂。在 Keras 中，只要写出每一步的递归函数；而在 tensorflow 中，则稍微复杂一点，需要将每一步的递归函数封装为一个 RNNCell 类。
    > 
    > 下面介绍用 Keras 实现最基本的一个 RNN：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/11602cf4-69f3-432d-85d6-f9f9eb042a77/1530250675563.png)
    > 
    > 代码非常简单：
    > 
    > ```
    > #! -*- coding: utf-8- -*-
    > 
    > from keras.layers import Layer
    > import keras.backend as K
    > 
    > class My_RNN(Layer):
    > 
    >     def __init__(self, output_dim, **kwargs):
    >         self.output_dim = output_dim # 输出维度
    >         super(My_RNN, self).__init__(**kwargs)
    > 
    >     def build(self, input_shape): # 定义可训练参数
    >         self.kernel1 = self.add_weight(name='kernel1',
    >                                       shape=(self.output_dim, self.output_dim),
    >                                       initializer='glorot_normal',
    >                                       trainable=True)
    >         self.kernel2 = self.add_weight(name='kernel2',
    >                                       shape=(input_shape[-1], self.output_dim),
    >                                       initializer='glorot_normal',
    >                                       trainable=True)
    >         self.bias = self.add_weight(name='kernel',
    >                                       shape=(self.output_dim,),
    >                                       initializer='glorot_normal',
    >                                       trainable=True)
    > 
    >     def step_do(self, step_in, states): # 定义每一步的迭代
    >         step_out = K.tanh(K.dot(states[0], self.kernel1) +
    >                           K.dot(step_in, self.kernel2) +
    >                           self.bias)
    >         return step_out, [step_out]
    > 
    >     def call(self, inputs): # 定义正式执行的函数
    >         init_states = [K.zeros((K.shape(inputs)[0],
    >                                 self.output_dim)
    >                               )] # 定义初始态(全零)
    >         outputs = K.rnn(self.step_do, inputs, init_states) # 循环执行step_do函数
    >         return outputs[0] # outputs是一个tuple，outputs[0]为最后时刻的输出，
    >                           # outputs[1]为整个输出的时间序列，output[2]是一个list，
    >                           # 是中间的隐藏状态。
    > 
    >     def compute_output_shape(self, input_shape):
    >         return (input_shape[0], self.output_dim)
    > ```
    > 
    > 可以看到，虽然代码行数不少，但大部分都只是固定格式的语句，**真正定义 RNN 的，是 step_do 这个函数**，这个函数接受两个输入：step_in 和 states。其中 step_in 是一个 (batch_size, input_dim) 的张量，代表当前时刻的样本 xt，而 states 是一个 list，代表 yt-1 及一些中间变量。
    > 
    > 特别要注意的是，states 是一个张量的 list，而不是单个张量，这是因为在递归过程中可能要同时传递多个中间变量，而不仅仅是 yt-1 一个，比如 LSTM 就需要有两个态张量。最后 step_do 要返回 yt 和新的 states，这是 step_do 这步的函数的编写规范。 
    > 
    > 而 K.rnn 这个函数，接受三个基本参数（还有其他参数，请自行看官方文档），其中第一个参数就是刚才写好的 step_do 函数，第二个参数则是输入的时间序列，第三个是初始态，跟前面说的 states 一致，所以很自然 init_states 也是一个张量的 list，默认情况下我们会选择全零初始化。
    > 
    > ### **ODE基本**
    > 
    > **什么是ODE？**
    > 
    > ODE 就是"常微分方程（Ordinary Differential Equation）"，这里指的是一般的常微分方程组：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/732c5c26-f62b-4176-8670-9203f0fa9a70/1530250675511.png)
    > 
    > 研究 ODE 的领域通常也直接称为"动力学"、"动力系统"，这是因为牛顿力学通常也就只是一组 ODE 而已。
    > 
    > ODE可以产生非常丰富的函数。比如 e^t 其实就是 x˙=x 的解，sin*t* 和 cos*t* 都是 x¨+x=0 的解（初始条件不同）。事实上，我记得确实有一些教程是直接通过微分方程 x˙=x 来定义 e^t 函数的。除了这些初等函数，很多我们能叫得上名字但不知道是什么鬼的特殊函数，都是通过 ODE 导出来的，比如超几何函数、勒让德函数、贝塞尔函数...
    > 
    > 总之，ODE 能产生并且已经产生了各种各样千奇百怪的函数～
    > 
    > **数值解ODE **
    > 
    > 能精确求出解析解的 ODE 其实是非常少的，所以很多时候我们都需要数值解法。 
    > 
    > ODE 的数值解已经是一门非常成熟的学科了，这里我们也不多做介绍，仅引入最基本的由数学家欧拉提出来的迭代公式：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/f7a872ae-d5d0-4b8e-8c2a-7af4b6e4d4d4/1530250675616.png)
    > 
    > 这里的 h 是步长。欧拉的解法来源很简单，就是用：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/c8c6e592-8adf-4afe-abea-5c11b8550372/1530250675721.png)
    > 
    > 来近似导数项 x˙(t)。只要给定初始条件 x(0)，我们就可以根据 (4) 一步步迭代算出每个时间点的结果。
    > 
    > ### **ODE与RNN **
    > 
    > **ODE也是RNN**
    > 
    > 大家仔细对比 (4) 和 (1)，发现有什么联系了吗？
    > 
    > 在 (1) 中，t 是一个整数变量，在 (4) 中，t 是一个浮点变量，除此之外，(4) 跟 (1) 貌似就没有什么明显的区别了。事实上，在 (4) 中我们可以以 h 为时间单位，记 t=nh，那么 (4) 变成了：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/2db05569-5ac0-4142-b25e-ead77922fe14/1530250675674.png)
    > 
    > 可以看到现在 (6) 中的时间变量 n 也是整数了。这样一来，我们就知道：**ODE 的欧拉解法 (4) 实际上就是 RNN 的一个特例罢了**。这里我们也许可以间接明白为什么 RNN 的拟合能力如此之强了（尤其是对于时间序列数据），我们看到 ODE 可以产生很多复杂的函数，而 ODE 只不过是 RNN 的一个特例罢了，所以 RNN 也就可以产生更为复杂的函数了。 
    > 
    > **用RNN解ODE **
    > 
    > 于是，我们就可以写一个 RNN 来解 ODE 了，比如《两生物种群竞争模型》[2] 中的例子：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/a8c932e3-a44f-41f3-b7ba-9b3aea85ae49/1530250675861.png)
    > 
    > 我们可以写出：
    > 
    > ```
    > #! -*- coding: utf-8- -*-
    > 
    > from keras.layers import Layer
    > import keras.backend as K
    > 
    > class ODE_RNN(Layer):
    > 
    >     def __init__(self, steps, h, **kwargs):
    >         self.steps = steps
    >         self.h = h
    >         super(ODE_RNN, self).__init__(**kwargs)
    > 
    >     def step_do(self, step_in, states): # 定义每一步的迭代
    >         x = states[0]
    >         r1,r2,a1,a2,iN1,iN2 = 0.1,0.3,0.0001,0.0002,0.002,0.003
    >         _1 = r1 * x[:,0] * (1 - iN1 * x[:,0]) - a1 * x[:,0] * x[:,1]
    >         _2 = r2 * x[:,1] * (1 - iN2 * x[:,1]) - a2 * x[:,0] * x[:,1]
    >         _1 = K.expand_dims(_1, 1)
    >         _2 = K.expand_dims(_2, 1)
    >         _ = K.concatenate([_1, _2], 1)
    >         step_out = x + self.h * _
    >         return step_out, [step_out]
    > 
    >     def call(self, inputs): # 这里的inputs就是初始条件
    >         init_states = [inputs]
    >         zeros = K.zeros((K.shape(inputs)[0],
    >                          self.steps,
    >                          K.shape(inputs)[1])) # 迭代过程用不着外部输入，所以
    >                                               # 指定一个全零输入，只为形式上的传入
    >         outputs = K.rnn(self.step_do, zeros, init_states) # 循环执行step_do函数
    >         return outputs[1] # 这次我们输出整个结果序列
    > 
    >     def compute_output_shape(self, input_shape):
    >         return (input_shape[0], self.steps, input_shape[1])
    > 
    > from keras.models import Sequential
    > import numpy as np
    > import matplotlib.pyplot as plt
    > 
    > steps,h = 1000,0.1
    > 
    > M = Sequential()
    > M.add(ODE_RNN(steps, h, input_shape=(2,)))
    > M.summary()
    > 
    > # 直接前向传播就输出解了
    > result = M.predict(np.array([[100, 150]]))[0] # 以[100, 150]为初始条件进行演算
    > times = np.arange(1, steps+1) * h
    > 
    > # 绘图
    > plt.plot(times, result[:,0])
    > plt.plot(times, result[:,1])
    > plt.savefig('test.png')
    > ```
    > 
    > 整个过程很容易理解，只不过有两点需要指出一下。首先，由于方程组 (7) 只有两维，而且不容易写成矩阵运算，因此我在 step_do 函数中是直接逐位操作的（代码中的 x[:,0],x[:,1]），如果方程本身维度较高，而且能写成矩阵运算，那么直接利用矩阵运算写会更加高效；然后，我们可以看到，写完整个模型之后，直接 predict 就输出结果了，不需要"训练"。
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/a939ee0d-5970-436f-8c40-57ad0b2605a5/1530250676000.png)
    > 
    > ▲ RNN解两物种的竞争模型
    > 
    > **反推ODE参数**
    > 
    > 前一节的介绍也就是说，RNN 的前向传播跟 ODE 的欧拉解法是对应的，那么反向传播又对应什么呢？
    > 
    > 在实际问题中，有一类问题称为"模型推断"，它是在已知实验数据的基础上，猜测这批数据符合的模型（机理推断）。这类问题的做法大概分两步，第一步是猜测模型的形式，第二步是确定模型的参数。假定这批数据可以由一个 ODE 描述，并且这个 ODE 的形式已经知道了，那么就需要估计里边的参数。
    > 
    > 如果能够用公式完全解出这个 ODE，那么这就只是一个非常简单的回归问题罢了。但前面已经说过，多数 ODE 都没有公式解，所以数值方法就必须了。这其实就是 ODE 对应的 RNN 的反向传播所要做的事情：前向传播就是解 ODE（RNN 的预测过程），反向传播自然就是推断 ODE 的参数了（RNN 的训练过程）。这是一个非常有趣的事实：**ODE 的参数推断是一个被研究得很充分的课题，然而在深度学习这里，只是 RNN 的一个最基本的应用罢了**。
    > 
    > 我们把刚才的例子的微分方程的解数据保存下来，然后只取几个点，看看能不能反推原来的微分方程出来，解数据为：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/f2b52210-2217-400c-b625-88b5f0b73aba/1530250677234.png)
    > 
    > 假设就已知这有限的点数据，然后假定方程 (7) 的形式，求方程的各个参数。我们修改一下前面的代码：
    > 
    > ```
    > #! -*- coding: utf-8- -*-
    > 
    > from keras.layers import Layer
    > import keras.backend as K
    > 
    > def my_init(shape, dtype=None): # 需要定义好初始化，这相当于需要实验估计参数的量级
    >     return K.variable([0.1, 0.1, 0.001, 0.001, 0.001, 0.001])
    > 
    > class ODE_RNN(Layer):
    > 
    >     def __init__(self, steps, h, **kwargs):
    >         self.steps = steps
    >         self.h = h
    >         super(ODE_RNN, self).__init__(**kwargs)
    > 
    >     def build(self, input_shape): # 将原来的参数设为可训练的参数
    >         self.kernel = self.add_weight(name='kernel', 
    >                                       shape=(6,),
    >                                       initializer=my_init,
    >                                       trainable=True)
    >     def step_do(self, step_in, states): # 定义每一步的迭代
    >         x = states[0]
    >         r1,r2,a1,a2,iN1,iN2 = (self.kernel[0], self.kernel[1],
    >                                self.kernel[2], self.kernel[3],
    >                                self.kernel[4], self.kernel[5])
    >         _1 = r1 * x[:,0] * (1 - iN1 * x[:,0]) - a1 * x[:,0] * x[:,1]
    >         _2 = r2 * x[:,1] * (1 - iN2 * x[:,1]) - a2 * x[:,0] * x[:,1]
    >         _1 = K.expand_dims(_1, 1)
    >         _2 = K.expand_dims(_2, 1)
    >         _ = K.concatenate([_1, _2], 1)
    >         step_out = x + self.h * K.clip(_, -1e5, 1e5) # 防止梯度爆炸
    >         return step_out, [step_out]
    > 
    >     def call(self, inputs): # 这里的inputs就是初始条件
    >         init_states = [inputs]
    >         zeros = K.zeros((K.shape(inputs)[0],
    >                          self.steps,
    >                          K.shape(inputs)[1])) # 迭代过程用不着外部输入，所以
    >                                               # 指定一个全零输入，只为形式上的传入
    >         outputs = K.rnn(self.step_do, zeros, init_states) # 循环执行step_do函数
    >         return outputs[1] # 这次我们输出整个结果序列
    > 
    >     def compute_output_shape(self, input_shape):
    >         return (input_shape[0], self.steps, input_shape[1])
    > 
    > from keras.models import Sequential
    > from keras.optimizers import Adam
    > import numpy as np
    > import matplotlib.pyplot as plt
    > 
    > steps,h = 50, 1 # 用大步长，减少步数，削弱长时依赖，也加快推断速度
    > series = {0: [100, 150],
    >           10: [165, 283],
    >           15: [197, 290],
    >           30: [280, 276],
    >           36: [305, 269],
    >           40: [318, 266],
    >           42: [324, 264]}
    > 
    > M = Sequential()
    > M.add(ODE_RNN(steps, h, input_shape=(2,)))
    > M.summary()
    > 
    > # 构建训练样本
    > # 其实就只有一个样本序列，X为初始条件，Y为后续时间序列
    > X = np.array([series[0]])
    > Y = np.zeros((1, steps, 2))
    > 
    > for i,j in series.items():
    >     if i != 0:
    >         Y[0, int(i/h)-1] += series[i]
    > 
    > # 自定义loss
    > # 在训练的时候，只考虑有数据的几个时刻，没有数据的时刻被忽略
    > def ode_loss(y_true, y_pred):
    >     T = K.sum(K.abs(y_true), 2, keepdims=True)
    >     T = K.cast(K.greater(T, 1e-3), 'float32')
    >     return K.sum(T * K.square(y_true - y_pred), [1, 2])
    > 
    > M.compile(loss=ode_loss,
    >           optimizer=Adam(1e-4))
    > 
    > M.fit(X, Y, epochs=10000) # 用低学习率训练足够多轮
    > 
    > # 用训练出来的模型重新预测，绘图，比较结果
    > result = M.predict(np.array([[100, 150]]))[0]
    > times = np.arange(1, steps+1) * h
    > 
    > plt.clf()
    > plt.plot(times, result[:,0], color='blue')
    > plt.plot(times, result[:,1], color='green')
    > plt.plot(series.keys(), [i[0] for i in series.values()], 'o', color='blue')
    > plt.plot(series.keys(), [i[1] for i in series.values()], 'o', color='green')
    > plt.savefig('test.png')
    > ```
    > 
    > 结果可以用一张图来看：
    > 
    > ![](https://image.jiqizhixin.com/uploads/editor/4af7174f-5321-4c44-baf3-0030ecd18bcc/1530250677673.png)
    > 
    > ▲ RNN做ODE的参数估计效果
    > 
    > （散点：有限的实验数据，曲线：估计出来的模型）
    > 
    > 显然结果是让人满意的。
    > 
    > ### **又到总结**
    > 
    > 本文在一个一般的框架下介绍了 RNN 模型及其在 Keras 下的自定义写法，然后揭示了 ODE 与 RNN 的联系。在此基础上，介绍了用 RNN 直接求解 ODE 以及用 RNN 反推 ODE 参数的基本思路。
    > 
    > 需要提醒读者的是，在 RNN 模型的反向传播中，要谨慎地做好初始化和截断处理处理，并且选择好学习率等，以防止梯度爆炸的出现（梯度消失只是优化得不够好，梯度爆炸则是直接崩溃了，解决梯度爆炸问题尤为重要）。
    > 
    > 总之，梯度消失和梯度爆炸在 RNN 中是一个很经典的困难，事实上，LSTM、GRU 等模型的引入，根本原因就是为了解决 RNN 的梯度消失问题，而梯度爆炸则是通过使用 tanh 或 sigmoid 激活函数来解决的。
    > 
    > 但是如果用 RNN 解决 ODE 的话，我们就没有选择激活函数的权利了（激活函数就是 ODE 的一部分），所以只能谨慎地做好初始化及其他处理。据说，只要谨慎做好初始化，普通 RNN 中用 relu 作为激活函数都是无妨的。
    > 
    > ### **相关链接**
    > 
    > [1]. Tian Qi C, Yulia R, Jesse B, David D. Neural Ordinary Differential Equations. arXiv preprint arXiv:1806.07366, 2018.
    > 
    > **[2]. 两生物种群竞争模型**
    > 
    > https://kexue.fm/archives/3120
    > 

## [1707.02568] Solving high-dimensional partial differential equations using deep learning

- [[1707.02568] Solving high-dimensional partial differential equations using deep learning](https://arxiv.org/abs/1707.02568)


# Audio

## Audio GAN

- [wavegan/README.md at master · chrisdonahue/wavegan](https://github.com/chrisdonahue/wavegan/blob/master/README.md)

## Music style transfer

### Midi file
- [Free Midi Music Songs Download - FreeMidi.org](https://freemidi.org/)

### paper

- [[1805.07848] A Universal Music Translation Network](https://arxiv.org/abs/1805.07848)

    <iframe width="560" height="315" src="https://www.youtube.com/embed/vdxCqNWTpUs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    
    - [[R] A Universal Music Translation Network：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/8lhrjf/r_a_universal_music_translation_network/)

    - [Paper Reading - A Universal Music Translation Network, from Facebook AI Research](https://medium.com/@bryanw/paper-reading-a-universal-music-translation-network-from-facebook-ai-rearch-754b4d645baa)


# Computer Graphics 電腦圖學


## Raytracing

- [Microsoft 發表 DirectX 12 Raytracing API 功能集 DXR，標準化光追蹤步驟 | T客邦 - 我只推薦好東西](https://www.techbang.com/posts/57408-microsoft-publishes-the-directx-raytracing-api-feature-set-dxr-standard-light-tracing-steps)

    > 若要瞭解光追蹤的成像原理，我們先來理解目前絕大部分 3D 遊戲畫面是如何繪製的。由於即時遊戲畫面需要每秒提供 30 張～60 張以上的畫面更新速率，因而需要 1 種相當有效率且易於平行運算的作業方式，Rasterization 光柵化就是目前 3D 遊戲的基礎。
    > 
    > 光柵化簡而言之就是把 3D 虛擬空間裡的所有物件，剃除視角以外的物件，也把被其它物件遮擋，物件背面部分全部刪除減少實際運算量（Z-buffer Culling），再壓縮至 1 張平面，而這平面就是你我所能看見的螢幕，接著再依據每個像素進行相關的貼圖、上色、打光作業。雖然這種方式能夠大量減少運算工作，同時產生不錯的畫面品質，卻也因為少了一些物件以及光線之間的交互作用，看起來與真實世界總有不小的差距。
    > 
    > 之後加入許多增進畫面品質的運算技巧，譬如 Bump Mapping 凹圖貼圖、Normal Mapping 法線貼圖、Parallax Mapping 視差貼圖、Global Illumination 全域照明、Ambient Occlusion 環境遮蔽等，都是為了加強畫面的品質與真實性。
    > 
    > ![](https://cdn0-techbang.pixfs.net/system/images/433803/original/29969e87bdc0ede4cb638c8cefc892e6.png?1521497551)  
    > ▲光柵化逐步剃除物件的步驟，由上而下分別為無剃除、剔除視錐以外物件、刪去視角以外部分、刪去背部看不見部分、刪去被其它物件遮蔽部分。
    > 
    > ![](https://cdn2-techbang.pixfs.net/system/images/433801/original/a59f717c9536fa56df1f77e5b266dd2d.png?1521497395)  
    > ▲三角形光柵化的效果。
    > 
    > Raytracing 光追蹤則是模擬真實世界光線運作方式，包含光在物體表面的折射、散射、繞射，或是穿過物體的透射，同時考慮到光在各個物體之間的交互作用，也就是計算場景的光場，最終以平面方式呈現在螢幕。為了減少運算量，實際上運算步驟並不從光源開始，而是由攝影機視角反向射出數條虛擬光線，追蹤這些虛擬光線的路徑。
    > 
    > ![](https://cdn2-techbang.pixfs.net/system/images/433804/original/e6f6301fb1078457ec0b633ca3280d37.png?1521497585)  
    > ▲光追蹤可以計算物件與物件之間的光線交互作用。
    > 
    > 因為內部原理與實際環境相當類似，光追蹤可以產生相當逼真的畫面效果，無需其它特殊的運算技巧。但是相對於光柵化還是需要相當大的運算資源，因此 3D 遊戲絕大部分依然選用光柵作為產生畫面的手段，光追蹤則是應用在動畫、電影特效等沒有時間限制的領域。部分遊戲貼圖材質也會先行以光追蹤技法處理，預先改變顏色後再貼上物件，產生較為逼真的結果。
    > 
    - [Announcing Microsoft DirectX Raytracing! – DirectX Developer Blog](https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/)

- [What's the Difference Between Ray Tracing, Rasterization? | NVIDIA Blog](https://blogs.nvidia.com/blog/2018/03/19/whats-difference-between-ray-tracing-rasterization/)




## GPU Rendering process

- [graphics card - Why are videos rendered by the cpu instead of the gpu? - Super User](https://superuser.com/questions/790418/why-are-videos-rendered-by-the-cpu-instead-of-the-gpu)

    > Before HD was a thing, CPUs could handle video decoding easily. When HD became popular about 8 years ago, GPU manufacturers started to implement accelerated video decoding in their chips. You could easily find graphics cards marketed as supporting HD videos and some other slogans. Today any GPU supports accelerated video, even integrated GPUs like Intel HD Graphics or their predecessors, Intel GMA. Without that addition your CPU would have a hard time trying to digest 1080p video with acceptable framerate, not to mention increased energy consumption. So you're already using accelerated video everyday.
    > 
    > Now when GPUs have more and more general use computational power, they are widely used to accelerate video processing too. This trend started around the same time when accelerated decoding was introduced. Programs like Badaboom started to gain popularity as it turned out that GPUs are much better at (re)encoding video than CPUs. It couldn't be done before, though, because GPUs lacked generic computational abilities.
    > 
    > But GPUs could already scale, rotate and transform pictures since middle ages, so why weren't we able to use these features for video processing? Well, these features were never implemented to be used in such way, so they were suboptimal for various reasons.
    > 
    > When you program a game, you first upload all graphics, effects etc. to the GPU and then you just render polygons and map appropriate objects to them. You don't have to send textures each time they are needed, you can load them and reuse them. **When it comes to video processing, you have to constantly feed frames to the GPU, process them and fetch them back to reencode them on CPU (remember, we're talking about pre-computational-GPU times)**. This wasn't how GPUs were supposed to work, so performance wasn't great.
    > 
    > Another thing is, **GPUs aren't quality-oriented when it comes to image transformations**. When you're playing a game at 40+ fps, you won't really notice slight pixel misrepresentations. Even if you would, game graphics weren't detailed enough for people to care. There are various hacks and tricks used to speed up rendering that can slightly affect quality. **Videos are played at rather high framerates too, so scaling them dynamically at playback is acceptable, but reencoding or rendering has to produce results that are pixel-perfect or at least as close as possible at reasonable cost.** You can't achieve that without proper features implemented directly in GPU.
    > 
    > Nowadays using GPUs to process videos is quite common because we have required technology in place. Why it's not the default choice is rather a question to program's publisher, not us - it's their choice. Maybe they believe that their clients have hardware oriented to process videos on CPU, so switching to GPU will negatively affect performance, but that's just my guess. Another possibility is that they still treat GPU rendering as experimental feature that's not stable enough to set it as a default yet. You don't want to waste hours rendering your video just to realize something is screwed up due to GPU rendering bug. If you decide to use it anyway, then you can't blame the software publisher - it was your decision.
    > 

- [How a GPU Works](https://www.cs.cmu.edu/afs/cs/academic/class/15462-f11/www/lec_slides/lec19.pdf)

    ![](https://screenshotscdn.firefoxusercontent.com/images/66d590a0-f112-4f68-9e6b-0a5840ef6025.png)

## 3D 建模

### GQN Neural rendering

<iframe width="560" height="315" src="https://www.youtube.com/embed/gnctSz2ofU4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

- [Neural scene representation and rendering | DeepMind](https://deepmind.com/blog/neural-scene-representation-and-rendering/)

- [自动「脑补」3D环境！DeepMind最新Science论文提出生成查询网络GQN | 机器之心](https://www.jiqizhixin.com/articles/Science-Neural-scene-representation-and-rendering)


    > 在这项发表在 Science 的研究中，DeepMind 引入了生成查询网络（Generative Query Network/GQN）的框架，其中机器通过到处走动并仅在由它们自己获取的数据中训练来感知周围环境。该行为和婴儿、动物很相似，GQN 通过尝试观察周围的世界并进行理解来学习。以此，GQN 得以学习合理的场景以及它们的几何性质，而不需要任何场景内容的人类标记。
    > 
    > GQN 模型由两部分构成：一个表征网络以及一个生成网络。表征网络将智能体的观察作为输入，并生成一个描述潜在场景的表征（向量）。然后生成网络从之前未观察过的视角来预测（想象）该场景。
    > 
    > 表征网络不知道生成网络将被要求预测哪些视角，因此必须找到尽可能准确描述场景真实布局的有效方法。表征网络能通过简明的分布式表示捕获最重要的元素，例如目标位置、颜色和房间布局。在训练过程中，生成器学习环境中的典型目标、特征、关系和规律。这组共享的「概念」使表征网络能够以高度压缩、抽象的方式来描述场景，让生成网络在必要时填写细节。例如，表征网络将把「蓝色立方体」简洁地表示为一个小的数值集合，生成网络将知道从特定的角度来看，这是如何以像素的形式表现出来的。
    > 
    > 我们在模拟 3D 世界里一组由程序生成的环境中对 GQN 进行了受控实验，这些环境包含随机位置、颜色、形状和纹理的多个目标，还有随机光源和严重遮挡。在这些环境下训练后，我们使用 GQN 的表征网络来生成新的、以前未见过的视角下的场景表征。我们在实验中表明，GQN 具有几个重要的特性：
    > 
    > -   GQN 的生成网络可以从新的视角非常精确地「想象」以前未见过视角下的场景。当给定场景表征和新视角时，它会生成清晰的图像，而不需要预先规定角度、遮挡或照明的规律。因此，生成网络是从数据中学习的近似渲染器（renderer）：
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ymzWBzccLf3HS5gOQrALlns7f5ax0aicgiaJSVrSI6we8kgQcjPCiaWAJfBa82CoNtCfdl493WibwJA/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
    > 
    > -   GQN 的表征网络可以学习计数、定位和分类目标，并且不需要任何目标级的标注。即使它的表征可能是很小的，GQN 在查询视角的预测也能达到很高的准确率，几乎和真实场景无法分辨。这意味着该表征网络可以准确地感知，例如识别积木块的精确配置：
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW8ymzWBzccLf3HS5gOQrALl3gxSZnKh4STsRmTq3GicPAj8oxwXLz2WGFzdLH6nuGn9ic8clc64rNvw/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)
    > 
    > -   GQN 可以表征、测量和减少不确定性。它可以计算关于场景可信度的不确定度，即使其内容不是完全可见的，并且它可以组合一个场景的多个部分视角来构建一致的整体。下图中展示了它的第一人称视角和自顶向下视角的预测。该模型通过预测的易变性来表达不确定度，并随着它在迷宫中移动而逐渐减小（灰色椎体表示观察位置，黄色椎体表示查询位置）。
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ymzWBzccLf3HS5gOQrALlmibDblbAZ3cmm6ydBMUQbe7e3MeDDK93lHDZiaacIKvVvJQ52XW96YKQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
    > 
    > -   GQN 的表征允许实现鲁棒性的、数据效率高的强化学习。当给定 GQN 的紧凑型表征时，如下所示，当前最优的深度强化学习智能体相比于 model-free 的基线智能体在学习完成任务上有更高的数据效率。对于这些智能体，通用网络中编码的信息能被视为环境的先验知识：
    > 
    > ![](https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ymzWBzccLf3HS5gOQrALlI5ZDO1ZvEqELkqwU2TJ1IeItRXBKXwHfDyiag37Niaql5GSIxr7Q2TMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
    > 
    > *相比于使用原像素的标准方法，使用 GQN 迭代次数少了 4 倍，但收敛表现一致且有更加数据高效的策略学习。*
    > 
    > GQN 建立在最近大量多视角的几何研究、生成式建模、无监督学习和预测学习的基础上，它展示了一种学习物理场景的紧凑、直观表征的全新方式。重要的是，提出的这种方法不需要特定域的工程以及消耗时间对场景内容打标签，使得同一模型能够应用到大量不同的环境。它也学习了一种强大的神经渲染器，能够产生准确的、全新视角的场景图像。
    > 
    > DeepMind 认为，相比于更多传统的计算机视觉技术，他们的方法还有许多缺陷，目前也只在合成场景下训练工作的。然而，随着新数据资源的产生、硬件能力的发展，DeepMind 希望探索 GQN 框架应用到更高分辨率真实场景图像的研究。未来，探索 GQN 应用到更广泛的场景理解的工作也非常重要，例如通过跨空间和时间的查询来学习物理和移动等常识概念，还有应用到虚拟和增强现实等。
    > 


### Photo realistic Neural rendering

- [Gaussian Material Synthesis – new! ACM Transactions on Graphics (SIGGRAPH 2018) Károly Zsolnai-Fehér, Peter Wonka, Michael Wimmer – Károly Zsolnai-Fehér – Research Scientist](https://users.cg.tuwien.ac.at/zsolnai/gfx/gaussian-material-synthesis/)

    <iframe width="560" height="315" src="https://www.youtube.com/embed/6FzVhIV_t3s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

    > In this paper, we teach an AI the concept of metallic, translucent materials and more. The newly synthesized materials can be visualized in real-time via neural rendering and we also propose an intuitive variant generation technique to enable the user to fine-tune these recommended materials. This project took more than 3000 work hours to complete – I hope you’ll enjoy the result!
    > 

### 共面性3D重建

- [ECCV 2018 | 國防科大&普林斯頓提出共面性檢測網絡：助力三維場景重建 - 幫趣](http://bangqu.com/611o8q.html)

    > 在此之前，傳統方法往往通過檢測不同圖片中的共同特徵點來實現圖片配準。但是由於圖片曝光不統一、相機運動太快導致圖像模糊等因素，關鍵點的檢測和特徵計算差強人意，基於特徵點的配准算法有很強的侷限性。
    > 
    > 爲了解決這一問題，人們開始嘗試利用較特徵點更大的幾何體來提高三維場景重建的精度。過去的五年裏，這方面的研究工作大多集中於利用平面的共面性對配準結果進行優化矯正。以 CVPR 2017 文章 [1] 中的方法爲例，該方法首先利用 SIFT 特徵點實現初步配準，然後通過將近似共面平面矯正成完全共面平面來優化相機位置。這種遞進式的優化方法雖然能夠在一定程度上提高三維場景重建精度，但是卻嚴重依賴於對相機位姿的初始估計。當初始位姿誤差很大的時候，該方法無法正確檢測幀間的共面平面，從而帶來巨大的重建誤差。那麼，如何才能更好地利用平面提高三維重建精度呢？
    > 
    > ![](http://i2.bangqu.com/j/news/20180810/611o8q1533877229321f2U6o.png)*圖 2 平面共面性預測*
    > 
    > 「我們注意到，人類在判斷兩個平面是否共面時並不需要估計相機的位姿。爲了解決上述問題，一個很直接的想法是讓機器具有像人類一樣對共面性進行判斷的能力。我們提出使用深度網絡預測不同幀中的兩個平面是否共面，這在三維重建領域尚屬首次。」論文的第一作者施逸飛這樣介紹這項工作。「人類在做這種判斷時，既會觀察兩個平面的紋理顏色是否一致，也會考慮平面的語義信息。例如，假設我知道兩個平面都是地面，那麼它們極有可能是共面的。」
    > 
    > ![](http://i2.bangqu.com/j/news/20180810/611o8q1533877230596C1m1D.png)*圖 3 PlaneMatch 算法流程圖*

