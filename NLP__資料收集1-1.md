# NLP__資料收集1-1

[toc]
<!-- toc --> 


## 資料集

### 康乃爾NLP

- [Cornell NLP :: Data](http://nlp.cornell.edu/data/)

### SemEval-2018 語意理解任務資料集

- [Tasks < SemEval-2018](http://alt.qcri.org/semeval2018/index.php?id=tasks)

### DART: a Dataset of Arguments and their Relations on Twitter

- [DART: a Dataset of Arguments and their Relations on Twitter - Semantic Scholar](https://www.semanticscholar.org/paper/DART%3A-a-Dataset-of-Arguments-and-their-Relations-on-Bosc-Cabrio/9b71b307a2f99fb404c6f6159b146547a0dc1cbc)



## Reference




### 我爱自然语言处理

- [自然语言处理 | 我爱自然语言处理](http://www.52nlp.cn/category/nlp)

### CS4650 and CS7650 ("Natural Language") at Georgia Tech

- [gt-nlp-class/notes at master · jacobeisenstein/gt-nlp-class](https://github.com/jacobeisenstein/gt-nlp-class/tree/master/notes)


### Text Classification using Neural Networks

- [Text Classification using Neural Networks – Machine Learnings](https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6)

### Supervised Sequence Labelling

- [Supervised Sequence Labelling | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-642-24797-2_2)



## Course

### CS224N

- [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)

    - [Lecture 1 | Natural Language Processing with Deep Learning - YouTube](https://www.youtube.com/watch?v=OQQ-W_63UgQ&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja)

## Tools

### NLPIR

- [推荐NLPIR大数据语义智能分析平台 | 我爱自然语言处理](http://www.52nlp.cn/%e6%8e%a8%e8%8d%90nlpir%e5%a4%a7%e6%95%b0%e6%8d%ae%e8%af%ad%e4%b9%89%e6%99%ba%e8%83%bd%e5%88%86%e6%9e%90%e5%b9%b3%e5%8f%b0)

    - [NLPIR/NLPIR-Parser at master · NLPIR-team/NLPIR](https://github.com/NLPIR-team/NLPIR/tree/master/NLPIR-Parser)

    > 1、精准采集：对境内外互联网海量信息实时精准采集，有主题采集（按照信息需求的主题采集）与站点采集两种模式（给定网址列表的站内定点采集功能）。
    > 
    > 2、文档抽取：对doc、excel、pdf与ppt等多种主流文档格式，进行文本信息抽取，信息抽取准确，效率达到大数据处理的要求。
    > 
    > 3、新词发现：从文本中挖掘出新词、新概念，用户可以用于专业词典的编撰，还可以进一步编辑标注，导入分词词典中，提高分词系统的准确度，并适应新的语言变化。
    > 
    > 4、批量分词：对原始语料进行分词，自动识别人名地名机构名等未登录词，新词标注以及词性标注。并可在分析过程中，导入用户定义的词典。
    > 
    > 5、语言统计：针对切分标注结果，系统可以自动地进行一元词频统计、二元词语转移概率统计。针对常用的术语，会自动给出相应的英文解释。
    > 
    > 6、文本聚类：能够从大规模数据中自动分析出热点事件，并提供事件话题的关键特征描述。同时适用于长文本和短信、微博等短文本的热点分析。
    > 
    > 7、文本分类：根据规则或训练的方法对大量文本进行分类，可用于新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面。
    > 
    > 8、摘要实体：对单篇或多篇文章，自动提炼出内容摘要，抽取人名、地名、机构名、时间及主题关键词；方便用户快速浏览文本内容。
    > 
    > 9、智能过滤：对文本内容的语义智能过滤审查，内置国内最全词库，智能识别多种变种：形变、音变、繁简等多种变形，语义精准排歧。
    > 
    > 10、情感分析：针对事先指定的分析对象，系统自动分析海量文档的情感倾向：情感极性及情感值测量，并在原文中给出正负面的得分和句子样例。
    > 
    > 11、文档去重：快速准确地判断文件集合或数据库中是否存在相同或相似内容的记录，同时找出所有的重复记录。
    > 
    > 12、全文检索：支持文本、数字、日期、字符串等各种数据类型，多字段的高效搜索，支持AND/OR/NOT以及NEAR邻近等查询语法，支持维语、藏语、蒙语、阿拉伯、韩语等多种少数民族语言的检索。
    > 
    > 13、编码转换：自动识别内容的编码，并把编码统一转换为其他编码。
    > 


## NLP 入門練習

- [CNN也能用于NLP任务，一文简述文本分类任务的7个模型](https://zhuanlan.zhihu.com/p/39054002)



## 信息熵(Entropy)


### 最大熵模型

- [最大熵模型 (MaxEnt) 實踐短字詞分類 – PyLadies Taiwan – Medium](https://medium.com/pyladies-taiwan/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B-maxent-%E5%AF%A6%E8%B8%90%E7%9F%AD%E5%AD%97%E8%A9%9E%E5%88%86%E9%A1%9E-b925665d9082)

    > > 我們要找的是其中最有可能的一種劃分，在這裡就是找出一種熵最大的劃分。
    > 
    > 這裡看了很久，因為我想，熵不是代表不確定性嗎？那為什麼是找最大，而不是最小？看到一篇文章這樣寫：
    > 
    > > 有一句俗话说的是“不要把鸡蛋都放到一个篮子里”，这是因为如果这个篮子出了问题，那么所有鸡蛋都没了。也就是当我们在作考虑的时候，不要人为的人为哪件事出现的概率会高一点，一句话，不要添加任何人为因素，要使不确定性最大。这就是最大熵原理，在一篇文章上这样说到：
    > > 
    > > Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum entropy estimate. It is least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information.
    > > 
    > > 科学网 — 最大熵模型（MaxEnt） — 何孝松的博文
    > > http://blog.sciencenet.cn/blog-802219-667105.html
    > 
    > 就短語分類的例子來說好了，我的想法是，未分類前的不確定性是最大的，因此具有最大的熵；然而分類後，由於必須滿足一定的約束條件，不確定性必然減少，因此熵會比未分類時來的小。顯然，若約束條件越多，不確定性就越小，最後結果的熵也會越小。
    > 
    > 現在假設我們人為設計幾條分類規則，而這些規則其實可視為另外的約束條件，讓最後結果的熵變得更小。因此，若要避免不小心引入人為的規則，就要盡可能在原始約束條件上最大化熵，這樣就可確定分類結果僅僅是受到原始約束條件的限制，模型沒有"學到"其他規則(Overfitting 正好相反，是模型學到太多規則)。
    > 
    > 這樣看來，最大熵模型其實也是避免Overfitting的模型，不過前提是，我們要先知道什麼是"原始"的約束條件。在NLP任務中，我們有大量語料統計出來的詞頻作為原始約束條件，但在其他類data上，似乎很難做這件事。想想做貓狗辨識的圖片分類問題，到底什麼才是他的原始約束條件呢？我想可能就是CNN 學到的那些features吧？是否neural network 內建了最大熵原理呢？盡可能做最少的假設來達成任務，也就是說，用最少的features 來達成任務，而非最多，聽起來也滿合理的。畢竟，feature 數量幾乎無限多種，沒有最多，只有更多。neural network 受限於參數量，顯然無法窮舉出所有features，只能被迫擠出幾種最有效的features(但是當資料太簡單時，就有可能窮舉出所有features，造成overfitting)。
    > [name=Ya-Lun Li]
    > 



## DEMO

### word2vec 小遊戲

- [讓你看見 AI 自然語言處理多強大！Google 發表一款搜尋引擎和兩個文字遊戲 - INSIDE 硬塞的網路趨勢觀察](https://www.inside.com.tw/2018/04/16/google-introducing-semantic-experiences-with-talk-to-book)

    - [Semantris](https://research.google.com/semantris/)













## gensim


Parameters:	

- __sg (int {1, 0})__ – Defines the training algorithm. If 1, skip-gram is employed; otherwise, CBOW is used.
- __size (int)__ – Dimensionality of the feature vectors.
- __window (int)__ – The maximum distance between the current and predicted word within a sentence.
- __alpha (float)__ – The initial learning rate.
- __min_alpha (float)__ – Learning rate will linearly drop to min_alpha as training progresses.
- __seed (int)__ – Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization).
- __min_count (int)__ – Ignores all words with total frequency lower than this.
- __max_vocab_size (int)__ – Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit.
- __sample (float)__ – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).
- __workers (int)__ – Use these many worker threads to train the model (=faster training with multicore machines).
- __hs (int {1,0})__ – If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.
- __negative (int)__ – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.
- __cbow_mean (int {1,0})__ – If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.
- __hashfxn (function)__ – Hash function to use to randomly initialize weights, for increased training reproducibility.
- __iter (int)__ – Number of iterations (epochs) over the corpus.
- __trim_rule (function)__ – Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count). Can be None (min_count will be used, look to keep_vocab_item()), or a callable that accepts parameters (word, count, min_count) and returns either gensim.utils.RULE_DISCARD, gensim.utils.RULE_KEEP or gensim.utils.RULE_DEFAULT. Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the model.
- __sorted_vocab (int {1,0})__ – If 1, sort the vocabulary by descending frequency before assigning word indexes.
- __batch_words (int)__ – Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)
- __compute_loss (bool)__ – If True, computes and stores loss value which can be retrieved using model.get_latest_training_loss().
- __callbacks__ – List of callbacks that need to be executed/run at specific stages during training.



## char-rnn

- [Tensorflow lyrics generation · Lei's Blog](http://leix.me/2016/11/28/tensorflow-lyrics-generation/)

    - [leido/char-rnn-cn: 基于char-rnn和tensorflow生成周杰伦歌词](https://github.com/leido/char-rnn-cn)

- [從字符級的語言建模開始，瞭解語言模型與序列建模的基本概念 - 幫趣](http://bangqu.com/24EJ36.html)

## auto tagging

- [memray/seq2seq-keyphrase](https://github.com/memray/seq2seq-keyphrase)

- [udibr/headlines: Automatically generate headlines to short articles](https://github.com/udibr/headlines)

- [fudannlp16/KeyPhrase-Extraction](https://github.com/fudannlp16/KeyPhrase-Extraction)

- [Tensorflow：基于LSTM轻松生成各种古诗 - CSDN博客](https://blog.csdn.net/meyh0x5vdtk48p2/article/details/78987402)

- [[代码]基于RNN的文本生成算法 - CSDN博客](https://blog.csdn.net/clayanddev/article/details/53955850)

- [How can I use machine learning to propose tags for content? - Quora](https://www.quora.com/How-can-I-use-machine-learning-to-propose-tags-for-content)

- [Deep Learning for Text Understanding from Scratch](https://www.kdnuggets.com/2015/03/deep-learning-text-understanding-from-scratch.html)

- [neural network - Keyword/phrase extraction from Text using Deep Learning libraries - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/10077/keyword-phrase-extraction-from-text-using-deep-learning-libraries)

- [attardi/deepnl: Deep Learning for Natural Language Processing](https://github.com/attardi/deepnl)

- [Intro to text classification with Keras: automatically tagging Stack Overflow posts | Google Cloud Big Data and Machine Learning Blog  |  Google Cloud](https://cloud.google.com/blog/big-data/2017/10/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts)

- [snkim/AutomaticKeyphraseExtraction: Data for Automatic Keyphrase Extraction Task](https://github.com/snkim/AutomaticKeyphraseExtraction)

- [lvsh/keywordfinder: Automatic keyword extraction - no alchemy required!](https://github.com/lvsh/keywordfinder)

- [Natural Language Toolkit — NLTK 3.2.5 documentation](http://www.nltk.org/)

- [nlp - How to auto-tag content, algorithms and suggestions needed - Stack Overflow](https://stackoverflow.com/questions/6039238/how-to-auto-tag-content-algorithms-and-suggestions-needed)











## Models
- [DeepMind丨深度學習最新生成記憶模型，遠超RNN的GTMM](http://www.bigdatafinance.tw/index.php/tech/557-deepmind-rnn-gtmm)

### RNN+CNN

- [Representing Language with Recurrent and Convolutional Layers: An Authorship Attribution Example](https://hergott.github.io/language-representation-rnn-cnn/)



## 詞性標註（POS）/詞語組快分析（Chunking）/命名實體識別（NER）/語意角色標註（SRL）

### Natural Language Processing (almost) from Scratch

- [[1103.0398] Natural Language Processing (almost) from Scratch](https://arxiv.org/abs/1103.0398)

- [【每周一文】Natural Language Processing (almost) From Scratch - CSDN博客](https://blog.csdn.net/fangqingan_java/article/details/50493948)

    > 概述
    > ==
    > 
    > 本文介绍了一个统一的神经网络架构用于解决自然语言处理各种的各种任务，主要是序列标注任务，包括词性标注（POS）、词语组块分析（Chunking）、命名实体识别（NER）以及语义角色标注（SRL）等。本文主要介绍如何构建这个统一的神经网络以及如何运用一些技巧去提高效果，结论是不需要特殊构建特征工程就可以得到State-of-art结果。


- [attardi/deepnl: Deep Learning for Natural Language Processing](https://github.com/attardi/deepnl)

    > `deepnl` is a Python library for Natural Language Processing tasks based on a Deep Learning neural network architecture.
    > 
    > The library currently provides tools for performing part-of-speech tagging, Named Entity tagging and Semantic Role Labeling.
    > 
    > `deepnl` also provides code for creating *word embeddings* from text, using either the Language Model approach by [[Collobert11]](https://github.com/attardi/deepnl#collobert11), or Hellinger PCA, as in [[Lebret14]](https://github.com/attardi/deepnl#lebret14).
    > 
    > It can also create *sentiment specific word embeddings* from a corpus of annotated Tweets.

### Entity extraction: 實體抽取 

- [Entity extraction using Deep Learning – Towards Data Science](https://towardsdatascience.com/entity-extraction-using-deep-learning-8014acac6bb8)

- [Sequence Tagging with Tensorflow](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)

- [dlnd-other/embeddings at master · dkarunakaran/dlnd-other](https://github.com/dkarunakaran/dlnd-other/tree/master/embeddings)

- [Pretrained Character Embeddings for Deep Learning and Automatic Text Generation](http://minimaxir.com/2017/04/char-embeddings/)

- [Introduction to Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)

### Lattice LSTM: 中文實體識別


- [[1805.02023] Chinese NER Using Lattice LSTM](https://arxiv.org/abs/1805.02023)

    - [ACL 2018 | 利用Lattice LSTM的最优中文命名实体识别方法](https://zhuanlan.zhihu.com/p/38941381)







## 語句解析

### Bi-LSTM-CRF

- [[1508.01991] Bidirectional LSTM-CRF Models for Sequence Tagging](https://arxiv.org/abs/1508.01991)

- [词法分析之Bi-LSTM-CRF框架 - CSDN博客](https://blog.csdn.net/qrlhl/article/details/78561342)




### Stack-augmented Parser-Interpreter Neural Network (SPINN)

- [[1603.06021] A Fast Unified Model for Parsing and Sentence Understanding](https://arxiv.org/abs/1603.06021)

- [Recursive Neural Networks with PyTorch | NVIDIA Developer Blog](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)

    - [如何用PyTorch實現遞歸神經網絡？ - 壹讀](https://read01.com/kdQym4.html)

    > The dataset comes with machine-generated syntactic parse trees, which group the words in each sentence into phrases and clauses that all have independent meaning and are each composed of two words or sub-phrases. Many linguists believe that humans understand language by combining meanings in a hierarchical way as described by trees like these, so it might be worth trying to build a neural network that works the same way. Here’s an example of a sentence from the dataset, with its parse tree represented by nested parentheses:
    > 
    > ```
    >     ( ( The church ) ( ( has ( cracks ( in ( the ceiling ) ) ) ) . ) )
    > ```
    > 
    > One way to encode this sentence using a neural network that takes the parse tree into account would be to build a neural network layer Reduce that combines pairs of words (represented by word embeddings like [GloVe](http://nlp.stanford.edu/projects/glove/)) and/or phrases, then apply this layer recursively, taking the result of the last Reduce operation as the encoding of the sentence:
    > 
    > ```
    > X = Reduce(“the”, “ceiling”)
    > Y = Reduce(“in”, X)
    > ... etc.
    > ```
    > 
    > But what if I want the network to work in an even more humanlike way, reading from left to right and maintaining sentence context while still combining phrases using the parse tree? Or, what if I want to train a network to construct its own parse tree as it reads the sentence, based on the words it sees? Here’s the same parse tree written a slightly different way:
    > 
    > ```
    >     The church ) has cracks in the ceiling ) ) ) ) . ) )
    > ```
    > 
    > Or a third way, again equivalent:
    > 
    > ```
    > WORDS:  The church   has cracks in the ceiling         .
    > PARSES: S   S      R S   S      S  S   S       R R R R S R R
    > ```
    > 
    > All I did was remove open parentheses, then tag words with “S” for “shift” and replace close parentheses with “R” for “reduce.” But now the information can be read from left to right as a set of instructions for manipulating a stack and a stack-like buffer, with exactly the same results as the recursive method described above:
    > 
    > 1.  Place the words into the buffer.
    > 2.  Pop “The” from the front of the buffer and push it onto stack, followed by “church”.
    > 3.  Pop top two stack values, apply Reduce, then push the result back to the stack.
    > 4.  Pop “has” from buffer and push to stack, then “cracks”, then “in”, then “the”, then “ceiling”.
    > 5.  Repeat four times: pop top two stack values, apply Reduce, then push the result.
    > 6.  Pop “.” from buffer and push onto stack.
    > 7.  Repeat two times: pop top two stack values, apply Reduce, then push the result.
    > 8.  Pop the remaining stack value and return it as the sentence encoding.
    > 
    > I also want to maintain sentence context to take into account information about the parts of the sentence the system has already read when performing Reduce operations on later parts of the sentence. So I’ll replace the two-argument `Reduce` function with a three-argument function that takes a left child phrase, a right child phrase, and the current sentence context state. This state is created by a second neural network layer, a recurrent unit called the `Tracker`. The `Tracker` produces a new state at every step of the stack manipulation (i.e., after reading each word or close parenthesis) given the current sentence context state, the top entry *b* in the buffer, and the top two entries *s1*, *s2* in the stack:
    > 
    > ```
    > context\[t+1\] = Tracker(context\[t\], b, s1, s2)
    > ```
    > 

### SLING

- [google/sling: SLING - A natural language frame semantics parser](https://github.com/google/sling)

- [自然語言理解技術大進展！免斷詞，Google語意框架剖析器SLING能自動找出語句架構 | iThome](https://www.ithome.com.tw/news/118415)

    > [Google最近開源釋出實驗性的語意框架剖析器（Parsing）SLING，](https://research.googleblog.com/2017/11/sling-natural-language-frame-semantic.html)有別於以往用斷詞的方式，SLING不需要靠人工的方式標註語句，而是可以透過語意框架（Frame Semantic Parsing）的方式自動抽取出文字所要描述的語意結構，再以語意框架圖（Semantic frame graph）的方式呈現，Google研究團隊表示，SLING是透過Tensorflow和Dragnn訓練過的標註語料庫，這是自然語言理解技術的一大進展，語意分析不再靠斷詞，而是從語言意義層面，自動標註出語句的結構。
    > 
    > SLING是採用一個特定用途的遞歸神經網路（Recurrent neural network，RNN）模型，在該框架圖上，透過輸入文字的遞增編輯動作，來計算輸出值，也就是說，該框架圖因為靈活的特性，可以擷取多個語意任務，SLING的語意剖析器只用了輸入詞句來訓練，沒有採用額外的生成的標註，像是語句相依性分析產生的標註。
    > 
    > ![](https://s4.itho.me/sites/default/files/images/sling.png)
    > 
    > 大部分的自然語言理解系統都是採用一種分析流程，從詞性標註 （Part-of-speech tagging），到透過語句相依性分析（Dependency parsing）來計算輸入的文字語意。這種模型較容易將不同的方析階段模組化，但是往往也導致一個問題，一旦產生錯誤將會影響整個模型的預測。
    > 
    > SLING輸出的語意框架圖可以直接擷取使用者感興趣的語意標示（Semantic annotation），也能避免系統流程中的設計缺陷，還能避免不必要的計算。
    > 
    > 舉例來說，傳統的自然語言理解系統會先執行語句相依性分析的工作，最後才會執行指代消解（Coreference resolution），指代消解是將指定代名詞還原為被替換的名詞，來避免重要的字詞因被替換為指定代名詞，而在計算權重時降低的問題，如果語句相依性分析過程若有錯誤，將會連帶影響最終輸出的結果。
    > 
    > ### 語意框架剖析的機制
    > 
    > 語意框架代表語句的意義，也是一個描述，每個描述都被稱為一個框架，該框架可被視為知識或是意義的單元，也包含了與其相關的概念或是框架的相互關係。SLING將每個框架組織成一個Slot的清單，每個Slot都有自己的角色或是名稱，以及代表的值，該代表值可以是個字詞的原意，或是與其他框架的連結。
    > 
    > 例如，Many people now claim to have predicted Black Monday這句話，SLING先辨識語句的實體、測量值和其他概念，實體像是人物、地點，事件，測量值像是時間、距離，其他概念則包含動詞，接著，將這些辨識出來的字詞分類到正確的語意角色，當作輸入值，因此，SLING會先將people視為人物框架、predicted是動詞類別框架、Black Monday是事件框架，predicted這個動詞表示為PREDICT-01框架，PREDICT-01框架與預測的主詞Slot有相互關係，因此，PREDICT-01與PERSON框架連接，除此之外，PREDICT-01框架也與被預測的受詞有相互關係，與Black Monday的EVENT框架連接。
    > 
    > ![](https://s4.itho.me/sites/default/files/images/%E7%AF%84%E4%BE%8B.PNG)
    > 
    > Google研究團隊認為，SLING透過語意框架，來訓練並優化遞歸神經網路。神經網路在隱藏層中學習到的知識，可以取代了人工標註特徵。
    > 
    > 該語意剖析器的輸入是以雙向長短記憶單元（Bi-directional LSTMs）演算法為基礎的轉換語意框架剖析方法，使用Transition Based Recurrent Unit (TBRU)來輸出，結合成一個訓練過的模型，只需要文字標註當作輸入，經過轉換系統，輸出語意框架圖形，不需要中間產生的標註（Intervening symbolic representation）。
    > 
    > 輸出層的文字在輸出後，還會經過轉換系統（Transition system），再重新進入輸入層，其中，轉換系統的一項關鍵機制是採用了固定大小的框架來記錄字詞對上下文預測的重要程度，也就是說，該框架是用來表示最近提及，或是在語句中被增強的關鍵字。Google研究團隊發現，透過這個簡單的機制，在擷取大量語意框架的關聯上，效率提升有非常多。
    > 
    > 目前，Google的研究團隊表示，SLING是研究語意剖析的實驗，Google已在Github將SLING開源釋出，提供開發人員預先訓練完成的語意剖析模型，可應用於知識萃取、解析複雜引用（Resolving complex references），以及對話理解等工作，未來，Google將會持續擴增SLING的功能。

## 文法改錯

### DeepFix

- [DeepFix: Fixing Common C Language Errors by Deep Learning - 13921](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14603/13921)










## 新聞

### 語音助理

- [個人語音助理時代已經來臨！ - EE Times Taiwan 電子工程專輯網](https://www.eettaiwan.com/news/article/20180416NT31-season-voice-based-personal-assistants?utm_source=EETT%20Article%20Alert&utm_medium=Email&utm_campaign=2018-04-17)

    > 過程和原理
    > 
    > 作為一名開發者和設計師，要充份使用這項技術，重要的是瞭解如下的完整命令互動過程：
    > 
    > * 虛擬助理使用一個觸發詞(如‘Ok Google’、‘Hey Siri’)來「喚醒」，以確保它只在命令下達時才執行。
    > * 音訊被記錄在設備上，經過壓縮並透過Wi-Fi傳輸到雲端。通常會採用降噪演算法來記錄音訊，以便雲端「大腦」更容易理解用戶的命令。
    > * 使用專有的「語音轉文本」(voice-to-text)平台將音訊轉換成文本命令。透過指定的頻率對類比訊號進行採樣，將類比聲波轉換為數位資料。分析數位資料以確定英語音素(‘bb’、‘oo’、‘sh’等)的出現位置。 一旦辨識別出音素，就使用統計建模演算法(如Hidden Markhov模型)來確定特定單詞的可能性。
    > * 使用自然語言處理技術來處理文本以確定所需的操作。 該演算法首先使用詞性標註來確定哪些詞是形容詞、動詞和名詞等，然後將這種標記與統計機器學習模型相結合起來，推斷句子的含義。
    > * 如果命令操作需要進一步的搜尋，系統將立即進行搜尋。例如，「嘿！Siri，什麼是Snapdragon行動平台？」將觸發網際網路搜尋，並返回所得到的資訊。如果該命令類似於「Ok Google，傳簡訊給媽媽」，那麼命令資料(操作：發送簡訊；收件人：媽媽)就會被直接傳送到虛擬助理。
    > 
    > 

### 寫稿機器人

- [百度AI开放平台-全球领先的人工智能服务平台](https://ai.baidu.com/support/news?action=detail&id=140)

- [只好跟著抄了！PTT 創世神的 AI Labs 為「記者快抄」打造寫稿機器人 | TechNews 科技新報](http://technews.tw/2017/08/09/ai-labs-is-using-ai-to-cover-ptt-news/)


## 機器翻譯

### seq2seq

- [从Encoder到Decoder实现Seq2Seq模型](https://zhuanlan.zhihu.com/p/27608348)

- [Neural Machine Translation (seq2seq) Tutorial  |  TensorFlow](https://www.tensorflow.org/tutorials/seq2seq)

- [深度学习笔记——Word2vec和Doc2vec训练实例以及参数解读 - CSDN博客](https://blog.csdn.net/mpk_no1/article/details/72510655)

- [序列到序列的语言翻译模型代码(tensorflow)解析 - grt1st博客](https://www.grt1st.cn/posts/seq2seq-code/)

- [用seq2seq with attention实现中文歌词生成](https://zhuanlan.zhihu.com/p/25280463)

- [tensorflow代码全解析 -3- seq2seq 自动生成文本 - 简书](https://www.jianshu.com/p/9766b317ffa4)

- [從零開始的 Sequence to Sequence | 雷德麥的藏書閣](https://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/)

- [教電腦寫作：AI球評——Seq2seq模型應用筆記(PyTorch + Python3) – Yi-Hsiang Kao – Medium](https://medium.com/@gau820827/%E6%95%99%E9%9B%BB%E8%85%A6%E5%AF%AB%E4%BD%9C-ai%E7%90%83%E8%A9%95-seq2seq%E6%A8%A1%E5%9E%8B%E6%87%89%E7%94%A8%E7%AD%86%E8%A8%98-pytorch-python3-31e853573dd0)

### ByteNet

- [《Neural Machine Translation in Linear Time》阅读笔记](https://zhuanlan.zhihu.com/p/23795111)

    - [[1610.10099] Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099)
    
    > **问题：** 提出了新型的source--target网络结构ByteNet，并通过两个扩张卷积神经网络（Dilated Convolution）堆叠实现,完成了机器翻译任务，并且将时间复杂度控制在线性范围。
    > 
    > **相关工作：**
    > 
    > 1、扩张卷积神经网络
    > 
    > Dilated Convolution network最近很火，2016ICLR上才被提出，本身是用在图像分割领域，但立马被deepmind拿来应用到语音(WaveNet)和nlp领域，且均取得不错的效果。
    > 
    > Dilated Convolution的产生是为了解决全卷积网络（FCN）在图像分割领域的问题，图像分割需要输入和输出在像素的shape保持一致，但由于池化层的存在导致FCN需要通过上采样扩增size,但是上采样并不能将丢失的信息无损的找回所以存在不足。
    > 
    > Dilated Convolution想法很粗暴，既然池化的下采样操作会带来信息损失，那么就把池化层去掉。但是池化层去掉随之带来的是网络各层的感受野变小，这样会降低整个模型的预测精度。Dilated convolution的主要贡献就是，如何在去掉池化下采样操作的同时，而不降低网络的感受野。
    > 
    > ![](https://pic1.zhimg.com/80/v2-3cd4e5ebcae5fa15019c9f4df03bc734_hd.png)
    > 
    > 以![3\times 3](http://www.zhihu.com/equation?tex=3%5Ctimes+3)的卷积核为例，传统卷积核在做卷积操作时，是将卷积核与输入张量中"连续"的![3\times 3](http://www.zhihu.com/equation?tex=3%5Ctimes+3)的patch逐点相乘再求和（如上图a，红色圆点为卷积核对应的输入"像素"，绿色为其在原输入中的感知野）。而dilated convolution中的卷积核则是将输入张量的![3\times 3](http://www.zhihu.com/equation?tex=3%5Ctimes+3)patch隔一定的像素进行卷积运算。如上图b所示，在去掉一层池化层后，需要在去掉的池化层后将传统卷积层换做一个"dilation=2"的dilated convolution层，此时卷积核将输入张量每隔一个"像素"的位置作为输入patch进行卷积计算，可以发现这时对应到原输入的感知野已经扩大（dilate）为![7\times 7](http://www.zhihu.com/equation?tex=7%5Ctimes+7)；同理，如果再去掉一个池化层，就要将其之后的卷积层换成"dilation=4"的dilated convolution层，如上图c所示。这样一来，即使去掉池化层也能保证网络的感受野，从而确保图像语义分割的精度。
    > 
    > ![](https://pic3.zhimg.com/80/v2-d1b7575900e42c5189997de03059d126_hd.png)
    > 
    > 上图是WaveNet里的Dilated Convolution的示意图，理解起来更容易，卷积的的输入像素的间距由1-2-4-8，虽然没有池化层，但是随着层数越深覆盖的原始输入信息依旧在增加。
    > 
    > **主要内容：**
    > 
    > （1）网络结构ByteNet
    > 
    > ![](https://pic1.zhimg.com/80/v2-0d72713562d4015e420f159e703958a8_hd.png)
    > 
    > ByteNet的核心分别是蓝色网络为target network, 红色网络为source network,相比其他encode-decode网络的区别在于，不再仅仅通过一个encode vector或者attention vector连接两个网络，而是将source network每一个输出都作为target vector对应位置上的输入。
    > 
    > 上图示例是以Dilated Convolution实现，实际本文提出的ByteNet的两个部分target network和source network均可以用RNN实现。
    > 
    > ![](https://pic3.zhimg.com/80/v2-b6f86867a9721e9874a65c65772d54ee_hd.png)
    > 
    > 本文就一个公式，理解起来就是当前预测词的输出概率由已预测词和当前token的source network encode vector决定。
    > 
    > 以下为本文的几个关键知识点
    > 
    > （2）Dynamic Unfolding
    > 
    > ![](https://pic2.zhimg.com/80/v2-e79507690ba294fbd8324cdd0033fb75_hd.png)
    > 
    > target网络的结束标志是以生成结束符EOS为标志，但由网络结构可以看到，target network的当前token的source vector输入长度和source input length是一致，由于翻译任务的长度存在变化，所以EOS出现位置可能长于source input length，因此byteNet当超出source input length时，target network的输出只有已解码词决定。
    > 
    > （3）Masked One-dimensional Convolutions
    > 
    > 为了避免在解决的时候卷积过程引入为解码词信息，引入Masked One-dimensional Convolutions，即卷积核只与当前token之前的输入进行卷积操作。
    > 
    > （4）Residual Blocks
    > 
    > ![](https://pic2.zhimg.com/80/v2-6163436971c333afb27f7567807be145_hd.png)
    > 
    > 卷积网络使用残差卷积网络。残差卷积网络的特点就是卷积层的最终输出由卷积结果和输入相加而得，因而变相理解为每一层只拟合输入和输出的残差，因而得名残差网络
    > 
    > （5）Sub-Batch Normalization
    > 
    > batch normalization是为了保持隐层的输出分布不发生漂移，而进行正规化操作，比如限定输出分布服从正态分布。好处是防止过拟合，防止梯度弥散，加大搜索跳出局部最小，保证源控件和目标空间一致（官方）
    > 
    > 标准的batch normalization会平均所有token的隐层输出，这会导致在target网络中要将未来时间步的考虑进来操作,这不符合时序关系，所以作者对BN层进行了改进，提出了sub batch normalization，大致意思，batch normalization分为两步，在全部训练完之前只用batch个sample里当前token以前的结果，训练完后再用所有结果一起normalization操作。
    > 
    > （6）Bag of Character n-Grams
    > 
    > 为了提高模型的信息容量，输入的每个token的vector不再单独是单独的词向量，而是相邻的n-grams的词的词向量求和。考虑到没有RNN模型特征融合的那么好而做的弥补吧。
    > 
    > **实验结果：**
    > 
    > （1）模型性能对比
    > 
    > ![](https://pic2.zhimg.com/80/v2-e17c18d8a0f70fa5ac08e76eeb4a311d_hd.png)
    > 
    > 作者还实验了将target network和source network分别用RNN实现，在上图实验列表里名为Recurrent ByteNet，注意和seq2seq的区别，seq2seq是取encode 网络的最后一个hidden state的输出作为decode网络的输入，而ByteNet中target network解码每个词的输入是由对应位置的source network输出决定，对应到RNN即source network第n个timeStep的输出作为target network第n个timestep的输入。
    > 
    > resolution preserving表示需要开辟额外空间保存历史信息。
    > 
    > Time代表时间复杂度，RP代表resolution preserving，Paths代表source网络从输入到输出的路径长度，Patht代表target网络从输入到输出的路径长度。Path越短代表反向传播的层数越少，网络越容易收敛，因为网络越浅越不容易出现梯度扩散。
    > 
    > （2）语言模型实验
    > 
    > 1、任务：字符生成预测，数据集为Hutter Prize version of the Wikipedia dataset
    > 
    > ![](https://pic4.zhimg.com/80/v2-7942cb10517176e4d562045f98bb4f13_hd.png)
    > 
    > ByteNet Decoder采用RNN实现，在交叉熵的评价指标上取得了state of the art结果。
    > 
    > 2、任务：机器翻译。数据集为WMT English to German translation task
    > 
    > ![](https://pic1.zhimg.com/80/v2-e81bdedfc7c6e9c6641325d8a0fadfdc_hd.png)
    > 
    > **简评：**
    > 
    > 1、虽然Dilated Convolution在某些实验任务上取得了不错的实验结果，但不能否认带池化层的卷积网络的优势，池化层存在特征选择的意义保留了强特征，从下采样的角度出发它降低了卷积网络的计算复杂度，因为随着卷积层的增加，通道个数也是呈倍数增加，如果不对应降低feature map的size,随着层数越深计算量就会变得相当大。
    > 
    > 2、但是没有池化层的Dilated Convolution使得卷积网络更适用于语音和文本，因为池化层的ave或者max操作要从全局出发，使得模型必须fix输入的规模。对于输入长度存在波动的语音和文本非常不方便。但是去掉池化层后使得前馈操作变得更灵活。
    > 
    > 3、本文的网络结构相比传统的seq2seq模型，优势在于训练时source network和target network均可并行计算，在预测时source network可并行计算。另外它在长输入问题上的优势应该更大。该网络可能将目前局限于两三句输入的自动摘要任务扩展到一个文档的输入。
    > 

### attention model

- [tensorflow/nmt: TensorFlow Neural Machine Translation Tutorial](https://github.com/tensorflow/nmt)

- [Attention Is All You Need：基於注意力機制的機器翻譯模型 – Youngmi huang – Medium](https://medium.com/@cyeninesky3/attention-is-all-you-need-%E5%9F%BA%E6%96%BC%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%E7%9A%84%E6%A9%9F%E5%99%A8%E7%BF%BB%E8%AD%AF%E6%A8%A1%E5%9E%8B-dcc12d251449)

- [[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- [Kyubyong/transformer: A TensorFlow Implementation of the Transformer: Attention Is All You Need](https://github.com/Kyubyong/transformer)

- [(4 封私信 / 23 条消息)如何理解谷歌团队的机器翻译新作《Attention is all you need》？ - 知乎](https://www.zhihu.com/question/61077555)

- [《Attention is All You Need》浅读（简介+代码） - 科学空间|Scientific Spaces](https://kexue.fm/archives/4765)








### UNSUPERVISED MACHINE TRANSLATION 

- [《UNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLY》阅读笔记](https://zhuanlan.zhihu.com/p/32375955)

- [【Science】無監督式機器翻譯，不需要人類干預和平行文本 - 壹讀](https://read01.com/Rnzx84N.html)

    > 這兩篇使用非常相似的方法的新論文也可以在句子層面進行翻譯。它們都使用兩種訓練策略，稱為反向翻譯和去噪（Back translation and Denoising）。在反向翻譯中，先把一種語言的句子大致翻譯成另一種語言，然後再翻譯回原來的語言。如果翻譯後的句子與最初的句子不一致，則調整神經網絡再次翻譯，直到變得越來越接近。
    > 
    > 去噪與反向翻譯類似，但不是從一種語言到另一種語言然後再回來，而是從一種語言（通過重新排列或刪除單詞）中添加噪聲，並嘗試將其翻譯回最開始的語言。這些方法的組合，能夠教給網絡更深層次的語言結構。
    > 
    > 但是兩種技術之間還是有著細微的差異。 UPV的系統在訓練期間更頻繁地進行反向翻譯。由位於賓夕法尼亞州匹茲堡的 Facebook 計算機科學家Guillaume Lample和合作者創建的另一個系統在翻譯過程中則增加了一個額外的步驟。在將一個語言解碼為另一種語言之前，這兩個系統都將其從一種語言編碼為更抽象的表示，但Facebook系統的研究員認為，其系統的「中間語言」是真正抽象的。 Artetxe和Lample都表示，他們可以通過應用對方論文的技巧來改善結果。
    > 
    > 兩篇論文之間唯一可以直接比較的結果是從以包含了3000萬句子的英法文本資料庫中進行的翻譯，兩個系統都在雙語評估替補評分（用來衡量翻譯的準確性）上的得分都在15分左右 。這個數字還比不上谷歌翻譯。谷歌翻譯使用有監督的方法，在同類測試上的得分是40多左右，人類水平是50分左右。但是，這些方法都比詞對詞的翻譯要好。


### Phrase-Based & Neural Unsupervised Machine Translation

- [[1804.07755] Phrase-Based & Neural Unsupervised Machine Translation](https://arxiv.org/abs/1804.07755)

    - [facebookresearch/UnsupervisedMT: Phrase-Based & Neural Unsupervised Machine Translation](https://github.com/facebookresearch/UnsupervisedMT)


- [FAIR新一代无监督机器翻译：模型更简洁，性能更优 | 机器之心](https://www.jiqizhixin.com/articles/Phrase-Based-Neural-Unsupervised-Machine-Translation)

    > ![](https://image.jiqizhixin.com/uploads/editor/84b75cea-df8a-4e0f-b315-8580b97e2703/1524896803075.jpg)
    > 
    > *图 1：无监督 MT 三原则的图示。*
    > 
    > A）两个单语数据集。标记对应于句子（详细信息请参见图例）。B）原则一：初始化。比如，这两个分布通过使用推断的双语词典执行逐词翻译而大致对齐。C）原则二：语言建模。在每个域中独立地学习语言模型，以推断数据中的结构（下面的连续曲线）；它在对句子进行去噪/纠正之前充当数据驱动（如图所示，借助弹簧将曲线外的句子拉回）。D）原则三：回译。从观察到的源语句（红色实心圆）开始，我们使用当前的源语→目标语模型进行翻译（虚线箭头），从而产生可能不正确的翻译（空心圆附近的蓝色十字）。从这次（反向）翻译开始，我们使用目标语→源语模型（连续箭头）来重建初始语言中的句子。重建结果与初始语句的差异为训练目标语→源语模型参数提供了误差信号。在相反的方向上应用相同的步骤来训练源语→目标语模型。
    > 
    > 



### CipherGAN

- [无平行文本照样破解密码，CipherGAN有望提升机器翻译水平](https://zhuanlan.zhihu.com/p/33672256)

    > 针对CipherGAN可以使用非平行文本作输入的特点，Gomez在接受Newsweek外媒采访的时候，也提到了，“密码破译的模型思路也能迁移到非监督学习的翻译上。”
    > 
    > 因为语言翻译常面临的难题是，缺乏足够的平行语料。
    > 
    > 正好和非配对明文密文的密码破译过程很相似。
    > 
    > 


## 情意分析

### CNN-SVM 諷刺偵測

- [[1610.08815] A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural Networks](https://arxiv.org/abs/1610.08815)

- [Detecting Sarcasm with Deep Convolutional Neural Networks](https://www.kdnuggets.com/2018/06/detecting-sarcasm-deep-convolutional-neural-networks.html#.Wyu7yLVD5OQ.linkedin)

    - [Detecting Sarcasm with Deep Convolutional Neural Networks](https://medium.com/dair-ai/detecting-sarcasm-with-deep-convolutional-neural-networks-4a0657f79e80)

    > ![](https://cdn-images-1.medium.com/max/1200/0*GVbW_tOQMN1F1lcw.)
    > 
    > To obtain the other features — sentiment (S), emotion (E), and personality (P) — CNN models are pre-trained and used to extract features from the sarcasm datasets. Different training datasets were used to train each model. (Refer to paper for more details)




## 閱讀理解

### SQuAD

- [机器这次击败人之后，争论一直没平息 | SQuAD风云](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247493419&idx=1&sn=73425fec04482f14f6b9b7316e425e63&chksm=e8d05059dfa7d94fc1457a36d4f62cb1b8a057ce18388fbad448aa6b53f4dbb1299cfd697724&scene=21#wechat_redirect)


    > SQuAD被称为行业公认的机器阅读理解顶级水平测试，可以理解为机器阅读理解领域的ImageNet。它们同样出自斯坦福，同样是一个数据集，搭配一个竞争激烈的竞赛。
    > 
    > 这个竞赛基于SQuAD问答数据集，考察两个指标：EM和F1。
    > 
    > EM是指精确匹配，也就是模型给出的答案与标准答案一模一样；F1，是根据模型给出的答案和标准答案之间的重合度计算出来的，也就是结合了召回率和精确率。
    > 
    > 目前阿里、微软团队并列第一，其中EM得分微软（r-net+融合模型）更高，F1得分阿里（SLQA+融合模型）更高。但是他们在EM成绩上都击败了“人类表现”。
    > 
    > ---
    > 
    > 2016年，斯坦福大学从维基百科上随机选取了536篇文章，随后采用众包的方式，由人类阅读这些文章后，提出问题并人工标注出答案，构成了包含10万多个问题的阅读理解数据集SQuAD。
    > 
    > 对于这样一个数据集，以色列巴伊兰大学的著名NLP研究者Yoav Goldberg的评价是太局限（restricted）了。
    > 
    > 
    > 早在好几个月之前，AI在SQuAD上接近人类得分的时候，Goldberg就专门写了个PPT，把SQuAD批判了一番。
    > 
    > ![](http://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtD6fFYcb6DzyJFYv9qXbIXrhnSnicBCVRib5QEQ9QvplO5Jb1gicibv4xfnK4VOlxMTBImGVvcnuAlibPQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
    > 
    > 他列举了SQuAD的三大不足：
    > 
    > -   受限于可以选择span来回答的问题；
    > 
    > -   需要在给定的段落里寻找答案；
    > 
    > -   段落里保证有答案。
    > 
    > 对于这些不足，DeepMind前不久发布的[NarrativeQA论文](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247492423&idx=2&sn=c02f90e09a49c5c41d7aac3472573b94&chksm=e8d05435dfa7dd235aeb66aae46bc619f3cb6940d88b5d00b34204c3e38757f48480de2c6f6f&scene=21#wechat_redirect)做了更详细的说明。
    > 
    > ---
    > 
    > 他们认为，由于SQuAD问题的答案必须是给定段落中的内容，这就导致很多评估阅读理解能力应该用到的合情合理的问题，根本没法问。
    > 
    > 同时，这种简单的答案通过文档表面的信号就能提取出来，对于无法用文中短语来回答、或者需要用文中几个不连续短语来回答的问题，SQuAD训练出来的模型无法泛化。
    > 
    > 另外，SQuAD虽然问题很多，但其实用到的文章又少又短，这就限制了整个数据集词汇和话题的多样性。
    > 
    > 因此，SQuAD上表现不错的模型，如果要用到更复杂的问题上，可扩展性和适用性都很成问题。
    > 


## 論證推理 Argument Reasoning



### SemEval-2018 Task 12

- [[1708.01425] The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants](https://arxiv.org/abs/1708.01425)

- [UKPLab/argument-reasoning-comprehension-task: The Argument Reasoning Comprehension Task: Source codes & Datasets](https://github.com/UKPLab/argument-reasoning-comprehension-task)

- [SemEval-2018 Task 12 - The Argument Reasoning Comprehension Task](https://competitions.codalab.org/competitions/17327)

- [habernal/semeval2018-task12-results: Official results of the SemEval 2018 Task 12: The Argument Reasoning Comprehension Task](https://github.com/habernal/semeval2018-task12-results)



## 論述挖掘、事實抽取、來源分類


### Argument Mining on Twitter

- [Argument Mining on Twitter: Arguments, Facts and Sources - D17-1245](http://aclweb.org/anthology/D17-1245)

- [mihaidusmanu/twitter-opinion-detection](https://github.com/mihaidusmanu/twitter-opinion-detection)



## 空間關係

### SLIM 空間語言編碼

- [[1807.01670] Encoding Spatial Relations from Natural Language](https://arxiv.org/abs/1807.01670)

- [DeepMind提出空間語言集成模型SLIM，有效編碼自然語言的空間關係 - 幫趣](http://bangqu.com/Q59Yc2.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > **3 模型描述**
    > 
    > 我們提出了一種模型，該模型學習將單個底層輸入的多種描述集成到單個表徵中，隨後在多模態設置中利用該表徵生成新數據。
    > 
    > 我們將該模型稱爲空間語言集成模型（Spatial Language Integrating Model，SLIM）。其靈感來自於生成查詢網絡（Generative Query Network，Eslami et al. 2018），該網絡集成了多個視覺輸入，可用於生成相同環境的新視圖。爲了讓表徵能編碼視點無關的場景描述，設置該模型使之在構建表徵之前不知道哪個視點會被解碼。在我們的例子裏，向模型輸入從 n 個不同視點所看到的場景的文本描述，以編碼成場景表徵向量。然後，利用該向量重建從新視點看到的場景圖像。
    > 
    > 如圖 2 所示，我們提出的模型由兩部分組成：一個表徵網絡，它從多視點場景的文本描述中生成聚合表徵（aggregated representation）；一個以場景表徵爲條件的生成網絡，它將場景渲染爲新視點下的圖像。我們對這兩個網絡進行了如下描述（詳見附錄 A）。
    > 
    > ![](http://i2.bangqu.com/j/news/20180724/Q59Yc21532408446134293qI.png)
    > 
    > *圖 2：模型圖示。表徵網絡解析多個攝像機座標拍攝的多視點場景的多個描述和文本描述。所有視點的表徵被聚合成一個場景表徵向量 r，然後生成網絡使用該向量 r 來重建從新的相機座標看到的場景的圖像。*
    > 
    > ![](http://i2.bangqu.com/j/news/20180724/Q59Yc215324084485682GE36.png)
    > 
    > *圖 3：從合成語言（頂部）和自然語言（底部）模型生成的樣本。相應的描述是：「There is a pink cone to the left of a red torus. There is a pink cone close to a purple cone. The cone is to the left of the cone. There is a red torus to the right of a purple cone.」；「There are two objects in the image. In the back left corner is a light green cone, about half the height of the wall. On the right side of the image is a bright red capsule. It is about the same height as the cone, but it is more forward in the plane of the image.」*




## 文章分類器


### 垃圾郵件偵測

- [How To Build a Simple Spam-Detecting Machine Learning Classifier](https://hackernoon.com/how-to-build-a-simple-spam-detecting-machine-learning-classifier-4471fe6b816e)

- [Spam detection using neural networks in Python – Emergent // Future – Medium](https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272)


- [Spam Classification | Kaggle](https://www.kaggle.com/benvozza/spam-classification/notebook)

- [[1606.01042] Machine Learning for E-mail Spam Filtering: Review,Techniques and Trends](https://arxiv.org/abs/1606.01042)



#### dataset

- [UCI Machine Learning Repository: Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/Spambase)

- [SMS Spam Collection Dataset | Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/kernels)

- [Fraudulent E-mail Corpus | Kaggle](https://www.kaggle.com/rtatman/fraudulent-email-corpus/data)



## 機器摘要


### deep reinforced summarization


- [Salesforce research](https://einstein.ai/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)

    - [[1705.04304] A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/abs/1705.04304)

    - [An Algorithm Summarizes Lengthy Text Surprisingly Well - MIT Technology Review](https://www.technologyreview.com/s/607828/an-algorithm-summarizes-lengthy-text-surprisingly-well/)



## 文字生成

- [Neural text generation – Phrasee – Medium](https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b)

### google smart compose

- [未來Email將預測你的心思自動寫完？Google大腦首席工程師在官方部落格詳細介紹了原理 - INSIDE 硬塞的網路趨勢觀察](https://www.inside.com.tw/2018/06/20/smart-compose)

    - [Google AI Blog: Smart Compose: Using Neural Networks to Help Write Emails](https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html)


    > Google 的方法是包含利用額外語境的一個方法，該方法是將問題轉換成一個序列到序列（seq2seq）的機器翻譯任務，其中源序列是郵件主題和上封郵件正文（假設存在上封郵件）的串聯，使用者正在寫的郵件是目標序列。儘管該方法在預測品質上表現良好，但它的延遲要比 Google 嚴苛的延遲標準超出了好幾個量級
    > 
    > 為了提高預測品質， Google 將一個 RNN-LM 神經網路與一個 BoW 模型結合起來，結合後的模型在速度上比 seq2seq 模型要快，且只輕微犧牲了預測品質。在該混合演算法中， Google 通過把詞嵌套們平均分配在每個區域內，來對郵件主題和此前的郵件內容進行編碼。隨後 Google 將這些平均分配後的嵌套連接在一起，並在每次執行解碼步驟時將它們提供給目標序列 RNN-LM，過程如下面的模型圖解。
    > 
    >  Smart Compose RNN-LM 模型架構。將郵件主題和此前郵件訊息進行編碼，採用的方法是將它們的詞嵌套平均分配在每一個區域內。隨後，平均後的嵌套會在每次執行解碼步驟時提供給目標序列 RNN-LM。
    > 
    > ![](https://i0.wp.com/www.inside.com.tw/wp-content/uploads/2018/06/model3.png?resize=640%2C236)
    > 


## 機器問答

### NTM+CGNN

- [[1806.09105] One-shot Learning for Question-Answering in Gaokao History Challenge](https://arxiv.org/abs/1806.09105)

    - [我一个理科生造的AI，怎么就去做历史高考题了呢？](https://zhuanlan.zhihu.com/p/38772246)




## 語音生成

### WaveNet

- [WaveNet: A Generative Model for Raw Audio | DeepMind](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
    - [[1609.03499] WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)
    
    ![](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif)

