# 機器學習理論

[toc]
<!-- toc --> 

## Reference

- [Machine Learning Foundations](https://www.slideshare.net/albertycchen/machine-learning-foundations-87730305)

## Activation function


### Softmax

### Hierarchical Softmax

- [類神經網路 -- Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax) « MARK CHANG'S BLOG](http://cpmarkchang.logdown.com/posts/276263--hierarchical-probabilistic-neural-networks-neural-network-language-model)

- [word2vec原理(二) 基于Hierarchical Softmax的模型 - 刘建平Pinard - 博客园](https://www.cnblogs.com/pinard/p/7243513.html)



## Loss function



### MSE 均方誤差(L2 損失)

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529651503728.jpg)
    > 
    > 均方誤差 (MSE) 是最常用的回歸損失函數，計算方法是求預測值與真實值之間距離的平方和，公式如圖。
    > 
    > 下圖是 MSE 函數的圖像，其中目標值是 100，預測值的範圍從 -10000 到 10000，Y 軸代表的 MSE 取值範圍是從 0 到正無窮，並且在預測值為 100 處達到最小。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650984294.jpg)

### MAE 平均絕對值誤差（也稱 L1 損失）

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650940657.jpg)
    > 
    > 平均絕對誤差（MAE）是另一種用於回歸模型的損失函數。MAE 是目標值和預測值之差的絕對值之和。其只衡量了預測值誤差的平均模長，而不考慮方向，取值範圍也是從 0 到正無窮（如果考慮方向，則是殘差/誤差的總和——平均偏差（MBE））。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650862540.jpg)
    > 
    > ---
    > 
    > MSE 對誤差取了平方（令 e=真實值-預測值），因此若 e>1，則 MSE 會進一步增大誤差。如果數據中存在異常點，那麼 e 值就會很大，而 e²則會遠大於|e|。
    > 
    > 因此，相對於使用 MAE 計算損失，使用 MSE 的模型會賦予異常點更大的權重。在第二個例子中，用 RMSE 計算損失的模型會以犧牲了其他樣本的誤差為代價，朝著減小異常點誤差的方向更新。然而這就會降低模型的整體性能。
    > 
    > 如果訓練數據被異常點所污染，那麼 MAE 損失就更好用（比如，在訓練數據中存在大量錯誤的反例和正例標記，但是在測試集中沒有這個問題）。
    > 
    > 直觀上可以這樣理解：如果我們最小化 MSE 來對所有的樣本點只給出一個預測值，那麼這個值一定是所有目標值的平均值。但如果是最小化 MAE，那麼這個值，則會是所有樣本點目標值的中位數。眾所周知，對異常值而言，中位數比均值更加魯棒，因此 MAE 對於異常值也比 MSE 更穩定。
    > 
    > 然而 MAE 存在一個嚴重的問題（特別是對於神經網絡）：更新的梯度始終相同，也就是說，即使對於很小的損失值，梯度也很大。這樣不利於模型的學習。為了解決這個缺陷，我們可以使用變化的學習率，在損失接近最小值時降低學習率。
    > 
    > 而 MSE 在這種情況下的表現就很好，即便使用固定的學習率也可以有效收斂。MSE 損失的梯度隨損失增大而增大，而損失趨於 0 時則會減小。這使得在訓練結束時，使用 MSE 模型的結果會更精確。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650753085.jpg)
    > 

### Huber 

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > Huber 損失對數據中的異常點沒有平方誤差損失那麼敏感。它在 0 也可微分。本質上，Huber 損失是絕對誤差，只是在誤差很小時，就變為平方誤差。誤差降到多小時變為二次誤差由超參數 δ（delta）來控制。當 Huber 損失在 \[0-δ,0+δ\] 之間時，等價為 MSE，而在 \[-∞,δ\] 和 \[δ,+∞\] 時為 MAE。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650718886.jpg)
    > 
    > 這裡超參數 delta 的選擇非常重要，因為這決定了你對與異常點的定義。當殘差大於 delta，應當採用 L1（對較大的異常值不那麼敏感）來最小化，而殘差小於超參數，則用 L2 來最小化。
    > 
    > **為何要使用 Huber 損失？**
    > 
    > 使用 MAE 訓練神經網絡最大的一個問題就是不變的大梯度，這可能導致在使用梯度下降快要結束時，錯過了最小點。而對於 MSE，梯度會隨著損失的減小而減小，使結果更加精確。
    > 
    > 在這種情況下，Huber 損失就非常有用。它會由於梯度的減小而落在最小值附近。比起 MSE，它對異常點更加魯棒。因此，Huber 損失結合了 MSE 和 MAE 的優點。但是，Huber 損失的問題是我們可能需要不斷調整超參數 delta。


### Log-Cosh

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > Log-cosh 是另一種應用於回歸問題中的，且比 L2 更平滑的的損失函數。它的計算方式是預測誤差的雙曲餘弦的對數。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650678423.jpg)
    > 
    > 優點：對於較小的 x，log(cosh(x)) 近似等於 (x^2)/2，對於較大的 x，近似等於 abs(x)-log(2)。這意味著’logcosh’ 基本類似於均方誤差，但不易受到異常點的影響。它具有 Huber 損失所有的優點，但不同於 Huber 損失的是，Log-cosh 二階處處可微。
    > 
    > 為什麼需要二階導數？許多機器學習模型如 XGBoost，就是採用牛頓法來尋找最優點。而牛頓法就需要求解二階導數（Hessian）。因此對於諸如 XGBoost 這類機器學習框架，損失函數的二階可微是很有必要的。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650587995.jpg)
    > 
    > 但 Log-cosh 損失也並非完美，其仍存在某些問題。比如誤差很大的話，一階梯度和 Hessian 會變成定值，這就導致 XGBoost 出現缺少分裂點的情況。

### Quantile

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > 當我們更關注區間預測而不僅是點預測時，分位數損失函數就很有用。使用最小二乘回歸進行區間預測，基於的假設是殘差（y-y_hat）是獨立變量，且方差保持不變。
    > 
    > 一旦違背了這條假設，那麼線性回歸模型就不成立。但是我們也不能因此就認為使用非線性函數或基於樹的模型更好，而放棄將線性回歸模型作為基線方法。這時，分位數損失和分位數回歸就派上用場了，因為即便對於具有變化方差或非正態分佈的殘差，基於分位數損失的回歸也能給出合理的預測區間。
    > 
    > 下面讓我們看一個實際的例子，以便更好地理解基於分位數損失的回歸是如何對異方差數據起作用的。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650532097.jpg)
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650491449.jpg)
    > 
    > ---
    > 
    > 如何選取合適的分位值取決於我們對正誤差和反誤差的重視程度。損失函數通過分位值（γ）對高估和低估給予不同的懲罰。例如，當分位數損失函數 γ=0.25 時，對高估的懲罰更大，使得預測值略低於中值。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650454121.jpg)
    > 
    > γ 是所需的分位數，其值介於 0 和 1 之間。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650428251.jpg)
    > 
    > 這個損失函數也可以在神經網絡或基於樹的模型中計算預測區間。以下是用 Sklearn 實現梯度提升樹回歸模型的示例。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650390909.jpg)
    > 
    > 上圖表明：在 sklearn 庫的梯度提升回歸中使用分位數損失可以得到 90％ 的預測區間。其中上限為 γ=0.95，下限為 γ=0.05。
    > 
    > ---
    > 
    > **對比研究**
    > 
    > 為了證明上述所有損失函數的特點，讓我們來一起看一個對比研究。首先，我們建立了一個從 sinc（x）函數中採樣得到的數據集，並引入了兩項人為噪聲：高斯噪聲分量 ε〜N（0，σ2）和脈衝噪聲分量 ξ〜Bern（p）。
    > 
    > 加入脈衝噪聲是為了說明模型的魯棒效果。以下是使用不同損失函數擬合 GBM 回歸器的結果。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650322786.jpg)
    > 
    > 連續損失函數：（A）MSE 損失函數；（B）MAE 損失函數；（C）Huber 損失函數；（D）分位數損失函數。將一個平滑的 GBM 擬合成有噪聲的 sinc（x）數據的示例：（E）原始 sinc（x）函數；（F）具有 MSE 和 MAE 損失的平滑 GBM；（G）具有 Huber 損失的平滑 GBM ，且δ={4,2,1}；（H）具有分位數損失的平滑的 GBM，且α={0.5,0.1,0.9}。
    > 
    > 仿真對比的一些觀察結果：
    > 
    > *   MAE 損失模型的預測結果受脈衝噪聲的影響較小，而 MSE 損失函數的預測結果受此影響略有偏移。
    > *   Huber 損失模型預測結果對所選超參數不敏感。
    > *   分位數損失模型在合適的置信水平下能給出很好的估計。
    > 



### Cross Entropy

- [A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)


## 夏農熵 (Shannon entropy)

- [Shannon entropy in the context of machine learning and AI](https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32)



## 奇異值分解

- [The SIngular Value Decomposition (SVD) song: It Had To Be U - YouTube](https://www.youtube.com/watch?v=fKVRSbFKnEw)



## Boosted tree

- [SGHMC+DDP Learning - BoostedTree.pdf](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)


## Blending and Bagging

- [207_handout.pdf](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)

## 評估方法 Accuracy Metrics

### Accuracy

||predicted 0|predicted 1|
|--|--|--|
|Actual 0|True Negative |False Positive|
|Actual 1| False Negative | True Positive |

$$ Accuracy = \frac{All\ True\ Predicted}{ All\ Possible}\\
(做出正確判定的機率)\\
= \frac{\#TN+\#TP}{\#TP+\#FP+\#TN+\#FN}
$$

### Precision and Recall

$$
Precision = \frac{Predicted\ 1\ and\ Actual\ 1}{Predicted\ 1} \\(判定為貓，真的為貓的機率)\\
= \frac{\#TP}{\#TP+\#FP}
$$

$$ 
Recall = \frac{Predicted\ 1\ and\ Actual\ 1}{Actual\ 1} \\(把全部的貓找回來的機率)\\
= \frac{\#TP}{\#TP+\#FN} 
$$

### F1 Score
$$
F_1 = \frac{2}{ \frac{1}{recall} + \frac{1}{precision}}\\
(recall\ 和\ precision\ 的調和平均)\\
= 2 * \frac{precision * recall}{precision + recall}
$$ 

- [如何理解与应用调和平均数？ - LIQiNG的回答 - 知乎](https://www.zhihu.com/question/23096098/answer/340657629)

    调和平均数，强调了较小值的重要性；在机器学习中。召回率为R, 准确率为P。使用他们对算法的评估，这两个值通常情况下相互制约。为了更加方便的评价算法的好坏。于是引入了F1值。F1为准确率P和召回率R的调和平均数。为什么F1使用调和平均数，而不是数字平均数。举个例子：当R 接近于1, P 接近于 0 时。采用调和平均数的F1值接近于0；而如果采用算数平均数F1的值为0.5；显然采用调和平均数能更好的评估算法的性能。等效于评价R和P的整体效果

### ROC Curves and Area Under the Curve(AUC)

<iframe width="560" height="315" src="https://www.youtube.com/embed/OAl6eAyP-yo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

- [机器学习和统计里面的auc怎么理解？ - 现在几点了的回答 - 知乎](https://www.zhihu.com/question/39840928/answer/83576302)

    >从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC



### Matthew's Correlation Coefficient
* 如果 sample size 有 unbalanced 的現象可以利用此 accuacy metric 去測量預測精準度
$$
MCC= \frac{TP*TN - FP*FN}{\sqrt{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}}
$$


## 驗證集、測試集與過擬合

- [常用測試集帶來過擬合？你真的能控制自己不根據測試集調參嗎 - 幫趣](http://bangqu.com/G53jP8.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > 儘管對比新模型與之前模型的結果是非常自然的想法，但很明顯當前的研究方法論削弱了一個關鍵假設：分類器與測試集是獨立的。這種不匹配帶來了一種顯而易見的危險，研究社區可能會輕易設計出只在特定測試集上性能良好，但無法泛化至新數據的模型 [1]。
    > 
    > ---
    > 
    > 該研究分爲三步：
    > 
    > 1. 首先，研究者創建一個新的測試集，將新測試集的子類別分佈與原始 CIFAR-10 數據集進行仔細匹配。
    > 
    > 2. 在收集了大約 2000 張新圖像之後，研究者在新測試集上評估 30 個圖像分類模型的性能。結果顯示出兩個重要現象。一方面，從原始測試集到新測試集的模型準確率顯著下降。例如，VGG 和 ResNet 架構 [7, 18] 的準確率從 93% 下降至新測試集上的 85%。另一方面，研究者發現在已有測試集上的性能可以高度預測新測試集上的性能。即使在 CIFAR-10 上的微小改進通常也能遷移至留出數據。
    > 
    > 3. 受原始準確率和新準確率之間差異的影響，第三步研究了多個解釋這一差距的假設。一種自然的猜想是重新調整標準超參數能夠彌補部分差距，但是研究者發現該舉措的影響不大，僅能帶來大約 0.6% 的改進。儘管該實驗和未來實驗可以解釋準確率損失，但差距依然存在。
    > 
    > 總之，研究者的結果使得當前機器學習領域的進展意味不明。適應 CIFAR-10 測試集的努力已經持續多年，模型表現的測試集適應性並沒有太大提升。頂級模型仍然是近期出現的使用 Cutout 正則化的 Shake-Shake 網絡 [3, 4]。此外，該模型比標準 ResNet 的優勢從 4% 上升至新測試集上的 8%。這說明當前對測試集進行長時間「攻擊」的研究方法具有驚人的抗過擬合能力。
    > 
    > 但是該研究結果令人對當前分類器的魯棒性產生質疑。儘管新數據集僅有微小的分佈變化，但廣泛使用的模型的分類準確率卻顯著下降。例如，前面提到的 VGG 和 ResNet 架構，其準確率損失相當於模型在 CIFAR-10 上的多年進展 [9]。注意該實驗中引入的分佈變化不是對抗性的，也不是不同數據源的結果。因此即使在良性設置中，分佈變化也對當前模型的真正泛化能力帶來了嚴峻挑戰。
    > 
    > 

### 不平衡數據(imbalanced data)

- [Classification when 80% of my training set is of one class.：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)

- [In classification, how do you handle an unbalanced training set? - Quora](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)



- [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)

    > ### 1) Can You Collect More Data?
    > 
    > You might think it's silly, but collecting more data is almost always overlooked.
    > 
    > Can you collect more data? Take a second and think about whether you are able to gather more data on your problem.
    > 
    > A larger dataset might expose a different and perhaps more balanced perspective on the classes.
    > 
    > More examples of minor classes may be useful later when we look at resampling your dataset.
    > 
    > ### 2) Try Changing Your Performance Metric
    > 
    > Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.
    > 
    > There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes.
    > 
    > I give more advice on selecting different performance measures in my post "[Classification Accuracy is Not Enough: More Performance Measures You Can Use](http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)".
    > 
    > In that post I look at an imbalanced dataset that characterizes the recurrence of breast cancer in patients.
    > 
    > From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:
    > 
    > -   **Confusion Matrix**: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).
    > -   **Precision**: A measure of a classifiers exactness.
    > -   **Recall**: A measure of a classifiers completeness
    > -   **F1 Score (or F-score)**: A weighted average of precision and recall.
    > 
    > I would also advice you to take a look at the following:
    > 
    > -   **Kappa (or [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))**: Classification accuracy normalized by the imbalance of the classes in the data.
    > -   **ROC Curves**: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.
    > 
    > You can learn a lot more about using ROC Curves to compare classification accuracy in our post "[Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)".
    > 
    > Still not sure? Start with kappa, it will give you a better idea of what is going on than classification accuracy.
    > 
    > ### 3) Try Resampling Your Dataset
    > 
    > You can change the dataset that you use to build your predictive model to have more balanced data.
    > 
    > This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:
    > 
    > 1.  You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or
    > 2.  You can delete instances from the over-represented class, called under-sampling.
    > 
    > These approaches are often very easy to implement and fast to run. They are an excellent starting point.
    > 
    > In fact, I would advise you to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred accuracy measures.
    > 
    > You can learn a little more in the the Wikipedia article titled "[Oversampling and undersampling in data analysis](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)".
    > 
    > #### Some Rules of Thumb
    > 
    > -   Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)
    > -   Consider testing over-sampling when you don't have a lot of data (tens of thousands of records or less)
    > -   Consider testing random and non-random (e.g. stratified) sampling schemes.
    > -   Consider testing different resampled ratios (e.g. you don't have to target a 1:1 ratio in a binary classification problem, try other ratios)
    > 
    > ### 4) Try Generate Synthetic Samples
    > 
    > A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.
    > 
    > You could sample them empirically within your dataset or you could use a method like Naive Bayes that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.
    > 
    > There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique.
    > 
    > As its name suggests, SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.
    > 
    > Learn more about SMOTE, see the original 2002 paper titled "[SMOTE: Synthetic Minority Over-sampling Technique](http://www.jair.org/papers/paper953.html)".
    > 
    > There are a number of implementations of the SMOTE algorithm, for example:
    > 
    > -   In Python, take a look at the "[UnbalancedDataset](https://github.com/fmfn/UnbalancedDataset)" module. It provides a number of implementations of SMOTE as well as various other resampling techniques that you could try.
    > -   In R, the [DMwR package](https://cran.r-project.org/web/packages/DMwR/index.html) provides an implementation of SMOTE.
    > -   In Weka, you can use the [SMOTE supervised filter](http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html).
    > 
    > ### 5) Try Different Algorithms
    > 
    > As always, I strongly advice you to not use your favorite algorithm on every problem. You should at least be spot-checking a variety of different types of algorithms on a given problem.
    > 
    > For more on spot-checking algorithms, see my post "Why you should be Spot-Checking Algorithms on your Machine Learning Problems".
    > 
    > That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed.
    > 
    > If in doubt, try a few popular decision tree algorithms like C4.5, C5.0, CART, and Random Forest.
    > 
    > For some example R code using decision trees, see my post titled "[Non-Linear Classification in R with Decision Trees](http://machinelearningmastery.com/non-linear-classification-in-r-with-decision-trees/)".
    > 
    > For an example of using CART in Python and scikit-learn, see my post titled "[Get Your Hands Dirty With Scikit-Learn Now](http://machinelearningmastery.com/get-your-hands-dirty-with-scikit-learn-now/)".
    > 
    > ### 6) Try Penalized Models
    > 
    > You can use the same algorithms but give them a different perspective on the problem.
    > 
    > Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.
    > 
    > Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA.
    > 
    > It is also possible to have generic frameworks for penalized models. For example, Weka has a [CostSensitiveClassifier](http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html) that can wrap any classifier and apply a custom penalty matrix for miss classification.
    > 
    > Using penalization is desirable if you are locked into a specific algorithm and are unable to resample or you're getting poor results. It provides yet another way to "balance" the classes. Setting up the penalty matrix can be complex. You will very likely have to try a variety of penalty schemes and see what works best for your problem.
    > 
    > ### 7) Try a Different Perspective
    > 
    > There are fields of study dedicated to imbalanced datasets. They have their own algorithms, measures and terminology.
    > 
    > Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas.
    > 
    > Two you might like to consider are **anomaly detection** and **change detection**.
    > 
    > [Anomaly detection](https://en.wikipedia.org/wiki/Anomaly_detection) is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by it's sequence of system calls. The events are rare and when compared to normal operation.
    > 
    > This shift in thinking considers the minor class as the outliers class which might help you think of new ways to separate and classify samples.
    > 
    > [Change detection](https://en.wikipedia.org/wiki/Change_detection) is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.
    > 
    > Both of these shifts take a more real-time stance to the classification problem that might give you some new ways of thinking about your problem and maybe some more techniques to try.
    > 
    > ### 8) Try Getting Creative
    > 
    > Really climb inside your problem and think about how to break it down into smaller problems that are more tractable.
    > 
    > For inspiration, take a look at the very creative answers on Quora in response to the question "[In classification, how do you handle an unbalanced training set?](http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)"
    > 
    > For example:
    > 
    > > Decompose your larger class into smaller number of other classes...
    > >
    > > ...use a One Class Classifier... (e.g. treat like outlier detection)
    > >
    > > ...resampling the unbalanced training set into not one balanced set, but several. Running an ensemble of classifiers on these sets could produce a much better result than one classifier alone
    > 
    > These are just a few of some interesting and creative ideas you could try.
    > 
    > For more ideas, check out these comments on the reddit post "[Classification when 80% of my training set is of one class](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)".
    > 
    > Pick a Method and Take Action
    > -----------------------------
    > 
    > You do not need to be an algorithm wizard or a statistician to build accurate and reliable models from imbalanced datasets.
    > 
    > We have covered a number of techniques that you can use to model an imbalanced dataset.
    > 
    > Hopefully there are one or two that you can take off the shelf and apply immediately, for example changing your accuracy metric and resampling your dataset. Both are fast and will have an impact straight away.
    > 
    > ***Which method are you going to try?***
    > 
    > A Final Word, Start Small
    > -------------------------
    > 
    > Remember that we cannot know which approach is going to best serve you and the dataset you are working on.
    > 
    > You can use some expert heuristics to pick this method or that, but in the end, the best advice I can give you is to "become the scientist" and empirically test each method and select the one that gives you the best results.
    > 
    > Start small and build upon what you learn.
    > 
    > Want More? Further Reading...
    > ---------------------------
    > 
    > There are resources on class imbalance if you know where to look, but they are few and far between.
    > 
    > I've looked and the following are what I think are the cream of the crop. If you'd like to dive deeper into some of the academic literature on dealing with class imbalance, check out some of the links below.
    > 
    > ### Books
    > 
    > -   [Imbalanced Learning: Foundations, Algorithms, and Applications](http://www.amazon.com/dp/1118074629?tag=inspiredalgor-20)
    > 
    > ### Papers
    > 
    > -   [Data Mining for Imbalanced Datasets: An Overview](http://link.springer.com/chapter/10.1007/978-0-387-09823-4_45)
    > -   [Learning from Imbalanced Data](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5128907)
    > -   [Addressing the Curse of Imbalanced Training Sets: One-Sided Selection](http://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf) (PDF)
    > -   [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](http://dl.acm.org/citation.cfm?id=1007735)
    > 



## 類聚 Clustering

### Quantum Mechanics to Cluster Time Series

- [[1805.01711] Using Quantum Mechanics to Cluster Time Series](https://arxiv.org/abs/1805.01711)

## 降維方法

### PCA （principal component analysis）主成份分析

- [淺談降維方法中的 PCA 與 t-SNE – DnD mag – Medium](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

    > > *將一個具有 n 個特徵空間的樣本，轉換為具有 k 個特徵空間的樣本，其中 k < n*
    > 
    > 以下是 PCA 的主要步驟：
    > 
    > 1.  將數據標準化
    > 2.  建立**共變異數矩陣（covariance matrix）**
    > 3.  利用**奇異值分解（SVD）**求得**特徵向量（eigenvector）**跟**特徵值（eigenvalue）**
    > 4.  通常特徵值會由大到小排列，選取 k 個特徵值與特徵向量
    > 5.  將原本的數據投影（映射）到特徵向量上，得到新的特徵數
    > 
    > PCA 最重要的部分就是奇異值分解，因此接下來的章節讓我們來談談**奇異值分解**
    > 
    > #### 直觀理解奇異值分解
    > 
    > 在矩陣分解當中，奇異值分解是個相當有名的方法。矩陣分解在高中數學當中最常見的用途就是解方程式（如 LU 分解），從奇異值分解的公式當中我們可以直觀地了解：
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*EW7y-TDCOHClCKINXFtlZQ.png)
    > 
    > 
    > 
    > 其中 A 為一個 m x n 的矩陣，𝑈 跟 V 都為正交矩陣，**𝛴 為奇異值矩陣**。奇異值矩陣為矩陣 A 對應的特徵值，在 PCA 當中又叫做**主成份**，代表對保存訊息的重要程度，通常由大到小遞減排列在對角中，是個對稱矩陣。
    > 
    > 那麼這邊的 A 對應什麼呢？當然就是我們的特徵，只是特別要注意的是這邊的 A 我們通常使用**共變異數矩陣（covariance martix）**來求算，記得資料一定要先正規化後在進行奇異值分解
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*xk7Vhzw5dI77BLNl3Dsdlw.png)
    > 
    > 
    > 
    > 因為共變異數矩陣**常用 Sigma 表示，不要跟上面的 𝛴 搞混囉**。因此如果要降維，我們可以用 U 的前 k 列乘上對應 𝛴 當中的特徵向量，就可以得出新的特徵了，而從幾何的角度來看
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*3nASowA14C1z-hIshmfg7w.png)
    > 
    > 
    > 
    > 這樣子的運算在幾何當中，其實是將 X 投影到 U 的前 k 個向量
    > 

### t-SNE 

- [淺談降維方法中的 PCA 與 t-SNE – DnD mag – Medium](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

    > t-SNE 主要是將高維的數據用高斯分佈的機率密度函數近似，而低維數據的部分使用 t 分佈的方式來近似，在使用 KL 距離計算相似度，最後再以梯度下降（或隨機梯度下降）求最佳解 。
    > 
    > #### 高斯分佈的機率密度函數
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*9CSkBUO3tvJAhq8Wzv9s2Q.png)
    > 
    > 
    > 
    > 其中，X 為隨機變量，𝝈 為變異數，𝜇 為平均。
    > 
    > 因此原本高維的數據可以這樣表示：
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*KxxzB_L8Wi1L-FgznjhkyA.png)
    > 
    > 
    > 
    > 而低維的數據用 t 分布的機率密度函數可以這樣表示（自由度為 1）
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*OwvAZgYHAfYvKehREGH9WQ.png)
    > 
    > 
    > 
    > 其中，x 為高維當中的數據，y 為低維當中的數據。P, Q 分別代表機率分佈。
    > 
    > 為什麼會使用 t 分佈來近似低維的數據呢？主要是因為轉換成低維之後一定會丟失許多訊息，所以為了不被異常值影響可以使用 t 分佈。
    > 
    > t 分佈在樣本數較少時，可以比較好模擬母體分布的情形，不容易被異常值所影響。
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1500/1*hkZuHGVNMmS4_MHfxoIDdA.jpeg) 
    > T 分佈與高斯分佈的機率密度函數
    > 
    > #### 兩個分佈之間的相似度
    > 
    > 求算兩個分佈之間的相似度，經常用 KL 距離（Kullback-Leibler Divergence）來表示，也叫做相對熵（Relative Entropy）。
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*geKaV3PDC2idMGUCXroJzw.png)
    > 
    > 
    > 
    > 在 t-SNE 中使用了困惑度（Perp）來當作超參數。
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*BiQ-gYxdbZsGP5KfYUF9Kg.png)
    > 
    > 
    > 
    > 論文中提出通常困惑度在 5 ~ 50 之間。
    > 
    > #### Cost function
    > 
    > 用 KL 距離計算 Cost
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*2XSnUoPaYB2_FMQ8xMzOuQ.png)
    > 
    > 
    > 求梯度可以寫成
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*MzouyFbnSKdeBA7nLJVCPw.png)
    > 
    > 
    > 最後再利用梯度下降法（或隨機梯度下降法）就可以找到最小值了。
    > 
    > ---
    > 
    > #### 實測：使用 MNIST 測試
    > 
    > 測試集可以到[這裡](http://yann.lecun.com/exdb/mnist/)下載，首先我們先用 PCA 降到二維看看。
    > 
    > **PCA**
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1250/1*Be6oSLuXgG0cDi2gJEL-wA.jpeg)
    > 
    > 
    > **t-SNE**
    > 
    > 接下來使用 t-SNE 測試
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1750/1*5i8McPBKmFOMOcCDjl8w4Q.jpeg)
    > 
    > ---
    > 
    > ### 小結
    > 
    > * 當特徵數量過多時，使用 PCA 可能會造成降維後的特徵欠擬合（underfitting），這時可以考慮使用 t-SNE 來降維
    > * t-SNE 的需要比較多的時間執行
    >
    > 

