# æ©Ÿå™¨å­¸ç¿’ç†è«–

[toc]
<!-- toc --> 

## Reference

- [Machine Learning Foundations](https://www.slideshare.net/albertycchen/machine-learning-foundations-87730305)

## Activation function


### Softmax

### Hierarchical Softmax

- [é¡ç¥ç¶“ç¶²è·¯ -- Hierarchical Probabilistic Neural Network Language Model (Hierarchical Softmax) Â« MARK CHANG'S BLOG](http://cpmarkchang.logdown.com/posts/276263--hierarchical-probabilistic-neural-networks-neural-network-language-model)

- [word2vecåŸç†(äºŒ) åŸºäºHierarchical Softmaxçš„æ¨¡å‹ - åˆ˜å»ºå¹³Pinard - åšå®¢å›­](https://www.cnblogs.com/pinard/p/7243513.html)



## Loss function



### MSE å‡æ–¹èª¤å·®(L2 æå¤±)

- [æ©Ÿå™¨å­¸ç¿’å¤§ç¥æœ€å¸¸ç”¨çš„ 5 å€‹å›æ­¸æå¤±å‡½æ•¸ï¼Œä½ çŸ¥é“å¹¾å€‹ï¼Ÿ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529651503728.jpg)
    > 
    > å‡æ–¹èª¤å·® (MSE) æ˜¯æœ€å¸¸ç”¨çš„å›æ­¸æå¤±å‡½æ•¸ï¼Œè¨ˆç®—æ–¹æ³•æ˜¯æ±‚é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼ä¹‹é–“è·é›¢çš„å¹³æ–¹å’Œï¼Œå…¬å¼å¦‚åœ–ã€‚
    > 
    > ä¸‹åœ–æ˜¯ MSE å‡½æ•¸çš„åœ–åƒï¼Œå…¶ä¸­ç›®æ¨™å€¼æ˜¯ 100ï¼Œé æ¸¬å€¼çš„ç¯„åœå¾ -10000 åˆ° 10000ï¼ŒY è»¸ä»£è¡¨çš„ MSE å–å€¼ç¯„åœæ˜¯å¾ 0 åˆ°æ­£ç„¡çª®ï¼Œä¸¦ä¸”åœ¨é æ¸¬å€¼ç‚º 100 è™•é”åˆ°æœ€å°ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650984294.jpg)

### MAE å¹³å‡çµ•å°å€¼èª¤å·®ï¼ˆä¹Ÿç¨± L1 æå¤±ï¼‰

- [æ©Ÿå™¨å­¸ç¿’å¤§ç¥æœ€å¸¸ç”¨çš„ 5 å€‹å›æ­¸æå¤±å‡½æ•¸ï¼Œä½ çŸ¥é“å¹¾å€‹ï¼Ÿ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650940657.jpg)
    > 
    > å¹³å‡çµ•å°èª¤å·®ï¼ˆMAEï¼‰æ˜¯å¦ä¸€ç¨®ç”¨æ–¼å›æ­¸æ¨¡å‹çš„æå¤±å‡½æ•¸ã€‚MAE æ˜¯ç›®æ¨™å€¼å’Œé æ¸¬å€¼ä¹‹å·®çš„çµ•å°å€¼ä¹‹å’Œã€‚å…¶åªè¡¡é‡äº†é æ¸¬å€¼èª¤å·®çš„å¹³å‡æ¨¡é•·ï¼Œè€Œä¸è€ƒæ…®æ–¹å‘ï¼Œå–å€¼ç¯„åœä¹Ÿæ˜¯å¾ 0 åˆ°æ­£ç„¡çª®ï¼ˆå¦‚æœè€ƒæ…®æ–¹å‘ï¼Œå‰‡æ˜¯æ®˜å·®/èª¤å·®çš„ç¸½å’Œâ€”â€”å¹³å‡åå·®ï¼ˆMBEï¼‰ï¼‰ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650862540.jpg)
    > 
    > ---
    > 
    > MSE å°èª¤å·®å–äº†å¹³æ–¹ï¼ˆä»¤ e=çœŸå¯¦å€¼-é æ¸¬å€¼ï¼‰ï¼Œå› æ­¤è‹¥ e>1ï¼Œå‰‡ MSE æœƒé€²ä¸€æ­¥å¢å¤§èª¤å·®ã€‚å¦‚æœæ•¸æ“šä¸­å­˜åœ¨ç•°å¸¸é»ï¼Œé‚£éº¼ e å€¼å°±æœƒå¾ˆå¤§ï¼Œè€Œ eÂ²å‰‡æœƒé å¤§æ–¼|e|ã€‚
    > 
    > å› æ­¤ï¼Œç›¸å°æ–¼ä½¿ç”¨ MAE è¨ˆç®—æå¤±ï¼Œä½¿ç”¨ MSE çš„æ¨¡å‹æœƒè³¦äºˆç•°å¸¸é»æ›´å¤§çš„æ¬Šé‡ã€‚åœ¨ç¬¬äºŒå€‹ä¾‹å­ä¸­ï¼Œç”¨ RMSE è¨ˆç®—æå¤±çš„æ¨¡å‹æœƒä»¥çŠ§ç‰²äº†å…¶ä»–æ¨£æœ¬çš„èª¤å·®ç‚ºä»£åƒ¹ï¼Œæœè‘—æ¸›å°ç•°å¸¸é»èª¤å·®çš„æ–¹å‘æ›´æ–°ã€‚ç„¶è€Œé€™å°±æœƒé™ä½æ¨¡å‹çš„æ•´é«”æ€§èƒ½ã€‚
    > 
    > å¦‚æœè¨“ç·´æ•¸æ“šè¢«ç•°å¸¸é»æ‰€æ±¡æŸ“ï¼Œé‚£éº¼ MAE æå¤±å°±æ›´å¥½ç”¨ï¼ˆæ¯”å¦‚ï¼Œåœ¨è¨“ç·´æ•¸æ“šä¸­å­˜åœ¨å¤§é‡éŒ¯èª¤çš„åä¾‹å’Œæ­£ä¾‹æ¨™è¨˜ï¼Œä½†æ˜¯åœ¨æ¸¬è©¦é›†ä¸­æ²’æœ‰é€™å€‹å•é¡Œï¼‰ã€‚
    > 
    > ç›´è§€ä¸Šå¯ä»¥é€™æ¨£ç†è§£ï¼šå¦‚æœæˆ‘å€‘æœ€å°åŒ– MSE ä¾†å°æ‰€æœ‰çš„æ¨£æœ¬é»åªçµ¦å‡ºä¸€å€‹é æ¸¬å€¼ï¼Œé‚£éº¼é€™å€‹å€¼ä¸€å®šæ˜¯æ‰€æœ‰ç›®æ¨™å€¼çš„å¹³å‡å€¼ã€‚ä½†å¦‚æœæ˜¯æœ€å°åŒ– MAEï¼Œé‚£éº¼é€™å€‹å€¼ï¼Œå‰‡æœƒæ˜¯æ‰€æœ‰æ¨£æœ¬é»ç›®æ¨™å€¼çš„ä¸­ä½æ•¸ã€‚çœ¾æ‰€å‘¨çŸ¥ï¼Œå°ç•°å¸¸å€¼è€Œè¨€ï¼Œä¸­ä½æ•¸æ¯”å‡å€¼æ›´åŠ é­¯æ£’ï¼Œå› æ­¤ MAE å°æ–¼ç•°å¸¸å€¼ä¹Ÿæ¯” MSE æ›´ç©©å®šã€‚
    > 
    > ç„¶è€Œ MAE å­˜åœ¨ä¸€å€‹åš´é‡çš„å•é¡Œï¼ˆç‰¹åˆ¥æ˜¯å°æ–¼ç¥ç¶“ç¶²çµ¡ï¼‰ï¼šæ›´æ–°çš„æ¢¯åº¦å§‹çµ‚ç›¸åŒï¼Œä¹Ÿå°±æ˜¯èªªï¼Œå³ä½¿å°æ–¼å¾ˆå°çš„æå¤±å€¼ï¼Œæ¢¯åº¦ä¹Ÿå¾ˆå¤§ã€‚é€™æ¨£ä¸åˆ©æ–¼æ¨¡å‹çš„å­¸ç¿’ã€‚ç‚ºäº†è§£æ±ºé€™å€‹ç¼ºé™·ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨è®ŠåŒ–çš„å­¸ç¿’ç‡ï¼Œåœ¨æå¤±æ¥è¿‘æœ€å°å€¼æ™‚é™ä½å­¸ç¿’ç‡ã€‚
    > 
    > è€Œ MSE åœ¨é€™ç¨®æƒ…æ³ä¸‹çš„è¡¨ç¾å°±å¾ˆå¥½ï¼Œå³ä¾¿ä½¿ç”¨å›ºå®šçš„å­¸ç¿’ç‡ä¹Ÿå¯ä»¥æœ‰æ•ˆæ”¶æ–‚ã€‚MSE æå¤±çš„æ¢¯åº¦éš¨æå¤±å¢å¤§è€Œå¢å¤§ï¼Œè€Œæå¤±è¶¨æ–¼ 0 æ™‚å‰‡æœƒæ¸›å°ã€‚é€™ä½¿å¾—åœ¨è¨“ç·´çµæŸæ™‚ï¼Œä½¿ç”¨ MSE æ¨¡å‹çš„çµæœæœƒæ›´ç²¾ç¢ºã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650753085.jpg)
    > 

### Huber 

- [æ©Ÿå™¨å­¸ç¿’å¤§ç¥æœ€å¸¸ç”¨çš„ 5 å€‹å›æ­¸æå¤±å‡½æ•¸ï¼Œä½ çŸ¥é“å¹¾å€‹ï¼Ÿ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > Huber æå¤±å°æ•¸æ“šä¸­çš„ç•°å¸¸é»æ²’æœ‰å¹³æ–¹èª¤å·®æå¤±é‚£éº¼æ•æ„Ÿã€‚å®ƒåœ¨ 0 ä¹Ÿå¯å¾®åˆ†ã€‚æœ¬è³ªä¸Šï¼ŒHuber æå¤±æ˜¯çµ•å°èª¤å·®ï¼Œåªæ˜¯åœ¨èª¤å·®å¾ˆå°æ™‚ï¼Œå°±è®Šç‚ºå¹³æ–¹èª¤å·®ã€‚èª¤å·®é™åˆ°å¤šå°æ™‚è®Šç‚ºäºŒæ¬¡èª¤å·®ç”±è¶…åƒæ•¸ Î´ï¼ˆdeltaï¼‰ä¾†æ§åˆ¶ã€‚ç•¶ Huber æå¤±åœ¨ \[0-Î´,0+Î´\] ä¹‹é–“æ™‚ï¼Œç­‰åƒ¹ç‚º MSEï¼Œè€Œåœ¨ \[-âˆ,Î´\] å’Œ \[Î´,+âˆ\] æ™‚ç‚º MAEã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650718886.jpg)
    > 
    > é€™è£¡è¶…åƒæ•¸ delta çš„é¸æ“‡éå¸¸é‡è¦ï¼Œå› ç‚ºé€™æ±ºå®šäº†ä½ å°èˆ‡ç•°å¸¸é»çš„å®šç¾©ã€‚ç•¶æ®˜å·®å¤§æ–¼ deltaï¼Œæ‡‰ç•¶æ¡ç”¨ L1ï¼ˆå°è¼ƒå¤§çš„ç•°å¸¸å€¼ä¸é‚£éº¼æ•æ„Ÿï¼‰ä¾†æœ€å°åŒ–ï¼Œè€Œæ®˜å·®å°æ–¼è¶…åƒæ•¸ï¼Œå‰‡ç”¨ L2 ä¾†æœ€å°åŒ–ã€‚
    > 
    > **ç‚ºä½•è¦ä½¿ç”¨ Huber æå¤±ï¼Ÿ**
    > 
    > ä½¿ç”¨ MAE è¨“ç·´ç¥ç¶“ç¶²çµ¡æœ€å¤§çš„ä¸€å€‹å•é¡Œå°±æ˜¯ä¸è®Šçš„å¤§æ¢¯åº¦ï¼Œé€™å¯èƒ½å°è‡´åœ¨ä½¿ç”¨æ¢¯åº¦ä¸‹é™å¿«è¦çµæŸæ™‚ï¼ŒéŒ¯éäº†æœ€å°é»ã€‚è€Œå°æ–¼ MSEï¼Œæ¢¯åº¦æœƒéš¨è‘—æå¤±çš„æ¸›å°è€Œæ¸›å°ï¼Œä½¿çµæœæ›´åŠ ç²¾ç¢ºã€‚
    > 
    > åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼ŒHuber æå¤±å°±éå¸¸æœ‰ç”¨ã€‚å®ƒæœƒç”±æ–¼æ¢¯åº¦çš„æ¸›å°è€Œè½åœ¨æœ€å°å€¼é™„è¿‘ã€‚æ¯”èµ· MSEï¼Œå®ƒå°ç•°å¸¸é»æ›´åŠ é­¯æ£’ã€‚å› æ­¤ï¼ŒHuber æå¤±çµåˆäº† MSE å’Œ MAE çš„å„ªé»ã€‚ä½†æ˜¯ï¼ŒHuber æå¤±çš„å•é¡Œæ˜¯æˆ‘å€‘å¯èƒ½éœ€è¦ä¸æ–·èª¿æ•´è¶…åƒæ•¸ deltaã€‚


### Log-Cosh

- [æ©Ÿå™¨å­¸ç¿’å¤§ç¥æœ€å¸¸ç”¨çš„ 5 å€‹å›æ­¸æå¤±å‡½æ•¸ï¼Œä½ çŸ¥é“å¹¾å€‹ï¼Ÿ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > Log-cosh æ˜¯å¦ä¸€ç¨®æ‡‰ç”¨æ–¼å›æ­¸å•é¡Œä¸­çš„ï¼Œä¸”æ¯” L2 æ›´å¹³æ»‘çš„çš„æå¤±å‡½æ•¸ã€‚å®ƒçš„è¨ˆç®—æ–¹å¼æ˜¯é æ¸¬èª¤å·®çš„é›™æ›²é¤˜å¼¦çš„å°æ•¸ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650678423.jpg)
    > 
    > å„ªé»ï¼šå°æ–¼è¼ƒå°çš„ xï¼Œlog(cosh(x)) è¿‘ä¼¼ç­‰æ–¼ (x^2)/2ï¼Œå°æ–¼è¼ƒå¤§çš„ xï¼Œè¿‘ä¼¼ç­‰æ–¼ abs(x)-log(2)ã€‚é€™æ„å‘³è‘—â€™logcoshâ€™ åŸºæœ¬é¡ä¼¼æ–¼å‡æ–¹èª¤å·®ï¼Œä½†ä¸æ˜“å—åˆ°ç•°å¸¸é»çš„å½±éŸ¿ã€‚å®ƒå…·æœ‰ Huber æå¤±æ‰€æœ‰çš„å„ªé»ï¼Œä½†ä¸åŒæ–¼ Huber æå¤±çš„æ˜¯ï¼ŒLog-cosh äºŒéšè™•è™•å¯å¾®ã€‚
    > 
    > ç‚ºä»€éº¼éœ€è¦äºŒéšå°æ•¸ï¼Ÿè¨±å¤šæ©Ÿå™¨å­¸ç¿’æ¨¡å‹å¦‚ XGBoostï¼Œå°±æ˜¯æ¡ç”¨ç‰›é “æ³•ä¾†å°‹æ‰¾æœ€å„ªé»ã€‚è€Œç‰›é “æ³•å°±éœ€è¦æ±‚è§£äºŒéšå°æ•¸ï¼ˆHessianï¼‰ã€‚å› æ­¤å°æ–¼è«¸å¦‚ XGBoost é€™é¡æ©Ÿå™¨å­¸ç¿’æ¡†æ¶ï¼Œæå¤±å‡½æ•¸çš„äºŒéšå¯å¾®æ˜¯å¾ˆæœ‰å¿…è¦çš„ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650587995.jpg)
    > 
    > ä½† Log-cosh æå¤±ä¹Ÿä¸¦éå®Œç¾ï¼Œå…¶ä»å­˜åœ¨æŸäº›å•é¡Œã€‚æ¯”å¦‚èª¤å·®å¾ˆå¤§çš„è©±ï¼Œä¸€éšæ¢¯åº¦å’Œ Hessian æœƒè®Šæˆå®šå€¼ï¼Œé€™å°±å°è‡´ XGBoost å‡ºç¾ç¼ºå°‘åˆ†è£‚é»çš„æƒ…æ³ã€‚

### Quantile

- [æ©Ÿå™¨å­¸ç¿’å¤§ç¥æœ€å¸¸ç”¨çš„ 5 å€‹å›æ­¸æå¤±å‡½æ•¸ï¼Œä½ çŸ¥é“å¹¾å€‹ï¼Ÿ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > ç•¶æˆ‘å€‘æ›´é—œæ³¨å€é–“é æ¸¬è€Œä¸åƒ…æ˜¯é»é æ¸¬æ™‚ï¼Œåˆ†ä½æ•¸æå¤±å‡½æ•¸å°±å¾ˆæœ‰ç”¨ã€‚ä½¿ç”¨æœ€å°äºŒä¹˜å›æ­¸é€²è¡Œå€é–“é æ¸¬ï¼ŒåŸºæ–¼çš„å‡è¨­æ˜¯æ®˜å·®ï¼ˆy-y_hatï¼‰æ˜¯ç¨ç«‹è®Šé‡ï¼Œä¸”æ–¹å·®ä¿æŒä¸è®Šã€‚
    > 
    > ä¸€æ—¦é•èƒŒäº†é€™æ¢å‡è¨­ï¼Œé‚£éº¼ç·šæ€§å›æ­¸æ¨¡å‹å°±ä¸æˆç«‹ã€‚ä½†æ˜¯æˆ‘å€‘ä¹Ÿä¸èƒ½å› æ­¤å°±èªç‚ºä½¿ç”¨éç·šæ€§å‡½æ•¸æˆ–åŸºæ–¼æ¨¹çš„æ¨¡å‹æ›´å¥½ï¼Œè€Œæ”¾æ£„å°‡ç·šæ€§å›æ­¸æ¨¡å‹ä½œç‚ºåŸºç·šæ–¹æ³•ã€‚é€™æ™‚ï¼Œåˆ†ä½æ•¸æå¤±å’Œåˆ†ä½æ•¸å›æ­¸å°±æ´¾ä¸Šç”¨å ´äº†ï¼Œå› ç‚ºå³ä¾¿å°æ–¼å…·æœ‰è®ŠåŒ–æ–¹å·®æˆ–éæ­£æ…‹åˆ†ä½ˆçš„æ®˜å·®ï¼ŒåŸºæ–¼åˆ†ä½æ•¸æå¤±çš„å›æ­¸ä¹Ÿèƒ½çµ¦å‡ºåˆç†çš„é æ¸¬å€é–“ã€‚
    > 
    > ä¸‹é¢è®“æˆ‘å€‘çœ‹ä¸€å€‹å¯¦éš›çš„ä¾‹å­ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£åŸºæ–¼åˆ†ä½æ•¸æå¤±çš„å›æ­¸æ˜¯å¦‚ä½•å°ç•°æ–¹å·®æ•¸æ“šèµ·ä½œç”¨çš„ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650532097.jpg)
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650491449.jpg)
    > 
    > ---
    > 
    > å¦‚ä½•é¸å–åˆé©çš„åˆ†ä½å€¼å–æ±ºæ–¼æˆ‘å€‘å°æ­£èª¤å·®å’Œåèª¤å·®çš„é‡è¦–ç¨‹åº¦ã€‚æå¤±å‡½æ•¸é€šéåˆ†ä½å€¼ï¼ˆÎ³ï¼‰å°é«˜ä¼°å’Œä½ä¼°çµ¦äºˆä¸åŒçš„æ‡²ç½°ã€‚ä¾‹å¦‚ï¼Œç•¶åˆ†ä½æ•¸æå¤±å‡½æ•¸ Î³=0.25 æ™‚ï¼Œå°é«˜ä¼°çš„æ‡²ç½°æ›´å¤§ï¼Œä½¿å¾—é æ¸¬å€¼ç•¥ä½æ–¼ä¸­å€¼ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650454121.jpg)
    > 
    > Î³ æ˜¯æ‰€éœ€çš„åˆ†ä½æ•¸ï¼Œå…¶å€¼ä»‹æ–¼ 0 å’Œ 1 ä¹‹é–“ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650428251.jpg)
    > 
    > é€™å€‹æå¤±å‡½æ•¸ä¹Ÿå¯ä»¥åœ¨ç¥ç¶“ç¶²çµ¡æˆ–åŸºæ–¼æ¨¹çš„æ¨¡å‹ä¸­è¨ˆç®—é æ¸¬å€é–“ã€‚ä»¥ä¸‹æ˜¯ç”¨ Sklearn å¯¦ç¾æ¢¯åº¦æå‡æ¨¹å›æ­¸æ¨¡å‹çš„ç¤ºä¾‹ã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650390909.jpg)
    > 
    > ä¸Šåœ–è¡¨æ˜ï¼šåœ¨ sklearn åº«çš„æ¢¯åº¦æå‡å›æ­¸ä¸­ä½¿ç”¨åˆ†ä½æ•¸æå¤±å¯ä»¥å¾—åˆ° 90ï¼… çš„é æ¸¬å€é–“ã€‚å…¶ä¸­ä¸Šé™ç‚º Î³=0.95ï¼Œä¸‹é™ç‚º Î³=0.05ã€‚
    > 
    > ---
    > 
    > **å°æ¯”ç ”ç©¶**
    > 
    > ç‚ºäº†è­‰æ˜ä¸Šè¿°æ‰€æœ‰æå¤±å‡½æ•¸çš„ç‰¹é»ï¼Œè®“æˆ‘å€‘ä¾†ä¸€èµ·çœ‹ä¸€å€‹å°æ¯”ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘å€‘å»ºç«‹äº†ä¸€å€‹å¾ sincï¼ˆxï¼‰å‡½æ•¸ä¸­æ¡æ¨£å¾—åˆ°çš„æ•¸æ“šé›†ï¼Œä¸¦å¼•å…¥äº†å…©é …äººç‚ºå™ªè²ï¼šé«˜æ–¯å™ªè²åˆ†é‡ Îµã€œNï¼ˆ0ï¼ŒÏƒ2ï¼‰å’Œè„ˆè¡å™ªè²åˆ†é‡ Î¾ã€œBernï¼ˆpï¼‰ã€‚
    > 
    > åŠ å…¥è„ˆè¡å™ªè²æ˜¯ç‚ºäº†èªªæ˜æ¨¡å‹çš„é­¯æ£’æ•ˆæœã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ä¸åŒæå¤±å‡½æ•¸æ“¬åˆ GBM å›æ­¸å™¨çš„çµæœã€‚
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650322786.jpg)
    > 
    > é€£çºŒæå¤±å‡½æ•¸ï¼šï¼ˆAï¼‰MSE æå¤±å‡½æ•¸ï¼›ï¼ˆBï¼‰MAE æå¤±å‡½æ•¸ï¼›ï¼ˆCï¼‰Huber æå¤±å‡½æ•¸ï¼›ï¼ˆDï¼‰åˆ†ä½æ•¸æå¤±å‡½æ•¸ã€‚å°‡ä¸€å€‹å¹³æ»‘çš„ GBM æ“¬åˆæˆæœ‰å™ªè²çš„ sincï¼ˆxï¼‰æ•¸æ“šçš„ç¤ºä¾‹ï¼šï¼ˆEï¼‰åŸå§‹ sincï¼ˆxï¼‰å‡½æ•¸ï¼›ï¼ˆFï¼‰å…·æœ‰ MSE å’Œ MAE æå¤±çš„å¹³æ»‘ GBMï¼›ï¼ˆGï¼‰å…·æœ‰ Huber æå¤±çš„å¹³æ»‘ GBM ï¼Œä¸”Î´={4,2,1}ï¼›ï¼ˆHï¼‰å…·æœ‰åˆ†ä½æ•¸æå¤±çš„å¹³æ»‘çš„ GBMï¼Œä¸”Î±={0.5,0.1,0.9}ã€‚
    > 
    > ä»¿çœŸå°æ¯”çš„ä¸€äº›è§€å¯Ÿçµæœï¼š
    > 
    > *   MAE æå¤±æ¨¡å‹çš„é æ¸¬çµæœå—è„ˆè¡å™ªè²çš„å½±éŸ¿è¼ƒå°ï¼Œè€Œ MSE æå¤±å‡½æ•¸çš„é æ¸¬çµæœå—æ­¤å½±éŸ¿ç•¥æœ‰åç§»ã€‚
    > *   Huber æå¤±æ¨¡å‹é æ¸¬çµæœå°æ‰€é¸è¶…åƒæ•¸ä¸æ•æ„Ÿã€‚
    > *   åˆ†ä½æ•¸æå¤±æ¨¡å‹åœ¨åˆé©çš„ç½®ä¿¡æ°´å¹³ä¸‹èƒ½çµ¦å‡ºå¾ˆå¥½çš„ä¼°è¨ˆã€‚
    > 



### Cross Entropy

- [A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)


## å¤è¾²ç†µ (Shannon entropy)

- [Shannon entropy in the context of machine learning and AI](https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32)



## å¥‡ç•°å€¼åˆ†è§£

- [The SIngular Value Decomposition (SVD) song: It Had To Be U - YouTube](https://www.youtube.com/watch?v=fKVRSbFKnEw)



## Boosted tree

- [SGHMC+DDP Learning - BoostedTree.pdf](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)


## Blending and Bagging

- [207_handout.pdf](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)

## è©•ä¼°æ–¹æ³• Accuracy Metrics

### Accuracy

||predicted 0|predicted 1|
|--|--|--|
|Actual 0|True Negative |False Positive|
|Actual 1| False Negative | True Positive |

$$ Accuracy = \frac{All\ True\ Predicted}{ All\ Possible}\\
(åšå‡ºæ­£ç¢ºåˆ¤å®šçš„æ©Ÿç‡)\\
= \frac{\#TN+\#TP}{\#TP+\#FP+\#TN+\#FN}
$$

### Precision and Recall

$$
Precision = \frac{Predicted\ 1\ and\ Actual\ 1}{Predicted\ 1} \\(åˆ¤å®šç‚ºè²“ï¼ŒçœŸçš„ç‚ºè²“çš„æ©Ÿç‡)\\
= \frac{\#TP}{\#TP+\#FP}
$$

$$ 
Recall = \frac{Predicted\ 1\ and\ Actual\ 1}{Actual\ 1} \\(æŠŠå…¨éƒ¨çš„è²“æ‰¾å›ä¾†çš„æ©Ÿç‡)\\
= \frac{\#TP}{\#TP+\#FN} 
$$

### F1 Score
$$
F_1 = \frac{2}{ \frac{1}{recall} + \frac{1}{precision}}\\
(recall\ å’Œ\ precision\ çš„èª¿å’Œå¹³å‡)\\
= 2 * \frac{precision * recall}{precision + recall}
$$ 

- [å¦‚ä½•ç†è§£ä¸åº”ç”¨è°ƒå’Œå¹³å‡æ•°ï¼Ÿ - LIQiNGçš„å›ç­” - çŸ¥ä¹](https://www.zhihu.com/question/23096098/answer/340657629)

    è°ƒå’Œå¹³å‡æ•°ï¼Œå¼ºè°ƒäº†è¾ƒå°å€¼çš„é‡è¦æ€§ï¼›åœ¨æœºå™¨å­¦ä¹ ä¸­ã€‚å¬å›ç‡ä¸ºR, å‡†ç¡®ç‡ä¸ºPã€‚ä½¿ç”¨ä»–ä»¬å¯¹ç®—æ³•çš„è¯„ä¼°ï¼Œè¿™ä¸¤ä¸ªå€¼é€šå¸¸æƒ…å†µä¸‹ç›¸äº’åˆ¶çº¦ã€‚ä¸ºäº†æ›´åŠ æ–¹ä¾¿çš„è¯„ä»·ç®—æ³•çš„å¥½åã€‚äºæ˜¯å¼•å…¥äº†F1å€¼ã€‚F1ä¸ºå‡†ç¡®ç‡På’Œå¬å›ç‡Rçš„è°ƒå’Œå¹³å‡æ•°ã€‚ä¸ºä»€ä¹ˆF1ä½¿ç”¨è°ƒå’Œå¹³å‡æ•°ï¼Œè€Œä¸æ˜¯æ•°å­—å¹³å‡æ•°ã€‚ä¸¾ä¸ªä¾‹å­ï¼šå½“R æ¥è¿‘äº1, P æ¥è¿‘äº 0 æ—¶ã€‚é‡‡ç”¨è°ƒå’Œå¹³å‡æ•°çš„F1å€¼æ¥è¿‘äº0ï¼›è€Œå¦‚æœé‡‡ç”¨ç®—æ•°å¹³å‡æ•°F1çš„å€¼ä¸º0.5ï¼›æ˜¾ç„¶é‡‡ç”¨è°ƒå’Œå¹³å‡æ•°èƒ½æ›´å¥½çš„è¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚ç­‰æ•ˆäºè¯„ä»·Rå’ŒPçš„æ•´ä½“æ•ˆæœ

### ROC Curves and Area Under the Curve(AUC)

<iframe width="560" height="315" src="https://www.youtube.com/embed/OAl6eAyP-yo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

- [æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡é‡Œé¢çš„aucæ€ä¹ˆç†è§£ï¼Ÿ - ç°åœ¨å‡ ç‚¹äº†çš„å›ç­” - çŸ¥ä¹](https://www.zhihu.com/question/39840928/answer/83576302)

    >ä»Mannâ€“Whitney U statisticçš„è§’åº¦æ¥è§£é‡Šï¼ŒAUCå°±æ˜¯ä»æ‰€æœ‰1æ ·æœ¬ä¸­éšæœºé€‰å–ä¸€ä¸ªæ ·æœ¬ï¼Œ ä»æ‰€æœ‰0æ ·æœ¬ä¸­éšæœºé€‰å–ä¸€ä¸ªæ ·æœ¬ï¼Œç„¶åæ ¹æ®ä½ çš„åˆ†ç±»å™¨å¯¹ä¸¤ä¸ªéšæœºæ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼ŒæŠŠ1æ ·æœ¬é¢„æµ‹ä¸º1çš„æ¦‚ç‡ä¸ºp1ï¼ŒæŠŠ0æ ·æœ¬é¢„æµ‹ä¸º1çš„æ¦‚ç‡ä¸ºp0ï¼Œp1>p0çš„æ¦‚ç‡å°±ç­‰äºAUC



### Matthew's Correlation Coefficient
* å¦‚æœ sample size æœ‰ unbalanced çš„ç¾è±¡å¯ä»¥åˆ©ç”¨æ­¤ accuacy metric å»æ¸¬é‡é æ¸¬ç²¾æº–åº¦
$$
MCC= \frac{TP*TN - FP*FN}{\sqrt{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}}
$$


## é©—è­‰é›†ã€æ¸¬è©¦é›†èˆ‡éæ“¬åˆ

- [å¸¸ç”¨æ¸¬è©¦é›†å¸¶ä¾†éæ“¬åˆï¼Ÿä½ çœŸçš„èƒ½æ§åˆ¶è‡ªå·±ä¸æ ¹æ“šæ¸¬è©¦é›†èª¿åƒå— - å¹«è¶£](http://bangqu.com/G53jP8.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > å„˜ç®¡å°æ¯”æ–°æ¨¡å‹èˆ‡ä¹‹å‰æ¨¡å‹çš„çµæœæ˜¯éå¸¸è‡ªç„¶çš„æƒ³æ³•ï¼Œä½†å¾ˆæ˜é¡¯ç•¶å‰çš„ç ”ç©¶æ–¹æ³•è«–å‰Šå¼±äº†ä¸€å€‹é—œéµå‡è¨­ï¼šåˆ†é¡å™¨èˆ‡æ¸¬è©¦é›†æ˜¯ç¨ç«‹çš„ã€‚é€™ç¨®ä¸åŒ¹é…å¸¶ä¾†äº†ä¸€ç¨®é¡¯è€Œæ˜“è¦‹çš„å±éšªï¼Œç ”ç©¶ç¤¾å€å¯èƒ½æœƒè¼•æ˜“è¨­è¨ˆå‡ºåªåœ¨ç‰¹å®šæ¸¬è©¦é›†ä¸Šæ€§èƒ½è‰¯å¥½ï¼Œä½†ç„¡æ³•æ³›åŒ–è‡³æ–°æ•¸æ“šçš„æ¨¡å‹ [1]ã€‚
    > 
    > ---
    > 
    > è©²ç ”ç©¶åˆ†çˆ²ä¸‰æ­¥ï¼š
    > 
    > 1. é¦–å…ˆï¼Œç ”ç©¶è€…å‰µå»ºä¸€å€‹æ–°çš„æ¸¬è©¦é›†ï¼Œå°‡æ–°æ¸¬è©¦é›†çš„å­é¡åˆ¥åˆ†ä½ˆèˆ‡åŸå§‹ CIFAR-10 æ•¸æ“šé›†é€²è¡Œä»”ç´°åŒ¹é…ã€‚
    > 
    > 2. åœ¨æ”¶é›†äº†å¤§ç´„ 2000 å¼µæ–°åœ–åƒä¹‹å¾Œï¼Œç ”ç©¶è€…åœ¨æ–°æ¸¬è©¦é›†ä¸Šè©•ä¼° 30 å€‹åœ–åƒåˆ†é¡æ¨¡å‹çš„æ€§èƒ½ã€‚çµæœé¡¯ç¤ºå‡ºå…©å€‹é‡è¦ç¾è±¡ã€‚ä¸€æ–¹é¢ï¼Œå¾åŸå§‹æ¸¬è©¦é›†åˆ°æ–°æ¸¬è©¦é›†çš„æ¨¡å‹æº–ç¢ºç‡é¡¯è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼ŒVGG å’Œ ResNet æ¶æ§‹ [7, 18] çš„æº–ç¢ºç‡å¾ 93% ä¸‹é™è‡³æ–°æ¸¬è©¦é›†ä¸Šçš„ 85%ã€‚å¦ä¸€æ–¹é¢ï¼Œç ”ç©¶è€…ç™¼ç¾åœ¨å·²æœ‰æ¸¬è©¦é›†ä¸Šçš„æ€§èƒ½å¯ä»¥é«˜åº¦é æ¸¬æ–°æ¸¬è©¦é›†ä¸Šçš„æ€§èƒ½ã€‚å³ä½¿åœ¨ CIFAR-10 ä¸Šçš„å¾®å°æ”¹é€²é€šå¸¸ä¹Ÿèƒ½é·ç§»è‡³ç•™å‡ºæ•¸æ“šã€‚
    > 
    > 3. å—åŸå§‹æº–ç¢ºç‡å’Œæ–°æº–ç¢ºç‡ä¹‹é–“å·®ç•°çš„å½±éŸ¿ï¼Œç¬¬ä¸‰æ­¥ç ”ç©¶äº†å¤šå€‹è§£é‡‹é€™ä¸€å·®è·çš„å‡è¨­ã€‚ä¸€ç¨®è‡ªç„¶çš„çŒœæƒ³æ˜¯é‡æ–°èª¿æ•´æ¨™æº–è¶…åƒæ•¸èƒ½å¤ å½Œè£œéƒ¨åˆ†å·®è·ï¼Œä½†æ˜¯ç ”ç©¶è€…ç™¼ç¾è©²èˆ‰æªçš„å½±éŸ¿ä¸å¤§ï¼Œåƒ…èƒ½å¸¶ä¾†å¤§ç´„ 0.6% çš„æ”¹é€²ã€‚å„˜ç®¡è©²å¯¦é©—å’Œæœªä¾†å¯¦é©—å¯ä»¥è§£é‡‹æº–ç¢ºç‡æå¤±ï¼Œä½†å·®è·ä¾ç„¶å­˜åœ¨ã€‚
    > 
    > ç¸½ä¹‹ï¼Œç ”ç©¶è€…çš„çµæœä½¿å¾—ç•¶å‰æ©Ÿå™¨å­¸ç¿’é ˜åŸŸçš„é€²å±•æ„å‘³ä¸æ˜ã€‚é©æ‡‰ CIFAR-10 æ¸¬è©¦é›†çš„åŠªåŠ›å·²ç¶“æŒçºŒå¤šå¹´ï¼Œæ¨¡å‹è¡¨ç¾çš„æ¸¬è©¦é›†é©æ‡‰æ€§ä¸¦æ²’æœ‰å¤ªå¤§æå‡ã€‚é ‚ç´šæ¨¡å‹ä»ç„¶æ˜¯è¿‘æœŸå‡ºç¾çš„ä½¿ç”¨ Cutout æ­£å‰‡åŒ–çš„ Shake-Shake ç¶²çµ¡ [3, 4]ã€‚æ­¤å¤–ï¼Œè©²æ¨¡å‹æ¯”æ¨™æº– ResNet çš„å„ªå‹¢å¾ 4% ä¸Šå‡è‡³æ–°æ¸¬è©¦é›†ä¸Šçš„ 8%ã€‚é€™èªªæ˜ç•¶å‰å°æ¸¬è©¦é›†é€²è¡Œé•·æ™‚é–“ã€Œæ”»æ“Šã€çš„ç ”ç©¶æ–¹æ³•å…·æœ‰é©šäººçš„æŠ—éæ“¬åˆèƒ½åŠ›ã€‚
    > 
    > ä½†æ˜¯è©²ç ”ç©¶çµæœä»¤äººå°ç•¶å‰åˆ†é¡å™¨çš„é­¯æ£’æ€§ç”¢ç”Ÿè³ªç–‘ã€‚å„˜ç®¡æ–°æ•¸æ“šé›†åƒ…æœ‰å¾®å°çš„åˆ†ä½ˆè®ŠåŒ–ï¼Œä½†å»£æ³›ä½¿ç”¨çš„æ¨¡å‹çš„åˆ†é¡æº–ç¢ºç‡å»é¡¯è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼Œå‰é¢æåˆ°çš„ VGG å’Œ ResNet æ¶æ§‹ï¼Œå…¶æº–ç¢ºç‡æå¤±ç›¸ç•¶æ–¼æ¨¡å‹åœ¨ CIFAR-10 ä¸Šçš„å¤šå¹´é€²å±• [9]ã€‚æ³¨æ„è©²å¯¦é©—ä¸­å¼•å…¥çš„åˆ†ä½ˆè®ŠåŒ–ä¸æ˜¯å°æŠ—æ€§çš„ï¼Œä¹Ÿä¸æ˜¯ä¸åŒæ•¸æ“šæºçš„çµæœã€‚å› æ­¤å³ä½¿åœ¨è‰¯æ€§è¨­ç½®ä¸­ï¼Œåˆ†ä½ˆè®ŠåŒ–ä¹Ÿå°ç•¶å‰æ¨¡å‹çš„çœŸæ­£æ³›åŒ–èƒ½åŠ›å¸¶ä¾†äº†åš´å³»æŒ‘æˆ°ã€‚
    > 
    > 

### ä¸å¹³è¡¡æ•¸æ“š(imbalanced data)

- [Classification when 80% of my training set is of one class.ï¼šMachineLearning](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)

- [In classification, how do you handle an unbalanced training set? - Quora](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)



- [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)

    > ### 1) Can You Collect More Data?
    > 
    > You might think it's silly, but collecting more data is almost always overlooked.
    > 
    > Can you collect more data? Take a second and think about whether you are able to gather more data on your problem.
    > 
    > A larger dataset might expose a different and perhaps more balanced perspective on the classes.
    > 
    > More examples of minor classes may be useful later when we look at resampling your dataset.
    > 
    > ### 2) Try Changing Your Performance Metric
    > 
    > Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.
    > 
    > There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes.
    > 
    > I give more advice on selecting different performance measures in my post "[Classification Accuracy is Not Enough: More Performance Measures You Can Use](http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)".
    > 
    > In that post I look at an imbalanced dataset that characterizes the recurrence of breast cancer in patients.
    > 
    > From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:
    > 
    > -   **Confusion Matrix**: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).
    > -   **Precision**: A measure of a classifiers exactness.
    > -   **Recall**: A measure of a classifiers completeness
    > -   **F1 Score (or F-score)**: A weighted average of precision and recall.
    > 
    > I would also advice you to take a look at the following:
    > 
    > -   **Kappa (or [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))**: Classification accuracy normalized by the imbalance of the classes in the data.
    > -   **ROC Curves**: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.
    > 
    > You can learn a lot more about using ROC Curves to compare classification accuracy in our post "[Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)".
    > 
    > Still not sure? Start with kappa, it will give you a better idea of what is going on than classification accuracy.
    > 
    > ### 3) Try Resampling Your Dataset
    > 
    > You can change the dataset that you use to build your predictive model to have more balanced data.
    > 
    > This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:
    > 
    > 1.  You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or
    > 2.  You can delete instances from the over-represented class, called under-sampling.
    > 
    > These approaches are often very easy to implement and fast to run. They are an excellent starting point.
    > 
    > In fact, I would advise you to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred accuracy measures.
    > 
    > You can learn a little more in the the Wikipedia article titled "[Oversampling and undersampling in data analysis](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)".
    > 
    > #### Some Rules of Thumb
    > 
    > -   Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)
    > -   Consider testing over-sampling when you don't have a lot of data (tens of thousands of records or less)
    > -   Consider testing random and non-random (e.g. stratified) sampling schemes.
    > -   Consider testing different resampled ratios (e.g. you don't have to target a 1:1 ratio in a binary classification problem, try other ratios)
    > 
    > ### 4) Try Generate Synthetic Samples
    > 
    > A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.
    > 
    > You could sample them empirically within your dataset or you could use a method like Naive Bayes that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.
    > 
    > There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique.
    > 
    > As its name suggests, SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.
    > 
    > Learn more about SMOTE, see the original 2002 paper titled "[SMOTE: Synthetic Minority Over-sampling Technique](http://www.jair.org/papers/paper953.html)".
    > 
    > There are a number of implementations of the SMOTE algorithm, for example:
    > 
    > -   In Python, take a look at the "[UnbalancedDataset](https://github.com/fmfn/UnbalancedDataset)" module. It provides a number of implementations of SMOTE as well as various other resampling techniques that you could try.
    > -   In R, the [DMwR package](https://cran.r-project.org/web/packages/DMwR/index.html) provides an implementation of SMOTE.
    > -   In Weka, you can use the [SMOTE supervised filter](http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html).
    > 
    > ### 5) Try Different Algorithms
    > 
    > As always, I strongly advice you to not use your favorite algorithm on every problem. You should at least be spot-checking a variety of different types of algorithms on a given problem.
    > 
    > For more on spot-checking algorithms, see my post "Why you should be Spot-Checking Algorithms on your Machine Learning Problems".
    > 
    > That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed.
    > 
    > If in doubt, try a few popular decision tree algorithms like C4.5, C5.0, CART, and Random Forest.
    > 
    > For some example R code using decision trees, see my post titled "[Non-Linear Classification in R with Decision Trees](http://machinelearningmastery.com/non-linear-classification-in-r-with-decision-trees/)".
    > 
    > For an example of using CART in Python and scikit-learn, see my post titled "[Get Your Hands Dirty With Scikit-Learn Now](http://machinelearningmastery.com/get-your-hands-dirty-with-scikit-learn-now/)".
    > 
    > ### 6) Try Penalized Models
    > 
    > You can use the same algorithms but give them a different perspective on the problem.
    > 
    > Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.
    > 
    > Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA.
    > 
    > It is also possible to have generic frameworks for penalized models. For example, Weka has a [CostSensitiveClassifier](http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html) that can wrap any classifier and apply a custom penalty matrix for miss classification.
    > 
    > Using penalization is desirable if you are locked into a specific algorithm and are unable to resample or you're getting poor results. It provides yet another way to "balance" the classes. Setting up the penalty matrix can be complex. You will very likely have to try a variety of penalty schemes and see what works best for your problem.
    > 
    > ### 7) Try a Different Perspective
    > 
    > There are fields of study dedicated to imbalanced datasets. They have their own algorithms, measures and terminology.
    > 
    > Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas.
    > 
    > Two you might like to consider are **anomaly detection** and **change detection**.
    > 
    > [Anomaly detection](https://en.wikipedia.org/wiki/Anomaly_detection) is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by it's sequence of system calls. The events are rare and when compared to normal operation.
    > 
    > This shift in thinking considers the minor class as the outliers class which might help you think of new ways to separate and classify samples.
    > 
    > [Change detection](https://en.wikipedia.org/wiki/Change_detection) is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.
    > 
    > Both of these shifts take a more real-time stance to the classification problem that might give you some new ways of thinking about your problem and maybe some more techniques to try.
    > 
    > ### 8) Try Getting Creative
    > 
    > Really climb inside your problem and think about how to break it down into smaller problems that are more tractable.
    > 
    > For inspiration, take a look at the very creative answers on Quora in response to the question "[In classification, how do you handle an unbalanced training set?](http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)"
    > 
    > For example:
    > 
    > > Decompose your larger class into smaller number of other classes...
    > >
    > > ...use a One Class Classifier... (e.g. treat like outlier detection)
    > >
    > > ...resampling the unbalanced training set into not one balanced set, but several. Running an ensemble of classifiers on these sets could produce a much better result than one classifier alone
    > 
    > These are just a few of some interesting and creative ideas you could try.
    > 
    > For more ideas, check out these comments on the reddit post "[Classification when 80% of my training set is of one class](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)".
    > 
    > Pick a Method and Take Action
    > -----------------------------
    > 
    > You do not need to be an algorithm wizard or a statistician to build accurate and reliable models from imbalanced datasets.
    > 
    > We have covered a number of techniques that you can use to model an imbalanced dataset.
    > 
    > Hopefully there are one or two that you can take off the shelf and apply immediately, for example changing your accuracy metric and resampling your dataset. Both are fast and will have an impact straight away.
    > 
    > ***Which method are you going to try?***
    > 
    > A Final Word, Start Small
    > -------------------------
    > 
    > Remember that we cannot know which approach is going to best serve you and the dataset you are working on.
    > 
    > You can use some expert heuristics to pick this method or that, but in the end, the best advice I can give you is to "become the scientist" and empirically test each method and select the one that gives you the best results.
    > 
    > Start small and build upon what you learn.
    > 
    > Want More? Further Reading...
    > ---------------------------
    > 
    > There are resources on class imbalance if you know where to look, but they are few and far between.
    > 
    > I've looked and the following are what I think are the cream of the crop. If you'd like to dive deeper into some of the academic literature on dealing with class imbalance, check out some of the links below.
    > 
    > ### Books
    > 
    > -   [Imbalanced Learning: Foundations, Algorithms, and Applications](http://www.amazon.com/dp/1118074629?tag=inspiredalgor-20)
    > 
    > ### Papers
    > 
    > -   [Data Mining for Imbalanced Datasets: An Overview](http://link.springer.com/chapter/10.1007/978-0-387-09823-4_45)
    > -   [Learning from Imbalanced Data](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5128907)
    > -   [Addressing the Curse of Imbalanced Training Sets: One-Sided Selection](http://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf) (PDF)
    > -   [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](http://dl.acm.org/citation.cfm?id=1007735)
    > 



## é¡èš Clustering

### Quantum Mechanics to Cluster Time Series

- [[1805.01711] Using Quantum Mechanics to Cluster Time Series](https://arxiv.org/abs/1805.01711)

## é™ç¶­æ–¹æ³•

### PCA ï¼ˆprincipal component analysisï¼‰ä¸»æˆä»½åˆ†æ

- [æ·ºè«‡é™ç¶­æ–¹æ³•ä¸­çš„ PCA èˆ‡ t-SNE â€“ DnD mag â€“ Medium](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

    > > *å°‡ä¸€å€‹å…·æœ‰ n å€‹ç‰¹å¾µç©ºé–“çš„æ¨£æœ¬ï¼Œè½‰æ›ç‚ºå…·æœ‰ k å€‹ç‰¹å¾µç©ºé–“çš„æ¨£æœ¬ï¼Œå…¶ä¸­ k < n*
    > 
    > ä»¥ä¸‹æ˜¯ PCA çš„ä¸»è¦æ­¥é©Ÿï¼š
    > 
    > 1.  å°‡æ•¸æ“šæ¨™æº–åŒ–
    > 2.  å»ºç«‹**å…±è®Šç•°æ•¸çŸ©é™£ï¼ˆcovariance matrixï¼‰**
    > 3.  åˆ©ç”¨**å¥‡ç•°å€¼åˆ†è§£ï¼ˆSVDï¼‰**æ±‚å¾—**ç‰¹å¾µå‘é‡ï¼ˆeigenvectorï¼‰**è·Ÿ**ç‰¹å¾µå€¼ï¼ˆeigenvalueï¼‰**
    > 4.  é€šå¸¸ç‰¹å¾µå€¼æœƒç”±å¤§åˆ°å°æ’åˆ—ï¼Œé¸å– k å€‹ç‰¹å¾µå€¼èˆ‡ç‰¹å¾µå‘é‡
    > 5.  å°‡åŸæœ¬çš„æ•¸æ“šæŠ•å½±ï¼ˆæ˜ å°„ï¼‰åˆ°ç‰¹å¾µå‘é‡ä¸Šï¼Œå¾—åˆ°æ–°çš„ç‰¹å¾µæ•¸
    > 
    > PCA æœ€é‡è¦çš„éƒ¨åˆ†å°±æ˜¯å¥‡ç•°å€¼åˆ†è§£ï¼Œå› æ­¤æ¥ä¸‹ä¾†çš„ç« ç¯€è®“æˆ‘å€‘ä¾†è«‡è«‡**å¥‡ç•°å€¼åˆ†è§£**
    > 
    > #### ç›´è§€ç†è§£å¥‡ç•°å€¼åˆ†è§£
    > 
    > åœ¨çŸ©é™£åˆ†è§£ç•¶ä¸­ï¼Œå¥‡ç•°å€¼åˆ†è§£æ˜¯å€‹ç›¸ç•¶æœ‰åçš„æ–¹æ³•ã€‚çŸ©é™£åˆ†è§£åœ¨é«˜ä¸­æ•¸å­¸ç•¶ä¸­æœ€å¸¸è¦‹çš„ç”¨é€”å°±æ˜¯è§£æ–¹ç¨‹å¼ï¼ˆå¦‚ LU åˆ†è§£ï¼‰ï¼Œå¾å¥‡ç•°å€¼åˆ†è§£çš„å…¬å¼ç•¶ä¸­æˆ‘å€‘å¯ä»¥ç›´è§€åœ°äº†è§£ï¼š
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*EW7y-TDCOHClCKINXFtlZQ.png)
    > 
    > 
    > 
    > å…¶ä¸­ A ç‚ºä¸€å€‹ m x n çš„çŸ©é™£ï¼Œğ‘ˆ è·Ÿ V éƒ½ç‚ºæ­£äº¤çŸ©é™£ï¼Œ**ğ›´ ç‚ºå¥‡ç•°å€¼çŸ©é™£**ã€‚å¥‡ç•°å€¼çŸ©é™£ç‚ºçŸ©é™£ A å°æ‡‰çš„ç‰¹å¾µå€¼ï¼Œåœ¨ PCA ç•¶ä¸­åˆå«åš**ä¸»æˆä»½**ï¼Œä»£è¡¨å°ä¿å­˜è¨Šæ¯çš„é‡è¦ç¨‹åº¦ï¼Œé€šå¸¸ç”±å¤§åˆ°å°éæ¸›æ’åˆ—åœ¨å°è§’ä¸­ï¼Œæ˜¯å€‹å°ç¨±çŸ©é™£ã€‚
    > 
    > é‚£éº¼é€™é‚Šçš„ A å°æ‡‰ä»€éº¼å‘¢ï¼Ÿç•¶ç„¶å°±æ˜¯æˆ‘å€‘çš„ç‰¹å¾µï¼Œåªæ˜¯ç‰¹åˆ¥è¦æ³¨æ„çš„æ˜¯é€™é‚Šçš„ A æˆ‘å€‘é€šå¸¸ä½¿ç”¨**å…±è®Šç•°æ•¸çŸ©é™£ï¼ˆcovariance martixï¼‰**ä¾†æ±‚ç®—ï¼Œè¨˜å¾—è³‡æ–™ä¸€å®šè¦å…ˆæ­£è¦åŒ–å¾Œåœ¨é€²è¡Œå¥‡ç•°å€¼åˆ†è§£
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*xk7Vhzw5dI77BLNl3Dsdlw.png)
    > 
    > 
    > 
    > å› ç‚ºå…±è®Šç•°æ•¸çŸ©é™£**å¸¸ç”¨ Sigma è¡¨ç¤ºï¼Œä¸è¦è·Ÿä¸Šé¢çš„ ğ›´ ææ··å›‰**ã€‚å› æ­¤å¦‚æœè¦é™ç¶­ï¼Œæˆ‘å€‘å¯ä»¥ç”¨ U çš„å‰ k åˆ—ä¹˜ä¸Šå°æ‡‰ ğ›´ ç•¶ä¸­çš„ç‰¹å¾µå‘é‡ï¼Œå°±å¯ä»¥å¾—å‡ºæ–°çš„ç‰¹å¾µäº†ï¼Œè€Œå¾å¹¾ä½•çš„è§’åº¦ä¾†çœ‹
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*3nASowA14C1z-hIshmfg7w.png)
    > 
    > 
    > 
    > é€™æ¨£å­çš„é‹ç®—åœ¨å¹¾ä½•ç•¶ä¸­ï¼Œå…¶å¯¦æ˜¯å°‡ X æŠ•å½±åˆ° U çš„å‰ k å€‹å‘é‡
    > 

### t-SNE 

- [æ·ºè«‡é™ç¶­æ–¹æ³•ä¸­çš„ PCA èˆ‡ t-SNE â€“ DnD mag â€“ Medium](https://medium.com/d-d-mag/%E6%B7%BA%E8%AB%87%E5%85%A9%E7%A8%AE%E9%99%8D%E7%B6%AD%E6%96%B9%E6%B3%95-pca-%E8%88%87-t-sne-d4254916925b)

    > t-SNE ä¸»è¦æ˜¯å°‡é«˜ç¶­çš„æ•¸æ“šç”¨é«˜æ–¯åˆ†ä½ˆçš„æ©Ÿç‡å¯†åº¦å‡½æ•¸è¿‘ä¼¼ï¼Œè€Œä½ç¶­æ•¸æ“šçš„éƒ¨åˆ†ä½¿ç”¨ t åˆ†ä½ˆçš„æ–¹å¼ä¾†è¿‘ä¼¼ï¼Œåœ¨ä½¿ç”¨ KL è·é›¢è¨ˆç®—ç›¸ä¼¼åº¦ï¼Œæœ€å¾Œå†ä»¥æ¢¯åº¦ä¸‹é™ï¼ˆæˆ–éš¨æ©Ÿæ¢¯åº¦ä¸‹é™ï¼‰æ±‚æœ€ä½³è§£ ã€‚
    > 
    > #### é«˜æ–¯åˆ†ä½ˆçš„æ©Ÿç‡å¯†åº¦å‡½æ•¸
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*9CSkBUO3tvJAhq8Wzv9s2Q.png)
    > 
    > 
    > 
    > å…¶ä¸­ï¼ŒX ç‚ºéš¨æ©Ÿè®Šé‡ï¼Œğˆ ç‚ºè®Šç•°æ•¸ï¼Œğœ‡ ç‚ºå¹³å‡ã€‚
    > 
    > å› æ­¤åŸæœ¬é«˜ç¶­çš„æ•¸æ“šå¯ä»¥é€™æ¨£è¡¨ç¤ºï¼š
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*KxxzB_L8Wi1L-FgznjhkyA.png)
    > 
    > 
    > 
    > è€Œä½ç¶­çš„æ•¸æ“šç”¨ t åˆ†å¸ƒçš„æ©Ÿç‡å¯†åº¦å‡½æ•¸å¯ä»¥é€™æ¨£è¡¨ç¤ºï¼ˆè‡ªç”±åº¦ç‚º 1ï¼‰
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*OwvAZgYHAfYvKehREGH9WQ.png)
    > 
    > 
    > 
    > å…¶ä¸­ï¼Œx ç‚ºé«˜ç¶­ç•¶ä¸­çš„æ•¸æ“šï¼Œy ç‚ºä½ç¶­ç•¶ä¸­çš„æ•¸æ“šã€‚P, Q åˆ†åˆ¥ä»£è¡¨æ©Ÿç‡åˆ†ä½ˆã€‚
    > 
    > ç‚ºä»€éº¼æœƒä½¿ç”¨ t åˆ†ä½ˆä¾†è¿‘ä¼¼ä½ç¶­çš„æ•¸æ“šå‘¢ï¼Ÿä¸»è¦æ˜¯å› ç‚ºè½‰æ›æˆä½ç¶­ä¹‹å¾Œä¸€å®šæœƒä¸Ÿå¤±è¨±å¤šè¨Šæ¯ï¼Œæ‰€ä»¥ç‚ºäº†ä¸è¢«ç•°å¸¸å€¼å½±éŸ¿å¯ä»¥ä½¿ç”¨ t åˆ†ä½ˆã€‚
    > 
    > t åˆ†ä½ˆåœ¨æ¨£æœ¬æ•¸è¼ƒå°‘æ™‚ï¼Œå¯ä»¥æ¯”è¼ƒå¥½æ¨¡æ“¬æ¯é«”åˆ†å¸ƒçš„æƒ…å½¢ï¼Œä¸å®¹æ˜“è¢«ç•°å¸¸å€¼æ‰€å½±éŸ¿ã€‚
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1500/1*hkZuHGVNMmS4_MHfxoIDdA.jpeg) 
    > T åˆ†ä½ˆèˆ‡é«˜æ–¯åˆ†ä½ˆçš„æ©Ÿç‡å¯†åº¦å‡½æ•¸
    > 
    > #### å…©å€‹åˆ†ä½ˆä¹‹é–“çš„ç›¸ä¼¼åº¦
    > 
    > æ±‚ç®—å…©å€‹åˆ†ä½ˆä¹‹é–“çš„ç›¸ä¼¼åº¦ï¼Œç¶“å¸¸ç”¨ KL è·é›¢ï¼ˆKullback-Leibler Divergenceï¼‰ä¾†è¡¨ç¤ºï¼Œä¹Ÿå«åšç›¸å°ç†µï¼ˆRelative Entropyï¼‰ã€‚
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*geKaV3PDC2idMGUCXroJzw.png)
    > 
    > 
    > 
    > åœ¨ t-SNE ä¸­ä½¿ç”¨äº†å›°æƒ‘åº¦ï¼ˆPerpï¼‰ä¾†ç•¶ä½œè¶…åƒæ•¸ã€‚
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*BiQ-gYxdbZsGP5KfYUF9Kg.png)
    > 
    > 
    > 
    > è«–æ–‡ä¸­æå‡ºé€šå¸¸å›°æƒ‘åº¦åœ¨ 5 ~ 50 ä¹‹é–“ã€‚
    > 
    > #### Cost function
    > 
    > ç”¨ KL è·é›¢è¨ˆç®— Cost
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*2XSnUoPaYB2_FMQ8xMzOuQ.png)
    > 
    > 
    > æ±‚æ¢¯åº¦å¯ä»¥å¯«æˆ
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1000/1*MzouyFbnSKdeBA7nLJVCPw.png)
    > 
    > 
    > æœ€å¾Œå†åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼ˆæˆ–éš¨æ©Ÿæ¢¯åº¦ä¸‹é™æ³•ï¼‰å°±å¯ä»¥æ‰¾åˆ°æœ€å°å€¼äº†ã€‚
    > 
    > ---
    > 
    > #### å¯¦æ¸¬ï¼šä½¿ç”¨ MNIST æ¸¬è©¦
    > 
    > æ¸¬è©¦é›†å¯ä»¥åˆ°[é€™è£¡](http://yann.lecun.com/exdb/mnist/)ä¸‹è¼‰ï¼Œé¦–å…ˆæˆ‘å€‘å…ˆç”¨ PCA é™åˆ°äºŒç¶­çœ‹çœ‹ã€‚
    > 
    > **PCA**
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1250/1*Be6oSLuXgG0cDi2gJEL-wA.jpeg)
    > 
    > 
    > **t-SNE**
    > 
    > æ¥ä¸‹ä¾†ä½¿ç”¨ t-SNE æ¸¬è©¦
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1750/1*5i8McPBKmFOMOcCDjl8w4Q.jpeg)
    > 
    > ---
    > 
    > ### å°çµ
    > 
    > * ç•¶ç‰¹å¾µæ•¸é‡éå¤šæ™‚ï¼Œä½¿ç”¨ PCA å¯èƒ½æœƒé€ æˆé™ç¶­å¾Œçš„ç‰¹å¾µæ¬ æ“¬åˆï¼ˆunderfittingï¼‰ï¼Œé€™æ™‚å¯ä»¥è€ƒæ…®ä½¿ç”¨ t-SNE ä¾†é™ç¶­
    > * t-SNE çš„éœ€è¦æ¯”è¼ƒå¤šçš„æ™‚é–“åŸ·è¡Œ
    >
    > 

