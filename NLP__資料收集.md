# NLP__è³‡æ–™æ”¶é›†

[toc]
<!-- toc --> 


## Reference Book

### CS4650 and CS7650 ("Natural Language") at Georgia Tech

- [gt-nlp-class/notes at master Â· jacobeisenstein/gt-nlp-class](https://github.com/jacobeisenstein/gt-nlp-class/tree/master/notes)

## Course

### CS224N

- [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)

    - [Lecture 1 | Natural Language Processing with Deep Learning - YouTube](https://www.youtube.com/watch?v=OQQ-W_63UgQ&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja)


## ä¿¡æ¯ç†µ

### æœ€å°ç†µåŸç†

- [ä»æ— ç›‘ç£æ„å»ºè¯åº“çœ‹ã€Œæœ€å°ç†µåŸç†ã€ï¼Œå¥—è·¯æ˜¯å¦‚ä½•ç‚¼æˆçš„](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247488802&idx=1&sn=eb35229374ee283d5c54d58ae277b9f0&chksm=96e9caa2a19e43b4f624eac3d56532cb9dc7ca017c9e0eaf96387e20e5f985e37da833fbddfd&scene=21#wechat_redirect)

- [å†è«‡æœ€å°ç†µåŸç†ï¼šã€Œé£›è±¡éæ²³ã€ä¹‹å¥æ¨¡ç‰ˆå’Œèªè¨€çµæ§‹ | é™„é–‹æºNLPåº« - å¹«è¶£](http://bangqu.com/8E2536.html#utm_source=Facebook_PicSee&utm_medium=Social)





## DEMO

### word2vec å°éŠæˆ²

- [è®“ä½ çœ‹è¦‹ AI è‡ªç„¶èªè¨€è™•ç†å¤šå¼·å¤§ï¼Google ç™¼è¡¨ä¸€æ¬¾æœå°‹å¼•æ“å’Œå…©å€‹æ–‡å­—éŠæˆ² - INSIDE ç¡¬å¡çš„ç¶²è·¯è¶¨å‹¢è§€å¯Ÿ](https://www.inside.com.tw/2018/04/16/google-introducing-semantic-experiences-with-talk-to-book)

    - [Semantris](https://research.google.com/semantris/)





## å¯¦ä½œä¸­æ–‡ auto tagging æµç¨‹

1. è¨“ç·´ä¸­æ–‡word2vec: 
https://zake7749.github.io/2016/08/28/word2vec-with-gensim/
   a. æœé›†å¤§é‡ä¸­æ–‡èªæ–™(wiki, ptt, æ–°è...)
   b. gensim åšæ ¼å¼è™•ç†ã€openccåšç°¡ç¹è½‰æ›
   c. jieba åšæ–·è©ï¼Œèª¿æ•´è¾­å…¸å°ç¹é«”çš„æ”¯æ´ï¼ŒåŠ å…¥åœç”¨è©(ä½ æˆ‘ä»–çš„ä¹‹....)
   d. gensimä¸­çš„word2vec åšè¨“ç·´ç”¢ç”Ÿmodel
   e. è¼‰å…¥model ä¸¦åšç›¸ä¼¼è©æ’åºçš„æª¢æŸ¥
2. è¨“ç·´seq2seq:
   a. ç”±word2vec model å»ºç«‹word2id, id2word å­—å…¸å°ç…§è¡¨
   b. jieba å°‡training set æ–‡ç« æ–·è©ï¼Œä¸¦èˆ‡target tag ä¸€èµ·è½‰æˆidï¼Œä½œç‚ºmodelçš„input, target
   c. è¨“ç·´model
3. ç”¢ç”Ÿtag:
   a. jieba å°‡testing set æ–‡ç« æ–·è©ä¸¦è½‰æˆid é€å…¥seq2seq model
   b. modelç”¢ç”Ÿå‡ºä¾†çš„åºåˆ—è½‰æˆwordå³ç‚ºtag

## ç¶²é ä»‹é¢å‰ç«¯ç¯„ä¾‹

- [zkwi/textSummary: æ–‡æœ¬æ™ºèƒ½æ‘˜è¦æå–](https://github.com/zkwi/textSummary)

## ä¸­æ–‡å­—

- [èªè­˜ä¸­æ–‡å­—å…ƒç¢¼](http://idv.sinica.edu.tw/bear/charcodes/Section05.htm)

## æ–·è©å·¥å…·

- [ysc/cws_evaluation: Javaå¼€æºé¡¹ç›®cws_evaluationï¼šä¸­æ–‡åˆ†è¯å™¨åˆ†è¯æ•ˆæœè¯„ä¼°å¯¹æ¯”](https://github.com/ysc/cws_evaluation)

- [ç«¹é–“æ™ºèƒ½ç§‘æŠ€å°åŒ— Emotibot Taipei - è²¼æ–‡](https://www.facebook.com/EmotibotTaipei/posts/1896621840559761:0)

    > ä¸­æ–‡æ–·è©åšä¸å¥½ï¼Œäººæ©Ÿè‡ªç„¶èªè¨€äº¤äº’ç•¶ç„¶é›£ä»¥å–å¾—çªç ´
    > 
    > åŸå‰µç«¹é–“æ™ºèƒ½ç§‘æŠ€Emotibot
    > ä¸­æ–‡æ–·è©æ˜¯ä¸­æ–‡æ–‡æœ¬è™•ç†çš„ä¸€å€‹åŸºç¤æ­¥é©Ÿï¼Œä¹Ÿæ˜¯ä¸­æ–‡äººæ©Ÿè‡ªç„¶èªè¨€äº¤äº’çš„åŸºç¤æ¨¡çµ„ã€‚ä¸åŒæ–¼è‹±æ–‡çš„æ˜¯ï¼Œä¸­æ–‡å¥å­ä¸­æ²’æœ‰è©çš„ç•Œé™ï¼Œå› æ­¤åœ¨é€²è¡Œä¸­æ–‡è‡ªç„¶èªè¨€è™•ç†æ™‚ï¼Œé€šå¸¸éœ€è¦å…ˆé€²è¡Œæ–·è©ï¼Œæ–·è©æ•ˆæœå°‡ç›´æ¥å½±éŸ¿è©æ€§ã€å¥æ³•æ¨¹ç­‰æ¨¡çµ„çš„æ•ˆæœã€‚ç•¶ç„¶æ–·è©åªæ˜¯ä¸€å€‹å·¥å…·ï¼Œå ´æ™¯ä¸åŒï¼Œè¦æ±‚ä¹Ÿä¸åŒã€‚
    > 
    > åœ¨äººæ©Ÿè‡ªç„¶èªè¨€äº¤äº’ä¸­ï¼Œæˆç†Ÿçš„ä¸­æ–‡æ–·è©ç®—æ³•èƒ½å¤ é”åˆ°æ›´å¥½çš„è‡ªç„¶èªè¨€è™•ç†æ•ˆæœï¼Œå¹«åŠ©è¨ˆç®—æ©Ÿç†è§£è¤‡é›œçš„ä¸­æ–‡èªè¨€ã€‚ç«¹é–“æ™ºèƒ½åœ¨æ§‹å»ºä¸­æ–‡è‡ªç„¶èªè¨€å°è©±ç³»çµ±æ™‚ï¼Œçµåˆèªè¨€å­¸ä¸æ–·å„ªåŒ–ï¼Œè¨“ç·´å‡ºäº†ä¸€å¥—å…·æœ‰è¼ƒå¥½æ–·è©æ•ˆæœçš„ç®—æ³•æ¨¡å‹ï¼Œçˆ²æ©Ÿå™¨æ›´å¥½åœ°ç†è§£ä¸­æ–‡è‡ªç„¶èªè¨€å¥ å®šäº†åŸºç¤ã€‚
    > 
    > åœ¨æ­¤ï¼Œå°æ–¼ä¸­æ–‡æ–·è©æ–¹æ¡ˆã€ç•¶å‰æ–·è©å™¨å­˜åœ¨çš„å•é¡Œï¼Œä»¥åŠä¸­æ–‡æ–·è©éœ€è¦è€ƒæ…®çš„å› ç´ åŠç›¸é—œè³‡æºï¼Œç«¹é–“æ™ºèƒ½ è‡ªç„¶èªè¨€èˆ‡æ·±åº¦å­¸ç¿’å°çµ„ åšäº†äº›æ•´ç†å’Œç¸½çµï¼Œå¸Œæœ›èƒ½çˆ²å¤§å®¶æä¾›ä¸€äº›åƒè€ƒã€‚
    > 
    > ä¸­æ–‡æ–·è©æ ¹æ“šå¯¦ç¾åŸç†å’Œç‰¹é»ï¼Œä¸»è¦åˆ†çˆ²ä»¥ä¸‹2å€‹é¡åˆ¥ï¼š
    > 
    > 1ã€åŸºäºè©å…¸æ–·è©ç®—æ³•
    > 
    > ä¹Ÿç¨±å­—ç¬¦ä¸²åŒ¹é…æ–·è©ç®—æ³•ã€‚è©²ç®—æ³•æ˜¯æŒ‰ç…§ä¸€å®šçš„ç­–ç•¥å°‡å¾…åŒ¹é…çš„å­—ç¬¦ä¸²å’Œä¸€å€‹å·²å»ºç«‹å¥½çš„â€œå……åˆ†å¤§çš„â€è©å…¸ä¸­çš„è©é€²è¡ŒåŒ¹é…ï¼Œè‹¥æ‰¾åˆ°æŸå€‹è©æ¢ï¼Œå‰‡èªªæ˜åŒ¹é…æˆåŠŸï¼Œè­˜åˆ¥äº†è©²è©ã€‚å¸¸è¦‹çš„åŸºäºè©å…¸çš„æ–·è©ç®—æ³•åˆ†çˆ²ä»¥ä¸‹å¹¾ç¨®ï¼šæ­£å‘æœ€å¤§åŒ¹é…æ³•ã€é€†å‘æœ€å¤§åŒ¹é…æ³•å’Œé›™å‘åŒ¹é…æ–·è©æ³•ç­‰ã€‚
    > 
    > åŸºäºè©å…¸çš„æ–·è©ç®—æ³•æ˜¯æ‡‰ç”¨æœ€å»£æ³›ã€æ–·è©é€Ÿåº¦æœ€å¿«çš„ã€‚å¾ˆé•·ä¸€æ®µæ™‚é–“å…§ç ”ç©¶è€…éƒ½åœ¨å°åŸºäºå­—ç¬¦ä¸²åŒ¹é…æ–¹æ³•é€²è¡Œå„ªåŒ–ï¼Œæ¯”å¦‚æœ€å¤§é•·åº¦è¨­å®šã€å­—ç¬¦ä¸²å­˜å„²å’ŒæŸ¥æ‰¾æ–¹å¼ä»¥åŠå°äºè©è¡¨çš„çµ„ç¹”çµæ§‹ï¼Œæ¯”å¦‚é‡‡ç”¨TRIEç´¢å¼•æ¨¹ã€å“ˆå¸Œç´¢å¼•ç­‰ã€‚
    > 
    > 2ã€åŸºäºçµ±è¨ˆçš„æ©Ÿå™¨å­¸ç¿’ç®—æ³•
    > 
    > é€™é¡ç›®å‰å¸¸ç”¨çš„æ˜¯ç®—æ³•æ˜¯HMMã€CRFã€SVMã€æ·±åº¦å­¸ç¿’ç­‰ç®—æ³•ï¼Œæ¯”å¦‚stanfordã€Hanlpæ–·è©å·¥å…·æ˜¯åŸºäºCRFç®—æ³•ã€‚ä»¥CRFçˆ²ä¾‹ï¼ŒåŸºæœ¬æ€è·¯æ˜¯å°æ¼¢å­—é€²è¡Œæ¨™æ³¨è¨“ç·´ï¼Œä¸åƒ…è€ƒæ…®äº†è©èªå‡ºç¾çš„é »ç‡ï¼Œé‚„è€ƒæ…®ä¸Šä¸‹æ–‡ï¼Œå…·å‚™è¼ƒå¥½çš„å­¸ç¿’èƒ½åŠ›ï¼Œå› æ­¤å…¶å°æ­§ç¾©è©å’Œæœªç™»éŒ„è©çš„è­˜åˆ¥éƒ½å…·æœ‰è‰¯å¥½çš„æ•ˆæœã€‚
    > 
    > Nianwen Xueåœ¨å…¶è«–æ–‡ã€ŠCombining Classifiers for Chinese Word Segmentationã€‹ä¸­é¦–æ¬¡æå‡ºå°æ¯å€‹å­—ç¬¦é€²è¡Œæ¨™æ³¨ï¼Œé€šéæ©Ÿå™¨å­¸ç¿’ç®—æ³•è¨“ç·´åˆ†é¡å™¨é€²è¡Œæ–·è©ï¼Œåœ¨è«–æ–‡ã€ŠChinese word segmentation as character taggingã€‹ä¸­è¼ƒçˆ²è©³ç´°åœ°é—¡è¿°äº†åŸºäºå­—æ¨™æ³¨çš„æ–·è©æ³•ã€‚
    > 
    > å¸¸è¦‹çš„æ–·è©å™¨éƒ½æ˜¯ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’ç®—æ³•å’Œè©å…¸ç›¸çµåˆï¼Œä¸€æ–¹é¢èƒ½å¤ æé«˜æ–·è©å‡†ç¢ºç‡ï¼Œå¦ä¸€æ–¹é¢èƒ½å¤ æ”¹å–„é ˜åŸŸé©æ‡‰æ€§ã€‚
    > 
    > éš¨è‘—æ·±åº¦å­¸ç¿’çš„èˆˆèµ·ï¼Œä¹Ÿå‡ºç¾äº†åŸºäºç¥ç¶“ç¶²çµ¡çš„æ–·è©å™¨ï¼Œä¾‹å¦‚æœ‰äººå“¡å˜—è©¦ä½¿ç”¨é›™å‘LSTM+CRFå¯¦ç¾æ–·è©å™¨ï¼Œå…¶æœ¬è³ªä¸Šæ˜¯åºåˆ—æ¨™æ³¨ï¼Œæ‰€ä»¥æœ‰é€šç”¨æ€§ï¼Œå‘½åå¯¦é«”è­˜åˆ¥ç­‰éƒ½å¯ä»¥ä½¿ç”¨è©²æ¨¡å‹ï¼Œæ“šå ±é“å…¶æ–·è©å™¨å­—ç¬¦å‡†ç¢ºç‡å¯é«˜é”97.5%ã€‚ç®—æ³•æ¡†æ¶çš„æ€è·¯èˆ‡è«–æ–‡ã€ŠNeural Architectures for Named Entity Recognitionã€‹é¡ä¼¼ï¼Œåˆ©ç”¨è©²æ¡†æ¶å¯ä»¥å¯¦ç¾ä¸­æ–‡æ–·è©ï¼Œå¦‚ä¸‹åœ–æ‰€ç¤ºï¼š
    > 
    > ![](https://scontent-tpe1-1.xx.fbcdn.net/v/t1.0-9/19429638_1896621840559761_8054285384926288490_n.png?_nc_cat=0&oh=8a0ff108dcc2e5c2487c5212b7ac805f&oe=5BB4F470)
    > 
    > é¦–å…ˆå°èªæ–™é€²è¡Œå­—ç¬¦åµŒå…¥ï¼Œå°‡å¾—åˆ°çš„ç‰¹å¾è¼¸å…¥çµ¦é›™å‘LSTMï¼Œç„¶å¾ŒåŠ ä¸€å€‹CRFå°±å¾—åˆ°æ¨™æ³¨çµæœã€‚
    > 
    > æ–·è©å™¨ç•¶å‰å­˜åœ¨å•é¡Œï¼š
    > 
    > ç›®å‰ä¸­æ–‡æ–·è©é›£é»ä¸»è¦æœ‰ä¸‰å€‹ï¼š
    > 
    > 1ã€æ–·è©æ¨™å‡†ï¼šæ¯”å¦‚äººåï¼Œå“ˆå·¥å¤§çš„æ¨™å‡†ä¸­å§“å’Œåæ˜¯åˆ†é–‹çš„ï¼Œä½†åœ¨Hanlpä¸­æ˜¯åˆåœ¨ä¸€èµ·çš„ã€‚é€™éœ€è¦æ ¹æ“šä¸åŒçš„éœ€æ±‚åˆ¶å®šä¸åŒçš„æ–·è©æ¨™å‡†ã€‚
    > 
    > 2ã€æ­§ç¾©ï¼šå°åŒä¸€å€‹å¾…åˆ‡åˆ†å­—ç¬¦ä¸²å­˜åœ¨å¤šå€‹æ–·è©çµæœã€‚
    > 
    > æ­§ç¾©åˆåˆ†çˆ²çµ„åˆå‹æ­§ç¾©ã€äº¤é›†å‹æ­§ç¾©å’ŒçœŸæ­§ç¾©ä¸‰ç¨®é¡å‹ã€‚
    > 
    > 1) çµ„åˆå‹æ­§ç¾©ï¼šæ–·è©æ˜¯æœ‰ä¸åŒçš„ç²’åº¦çš„ï¼ŒæŒ‡æŸå€‹è©æ¢ä¸­çš„ä¸€éƒ¨åˆ†ä¹Ÿå¯ä»¥åˆ‡åˆ†çˆ²ä¸€å€‹ç¨ç«‹çš„è©æ¢ã€‚æ¯”å¦‚â€œä¸­è¯æ°‘åœ‹â€ï¼Œç²—ç²’åº¦çš„æ–·è©å°±æ˜¯â€œä¸­è¯æ°‘åœ‹â€ï¼Œç´°ç²’åº¦çš„æ–·è©å¯èƒ½æ˜¯â€œä¸­è¯/æ°‘åœ‹â€ã€‚
    > 
    > 2) äº¤é›†å‹æ­§ç¾©ï¼šåœ¨â€œé«˜é›„å¤©å’Œæœè£å» â€ä¸­ï¼Œâ€œå¤©å’Œâ€æ˜¯å» åï¼Œæ˜¯ä¸€å€‹å°ˆæœ‰è©ï¼Œâ€œå’Œæœâ€ä¹Ÿæ˜¯ä¸€å€‹è©ï¼Œå®ƒå€‘å…±ç”¨äº†â€œå’Œâ€å­—ã€‚
    > 
    > 3) çœŸæ­§ç¾©ï¼šæœ¬èº«çš„èªæ³•å’Œèªç¾©éƒ½æ²’æœ‰å•é¡Œ, å³ä¾¿é‡‡ç”¨äººå·¥åˆ‡åˆ†ä¹Ÿæœƒç”£ç”ŸåŒæ¨£çš„æ­§ç¾©ï¼Œåªæœ‰é€šéä¸Šä¸‹æ–‡çš„èªç¾©ç’°å¢ƒæ‰èƒ½çµ¦å‡ºæ­£ç¢ºçš„åˆ‡åˆ†çµæœã€‚ä¾‹å¦‚ï¼šå°äºå¥å­â€œç¾åœ‹æœƒé€šéå°å°å”®æ­¦æ³•æ¡ˆâ€ï¼Œæ—¢å¯ä»¥åˆ‡åˆ†æˆâ€œç¾åœ‹/æœƒ/é€šéå°å°è»å”®æ³•æ¡ˆâ€ï¼Œåˆå¯ä»¥åˆ‡åˆ†æˆâ€œç¾/åœ‹æœƒ/é€šéå°å°è»å”®æ³•æ¡ˆâ€ã€‚
    > 
    > ä¸€èˆ¬åœ¨æœç´¢å¼•æ“ä¸­ï¼Œæ§‹å»ºç´¢å¼•æ™‚å’ŒæŸ¥è©¢æ™‚æœƒä½¿ç”¨ä¸åŒçš„æ–·è©ç®—æ³•ã€‚å¸¸ç”¨çš„æ–¹æ¡ˆæ˜¯ï¼Œåœ¨ç´¢å¼•çš„æ™‚å€™ä½¿ç”¨ç´°ç²’åº¦çš„æ–·è©ä»¥ä¿è­‰å¬å›ï¼Œåœ¨æŸ¥è©¢çš„æ™‚å€™ä½¿ç”¨ç²—ç²’åº¦çš„æ–·è©ä»¥ä¿è­‰ç²¾åº¦ã€‚
    > 
    > 3ã€æ–°è©ï¼šä¹Ÿç¨±æœªè¢«è©å…¸æ”¶éŒ„çš„è©ï¼Œè©²å•é¡Œçš„è§£æ±ºä¾è³´äºäººå€‘å°æ–·è©æŠ€è¡“å’Œæ¼¢èªèªè¨€çµæ§‹çš„é€²ä¸€æ­¥èªè­˜ã€‚
    > 
    > å¦å¤–ï¼Œæˆ‘å€‘æ”¶é›†äº†å¦‚ä¸‹éƒ¨åˆ†æ–·è©å·¥å…·ï¼Œä¾›åƒè€ƒï¼š
    > 
    > ä¸­åœ‹ç§‘å­¸é™¢è¨ˆç®—æ‰€NLPIRï¼š http://ictclas.nlpir.org/nlpir/
    > ansjæ–·è©å™¨ï¼šhttps://github.com/NLPchina/ansj_seg
    > å“ˆå·¥å¤§çš„LTPï¼š https://github.com/HIT-SCIR/ltp
    > åŒ—äº¬æ¸…è¯å¤§å­¸THULACï¼š https://github.com/thunlp/THULAC
    > Stanfordæ–·è©å™¨ï¼šhttps://nlp.stanford.edu/software/segmenter.shtml
    > Hanlpæ–·è©å™¨ï¼šhttps://github.com/hankcs/HanLP
    > çµå·´æ–·è©ï¼šhttps://github.com/yanyiwu/cppjieba
    > KCWSæ–·è©å™¨(å­—åµŒå…¥+Bi-LSTM+CRF) ï¼šhttps://github.com/koth/kcws
    > ZParï¼šhttps://github.com/frcchang/zpar/releases
    > IKAnalyzerï¼š https://github.com/wks/ik-analyzer
    > 
    > ä»¥åŠéƒ¨åˆ†æ–·è©å™¨çš„ç°¡å–®èªªæ˜ï¼š
    > 
    > å“ˆå·¥å¤§çš„æ–·è©å™¨ï¼šä¸»é ä¸Šçµ¦éèª¿ç”¨æ¥å£ï¼Œæ¯ç§’è«‹æ±‚çš„æ¬¡æ•¸æœ‰é™åˆ¶ã€‚
    > åŒ—äº¬æ¸…è¯å¤§å­¸THULACï¼šç›®å‰å·²ç¶“æœ‰Javaã€Pythonå’ŒC++ ç‰ˆæœ¬ï¼Œä¸¦ä¸”ä»£ç¢¼é–‹æºã€‚
    > Stanfordæ–·è©å™¨ï¼šä½œçˆ²è¡†å¤šæ–¯å¦ç¦è‡ªç„¶èªè¨€è™•ç†ä¸­çš„ä¸€å€‹åŒ…ï¼Œç›®å‰æœ€æ–°ç‰ˆæœ¬3.7.0ï¼Œ Javaå¯¦ç¾çš„CRFç®—æ³•ã€‚å¯ä»¥ç›´æ¥ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œä¹Ÿæä¾›è¨“ç·´æ¨¡å‹æ¥å£ã€‚
    > Hanlpæ–·è©ï¼šæ±‚è§£çš„æ˜¯æœ€çŸ­è·¯å¾‘ã€‚å„ªé»ï¼šé–‹æºã€æœ‰äººç¶­è­·ã€å¯ä»¥è§£ç­”ã€‚åŸå§‹æ¨¡å‹ç”¨çš„è¨“ç·´èªæ–™æ˜¯äººæ°‘æ—¥å ±çš„èªæ–™ï¼Œç•¶ç„¶å¦‚æœä½ æœ‰è¶³å¤ çš„èªæ–™ä¹Ÿå¯ä»¥è‡ªå·±è¨“ç·´ã€‚
    > çµå·´æ–·è©å·¥å…·ï¼šåŸºäºå‰ç¶´è©å…¸å¯¦ç¾é«˜æ•ˆçš„è©åœ–æƒæï¼Œç”Ÿæˆå¥å­ä¸­æ¼¢å­—æ‰€æœ‰å¯èƒ½æˆè©æƒ…æ³æ‰€æ§‹æˆçš„æœ‰å‘ç„¡ç’°åœ– (DAG)ï¼›é‡‡ç”¨äº†å‹•æ…‹è¦åŠƒæŸ¥æ‰¾æœ€å¤§æ¦‚ç‡è·¯å¾‘, æ‰¾å‡ºåŸºäºè©é »çš„æœ€å¤§åˆ‡åˆ†çµ„åˆï¼›å°äºæœªç™»éŒ„è©ï¼Œé‡‡ç”¨äº†åŸºäºæ¼¢å­—æˆè©èƒ½åŠ›çš„ HMM æ¨¡å‹ï¼Œä½¿ç”¨äº† Viterbi ç®—æ³•ã€‚
    > å­—åµŒå…¥+Bi-LSTM+CRFæ–·è©å™¨ï¼šæœ¬è³ªä¸Šæ˜¯åºåˆ—æ¨™æ³¨ï¼Œé€™å€‹æ–·è©å™¨ç”¨äººæ°‘æ—¥å ±çš„80è¬èªæ–™ï¼Œæ“šèªªæŒ‰ç…§å­—ç¬¦æ­£ç¢ºç‡è©•ä¼°æ¨™å‡†èƒ½é”åˆ°97.5%çš„å‡†ç¢ºç‡ï¼Œå„ä½æ„Ÿèˆˆè¶£å¯ä»¥å»çœ‹çœ‹ã€‚
    > ZParæ–·è©å™¨ï¼šæ–°åŠ å¡ç§‘æŠ€è¨­è¨ˆå¤§å­¸é–‹ç™¼çš„ä¸­æ–‡æ–·è©å™¨ï¼ŒåŒ…æ‹¬æ–·è©ã€è©æ€§æ¨™æ³¨å’ŒParserï¼Œæ”¯æŒå¤šèªè¨€ï¼Œæ“šèªªæ•ˆæœæ˜¯å…¬é–‹çš„æ–·è©å™¨ä¸­æœ€å¥½çš„ï¼ŒC++ èªè¨€ç·¨å¯«ã€‚
    > 
    > é—œäºé€Ÿåº¦ï¼š
    > 
    > ç”±äºæ–·è©æ˜¯åŸºç¤çµ„ä»¶ï¼Œå…¶æ€§èƒ½ä¹Ÿæ˜¯é—œéµçš„è€ƒé‡å› ç´ ã€‚é€šå¸¸ï¼Œæ–·è©é€Ÿåº¦è·Ÿç³»çµ±çš„è»Ÿç¡¬ä»¶ç’°å¢ƒæœ‰ç›¸é—œå¤–ï¼Œé‚„èˆ‡è©å…¸çš„çµæ§‹è¨­è¨ˆå’Œç®—æ³•è¤‡é›œåº¦ç›¸é—œã€‚æ¯”å¦‚æˆ‘å€‘ä¹‹å‰è·‘éå­—åµŒå…¥+Bi-LSTM+CRFæ–·è©å™¨ï¼Œå…¶é€Ÿåº¦ç›¸å°è¼ƒæ…¢ã€‚å¦å¤–ï¼Œé–‹æºé …ç›® https://github.com/ysc/cws_evaluation æ›¾å°å¤šæ¬¾æ–·è©å™¨é€Ÿåº¦å’Œæ•ˆæœé€²è¡Œéå°æ¯”ï¼Œå¯ä¾›å¤§å®¶åƒè€ƒã€‚
    > 
    > 


### çµå·´ä¸­æ–‡åˆ†è©/æ–·è©

- [fxsjy/jieba: ç»“å·´ä¸­æ–‡åˆ†è¯](https://github.com/fxsjy/jieba)

### HuhuSeg

- [Colearo/HuhuSeg: Simple Chinese segmentator, keywords extractor and other examples](https://github.com/Colearo/HuhuSeg)

    > åœ¨TF-IDFå®ç°çš„å…³é”®è¯æå–ä¹‹å¤–ï¼Œè¿™é‡Œè¿˜å®ç°äº†åŸºäºTextRank[5]çš„æå–ç®—æ³•ï¼Œä¸ä¾èµ–äºåºå¤§çš„IDFæ¨¡å‹ï¼Œè€Œæ˜¯è¯•å›¾åœ¨æ–‡æœ¬ä¸­è¯è¯­çš„å…±ç°å…³ç³»å›¾é‡Œæ‰¾åˆ°è¢«Rankæœ€é«˜çš„è¯è¯­ã€‚å½“ç„¶ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œè¿™é‡Œçš„ä»£ç å®ç°çš„å…³é”®è¯æå–è¿˜ä½¿ç”¨äº†ä¸€ä¸ªå°trickï¼Œåœ¨é€šè¿‡TextRankæå–å®Œå…³é”®è¯ä¹‹åï¼Œä¼šå†æ¬¡æ‰«ææ–‡æœ¬ï¼Œæ‰¾åˆ°topå…³é”®è¯ä¸­æ˜¯å¦æœ‰é‚»æ¥è¯å¯ä»¥ç»„æˆçŸ­è¯­


## gensim


Parameters:	

- __sg (int {1, 0})__ â€“ Defines the training algorithm. If 1, skip-gram is employed; otherwise, CBOW is used.
- __size (int)__ â€“ Dimensionality of the feature vectors.
- __window (int)__ â€“ The maximum distance between the current and predicted word within a sentence.
- __alpha (float)__ â€“ The initial learning rate.
- __min_alpha (float)__ â€“ Learning rate will linearly drop to min_alpha as training progresses.
- __seed (int)__ â€“ Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization).
- __min_count (int)__ â€“ Ignores all words with total frequency lower than this.
- __max_vocab_size (int)__ â€“ Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit.
- __sample (float)__ â€“ The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).
- __workers (int)__ â€“ Use these many worker threads to train the model (=faster training with multicore machines).
- __hs (int {1,0})__ â€“ If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.
- __negative (int)__ â€“ If > 0, negative sampling will be used, the int for negative specifies how many â€œnoise wordsâ€ should be drawn (usually between 5-20). If set to 0, no negative sampling is used.
- __cbow_mean (int {1,0})__ â€“ If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.
- __hashfxn (function)__ â€“ Hash function to use to randomly initialize weights, for increased training reproducibility.
- __iter (int)__ â€“ Number of iterations (epochs) over the corpus.
- __trim_rule (function)__ â€“ Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count). Can be None (min_count will be used, look to keep_vocab_item()), or a callable that accepts parameters (word, count, min_count) and returns either gensim.utils.RULE_DISCARD, gensim.utils.RULE_KEEP or gensim.utils.RULE_DEFAULT. Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the model.
- __sorted_vocab (int {1,0})__ â€“ If 1, sort the vocabulary by descending frequency before assigning word indexes.
- __batch_words (int)__ â€“ Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)
- __compute_loss (bool)__ â€“ If True, computes and stores loss value which can be retrieved using model.get_latest_training_loss().
- __callbacks__ â€“ List of callbacks that need to be executed/run at specific stages during training.



## char-rnn

- [Tensorflow lyrics generation Â· Lei's Blog](http://leix.me/2016/11/28/tensorflow-lyrics-generation/)

    - [leido/char-rnn-cn: åŸºäºchar-rnnå’Œtensorflowç”Ÿæˆå‘¨æ°ä¼¦æ­Œè¯](https://github.com/leido/char-rnn-cn)

- [å¾å­—ç¬¦ç´šçš„èªè¨€å»ºæ¨¡é–‹å§‹ï¼Œç­è§£èªè¨€æ¨¡å‹èˆ‡åºåˆ—å»ºæ¨¡çš„åŸºæœ¬æ¦‚å¿µ - å¹«è¶£](http://bangqu.com/24EJ36.html)



## 1D-CNN for text

- [Implementing a CNN for Text Classification in TensorFlow â€“ WildML](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)

- [dennybritz/cnn-text-classification-tf: Convolutional Neural Network for Text Classification in Tensorflow](https://github.com/dennybritz/cnn-text-classification-tf)

- [[Deep Learning] Convolution Neural Network on NLP](https://unilight.github.io/2017/07/04/Convolution-Neural-Network-on-NLP/)

- [Convolutional Methods for Text â€“ Tal Perry â€“ Medium](https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f)

- [Understanding Convolutional Neural Networks for NLP â€“ WildML](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)

## è©å‘é‡


### word2Vec

#### åŸç†

- [é¡ç¥ç¶“ç¶²è·¯ -- word2vec (part 1 : Overview) Â« MARK CHANG'S BLOG](http://cpmarkchang.logdown.com/posts/773062-neural-network-word2vec-part-1-overview)

- [ç†è§£ Word2Vec ä¹‹ Skip-Gram æ¨¡å‹](https://zhuanlan.zhihu.com/p/27234078)

- [Word2vec (ä¸­æ–‡)](https://www.slideshare.net/ywchen88/word2vec-70245972)


#### å¯¦ä½œ

- [ä»¥ gensim è¨“ç·´ä¸­æ–‡è©å‘é‡ | é›·å¾·éº¥çš„è—æ›¸é–£](https://zake7749.github.io/2016/08/28/word2vec-with-gensim/)

- [zake7749/word2vec-tutorial: ä¸­æ–‡è©å‘é‡è¨“ç·´æ•™å­¸](https://github.com/zake7749/word2vec-tutorial)

- [ä½¿ç”¨tensorflowå®ç°word2vecä¸­æ–‡è¯å‘é‡çš„è®­ç»ƒ](https://zhuanlan.zhihu.com/p/28979653)

- [é¡ç¥ç¶“ç¶²è·¯ -- word2vec (part 1 : Overview) Â« MARK CHANG'S BLOG](http://cpmarkchang.logdown.com/posts/773062-neural-network-word2vec-part-1-overview)

- [Alex-CHUN-YU/Word2vec: è¨“ç·´ä¸­æ–‡è©å‘é‡ Word2vec, Word2vec was created by a team of researchers led by Tomas Mikolov at Google.](https://github.com/Alex-CHUN-YU/Word2vec)

- [ç”¨ä¸­æ–‡è³‡æ–™æ¸¬è©¦ word2vec - ç¿¼ä¹‹éƒ½, City of Wings](http://city.shaform.com/blog/2014/11/04/word2vec.html)

- [è‡ªç„¶èªè¨€è™•ç†å…¥é–€- Word2vecå°å¯¦ä½œ â€“ PyLadies Taiwan â€“ Medium](https://medium.com/pyladies-taiwan/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E5%85%A5%E9%96%80-word2vec%E5%B0%8F%E5%AF%A6%E4%BD%9C-f8832d9677c8)

- [è©å‘é‡ä»‹ç´¹](https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1)

### Skip-gram / CBOW

- [Is skip gram (negative sampling) better than CBOW (NS) for word2vec? If so, why? - Quora](https://www.quora.com/Is-skip-gram-negative-sampling-better-than-CBOW-NS-for-word2vec-If-so-why)

    > Please see [Stephan Gouws](https://www.quora.com/profile/Stephan-Gouws) excellent answer here: [How does word2vec work? Can someone walk through a specific example?](https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example)
    > 
    > >For the skipgram, the direction of the prediction is simply inverted, i.e. now we try to predict P(citizens | X), P(of | X), etc. This turns out to learn finer-grained vectors when one trains over more data. The main reason is that the CBOW smooths over a lot of the distributional statistics by averaging over all context words while the skipgram does not. With little data, this "regularizing" effect of the CBOW turns out to be helpful, but since data is the ultimate regularizer the skipgram is able to extract more information when more data is available.
    > 


### fastText

- [Word vectors for 157 languages Â· fastText](https://fasttext.cc/docs/en/crawl-vectors.html)

- [fastTextï¼Œæ™ºæ…§èˆ‡ç¾è²Œä¸¦é‡çš„æ–‡æœ¬åˆ†é¡åŠå‘é‡åŒ–å·¥å…· - å¹«è¶£](http://bangqu.com/749a22.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > ç°¡å–®ä»‹ç´¹ä¸€ä¸‹fastTextçš„ä¸»è¦ä½œè€…ï¼Œé€™ä½é«˜é¡å€¼çš„Facebookç§‘å­¸å®¶Tomas Mikolovå°å“¥ã€‚ä»–2012å¹´åˆ°2014å¹´å°±è·æ–¼Googleï¼Œéš¨å¾Œè·³åˆ°äº†Facebookè‡³ä»Šã€‚
    >  
    > ä»–åæšå¤©ä¸‹çš„ï¼Œä¸»è¦æ˜¯ä»¥ä¸‹ä¸‰ç¯‡é‡è¦è«–æ–‡ï¼š
    > 
    > 1.Efficient Estimation of WordRepresentation in Vector Space, 2013 â€”â€” é€™ç¯‡æ˜¯word2vecçš„é–‹è’™ä¹‹ä½œï¼›
    > 
    > 2.Distributed Representations ofSentences and Documents, 2014 â€”â€” é€™ç¯‡å°‡è©å‘é‡çš„æ€æƒ³æ“´å±•åˆ°äº†æ®µè½å’Œæ–‡æª”ä¸Šï¼›
    > 
    > 3.Enriching Word Vectors withSubword Information, 2016 â€”â€” é€™ç¯‡å’ŒfastTextç›¸é—œï¼Œå¼•å…¥äº†è©å…§çš„n-gramä¿¡æ¯ï¼Œè±å¯Œäº†è©å‘é‡çš„èªç¾©ã€‚
    > 
    > fastTextèƒ½å¤ åšåˆ°æ•ˆæœå¥½ï¼Œé€Ÿåº¦å¿«ï¼Œä¸»è¦ä¾é å…©å€‹ç¥•å¯†æ­¦å™¨ï¼šä¸€æ˜¯åˆ©ç”¨äº†è©å…§çš„n-gramä¿¡æ¯(subword n-gram information)ï¼ŒäºŒæ˜¯ç”¨åˆ°äº†å±¤æ¬¡åŒ–Softmaxè¿´æ­¸(Hierarchical Softmax)çš„è¨“ç·´trickã€‚æˆ‘å€‘åˆ†åˆ¥ä»‹ç´¹ä¸€ä¸‹ã€‚
    > 
    > 
    > __Subword n-gramlnformation__
    > 
    > åœ¨fastTextçš„å·¥ä½œä¹‹å‰ï¼Œå¤§éƒ¨åˆ†çš„æ–‡æœ¬å‘é‡åŒ–çš„å·¥ä½œï¼Œéƒ½æ˜¯ä»¥è©å½™è¡¨ä¸­çš„ç¨ç«‹å–®è©ä½œçˆ²åŸºæœ¬å–®å…ƒä¾†é€²è¡Œè¨“ç·´å­¸ç¿’çš„ã€‚é€™ç¨®æƒ³æ³•éå¸¸è‡ªç„¶ï¼Œä½†ä¹Ÿæœƒå¸¶ä¾†å¦‚ä¸‹çš„å•é¡Œï¼š
    > 
    > Â· ä½é »è©ã€ç½•è¦‹è©ï¼Œç”±æ–¼åœ¨èªæ–™ä¸­æœ¬èº«å‡ºç¾çš„æ¬¡æ•¸å°±å°‘ï¼Œå¾—ä¸åˆ°è¶³å¤ çš„è¨“ç·´ï¼Œæ•ˆæœä¸ä½³ï¼›
    > 
    > Â· æœªç™»éŒ„è©ï¼Œå¦‚æœå‡ºç¾äº†ä¸€äº›åœ¨è©å…¸ä¸­éƒ½æ²’æœ‰å‡ºç¾éçš„è©ï¼Œæˆ–è€…å¸¶æœ‰æŸäº›æ‹¼å¯«éŒ¯èª¤çš„è©ï¼Œå‚³çµ±æ¨¡å‹æ›´åŠ ç„¡èƒ½çˆ²åŠ›ã€‚
    > 
    > fastTextå¼•å…¥äº†subword n-gramçš„æ¦‚å¿µä¾†è§£æ±ºè©å½¢è®ŠåŒ–(morphology)çš„å•é¡Œã€‚ç›´è§€ä¸Šï¼Œå®ƒå°‡ä¸€å€‹å–®è©æ‰“æ•£åˆ°å­—ç¬¦ç´šåˆ¥ï¼Œä¸¦ä¸”åˆ©ç”¨å­—ç¬¦ç´šåˆ¥çš„n-gramä¿¡æ¯ä¾†æ•æ‰å­—ç¬¦é–“çš„é †åºé—œä¿‚ï¼Œå¸Œæœ›èƒ½å¤ ä»¥æ­¤è±å¯Œå–®è©å…§éƒ¨æ›´ç´°å¾®çš„èªç¾©ã€‚æˆ‘å€‘çŸ¥é“ï¼Œè¥¿æ–¹èªè¨€æ–‡å­—å¸¸å¸¸é€šéå‰ç¶´ã€å¾Œç¶´ã€å­—æ ¹ä¾†æ§‹è©ï¼Œæ¼¢èªä¹Ÿæœ‰å–®å­—è¡¨ç¾©çš„å‚³çµ±ï¼Œæ‰€ä»¥é€™æ¨£çš„åšæ³•è½èµ·ä¾†é‚„æ˜¯æœ‰ä¸€å®šçš„é“ç†ã€‚
    > 
    > èˆ‰å€‹ä¾‹å­ã€‚å°æ–¼ä¸€å€‹å–®è©ã€Œgoogleã€ï¼Œçˆ²äº†è¡¨é”å–®è©å‰å¾Œé‚Šç•Œï¼Œæˆ‘å€‘åŠ å…¥<>å…©å€‹å­—ç¬¦ï¼Œå³è®Šå½¢çˆ²ã€Œ ã€ã€‚å‡è¨­æˆ‘å€‘å¸Œæœ›æŠ½å–æ‰€æœ‰çš„tri-gramä¿¡æ¯ï¼Œå¯ä»¥å¾—åˆ°å¦‚ä¸‹é›†åˆï¼šG = { }ã€‚åœ¨å¯¦è¸ä¸­ï¼Œæˆ‘å€‘å¾€å¾€æœƒåŒæ™‚æå–å–®è©çš„å¤šç¨®n-gramä¿¡æ¯ï¼Œå¦‚2/3/4/5-gramã€‚é€™æ¨£ï¼ŒåŸå§‹çš„ä¸€å€‹å–®è©googleï¼Œå°±è¢«ä¸€å€‹å­—ç¬¦ç´šåˆ¥çš„n-gramé›†åˆæ‰€è¡¨é”ã€‚
    > 
    > åœ¨è¨“ç·´éç¨‹ä¸­ï¼Œæ¯å€‹n-graméƒ½æœƒå°æ‡‰è¨“ç·´ä¸€å€‹å‘é‡ï¼Œè€ŒåŸä¾†å®Œæ•´å–®è©çš„è©å‘é‡å°±ç”±å®ƒå°æ‡‰çš„æ‰€æœ‰n-gramçš„å‘é‡æ±‚å’Œå¾—åˆ°ã€‚æ‰€æœ‰çš„å–®è©å‘é‡ä»¥åŠå­—ç¬¦ç´šåˆ¥çš„n-gramå‘é‡æœƒåŒæ™‚ç›¸åŠ æ±‚å¹³å‡ä½œçˆ²è¨“ç·´æ¨¡å‹çš„è¼¸å…¥ã€‚
    > 
    > å¾å¯¦é©—æ•ˆæœä¾†çœ‹ï¼Œsubword n-gramä¿¡æ¯çš„åŠ å…¥ï¼Œä¸ä½†è§£æ±ºäº†ä½é »è©æœªç™»éŒ„è©çš„è¡¨é”çš„å•é¡Œï¼Œè€Œä¸”å°æ–¼æœ€çµ‚ä»»å‹™ç²¾åº¦ä¸€èˆ¬æœƒæœ‰å¹¾å€‹ç™¾åˆ†é»çš„æå‡ã€‚å”¯ä¸€çš„å•é¡Œå°±æ˜¯ç”±æ–¼éœ€è¦ä¼°è¨ˆçš„åƒæ•¸å¤šï¼Œæ¨¡å‹å¯èƒ½æœƒæ¯”è¼ƒè†¨è„¹ã€‚ä¸éï¼ŒFacebookä¹Ÿæä¾›äº†å¹¾é»å£“ç¸®æ¨¡å‹çš„å»ºè­°ï¼š
    > 
    > Â· æ¡ç”¨hash-trickã€‚ç”±æ–¼n-gramåŸå§‹çš„ç©ºé–“å¤ªå¤§ï¼Œå¯ä»¥ç”¨æŸç¨®hashå‡½æ•¸å°‡å…¶æ˜ å°„åˆ°å›ºå®šå¤§å°çš„bucketsä¸­å»ï¼Œå¾è€Œå¯¦ç¾å…§å­˜å¯æ§ï¼›
    > 
    > Â· æ¡ç”¨quantizeå‘½ä»¤ï¼Œå°ç”Ÿæˆçš„æ¨¡å‹é€²è¡Œåƒæ•¸é‡åŒ–å’Œå£“ç¸®ï¼›
    > 
    > Â· æ¸›å°æœ€çµ‚å‘é‡çš„ç¶­åº¦ã€‚
    > 
    > __Hierarchical Softmax__
    > 
    > å¦ä¸€å€‹æ•ˆç‡å„ªåŒ–çš„é»æ˜¯æ‰€è¬‚çš„å±¤æ¬¡åŒ–Softmaxã€‚
    > 
    > Softmaxå¤§å®¶éƒ½æ¯”è¼ƒç†Ÿæ‚‰ï¼Œå®ƒæ˜¯é‚è¼¯è¿´æ­¸(logisticregression)åœ¨å¤šåˆ†é¡ä»»å‹™ä¸Šçš„æ¨å»£ï¼Œæ˜¯æˆ‘å€‘è¨“ç·´çš„ç¥ç¶“ç¶²çµ¡ä¸­çš„æœ€å¾Œä¸€å±¤ã€‚ä¸€èˆ¬åœ°ï¼ŒSoftmaxä»¥éš±è—å±¤çš„è¼¸å‡ºhçˆ²è¼¸å…¥ï¼Œç¶“éç·šæ€§å’ŒæŒ‡æ•¸è®Šæ›å¾Œï¼Œå†é€²è¡Œå…¨å±€çš„æ­¸ä¸€åŒ–è™•ç†ï¼Œæ‰¾åˆ°æ¦‚ç‡æœ€å¤§çš„è¼¸å‡ºé …ã€‚ç•¶è©å½™æ•¸é‡Vè¼ƒå¤§æ™‚ï¼ˆä¸€èˆ¬æœƒåˆ°å¹¾åè¬é‡ç´šï¼‰ï¼ŒSoftmaxè¨ˆç®—ä»£åƒ¹å¾ˆå¤§ï¼Œæ˜¯O(V)é‡ç´šã€‚
    > 
    > å±¤æ¬¡åŒ–çš„Softmaxçš„æ€æƒ³å¯¦è³ªä¸Šæ˜¯å°‡ä¸€å€‹å…¨å±€å¤šåˆ†é¡çš„å•é¡Œï¼Œè½‰åŒ–æˆçˆ²äº†è‹¥å¹²å€‹äºŒå…ƒåˆ†é¡å•é¡Œï¼Œå¾è€Œå°‡è¨ˆç®—è¤‡é›œåº¦å¾O(V)é™åˆ°O(logV)ã€‚
    > 
    > æ¯å€‹äºŒå…ƒåˆ†é¡å•é¡Œï¼Œç”±ä¸€å€‹åŸºæœ¬çš„é‚è¼¯è¿´æ­¸å–®å…ƒä¾†å¯¦ç¾ã€‚å¦‚ä¸‹åœ–æ‰€ç¤ºï¼Œå¾æ ¹çµé»é–‹å§‹ï¼Œæ¯å€‹ä¸­é–“çµé»ï¼ˆæ¨™è¨˜æˆç°è‰²ï¼‰éƒ½æ˜¯ä¸€å€‹é‚è¼¯è¿´æ­¸å–®å…ƒï¼Œæ ¹æ“šå®ƒçš„è¼¸å‡ºä¾†é¸æ“‡ä¸‹ä¸€æ­¥æ˜¯å‘å·¦èµ°é‚„æ˜¯å‘å³èµ°ã€‚ä¸‹åœ–ç¤ºä¾‹ä¸­å¯¦éš›ä¸Šèµ°äº†ä¸€æ¢ã€Œå·¦-å·¦-å³ã€çš„è·¯ç·šï¼Œå¾è€Œæ‰¾åˆ°å–®è©wâ‚‚ã€‚è€Œæœ€çµ‚è¼¸å‡ºå–®è©wâ‚‚çš„æ¦‚ç‡ï¼Œç­‰æ–¼ä¸­é–“è‹¥å¹²é‚è¼¯è¿´æ­¸å–®å…ƒè¼¸å‡ºæ¦‚ç‡çš„é€£ä¹˜ç©ã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180606/749a22152825763470852j19.png)
    > 
    >  ![](http://i2.bangqu.com/j/news/20180606/749a221528257635684823ER.png)
    > 
    > è‡³æ­¤ï¼Œæˆ‘å€‘é‚„å‰©ä¸‹å…©å€‹å•é¡Œï¼Œä¸€æ˜¯å¦‚ä½•æ§‹é€ æ¯å€‹é‚è¼¯è¿´æ­¸å–®å…ƒçš„è¼¸å…¥ï¼Œå¦ä¸€å€‹æ˜¯å¦‚ä½•å»ºç«‹é€™æ£µç”¨æ–¼åˆ¤æ–·çš„æ¨¹å½¢çµæ§‹ã€‚
    > 
    > __fastTextå’Œå‚³çµ±CBOWæ¨¡å‹å°æ¯”__
    > 
    > é€™è£å‡è¨­ä½ å°word2vecçš„CBOWæ¨¡å‹æ¯”è¼ƒç†Ÿæ‚‰ï¼Œæˆ‘å€‘ä¾†å°çµä¸€ä¸‹CBOWå’ŒfastTextçš„è¨“ç·´éç¨‹æœ‰ä»€éº¼ä¸åŒã€‚ä¸‹é¢å…©å¼µåœ–åˆ†åˆ¥å°æ‡‰CBOWå’ŒfastTextçš„ç¶²çµ¡çµæ§‹åœ–ã€‚
    > 
    > å…©è€…çš„ä¸åŒä¸»è¦é«”ç¾åœ¨å¦‚ä¸‹å¹¾å€‹æ–¹é¢ï¼š
    > 
    > Â· è¼¸å…¥å±¤ï¼šCBOWçš„è¼¸å…¥æ˜¯ç›®æ¨™å–®è©çš„ä¸Šä¸‹æ–‡ä¸¦é€²è¡Œone-hotç·¨ç¢¼ï¼ŒfastTextçš„è¼¸å…¥æ˜¯å¤šå€‹å–®è©embeddingå‘é‡ï¼Œä¸¦å°‡å–®è©çš„å­—ç¬¦ç´šåˆ¥çš„n-gramå‘é‡ä½œçˆ²é¡å¤–çš„ç‰¹å¾µï¼›
    > 
    > Â· å¾è¼¸å…¥å±¤åˆ°éš±è—å±¤ï¼ŒCBOWæœƒå°‡ä¸Šä¸‹æ–‡å–®è©å‘é‡ç–ŠåŠ èµ·ä¾†ä¸¦ç¶“éä¸€æ¬¡çŸ©é™£ä¹˜æ³•ï¼ˆç·šæ€§è®ŠåŒ–ï¼‰ä¸¦æ‡‰ç”¨æ¿€æ´»å‡½æ•¸ï¼Œè€ŒfastTextçœç•¥äº†é€™ä¸€éç¨‹ï¼Œç›´æ¥å°‡embeddingéçš„å‘é‡ç‰¹å¾µæ±‚å’Œå–å¹³å‡ï¼›
    > 
    > Â· è¼¸å‡ºå±¤ï¼Œä¸€èˆ¬çš„CBOWæ¨¡å‹æœƒæ¡ç”¨Softmaxä½œçˆ²è¼¸å‡ºï¼Œè€ŒfastTextå‰‡æ¡ç”¨äº†Hierarchical Softmaxï¼Œå¤§å¤§é™ä½äº†æ¨¡å‹è¨“ç·´æ™‚é–“ï¼›
    > 
    > Â· CBOWçš„è¼¸å‡ºæ˜¯ç›®æ¨™è©å½™ï¼ŒfastTextçš„è¼¸å‡ºæ˜¯æ–‡æª”å°æ‡‰çš„é¡æ¨™ã€‚
    > 



### ELMo

- [[1802.05365] Deep contextualized word representations](https://arxiv.org/abs/1802.05365)








## æ–‡ç« å‘é‡

### æ¦‚è§€

- [ç•¶å‰æœ€å¥½çš„è©å¥åµŒå…¥æŠ€è¡“æ¦‚è¦½ï¼šå¾ç„¡ç›£ç£å­¸ç¿’åˆ°ç›£ç£ã€å¤šä»»å‹™å­¸ç¿’ - å¹«è¶£](http://bangqu.com/59R436.html#utm_source=Facebook_PicSee&utm_medium=Social)

    - [ğŸ“šThe Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)

    > FastText ç›¸å°æ–¼åŸå§‹çš„ word2vec å‘é‡æœ€ä¸»è¦çš„æå‡æ˜¯å®ƒå¼•å…¥äº† n å…ƒå­—ç¬¦ï¼ˆn-gramï¼‰ï¼Œé€™ä½¿å¾—å°æ²’æœ‰åœ¨è¨“ç·´æ•¸æ“šä¸­å‡ºç¾çš„å–®è©ï¼ˆè©å½™è¡¨å¤–çš„å–®è©ï¼‰è¨ˆç®—å–®è©çš„è¡¨å¾µæˆçˆ²äº†å¯èƒ½ã€‚
    >     
    > ---
    >     
    > æ·±åº¦ä¸Šä¸‹æ–‡å–®è©è¡¨å¾µï¼ˆELMoï¼‰åœ¨å¾ˆå¤§çš„ç¨‹åº¦ä¸Šæé«˜äº†ç›®å‰æœ€å…ˆé€²çš„è©åµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒå€‘ç”± Allen äººå·¥æ™ºèƒ½ç ”ç©¶æ‰€ç ”ç™¼ï¼Œä¸¦å°‡åœ¨ 6 æœˆåˆçš„ NAACL 2018 ï¼ˆ https://arxiv.org/abs/1802.05365 ï¼‰ ä¸­å±•ç¤ºã€‚
    > 
    > ELMo æ¨¡å‹æœƒçˆ²æ¯ä¸€å€‹å–®è©åˆ†é…ä¸€å€‹è¡¨å¾µï¼Œè©²è¡¨å¾µæ˜¯å®ƒå€‘æ‰€å±¬çš„æ•´å€‹èªæ–™åº«ä¸­çš„å¥å­çš„ä¸€å€‹å‡½æ•¸ã€‚è©åµŒå…¥å°‡å¾ä¸€å€‹å…©å±¤çš„é›™å‘èªè¨€æ¨¡å‹ï¼ˆLMï¼‰çš„å…§éƒ¨ç‹€æ…‹ä¸­è¨ˆç®—å‡ºä¾†ï¼Œå› æ­¤è©²æ¨¡å‹è¢«å‘½åçˆ²ã€ŒELMoã€ï¼š Embeddings from Language Modelsï¼ˆE ä»£è¡¨ã€ŒåµŒå…¥ã€ï¼ŒLM ä»£è¡¨ã€Œèªè¨€æ¨¡å‹ã€ï¼‰ã€‚
    > 
    > ELMo æ¨¡å‹çš„ç‰¹é»ï¼š
    > 
    > - ELMo æ¨¡å‹çš„è¼¸å…¥æ˜¯å­—ç¬¦è€Œä¸æ˜¯å–®è©ã€‚å› æ­¤ï¼Œå®ƒå€‘å¯ä»¥åˆ©ç”¨å­è©å–®å…ƒçš„å„ªå‹¢ä¾†è¨ˆç®—æœ‰æ„ç¾©çš„å–®è©è¡¨ç¤ºï¼Œå³ä½¿é€™äº›å–®è©å¯èƒ½åœ¨è©å½™è¡¨ä¹‹å¤–ï¼ˆå°±åƒ FastText ä¸€æ¨£ï¼‰ã€‚
    > 
    > - ELMo æ˜¯åœ¨é›™å‘èªè¨€æ¨¡å‹ä¸­çš„ä¸€äº›å±¤ä¸Šçš„æ¿€å‹µå‡½æ•¸çš„ä¸²æ¥ã€‚ä¸€å€‹èªè¨€æ¨¡å‹çš„ä¸åŒå±¤æœƒå°ä¸€å€‹å–®è©çš„ä¸åŒé¡å‹çš„ä¿¡æ¯é€²è¡Œç·¨ç¢¼ï¼ˆä¾‹å¦‚ï¼Œè©æ€§æ¨™è¨»ï¼ˆPart-Of-Speech taggingï¼‰ç”±é›™å‘ LSTMï¼ˆbiLSTMï¼‰çš„è¼ƒä½å±¤å¾ˆå¥½åœ°é æ¸¬ï¼Œè€Œè©ç¾©æ’æ­§å‰‡ç”±è¼ƒé«˜å±¤æ›´å¥½åœ°é€²è¡Œç·¨ç¢¼ï¼‰ã€‚å°‡æ‰€æœ‰çš„å±¤ä¸²æ¥èµ·ä¾†ä½¿å¾—è‡ªç”±çµ„åˆå„ç¨®ä¸åŒçš„å–®è©è¡¨å¾µæˆçˆ²äº†å¯èƒ½ï¼Œå¾è€Œåœ¨ä¸‹æ¸¸ä»»å‹™ä¸­å¾—åˆ°æ›´å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚
    > 
    > ---
    > 
    > åœ¨é€™å€‹é ˜åŸŸæœ‰ä¸€å€‹å»£æ³›çš„å…±è­˜ï¼ˆ http://arxiv.org/abs/1805.01070 ï¼‰ ï¼Œé‚£å°±æ˜¯ï¼šç›´æ¥å°å¥å­çš„è©åµŒå…¥å–å¹³å‡ï¼ˆæ‰€è¬‚çš„è©è¢‹æ¨¡å‹ï¼ˆBag-of-Wordï¼ŒBoWï¼‰ï¼‰é€™æ¨£ç°¡å–®çš„æ–¹æ³•å¯ä»¥çˆ²è¨±å¤šä¸‹æ¸¸ä»»å‹™æä¾›ä¸€å€‹å¾ˆå¼·å¤§çš„å°æ¯”åŸºç·šã€‚
    > 
    > Arora ç­‰äººåœ¨ ICLR 2017 ä¸Šæå‡ºäº†ã€ŒA Simple but Tough-to-Beat Baseline for Sentence Embeddingsã€ ï¼ˆ https://openreview.net/forum?id=SyK00v5xx ï¼‰ ï¼Œé€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„èƒ½å¤ è¢«ç”¨æ–¼è¨ˆç®—é€™å€‹åŸºç·šï¼ˆBoWï¼‰çš„ç®—æ³•ï¼Œç®—æ³•çš„å¤§è‡´æè¿°å¦‚ä¸‹ï¼šé¸æ“‡ä¸€å€‹æµè¡Œçš„è©åµŒå…¥æ–¹æ³•ï¼Œé€šéè©å‘é‡çš„ç·šæ€§çš„åŠ æ¬Šçµ„åˆå°ä¸€å€‹å¥å­é€²è¡Œç·¨ç¢¼ï¼Œä¸¦ä¸”åˆªé™¤å…±æœ‰çš„éƒ¨åˆ†ï¼ˆåˆªé™¤å®ƒå€‘çš„ç¬¬ä¸€å€‹ä¸»æˆåˆ†ä¸Šçš„æŠ•å½±ï¼‰ã€‚
    > 
    > ---
    > 
    > é™¤äº†ç°¡å–®çš„è©å‘é‡å¹³å‡ï¼Œç¬¬ä¸€å€‹ä¸»è¦çš„æè­°æ˜¯ä½¿ç”¨ç„¡ç›£ç£å­¸ç¿’è¨“ç·´ç›®æ¨™ï¼Œé€™é …å·¥ä½œæ˜¯èµ·å§‹æ–¼ Jamie Kiros å’Œä»–çš„åŒäº‹å€‘åœ¨ 2015 å¹´æå‡ºçš„ã€ŒSkip-thought vectorsã€ï¼ˆ https://arxiv.org/abs/1506.06726 ï¼‰ ã€‚
    > 
    > ã€ŒSkip-thoughts vectorã€æ˜¯ä¸€å€‹å…¸å‹çš„å­¸ç¿’ç„¡ç›£ç£å¥å­åµŒå…¥çš„æ¡ˆä¾‹ã€‚å®ƒå¯ä»¥è¢«èªçˆ²ç›¸ç•¶æ–¼çˆ²è©åµŒå…¥è€Œé–‹ç™¼çš„ã€Œskip-gramã€æ¨¡å‹çš„å¥å­å‘é‡ï¼Œæˆ‘å€‘åœ¨é€™è£è©¦åœ–é æ¸¬ä¸€å€‹çµ¦å®šçš„å¥å­å‘¨åœçš„å¥å­ï¼Œè€Œä¸æ˜¯é æ¸¬ä¸€å€‹å–®è©å‘¨åœçš„å…¶ä»–å–®è©ã€‚è©²æ¨¡å‹ç”±ä¸€å€‹åŸºæ–¼å¾ªç’°ç¥ç¶“ç¶²çµ¡çš„ç·¨ç¢¼å™¨â€”è§£ç¢¼å™¨çµæ§‹çµ„æˆï¼Œç ”ç©¶è€…é€šéè¨“ç·´é€™å€‹æ¨¡å‹å¾ç•¶å‰å¥å­ä¸­é‡æ§‹å‘¨åœçš„å¥å­ã€‚
    > 
    > Skip-Thoughts çš„è«–æ–‡ä¸­æœ€ä»¤äººæ„Ÿèˆˆè¶£çš„è§€é»æ˜¯ä¸€ç¨®è©å½™è¡¨æ“´å±•æ–¹æ¡ˆï¼šKiros ç­‰äººé€šéåœ¨ä»–å€‘çš„å¾ªç’°ç¥ç¶“ç¶²çµ¡è©åµŒå…¥ç©ºé–“å’Œä¸€å€‹æ›´å¤§çš„è©åµŒå…¥ç©ºé–“ï¼ˆä¾‹å¦‚ï¼Œword2vecï¼‰ä¹‹é–“å­¸ç¿’ä¸€ç¨®ç·šæ€§è®Šæ›ä¾†è™•ç†è¨“ç·´éç¨‹ä¸­æ²’æœ‰å‡ºç¾çš„å–®è©ã€‚
    > 
    > ã€ŒQuick-thoughts vectorsã€ï¼ˆ https://openreview.net/forum?id=rJvJXZb0W ï¼‰ æ˜¯ç ”ç©¶äººå“¡æœ€è¿‘å°ã€ŒSkip-thoughts vectorsã€çš„ä¸€å€‹æ”¹é€²ï¼Œå®ƒåœ¨ä»Šå¹´çš„ ICLR ä¸Šè¢«æå‡ºã€‚åœ¨é€™é …å·¥ä½œä¸­ï¼Œåœ¨çµ¦å®šå‰ä¸€å€‹å¥å­çš„æ¢ä»¶ä¸‹é æ¸¬ä¸‹ä¸€å€‹å¥å­çš„ä»»å‹™è¢«é‡æ–°å®šç¾©çˆ²äº†ä¸€å€‹åˆ†é¡å•é¡Œï¼šç ”ç©¶äººå“¡å°‡ä¸€å€‹ç”¨æ–¼åœ¨è¡†å¤šå€™é¸è€…ä¸­é¸å‡ºä¸‹ä¸€å€‹å¥å­çš„åˆ†é¡å™¨ä»£æ›¿ç­è§£ç¢¼å™¨ã€‚å®ƒå¯ä»¥è¢«è§£é‡‹çˆ²å°ç”Ÿæˆå•é¡Œçš„ä¸€å€‹åˆ¤åˆ¥åŒ–çš„è¿‘ä¼¼ã€‚
    > 
    > è©²æ¨¡å‹çš„é‹è¡Œé€Ÿåº¦æ˜¯å®ƒçš„å„ªé»ä¹‹ä¸€ï¼ˆèˆ‡ Skip-thoughts æ¨¡å‹å±¬æ–¼åŒä¸€å€‹æ•¸é‡ç´šï¼‰ï¼Œä½¿å…¶æˆçˆ²åˆ©ç”¨æµ·é‡æ•¸æ“šé›†çš„ä¸€å€‹å…·æœ‰ç«¶çˆ­åŠ›çš„è§£æ±ºæ–¹æ¡ˆã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180606/59R4361528261210081r28S2.png)*ã€ŒQuick-thoughtsã€åˆ†é¡ä»»å‹™ç¤ºæ„åœ–ã€‚åˆ†é¡å™¨éœ€è¦å¾ä¸€çµ„å¥å­åµŒå…¥ä¸­é¸å‡ºä¸‹ä¸€å€‹å¥å­ã€‚åœ–ç‰‡ä¾†è‡ª Logeswaran ç­‰äººæ‰€è‘—çš„ã€ŒAn efficient framework for learning sentence representationsã€ã€‚*
    > 
    > ---
    > 
    > åœ¨å¾ˆé•·ä¸€æ®µæ™‚é–“å…§ï¼Œäººå€‘èªçˆ²ç›£ç£å­¸ç¿’æŠ€è¡“æ¯”ç„¡ç›£ç£å­¸ç¿’æŠ€è¡“å¾—åˆ°çš„å¥å­åµŒå…¥çš„è³ªé‡è¦ä½ä¸€äº›ã€‚ç„¶è€Œï¼Œé€™ç¨®å‡èªªæœ€è¿‘è¢«æ¨ç¿»äº†ï¼Œé€™è¦éƒ¨åˆ†æ­¸åŠŸæ–¼ã€ŒInferSentã€ï¼ˆ https://arxiv.org/abs/1705.02364 ï¼‰ çš„æå‡ºã€‚
    > 
    > InferSent å…·æœ‰éå¸¸ç°¡å–®çš„æ¶æ§‹ï¼Œé€™ä½¿å¾—å®ƒæˆçˆ²äº†ä¸€ç¨®éå¸¸æœ‰è¶£çš„æ¨¡å‹ã€‚å®ƒä½¿ç”¨ Sentence Natural Language Inferenceï¼ˆNLIï¼‰æ•¸æ“šé›†ï¼ˆè©²æ•¸æ“šé›†åŒ…å« 570,000 å°å¸¶æ¨™ç±¤çš„å¥å­ï¼Œå®ƒå€‘è¢«åˆ†æˆäº†ä¸‰é¡ï¼šä¸­ç«‹ã€çŸ›ç›¾ä»¥åŠè˜Šå«ï¼‰è¨“ç·´ä¸€å€‹ä½æ–¼å¥å­ç·¨ç¢¼å™¨é ‚å±¤çš„åˆ†é¡å™¨ã€‚å…©å€‹å¥å­ä½¿ç”¨åŒä¸€å€‹ç·¨ç¢¼å™¨é€²è¡Œç·¨ç¢¼ï¼Œè€Œåˆ†é¡å™¨å‰‡æ˜¯ä½¿ç”¨é€šéå…©å€‹å¥å­åµŒå…¥æ§‹å»ºçš„ä¸€å°å¥å­è¡¨å¾µè¨“ç·´çš„ã€‚Conneau ç­‰äººæ¡ç”¨äº†ä¸€å€‹é€šéæœ€å¤§æ± åŒ–æ“ä½œå¯¦ç¾çš„é›™å‘ LSTM ä½œçˆ²ç·¨ç¢¼å™¨ã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180606/59R4361528261211217433N2.png)
    > 
    > 
    > ---
    >
    > åœ¨ ICLR 2018 ä¸Šç™¼è¡¨çš„æè¿° MILA å’Œå¾®è»Ÿè’™ç‰¹åˆ©çˆ¾ç ”ç©¶é™¢çš„å·¥ä½œçš„è«–æ–‡ã€ŠLearning General Purpose Distributed Sentence Representation via Large Scale Multi-Task Learningã€‹ï¼ˆ https://arxiv.org/abs/1804.00079 ï¼‰ä¸­ï¼ŒSubramanian ç­‰äººè§€å¯Ÿåˆ°ï¼Œçˆ²äº†èƒ½å¤ åœ¨å„ç¨®å„æ¨£çš„ä»»å‹™ä¸­æ³›åŒ–å¥å­è¡¨å¾µï¼Œå¾ˆæœ‰å¿…è¦å°‡ä¸€å€‹å¥å­çš„å¤šå€‹å±¤é¢çš„ä¿¡æ¯é€²è¡Œç·¨ç¢¼ã€‚
    > 
    > å› æ­¤ï¼Œé€™ç¯‡æ–‡ç« çš„ä½œè€…åˆ©ç”¨äº†ä¸€å€‹ä¸€å°å¤šçš„å¤šä»»å‹™å­¸ç¿’æ¡†æ¶ï¼Œé€šéåœ¨ä¸åŒçš„ä»»å‹™ä¹‹é–“é€²è¡Œåˆ‡æ›å»å­¸ç¿’ä¸€å€‹é€šç”¨çš„å¥å­åµŒå…¥ã€‚è¢«é¸ä¸­çš„ 6 å€‹ä»»å‹™ï¼ˆå°æ–¼ä¸‹ä¸€å€‹/ä¸Šä¸€å€‹å¥å­çš„ Skip-thoughts é æ¸¬ã€ç¥ç¶“æ©Ÿå™¨ç¿»è­¯ã€çµ„åˆ¥è§£æï¼ˆconstituency parsingï¼‰ï¼Œä»¥åŠç¥ç¶“èªè¨€æ¨ç†ï¼‰å…±äº«ç›¸åŒçš„ç”±ä¸€å€‹é›™å‘é–€æ§å¾ªç’°å–®å…ƒå¾—åˆ°çš„å¥å­åµŒå…¥ã€‚å¯¦é©—è¡¨æ˜ï¼Œåœ¨å¢æ·»äº†ä¸€å€‹å¤šèªè¨€ç¥ç¶“æ©Ÿå™¨ç¿»è­¯ä»»å‹™æ™‚ï¼Œå¥æ³•å±¬æ€§èƒ½å¤ è¢«æ›´å¥½åœ°å­¸ç¿’åˆ°ï¼Œå¥å­é•·åº¦å’Œè©åºèƒ½å¤ é€šéä¸€å€‹å¥æ³•åˆ†æä»»å‹™å­¸ç¿’åˆ°ï¼Œä¸¦ä¸”è¨“ç·´ä¸€å€‹ç¥ç¶“èªè¨€æ¨ç†èƒ½å¤ ç·¨ç¢¼èªæ³•ä¿¡æ¯ã€‚
    > 
    > è°·æ­Œåœ¨ 2018 å¹´åˆç™¼å¸ƒçš„çš„é€šç”¨å¥å­ç·¨ç¢¼å™¨ï¼ˆ https://arxiv.org/abs/1803.11175 ï¼‰ä¹Ÿä½¿ç”¨äº†åŒæ¨£çš„æ–¹æ³•ã€‚ä»–å€‘çš„ç·¨ç¢¼å™¨ä½¿ç”¨ä¸€å€‹åœ¨å„ç¨®å„æ¨£çš„æ•¸æ“šæºå’Œå„ç¨®å„æ¨£çš„ä»»å‹™ä¸Šè¨“ç·´çš„è½‰æ›ç¶²çµ¡ï¼Œæ—¨åœ¨å‹•æ…‹åœ°é©æ‡‰å„é¡è‡ªç„¶èªè¨€ç†è§£ä»»å‹™ã€‚è©²æ¨¡å‹çš„ä¸€å€‹é è¨“ç·´å¥½çš„ç‰ˆæœ¬å¯ä»¥åœ¨ TensorFlow ç²å¾—ã€‚


### TF-IDF

- [tfâ€“idf - Wikiwand](https://www.wikiwand.com/en/Tf%E2%80%93idf)

- [[æ–‡ä»¶æ¢å‹˜] TF-IDF æ¼”ç®—æ³•ï¼šå¿«é€Ÿè¨ˆç®—å–®å­—èˆ‡æ–‡ç« çš„é—œè¯ â€“ David's Perspective](https://taweihuang.hpd.io/2017/03/01/tfidf/)

    > ### BoW (Bag of Words) èˆ‡è©å½™æ•¸é‡-æ–‡ä»¶çŸ©é™£
    > 
    > å‡è¨­ç¾åœ¨æœ‰ ![D](https://s0.wp.com/latex.php?latex=D&bg=ffffff&fg=000&s=0 "D") ç¯‡æ–‡ä»¶ (document)ï¼Œè€Œæ‰€æœ‰æ–‡ä»¶ä¸­ç¸½å…±ä½¿ç”¨äº† ![~T~](https://s0.wp.com/latex.php?latex=%7ET%7E&bg=ffffff&fg=000&s=0 "~T~") å€‹è©å½™ (term)ï¼Œæˆ‘å€‘å°±å¯ä»¥å°‡æ–‡ç« è½‰æ›æˆä»¥ä¸‹é¡å‹çš„çŸ©é™£ï¼Œå…¶ä¸­ç¬¬ä¸€æ¬„ç¬¬ä¸€åˆ—çš„ã€Œ12ã€ä»£è¡¨çš„æ˜¯ã€Œæ–‡ä»¶ 1ã€ ä¸­å‡ºç¾äº†12å€‹ã€Œæ–‡å­— 1ã€ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å¯ä»¥ç”¨ ![[12,~0,~3,~\cdots,~2]](https://s0.wp.com/latex.php?latex=%5B12%2C%7E0%2C%7E3%2C%7E%5Ccdots%2C%7E2%5D&bg=ffffff&fg=000&s=0 "[12,~0,~3,~\cdots,~2]") é€™å€‹å‘é‡ä¾†ä»£è¡¨ã€Œæ–‡ä»¶ 1ã€ï¼ŒåŒç†ä¹Ÿå¯ç”¨ã€Œæ–‡ä»¶ Dã€ä¹Ÿå¯ä»¥ç”¨ ![[0,~2,~8,~\cdots,~0]](https://s0.wp.com/latex.php?latex=%5B0%2C%7E2%2C%7E8%2C%7E%5Ccdots%2C%7E0%5D&bg=ffffff&fg=000&s=0 "[0,~2,~8,~\cdots,~0]") ä¾†è¡¨ç¤ºã€‚
    > 
    > ![åœ–ç‰‡1](https://taweihuang.hpd.io/wp-content/uploads/2017/03/åœ–ç‰‡1.png)
    > 
    > é€™æ¨£çš„æ–¹æ³•å°±æ˜¯ã€ŒBoW (Bag of Word)æ¼”ç®—æ³•ã€ï¼Œé€™ç¨®æ–¹æ³•é›–ç„¶å¾ˆç°¡å–®ï¼Œä½†æœ‰2å€‹ä¸»è¦çš„å•é¡Œ â”€ ä¸€æ˜¯æ¯ç¯‡æ–‡ç« çš„ç¸½å­—æ•¸ä¸ä¸€æ¨£ï¼Œæ¯”å¦‚èªªæ–‡å­— 2åœ¨æ–‡ä»¶ 2ä¸­å‡ºç¾9æ¬¡ï¼Œåœ¨æ–‡ä»¶ Dä¸­å»åªå‡ºç¾2æ¬¡ï¼Œé€™æ¨£æ˜¯å¦ä»£è¡¨æ–‡å­— 2 å°æ–‡ä»¶ 2 æ¯”è¼ƒé‡è¦ï¼Œå°æ–‡ä»¶ D æ¯”è¼ƒä¸é‡è¦å‘¢ï¼Ÿç­”æ¡ˆæ˜¯å¦å®šçš„ï¼Œèªªä¸å®šæ–‡ä»¶2æœ‰10000å€‹å­—ï¼Œè€Œæ–‡ä»¶Dåªæœ‰50å€‹å­—ï¼Œå¦‚æ­¤ä¸€ä¾†æ–‡å­—2æ‡‰è©²å°æ–‡ä»¶Dæ¯”è¼ƒé‡è¦æ‰å°ã€‚
    > 
    > å¦ä¸€å€‹å•é¡Œæ˜¯ï¼Œæ™‚å¸¸é‡è¤‡å‡ºç¾çš„æ…£ç”¨è©å½™å°ä¸€å€‹æ–‡ä»¶çš„å½±éŸ¿å¾ˆå¤§ã€‚æ¯”å¦‚èªªï¼Œä¸Šåœ–ä¸­çš„æ–‡å­— 3åœ¨æ¯å€‹æ–‡ä»¶ä¸­éƒ½å‡ºç¾å¥½å¤šæ¬¡ï¼Œå¯èƒ½æ˜¯ã€Œtheã€ä¹‹é¡çš„å¸¸ç”¨å­—ï¼Œå¦‚æ­¤ä¸€ä¾†ã€Œæ–‡ä»¶ Dã€çš„å‘é‡å°±æœƒè¢«  the é€™å€‹å­—æ‰€ä¸»å°ï¼Œä½† the é€™å€‹å­—å…¶å¯¦æ²’ä»€éº¼ç‰¹åˆ¥çš„æ„ç¾©ã€‚
    > 
    > ç‚ºäº†è™•ç†ä»¥ä¸Šå…©å€‹å•é¡Œï¼Œæ­·å²æ‚ ä¹…ä½†éå¸¸å¥½ç”¨çš„ TF-IDFæ¼”ç®—æ³•å°±è¢«ç™¼æ˜å‡ºä¾†äº†ã€‚
    > 
    > ### TF-IDF æ¼”ç®—æ³•
    > 
    > TF-IDF æ¼”ç®—æ³•åŒ…å«äº†å…©å€‹éƒ¨åˆ†ï¼š**è©é »**ï¼ˆterm frequencyï¼ŒTFï¼‰è·Ÿ**é€†å‘æ–‡ä»¶é »ç‡**ï¼ˆinverse document frequencyï¼ŒIDFï¼‰ã€‚è©é »æŒ‡çš„æ˜¯æŸä¸€å€‹çµ¦å®šçš„è©èªåœ¨è©²æ–‡ä»¶ä¸­å‡ºç¾çš„é »ç‡ï¼Œç¬¬ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")å€‹è©å‡ºç¾åœ¨ç¬¬ ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") ç¯‡æ–‡ä»¶çš„é »ç‡è¨˜åš ![~tf_{t,d}~](https://s0.wp.com/latex.php?latex=%7Etf_%7Bt%2Cd%7D%7E&bg=ffffff&fg=000&s=0 "~tf_{t,d}~")ï¼Œèˆ‰ä¾‹ä¾†èªªï¼Œå¦‚æœæ–‡ä»¶ 1 ç¸½å…±æœ‰100å€‹å­—ï¼Œè€Œç¬¬ 1 å€‹å­—åœ¨æ–‡ä»¶ 1 å‡ºç¾çš„æ¬¡æ•¸æ˜¯12æ¬¡ï¼Œå› æ­¤![~tf_{1,1}=12/100~](https://s0.wp.com/latex.php?latex=%7Etf_%7B1%2C1%7D%3D12%2F100%7E&bg=ffffff&fg=000&s=0 "~tf_{1,1}=12/100~")ï¼Œå¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å°±å¯ä»¥é‡å°ä¸Šè¿°çš„ç¬¬ä¸€å€‹å•é¡Œé€²è¡Œä¿®æ­£ï¼Œä»¥é »ç‡è€Œä¸æ˜¯æ¬¡æ•¸ä¾†çœ‹å¾…æ–‡å­—çš„é‡è¦æ€§ï¼Œè®“æ–‡ç« èˆ‡æ–‡ç« ä¹‹é–“æ¯”è¼ƒæœ‰å¯æ¯”è¼ƒæ€§ã€‚
    > 
    > è€Œé€†å‘æ–‡ä»¶é »ç‡å‰‡æ˜¯ç”¨ä¾†è™•ç†å¸¸ç”¨å­—çš„å•é¡Œã€‚å‡è¨­è©å½™ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t") ç¸½å…±åœ¨ ![d_t](https://s0.wp.com/latex.php?latex=d_t&bg=ffffff&fg=000&s=0 "d_t") ç¯‡æ–‡ç« ä¸­å‡ºç¾éï¼Œå‰‡è©å½™ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t") çš„ IDF å®šç¾©æˆ ![~idf_t = \log\left(\frac{D}{d_t}\right)~](https://s0.wp.com/latex.php?latex=%7Eidf_t+%3D+%5Clog%5Cleft%28%5Cfrac%7BD%7D%7Bd_t%7D%5Cright%29%7E&bg=ffffff&fg=000&s=0 "~idf_t = \log\left(\frac{D}{d_t}\right)~")ã€‚æ¯”å¦‚èªªï¼Œå‡è¨­æ–‡å­— 1 ç¸½å…±å‡ºç¾åœ¨ 25 ç¯‡ä¸åŒçš„æ–‡ä»¶ï¼Œå‰‡ ![~~idf_1 = \log\left(\frac{D}{25}\right)~~](https://s0.wp.com/latex.php?latex=%7E%7Eidf_1+%3D+%5Clog%5Cleft%28%5Cfrac%7BD%7D%7B25%7D%5Cright%29%7E%7E&bg=ffffff&fg=000&s=0 "~~idf_1 = \log\left(\frac{D}{25}\right)~~")ã€‚å¦‚æœè©å½™ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")åœ¨éå¸¸å¤šç¯‡æ–‡ç« ä¸­éƒ½å‡ºç¾éï¼Œå°±ä»£è¡¨ ![~d_t~](https://s0.wp.com/latex.php?latex=%7Ed_t%7E&bg=ffffff&fg=000&s=0 "~d_t~") å¾ˆå¤§ï¼Œæ­¤æ™‚![~idf_t~](https://s0.wp.com/latex.php?latex=%7Eidf_t%7E&bg=ffffff&fg=000&s=0 "~idf_t~") å°±æœƒæ¯”è¼ƒå°ã€‚
    > 
    > è€Œä¸€å€‹å­—å°æ–¼ä¸€ç¯‡æ–‡ä»¶é‡è¦æ€§çš„åˆ†æ•¸ (score) å°±å¯ä»¥é€éTFèˆ‡IDFå…©å€‹æŒ‡æ¨™è¨ˆç®—å‡ºä¾†ï¼Œæˆ‘å€‘å°‡ç¬¬ ![t](https://s0.wp.com/latex.php?latex=t&bg=ffffff&fg=000&s=0 "t")å€‹è©å½™å°æ–¼ç¬¬ ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") ç¯‡æ–‡ä»¶çš„TF-IDFæ¬Šé‡å®šç¾©ç‚º ![~w_{t,d} =  tf_{t,d} \times  idf_t ~](https://s0.wp.com/latex.php?latex=%7Ew_%7Bt%2Cd%7D+%3D%C2%A0+tf_%7Bt%2Cd%7D+%5Ctimes+%C2%A0idf_t+%7E&bg=ffffff&fg=000&s=0 "~w_{t,d} =  tf_{t,d} \times  idf_t ~")ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œç•¶è©å½™ ![~t~](https://s0.wp.com/latex.php?latex=%7Et%7E&bg=ffffff&fg=000&s=0 "~t~") å¾ˆå¸¸å‡ºç¾åœ¨æ–‡ä»¶  ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") æ™‚ï¼Œä»–çš„ ![~tf_{t,d} ~](https://s0.wp.com/latex.php?latex=%7Etf_%7Bt%2Cd%7D+%7E&bg=ffffff&fg=000&s=0 "~tf_{t,d} ~") å°±æœƒæ¯”è¼ƒå¤§ï¼Œè€Œå¦‚æœè©å½™![~~ t](https://s0.wp.com/latex.php?latex=%7E%7Et&bg=ffffff&fg=000&s=0 "~~t") ä¹Ÿå¾ˆå°‘å‡ºç¾åœ¨å…¶ä»–ç¯‡æ–‡ç« ï¼Œå‰‡ ![~idf_t~](https://s0.wp.com/latex.php?latex=%7Eidf_t%7E&bg=ffffff&fg=000&s=0 "~idf_t~") ä¹Ÿæœƒæ¯”è¼ƒå¤§ï¼Œä½¿ ![~w_{t,d}~](https://s0.wp.com/latex.php?latex=%7Ew_%7Bt%2Cd%7D%7E&bg=ffffff&fg=000&s=0 "~w_{t,d}~")æ•´é«”ä¾†èªªæ¯”è¼ƒå¤§ï¼Œä¹Ÿå°±æ˜¯èªªè©å½™![~t~](https://s0.wp.com/latex.php?latex=%7Et%7E&bg=ffffff&fg=000&s=0 "~t~") å°æ–¼æ–‡ä»¶  ![~d~](https://s0.wp.com/latex.php?latex=%7Ed%7E&bg=ffffff&fg=000&s=0 "~d~") ä¾†èªªæ˜¯å¾ˆé‡è¦çš„ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œæˆ‘å€‘å°±å¯ä»¥è¨ˆç®—å‡º TF-IDF çŸ©é™£ï¼Œå¦‚ä¸‹åœ–æ‰€ç¤ºã€‚
    > 
    > ![åœ–ç‰‡1](https://taweihuang.hpd.io/wp-content/uploads/2017/03/åœ–ç‰‡1-1.png)
    > 
    > å¦ä¸€æ–¹é¢ï¼Œ TF-IDF æ™‚å¸¸è¢«ç”¨ä¾†ä½œè³‡è¨Šæª¢ç´¢ (information retrieval)ï¼Œæ¯”å¦‚èªªï¼Œçµ¦å®šä¸€ä¸²æŒ‡ä»¤ (query)ã€Œæ–‡å­— 1 + æ–‡å­— 3 + ä¸åœ¨ç¾æœ‰è©å½™è£¡é¢çš„æ–‡å­—ã€ï¼Œå‰‡é€™ä¸²æŒ‡ä»¤è·Ÿç¬¬ ![d](https://s0.wp.com/latex.php?latex=d&bg=ffffff&fg=000&s=0 "d") çš„é—œè¯åˆ†æ•¸å°±å¯ä»¥å®šç¾©æ–‡ ![tf_{1,d} + tf_{3,d} ](https://s0.wp.com/latex.php?latex=tf_%7B1%2Cd%7D+%2B+tf_%7B3%2Cd%7D%C2%A0&bg=ffffff&fg=000&s=0 "tf_{1,d} + tf_{3,d} ")ã€‚
    > 
    > ### å¤§é¼»æ˜¯æ€éº¼ç”¨ TF-IDFï¼Ÿ
    > 
    > TF-IDF å¸¸è¢«æˆ‘ç”¨åœ¨3å€‹åœ°æ–¹ï¼Œä¸€å€‹æ˜¯ä½œç‚º baseline modelçš„ç‰¹å¾µ (feature)ï¼Œæ¯”å¦‚èªªä½œæ–‡ä»¶åˆ†é¡ (text classification) æ™‚ï¼Œæˆ‘å°±æœƒæŠŠ tf è·Ÿ idf éƒ½ç•¶ä½œæ–‡ä»¶çš„ç‰¹å¾µ(æ‰€ä»¥ä¸€ç¯‡æ–‡ç« ç¸½å¤ æœƒæœ‰ ![2T](https://s0.wp.com/latex.php?latex=2T&bg=ffffff&fg=000&s=0 "2T")å€‹ç‰¹å¾µ)ï¼Œå»è·‘åˆ†é¡æ¨¡å‹ï¼Œä½œç‚º baselineã€‚æœ‰æ™‚å€™æˆ‘æœƒæŠŠä¸€å€‹è©å½™å°æ–¼æ¯ç¯‡çš„æ–‡ç« çš„ tf-idf å€¼ç•¶ä½œè©²è©å½™çš„ç‰¹å¾µï¼Œå»è·‘æ–‡å­—çš„åˆ†ç¾¤ã€‚é‚„æœ‰ä¸€å€‹æ˜¯å¤§é¼»å¥½å‹ã„ã„“å‘Šè¨´æˆ‘çš„å¦™æ‹›ï¼Œå°±æ˜¯å¦‚æœæˆ‘å€‘æƒ³è¦ç”¨word2vecå¾—å‡ºä¾†çš„è©å‘é‡å»è¡¨ç¤ºä¸€å€‹æ–‡ä»¶çš„è©±ï¼Œå¯ä»¥ç”¨tf-idfçš„å€¼ç•¶ä½œæ¬Šé‡ï¼Œå†æŠŠè©²æ–‡ä»¶ä¸­æ¯å€‹è©å‘é‡ç”¨tf-idfç•¶ä½œæ¬Šé‡åŠ èµ·ä¾†ï¼Œæ•ˆæœä¹Ÿå¾ˆä¸éŒ¯å–”ï¼






### Doc2Vec

#### åŸç†

- [æ·±åº¦å­¦ä¹ ç¬”è®°â€”â€”Word2vecå’ŒDoc2vecåŸç†ç†è§£å¹¶ç»“åˆä»£ç åˆ†æ - CSDNåšå®¢](https://blog.csdn.net/mpk_no1/article/details/72458003)

- [A gentle introduction to Doc2Vec â€“ ScaleAbout â€“ Medium](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)

- [How does doc2vec represent feature vector of a document? Can anyone explain mathematically how the process is done? - Quora](https://www.quora.com/How-does-doc2vec-represent-feature-vector-of-a-document-Can-anyone-explain-mathematically-how-the-process-is-done)

    > Doc2Vec explores the above observation by adding additional input nodes representing documents as additional context. Each additional node can be thought of just as an id for each input document.
    > 
    > ![](https://qph.ec.quoracdn.net/main-qimg-3449ced3d53f653ebfd9b73db1065056)
    > 
    > D represent the features representing the document context and W
    > 
    > represent the word context in a window surrounding the target word. Training is similar to word2vec, with additional document context. The objective of doc2vec learning is
    > 
    > maxâˆ‘âˆ€(tar,con,doc)logP
    > 
    > (target word|context words, document context)
    > 
    > At the end of the training process, you will have word embeddings, W
    > 
    > and document embedding D
    > 
    > for documents in the training corpus.
    > 
    > Great! We got document embeddings for input corpus but what about new unseen documents that appear in the test set. The idea is to learn their representation at test time by solving an optimization problem for inference.
    > 
    > ![](https://qph.ec.quoracdn.net/main-qimg-0112c98875541c958abee17d10fccdaa)
    > 
    > The optimization problem is not any different from the training problem. maxâˆ‘âˆ€(tar,con)logP
    > 
    > (target word|context words, document = test doc). One may however, choose to keep W and Wâ€² fixed and learn variable D as document embedding.

- Piyush Bhardwaj **Understanding Word2Vec and Document2Vec.** *Unfinished Notes* [[pdf]](https://piyushbhardwaj.github.io/documents/w2v_p2vupdates.pdf)


#### å¯¦ä½œ

- [åŸºäºgensimçš„Doc2Vecç®€æ - CSDNåšå®¢](https://blog.csdn.net/lenbow/article/details/52120230)

- [åŸºäºjiebaå’Œdoc2vecçš„ä¸­æ–‡æƒ…æ„Ÿè¯­æ–™åˆ†ç±» - ä¸ªäººæ–‡ç«  - SegmentFault æ€å¦](https://segmentfault.com/a/1190000012203525)

- [ç”¨ Doc2Vec å¾—åˆ°æ–‡æ¡£ï¼æ®µè½ï¼å¥å­çš„å‘é‡è¡¨è¾¾ - ç®€ä¹¦](https://www.jianshu.com/p/854a59b93e09)



- [jhlau/doc2vec: Python scripts for training/testing paragraph vectors](https://github.com/jhlau/doc2vec)

- [RaRe-Technologies/gensim: Topic Modelling for Humans](https://github.com/RaRe-Technologies/gensim)
    - [gensim/doc2vec-lee.ipynb at develop Â· RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)
    - [gensim/doc2vec-IMDB.ipynb at develop Â· RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)
    - [gensim/doc2vec-wikipedia.ipynb at develop Â· RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb)

- [abtpst/Doc2Vec: Doc2Vec algorithm for solving moview review sentiment analysis](https://github.com/abtpst/Doc2Vec)

- [hiyijian/doc2vec: C++ implement of Tomas Mikolov's word/document embedding](https://github.com/hiyijian/doc2vec)

- [fbkarsdorp/doc2vec: Tutorial and review of word2vec / doc2vec](https://github.com/fbkarsdorp/doc2vec)

- [ibrahimsharaf/doc2vec: Text classification using Doc2Vec & Random Forest](https://github.com/ibrahimsharaf/doc2vec)

- [Doc2vec tutorial | RARE Technologies](https://rare-technologies.com/doc2vec-tutorial/)

- [How Quid uses deep learning with small data](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)

- [[1405.4053] Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)


- [æƒ…æ„Ÿåˆ†æçš„æ–°æ–¹æ³•â€”â€”åŸºäºWord2Vec/Doc2Vec/Python - OPEN å¼€å‘ç»éªŒåº“](http://www.open-open.com/lib/view/open1444351655682.html)

- [doc2vecè®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦ - ç¨‹åºå›­](http://www.voidcn.com/article/p-fiwnobtj-dx.html)

- [Doc2Vec tutorial using Gensim â€“ Andreas Klintberg â€“ Medium](https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1)

- [How does doc2vec represent feature vector of a document? Can anyone explain mathematically how the process is done? - Quora](https://www.quora.com/How-does-doc2vec-represent-feature-vector-of-a-document-Can-anyone-explain-mathematically-how-the-process-is-done)

- [jhlau/doc2vec: Python scripts for training/testing paragraph vectors](https://github.com/jhlau/doc2vec)


### TreeRNN(TreeLSTM)

- [[1503.00075] Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](https://arxiv.org/abs/1503.00075)


- [åŸºäºTreeLSTMçš„æƒ…æ„Ÿåˆ†æ](https://zhuanlan.zhihu.com/p/35252733)



### Tree-Based CNN

- [[1409.5718] Convolutional Neural Networks over Tree Structures for Programming Language Processing](https://arxiv.org/abs/1409.5718)

- [[1504.01106] Discriminative Neural Sentence Modeling by Tree-Based Convolution](https://arxiv.org/abs/1504.01106)

- [[1512.08422] Natural Language Inference by Tree-Based Convolution and Heuristic Matching](https://arxiv.org/abs/1512.08422)



### Neural Bag-of-Ngrams

- [Neural Bag-of-Ngrams - Association for the Advancement of Artificial ...](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14513/14079)

- [libofang/Neural-BoN: code for AAAI-17 paper "Neural Bag-of-Ngrams"](https://github.com/libofang/Neural-BoN)

- [Neural Bag-of-Ngrams - CSDNåšå®¢](https://blog.csdn.net/qq_22548549/article/details/78213004)

    > è¯¥æ–‡ä¸€å…±æå‡ºäº†ä¸‰ç§ngramå‘é‡åˆ›å»ºçš„æ–¹æ³•ã€‚åˆ†åˆ«æ˜¯Context guided N-gram representation, Text Guided N-gram representationä»¥åŠLabel guided N-gram representation. å…¶ä¸­å‰ä¸¤ä¸ªæ¨¡å‹éƒ½æ˜¯æ— ç›‘ç£çš„ï¼Œæœ€åä¸€ä¸ªæ¨¡å‹æ˜¯ç›‘ç£ç®—æ³•ï¼Œéœ€è¦å¯¹æ–‡æœ¬æ‰“æ ‡ç­¾ã€‚\
    > ![](https://img-blog.csdn.net/20171012123832474?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
    > 
    > Context guided N-gram representation
    > ------------------------------------
    > 
    > Context guided N-gram representationç®€ç§°CGNRï¼Œå€Ÿé‰´çš„æ˜¯Skip-gramæ¨¡å‹ç”Ÿæˆè¯å‘é‡çš„æ–¹æ³•ã€‚CGNRç”¨å½“å‰ngramå‘é‡é¢„æµ‹å…¶ä¸Šä¸‹æ–‡ä¸­çš„ngramå‘é‡ã€‚å…¶å½¢å¼åŒ–è¡¨ç¤ºå¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012124622467?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­gè¡¨ç¤ºå…¶æ˜¯ç¬¬iç¯‡æ–‡ç« ç¬¬jä¸ªngramï¼Œvgæ˜¯å…¶å¯¹åº”çš„å‘é‡ï¼Œcè¡¨ç¤ºgçš„ç¬¬qä¸ªä¸Šä¸‹æ–‡ä¸­çš„ngramã€‚å…¶ä¸­ä¸Šä¸‹æ–‡é›†åˆè¡¨ç¤ºå¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012124917166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­winæ˜¯ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°ï¼Œmæ˜¯æœ€å¤§çš„gramå€¼ã€‚ä¸Šä¸‹æ–‡é›†åˆåˆ›å»ºçš„ä¾‹å­å¦‚ä¸‹å›¾æ‰€ç¤º\
    > ![](https://img-blog.csdn.net/20171012125347703?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­ç»¿è‰²è¡¨ç¤ºç›®æ ‡gramï¼Œè“è‰²è¡¨ç¤ºè¯¥gramçš„ä¸Šä¸‹æ–‡é›†åˆï¼Œgramæœ€å¤§å€¼æ˜¯3ï¼Œå³åŒ…å«1gram, 2gramå’Œ3gramã€‚\
    > CGNRä¾ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå› ä¸ºæœ‰ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„è¯å®ƒä»¬çš„è¯ä¹‰ä¸ä¸€å®šç›¸ä¼¼ï¼Œå¯èƒ½åˆšå¥½ç›¸åï¼Œå¦‚è¯"good"å’Œ"bad"ã€‚æ‰€ä»¥ä½œè€…åˆæå‡ºäº†Text Guided N-gram representationä»¥åŠLabel guided N-gram representation.
    > 
    > Text Guided N-gram representation
    > ---------------------------------
    > 
    > ä¸€èˆ¬è€Œè¨€ï¼ŒåŒä¸€ç¯‡æ–‡ç« é‡Œå‡ºç°çš„è¯ï¼Œä»–ä»¬çš„è¯ä¹‰ä¹Ÿæ˜¯ç›¸ä¼¼çš„ï¼Œå°¤å…¶æ˜¯æœ‰å…³è¯„è®ºçš„æ–‡ç« é‡Œï¼Œç»å¸¸ä¼šå‡ºç°"good", "not bad"ç­‰è¯ã€‚ä»–ä»¬æ‰€è¡¨è¾¾çš„è¯ä¹‰æ˜¯ä¸€æ ·çš„ã€‚å› æ­¤ï¼Œè¯¥æ–‡æå‡ºè¦ç”¨ngramå‘é‡é¢„æµ‹è¯¥ngramæ‰€å±çš„æ–‡æœ¬ï¼Œå½¢å¼åŒ–è¡¨è¿°å¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012130421982?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > å…¶ä¸­tiè¡¨ç¤ºç¬¬iç¯‡æ–‡æ¡£ã€‚TGNRæ¨¡å‹é€‚åˆäºé•¿æ–‡æœ¬ï¼Œæ¯”å¦‚åœ¨ä¸€ç¯‡æ¶ˆæçš„ç”µå½±å½±è¯„ä¸­ï¼Œå¯èƒ½ä¼šåŒæ—¶å‡ºç°"terrible", "waste of time"ç­‰è¯ï¼ŒTGNRæ¨¡å‹èƒ½å°†è¿™äº›è¯èšé›†åœ¨ä¸€èµ·ï¼Œè€Œå¯¹äºçŸ­æ–‡æœ¬ï¼Œå¯èƒ½å¹¶ä¸ä¼šå‡ºç°è¿™ä¹ˆå¤šè¯ã€‚ç”±æ­¤ï¼Œè¯¥æ–‡è¿›ä¸€æ­¥æå‡ºäº†Label guided N-gram representationæ¨¡å‹ã€‚
    > 
    > Label guided N-gram representation
    > ----------------------------------
    > 
    > å½“è¯"good"å’Œ"perfect"å‡ºç°åœ¨ä¸åŒçš„æ–‡æ¡£é‡Œæ˜¯ï¼ŒCGNRå’ŒTGNRå¯èƒ½éƒ½ä¸èƒ½è¾¾åˆ°è¾ƒå¥½çš„æ•ˆæœã€‚ä½†æ˜¯å½“ä¸¤ç¯‡æ–‡æ¡£å½’å±äºåŒä¸€ä¸ªæ ‡ç­¾æ—¶ï¼Œå…¶ä¸­çš„è¯å¯èƒ½å…·æœ‰ç›¸ä¼¼çš„è¯­ä¹‰ã€‚ç”±æ­¤è¯¥æ–‡æå‡ºäº†Label guided N-gram representationæ–¹æ³•ï¼Œç”¨å½“å‰ngramé¢„æµ‹å…¶æ‰€å±çš„æ–‡æ¡£çš„æ ‡ç­¾ï¼Œå½¢å¼åŒ–è¡¨ç¤ºå¦‚ä¸‹\
    > ![](https://img-blog.csdn.net/20171012131351485?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjI1NDg1NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\
    > ç›¸å¯¹è€Œè¨€ï¼ŒLGNRæ›´é€‚åˆçŸ­æ–‡æœ¬ã€‚




### Universal Sentence Encoder

- [é€šç”¨å¥å­èªç¾©ç·¨ç¢¼å™¨ï¼Œè°·æ­Œåœ¨èªç¾©æ–‡æœ¬ç›¸ä¼¼æ€§ä¸Šçš„æ¢ç´¢ - å¹«è¶£](http://bangqu.com/gY7194.html)

    > **èªç¾©æ–‡æœ¬ç›¸ä¼¼åº¦**
    > 
    > åœ¨ã€ŒLearning Semantic Textual Similarity from Conversationsã€é€™ç¯‡è«–æ–‡ä¸­ï¼Œæˆ‘å€‘å¼•å…¥ä¸€ç¨®æ–°çš„æ–¹å¼ä¾†å­¸ç¿’èªç¾©æ–‡æœ¬ç›¸ä¼¼çš„å¥å­è¡¨ç¤ºã€‚ç›´è§€çš„èªªï¼Œå¦‚æœå¥å­çš„å›ç­”åˆ†ä½ˆç›¸ä¼¼ï¼Œå‰‡å®ƒå€‘åœ¨èªç¾©ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚ä¾‹å¦‚ï¼Œã€Œä½ å¤šå¤§äº†ï¼Ÿã€ä»¥åŠã€Œä½ çš„å¹´é½¡æ˜¯å¤šå°‘ï¼Ÿã€éƒ½æ˜¯é—œæ–¼å¹´é½¡çš„å•é¡Œï¼Œå¯ä»¥é€šéé¡ä¼¼çš„å›ç­”ï¼Œä¾‹å¦‚ã€Œæˆ‘ 20 æ­²ã€ä¾†å›ç­”ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé›–ç„¶ã€Œä½ å¥½å—ï¼Ÿã€å’Œã€Œä½ å¤šå¤§äº†ï¼Ÿã€åŒ…å«çš„å–®è©å¹¾ä¹ç›¸åŒï¼Œä½†å®ƒå€‘çš„å«ç¾©å»å¤§ç›¸å¾‘åº­ï¼Œæ‰€ä»¥å°æ‡‰çš„å›ç­”ä¹Ÿç›¸å»ç”šé ã€‚
    > 
    > è«–æ–‡åœ°å€ï¼šhttps://arxiv.org/abs/1804.07754
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY719415273972350175UWgr.png)
    > 
    > *å¦‚æœå¥å­å¯ä»¥é€šéç›¸åŒçš„ç­”æ¡ˆä¾†å›ç­”ï¼Œé‚£éº¼å¥å­åœ¨èªç¾©ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚å¦å‰‡ï¼Œå®ƒå€‘åœ¨èªç¾©ä¸Šæ˜¯ä¸åŒçš„ã€‚*
    > 
    > é€™é …å·¥ä½œä¸­ï¼Œæˆ‘å€‘å¸Œæœ›é€šéçµ¦å›ç­”åˆ†é¡çš„æ–¹å¼å­¸ç¿’èªç¾©ç›¸ä¼¼æ€§ï¼šçµ¦å®šä¸€å€‹å°è©±è¼¸å…¥ï¼Œæˆ‘å€‘å¸Œæœ›å¾ä¸€æ‰¹éš¨æ©Ÿé¸æ“‡çš„å›è¦†ä¸­åˆ†é¡å¾—åˆ°æ­£ç¢ºçš„ç­”æ¡ˆã€‚ä½†æ˜¯ï¼Œä»»å‹™çš„æœ€çµ‚ç›®æ¨™æ˜¯å­¸ç¿’ä¸€å€‹å¯ä»¥è¿”å›è¡¨ç¤ºå„ç¨®è‡ªç„¶èªè¨€é—œä¿‚ï¼ˆåŒ…æ‹¬ç›¸ä¼¼æ€§å’Œç›¸é—œæ€§ï¼‰çš„ç·¨ç¢¼æ¨¡å‹ã€‚æˆ‘å€‘æå‡ºäº†å¦ä¸€é æ¸¬ä»»å‹™ï¼ˆæ­¤è™•æ˜¯æŒ‡ SNLI è˜Šå«æ•¸æ“šé›†ï¼‰ï¼Œä¸¦é€šéå…±äº«çš„ç·¨ç¢¼å±¤åŒæ™‚æ¨é€²å…©é …ä»»å‹™ã€‚åˆ©ç”¨é€™ç¨®æ–¹å¼ï¼Œæˆ‘å€‘åœ¨ STSBenchmark å’Œ CQA task B ç­‰ç›¸ä¼¼åº¦åº¦é‡æ¨™æº–ä¸Šå–å¾—äº†æ›´å¥½çš„è¡¨ç¾ï¼Œç©¶å…¶åŸå› ï¼Œæ˜¯ç°¡å–®ç­‰åƒ¹é—œä¿‚èˆ‡é‚è¼¯è˜Šå«ä¹‹é–“å­˜åœ¨å·¨å¤§ä¸åŒï¼Œå¾Œè€…çˆ²å­¸ç¿’è¤‡é›œèªç¾©è¡¨ç¤ºæä¾›äº†æ›´å¤šå¯ä¾›ä½¿ç”¨çš„ä¿¡æ¯ã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397235681nLm72.png)
    > 
    > *å°æ–¼çµ¦å®šçš„è¼¸å…¥ï¼Œåˆ†é¡å¯ä»¥èªçˆ²æ˜¯ä¸€ç¨®å°æ‰€æœ‰å¯èƒ½å€™é¸ç­”æ¡ˆçš„æ’åºå•é¡Œã€‚*
    > 
    > **é€šç”¨å¥å­ç·¨ç¢¼å™¨**
    > 
    > ã€ŒUniversal Sentence Encoderã€é€™ç¯‡è«–æ–‡ä»‹ç´¹äº†ä¸€ç¨®æ¨¡å‹ï¼Œå®ƒé€šéå¢åŠ æ›´å¤šä»»å‹™ä¾†æ“´å±•ä¸Šè¿°çš„å¤šä»»å‹™è¨“ç·´ï¼Œä¸¦èˆ‡ä¸€å€‹é¡ä¼¼ skip-thought çš„æ¨¡å‹è¯åˆè¨“ç·´ï¼Œå¾è€Œåœ¨çµ¦å®šæ–‡æœ¬ç‰‡æ®µä¸‹é æ¸¬å¥å­ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œæˆ‘å€‘ä¸ä½¿ç”¨åŸ skip-thought æ¨¡å‹ä¸­çš„ç·¨ç¢¼å™¨ - è§£ç¢¼å™¨æ¶æ§‹ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç¨®åªæœ‰ç·¨ç¢¼å™¨çš„æ¨¡å‹ï¼Œä¸¦é€šéå…±äº«ç·¨ç¢¼å™¨ä¾†æ¨é€²é æ¸¬ä»»å‹™ã€‚åˆ©ç”¨é€™ç¨®æ–¹å¼ï¼Œæ¨¡å‹è¨“ç·´æ™‚é–“å¤§å¤§æ¸›å°‘ï¼ŒåŒæ™‚é‚„èƒ½ä¿è­‰å„é¡é·ç§»å­¸ç¿’ä»»å‹™ï¼ˆåŒ…æ‹¬æƒ…æ„Ÿå’Œèªç¾©ç›¸ä¼¼åº¦åˆ†é¡ï¼‰çš„æ€§èƒ½ã€‚é€™ç¨®æ¨¡å‹çš„ç›®çš„æ˜¯çˆ²å„˜å¯èƒ½å¤šçš„æ‡‰ç”¨ï¼ˆé‡‹ç¾©æª¢æ¸¬ã€ç›¸é—œæ€§ã€èšé¡å’Œè‡ªå®šç¾©æ–‡æœ¬åˆ†é¡ï¼‰æä¾›ä¸€ç¨®é€šç”¨çš„ç·¨ç¢¼å™¨ã€‚
    > 
    > è«–æ–‡åœ°å€ï¼šhttps://arxiv.org/abs/1803.11175
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397237211r96N9.png)
    > 
    > *æˆå°èªç¾©ç›¸ä¼¼æ€§æ¯”è¼ƒï¼Œçµæœçˆ² TensorFlow Hub é€šç”¨å¥å­ç·¨ç¢¼å™¨æ¨¡å‹çš„è¼¸å‡ºã€‚*
    > 
    > æ­£å¦‚æ–‡ä¸­æ‰€èªªï¼Œé€šç”¨å¥å­ç·¨ç¢¼å™¨æ¨¡å‹çš„ä¸€å€‹è®Šé«”ä½¿ç”¨äº†æ·±åº¦å¹³å‡ç¶²çµ¡ï¼ˆDANï¼‰ç·¨ç¢¼å™¨ï¼Œè€Œå¦ä¸€å€‹è®Šé«”ä½¿ç”¨äº†æ›´åŠ è¤‡é›œçš„è‡ªæ³¨æ„åŠ›ç¶²çµ¡æ¶æ§‹ Transformerã€‚
    > 
    > ![](http://i2.bangqu.com/j/news/20180527/gY71941527397238079233wo.png)
    > 
    > *ã€ŒUniversal Sentence Encoderã€ä¸€æ–‡ä¸­æåˆ°çš„å¤šä»»å‹™è¨“ç·´ã€‚å„é¡ä»»å‹™åŠçµæ§‹é€šéå…±äº«çš„ç·¨ç¢¼å±¤/åƒæ•¸ï¼ˆç°è‰²æ¡†ï¼‰é€²è¡Œé€£æ¥ã€‚*
    > 
    > éš¨ç€å…¶é«”ç³»çµæ§‹çš„è¤‡é›œåŒ–ï¼ŒTransformer æ¨¡å‹åœ¨å„ç¨®æƒ…æ„Ÿå’Œç›¸ä¼¼åº¦åˆ†é¡ä»»å‹™ä¸Šçš„è¡¨ç¾éƒ½å„ªæ–¼ç°¡å–®çš„ DAN æ¨¡å‹ï¼Œä¸”åœ¨è™•ç†çŸ­å¥å­æ™‚åªç¨æ…¢ä¸€äº›ã€‚ç„¶è€Œï¼Œéš¨ç€å¥å­é•·åº¦çš„å¢åŠ ï¼Œä½¿ç”¨ Transformer çš„è¨ˆç®—æ™‚é–“æ˜é¡¯å¢åŠ ï¼Œä½†æ˜¯ DAN æ¨¡å‹çš„è¨ˆç®—è€—æ™‚å»å¹¾ä¹ä¿æŒä¸è®Šã€‚
    > 
    > **æ–°æ¨¡å‹**
    > 
    > é™¤äº†ä¸Šè¿°çš„é€šç”¨å¥å­ç·¨ç¢¼å™¨æ¨¡å‹ä¹‹å¤–ï¼Œæˆ‘å€‘é‚„åœ¨ TensorFlow Hub ä¸Šå…±äº«äº†å…©å€‹æ–°æ¨¡å‹ï¼šå¤§å‹é€šç”¨å¥å‹ç·¨ç¢¼å™¨é€šå’Œç²¾ç°¡ç‰ˆé€šç”¨å¥å‹ç·¨ç¢¼å™¨ã€‚
    > 
    > -   å¤§å‹ï¼šhttps://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-large/1
    > 
    > -   ç²¾ç°¡ï¼šhttps://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-lite/1
    > 
    >  é€™äº›éƒ½æ˜¯é è¨“ç·´å¥½çš„ Tensorflow æ¨¡å‹ï¼Œçµ¦å®šé•·åº¦ä¸å®šçš„æ–‡æœ¬è¼¸å…¥ï¼Œè¿”å›ä¸€å€‹èªç¾©ç·¨ç¢¼ã€‚é€™äº›ç·¨ç¢¼å¯ç”¨æ–¼èªç¾©ç›¸ä¼¼æ€§åº¦é‡ã€ç›¸é—œæ€§åº¦é‡ã€åˆ†é¡æˆ–è‡ªç„¶èªè¨€æ–‡æœ¬çš„èšé¡ã€‚
    > 
    > å¤§å‹é€šç”¨å¥å‹ç·¨ç¢¼å™¨æ¨¡å‹æ˜¯ç”¨æˆ‘å€‘ä»‹ç´¹çš„ç¬¬äºŒç¯‡æ–‡ç« ä¸­æåˆ°çš„ Transformer ç·¨ç¢¼å™¨è¨“ç·´çš„ã€‚å®ƒé‡å°éœ€è¦é«˜ç²¾åº¦èªç¾©è¡¨ç¤ºçš„å ´æ™¯ï¼ŒçŠ§ç‰²äº†é€Ÿåº¦å’Œé«”ç©ä¾†ç²å¾—æœ€ä½³çš„æ€§èƒ½ã€‚
    > 
    > ç²¾ç°¡ç‰ˆæ¨¡å‹ä½¿ç”¨ Sentence Piece è©å½™åº«è€Œéå–®è©é€²è¡Œè¨“ç·´ï¼Œé€™ä½¿å¾—æ¨¡å‹å¤§å°é¡¯è‘—æ¸›å°ã€‚å®ƒé‡å°å…§å­˜å’Œ CPU ç­‰è³‡æºæœ‰é™çš„å ´æ™¯ï¼ˆå¦‚å°å‹è¨­å‚™æˆ–ç€è¦½å™¨ï¼‰ã€‚
    > 
    > æˆ‘å€‘å¾ˆé«˜èˆˆèˆ‡å¤§å®¶åˆ†äº«é€™é …ç ”ç©¶ä»¥åŠé€™äº›æ¨¡å‹ã€‚é€™åªæ˜¯ä¸€å€‹é–‹å§‹ï¼Œä¸¦ä¸”ä»ç„¶é‚„æœ‰å¾ˆå¤šå•é¡ŒäºŸå¾…è§£æ±ºï¼Œå¦‚å°‡æŠ€è¡“æ“´å±•åˆ°æ›´å¤šèªè¨€ä¸Šï¼ˆä¸Šè¿°æ¨¡å‹ç›®å‰åƒ…æ”¯æŒè‹±èªï¼‰ã€‚æˆ‘å€‘ä¹Ÿå¸Œæœ›é€²ä¸€æ­¥åœ°é–‹ç™¼é€™ç¨®æŠ€è¡“ï¼Œä½¿å…¶èƒ½å¤ ç†è§£æ®µè½ç”šè‡³æ•´å€‹æ–‡æª”ã€‚åœ¨å¯¦ç¾é€™äº›ç›®æ¨™çš„éç¨‹ä¸­ï¼Œå¾ˆæœ‰å¯èƒ½æœƒç”¢ç”Ÿå‡ºçœŸæ­£çš„ã€Œé€šç”¨ã€ç·¨ç¢¼å™¨ã€‚
    > 
    > *åŸæ–‡éˆæ¥ï¼šhttps://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html*


- [ã€ŠUniversal Sentence Encoderã€‹è®ºæ–‡åˆ†äº«](https://zhuanlan.zhihu.com/p/35174235)

    > è®ºæ–‡ä¸»è¦æ˜¯æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¥å­ç¼–ç æ¡†æ¶ï¼Œå¥å­çº§åˆ«çš„encodeæ¯”Word2vecä½¿ç”¨èµ·æ¥ä¼šæ›´åŠ çš„æ–¹ä¾¿ï¼Œå› ä¸ºå¯ä»¥ç›´æ¥æ‹¿æ¥åšå¥å­åˆ†ç±»ç­‰ä»»åŠ¡ã€‚
    > 
    > æœ¬æ–‡ä¸»è¦æå‡ºäº†ä¸¤ä¸ªå¥å­encodeçš„æ¡†æ¶ï¼Œä¸€ä¸ªæ˜¯ä¹‹å‰ã€Šattention is all you needã€‹é‡Œé¢çš„ä¸€ä¸ªencodeæ¡†æ¶ï¼Œå¦ä¸€ä¸ªæ˜¯DANï¼ˆdeep average networkï¼‰çš„encodeæ–¹å¼ã€‚ä¸¤ä¸ªçš„è®­ç»ƒæ–¹å¼è¾ƒä¸ºç±»ä¼¼ï¼Œéƒ½æ˜¯é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ï¼Œå°†encodeç”¨åœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬åˆ†ç±»ï¼Œç”Ÿæˆç­‰ç­‰ï¼Œä»¥ä¸åŒçš„ä»»åŠ¡æ¥è®­ç»ƒä¸€ä¸ªç¼–ç å™¨ï¼Œä»è€Œå®ç°æ³›åŒ–çš„ç›®çš„ã€‚
    > 
    > Transformerï¼š
    > 
    > ç¬¬ä¸€ä¸ªå¥å­ç¼–ç æ¨¡å‹ä½¿ç”¨äº†transformeré‡Œé¢çš„ä¸€ä¸ªå­å›¾ï¼Œè¿™ä¸ªå­å›¾é€šè¿‡attentionæ¥ä¸€ä¸ªå¥å­ä¸­è¯è¯­çš„contextè¡¨ç¤ºï¼ŒåŒæ—¶è€ƒè™‘æ‰€æœ‰çš„è¯å¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªfixed lengthå‘é‡ã€‚å…·ä½“äº†è§£å¯ä»¥å»çœ‹é‚£ç¯‡è®ºæ–‡ï¼Œè¿™ç¯‡è®ºæ–‡æœ¬èº«ä¹Ÿæ²¡æœ‰ç»™å‡ºå…·ä½“çš„å›¾ã€‚
    > 
    > ![](https://pic4.zhimg.com/80/v2-540dc62cdb5fe8939c5185a0eeb14c7b_hd.jpg)
    > 
    > DANï¼š
    > 
    > ç¬¬äºŒä¸ªå¥å­ç¼–ç æ¨¡å‹ä½¿ç”¨äº†è¾ƒä¸ºç®€å•çš„ä¸€ä¸ªDANæ¨¡å‹ï¼ˆ[Mohit Iyyer](https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/P/P15/P15-1162.pdf)ï¼‰ï¼Œé€šè¿‡å¯¹embeddingæ±‚å¹³å‡åè¾“å…¥åˆ°feed-forward networkï¼Œç„¶åä¸åŠ softmaxå³å¯å¾—åˆ°éšå±‚è¡¨ç¤ºï¼Œç„¶åæŠŠè¿™ä¸ªå‘é‡ç±»ä¼¼äºä¸Šé¢çš„æ–¹æ³•ï¼Œç”¨å¤šä»»åŠ¡æ¥è¿›è¡Œè®­ç»ƒæ¨¡å‹ã€‚
    > 
    > ![](https://pic3.zhimg.com/80/v2-bb79f8465693d9834e45a616b6a53631_hd.jpg)
    > 
    > è®ºæ–‡ä¹‹æ‰€ä»¥æå‡ºä¸¤ä¸ªæ˜¯è€ƒè™‘åˆ°æ¨¡å‹çš„å‡†ç¡®åº¦å’Œå¤æ‚ç¨‹åº¦ï¼Œä»¥åŠè®­ç»ƒçš„èµ„æºæ¶ˆè€—ã€‚
    > 
    > attentioné‚£ä¸ªencodeæ¨¡å‹å‡†ç¡®ç‡é«˜ï¼Œä½†æ˜¯æ¨¡å‹å¤æ‚åº¦é«˜ï¼Œè®­ç»ƒæ—¶é—´é•¿ï¼Œæ¶ˆè€—gpuèµ„æºå¤š
    > 
    > è€ŒDANæ¨¡å‹è™½ç„¶å‡†ç¡®åº¦ä½ï¼Œä½†æ˜¯è®­ç»ƒå¿«ï¼Œæ¨¡å‹å¤æ‚åº¦ä½ã€‚
    > 
    > æ–‡ä¸­ä¹Ÿç»™å‡ºäº†æ ·ä¾‹ï¼Œæœ‰äº†ä¸€ä¸ªåˆå§‹è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚å†åŠ å…¥ä¸€äº›æ–‡æœ¬è¿›è¡Œå†æ¬¡è®­ç»ƒã€‚ï¼ˆ[ä½¿ç”¨åœ°å€](https://link.zhihu.com/?target=https%3A//www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1)ï¼‰
    > 
    > ![](https://pic4.zhimg.com/80/v2-51aac31df9e31c1d907171f20e96fc0f_hd.jpg)
    > 
    > æ–‡ä¸­è¿˜æå‡ºäº†æ¯”è¾ƒä¸¤ä¸ªå¥å­å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦æ—¶å€™ä¸ç”¨åŸç”Ÿçš„cosè·ç¦»ï¼Œè€Œæ˜¯è½¬åŒ–ä¸ºå¼§åº¦ä¹‹åè®¡ç®—ï¼Œæ•ˆæœè¦å¥½äºåŸç”Ÿçš„ï¼š
    > 
    > ![](https://pic3.zhimg.com/80/v2-cb699c1869f91223cffe3d77cc524f79_hd.jpg)


## èªå¥è§£æ

### Bi-LSTM-CRF

- [[1508.01991] Bidirectional LSTM-CRF Models for Sequence Tagging](https://arxiv.org/abs/1508.01991)

- [è¯æ³•åˆ†æä¹‹Bi-LSTM-CRFæ¡†æ¶ - CSDNåšå®¢](https://blog.csdn.net/qrlhl/article/details/78561342)




### Stack-augmented Parser-Interpreter Neural Network (SPINN)

- [[1603.06021] A Fast Unified Model for Parsing and Sentence Understanding](https://arxiv.org/abs/1603.06021)

- [Recursive Neural Networks with PyTorch | NVIDIA Developer Blog](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)

    - [å¦‚ä½•ç”¨PyTorchå¯¦ç¾éæ­¸ç¥ç¶“ç¶²çµ¡ï¼Ÿ - å£¹è®€](https://read01.com/kdQym4.html)

    > The dataset comes with machine-generated syntactic parse trees, which group the words in each sentence into phrases and clauses that all have independent meaning and are each composed of two words or sub-phrases. Many linguists believe that humans understand language by combining meanings in a hierarchical way as described by trees like these, so it might be worth trying to build a neural network that works the same way. Hereâ€™s an example of a sentence from the dataset, with its parse tree represented by nested parentheses:
    > 
    > ```
    >     ( ( The church ) ( ( has ( cracks ( in ( the ceiling ) ) ) ) . ) )
    > ```
    > 
    > One way to encode this sentence using a neural network that takes the parse tree into account would be to build a neural network layer Reduce that combines pairs of words (represented by word embeddings like [GloVe](http://nlp.stanford.edu/projects/glove/)) and/or phrases, then apply this layer recursively, taking the result of the last Reduce operation as the encoding of the sentence:
    > 
    > ```
    > X = Reduce(â€œtheâ€, â€œceilingâ€)
    > Y = Reduce(â€œinâ€, X)
    > ... etc.
    > ```
    > 
    > But what if I want the network to work in an even more humanlike way, reading from left to right and maintaining sentence context while still combining phrases using the parse tree? Or, what if I want to train a network to construct its own parse tree as it reads the sentence, based on the words it sees? Hereâ€™s the same parse tree written a slightly different way:
    > 
    > ```
    >     The church ) has cracks in the ceiling ) ) ) ) . ) )
    > ```
    > 
    > Or a third way, again equivalent:
    > 
    > ```
    > WORDS:  The church   has cracks in the ceiling         .
    > PARSES: S   S      R S   S      S  S   S       R R R R S R R
    > ```
    > 
    > All I did was remove open parentheses, then tag words with â€œSâ€ for â€œshiftâ€ and replace close parentheses with â€œRâ€ for â€œreduce.â€ But now the information can be read from left to right as a set of instructions for manipulating a stack and a stack-like buffer, with exactly the same results as the recursive method described above:
    > 
    > 1.  Place the words into the buffer.
    > 2.  Pop â€œTheâ€ from the front of the buffer and push it onto stack, followed by â€œchurchâ€.
    > 3.  Pop top two stack values, apply Reduce, then push the result back to the stack.
    > 4.  Pop â€œhasâ€ from buffer and push to stack, then â€œcracksâ€, then â€œinâ€, then â€œtheâ€, then â€œceilingâ€.
    > 5.  Repeat four times: pop top two stack values, apply Reduce, then push the result.
    > 6.  Pop â€œ.â€ from buffer and push onto stack.
    > 7.  Repeat two times: pop top two stack values, apply Reduce, then push the result.
    > 8.  Pop the remaining stack value and return it as the sentence encoding.
    > 
    > I also want to maintain sentence context to take into account information about the parts of the sentence the system has already read when performing Reduce operations on later parts of the sentence. So Iâ€™ll replace the two-argument `Reduce` function with a three-argument function that takes a left child phrase, a right child phrase, and the current sentence context state. This state is created by a second neural network layer, a recurrent unit called the `Tracker`. The `Tracker` produces a new state at every step of the stack manipulation (i.e., after reading each word or close parenthesis) given the current sentence context state, the top entry *b* in the buffer, and the top two entries *s1*, *s2* in the stack:
    > 
    > ```
    > context\[t+1\] = Tracker(context\[t\], b, s1, s2)
    > ```
    > 

### SLING

- [google/sling: SLING - A natural language frame semantics parser](https://github.com/google/sling)

- [è‡ªç„¶èªè¨€ç†è§£æŠ€è¡“å¤§é€²å±•ï¼å…æ–·è©ï¼ŒGoogleèªæ„æ¡†æ¶å‰–æå™¨SLINGèƒ½è‡ªå‹•æ‰¾å‡ºèªå¥æ¶æ§‹ | iThome](https://www.ithome.com.tw/news/118415)

    > [Googleæœ€è¿‘é–‹æºé‡‹å‡ºå¯¦é©—æ€§çš„èªæ„æ¡†æ¶å‰–æå™¨ï¼ˆParsingï¼‰SLINGï¼Œ](https://research.googleblog.com/2017/11/sling-natural-language-frame-semantic.html)æœ‰åˆ¥æ–¼ä»¥å¾€ç”¨æ–·è©çš„æ–¹å¼ï¼ŒSLINGä¸éœ€è¦é äººå·¥çš„æ–¹å¼æ¨™è¨»èªå¥ï¼Œè€Œæ˜¯å¯ä»¥é€éèªæ„æ¡†æ¶ï¼ˆFrame Semantic Parsingï¼‰çš„æ–¹å¼è‡ªå‹•æŠ½å–å‡ºæ–‡å­—æ‰€è¦æè¿°çš„èªæ„çµæ§‹ï¼Œå†ä»¥èªæ„æ¡†æ¶åœ–ï¼ˆSemantic frame graphï¼‰çš„æ–¹å¼å‘ˆç¾ï¼ŒGoogleç ”ç©¶åœ˜éšŠè¡¨ç¤ºï¼ŒSLINGæ˜¯é€éTensorflowå’ŒDragnnè¨“ç·´éçš„æ¨™è¨»èªæ–™åº«ï¼Œé€™æ˜¯è‡ªç„¶èªè¨€ç†è§£æŠ€è¡“çš„ä¸€å¤§é€²å±•ï¼Œèªæ„åˆ†æä¸å†é æ–·è©ï¼Œè€Œæ˜¯å¾èªè¨€æ„ç¾©å±¤é¢ï¼Œè‡ªå‹•æ¨™è¨»å‡ºèªå¥çš„çµæ§‹ã€‚
    > 
    > SLINGæ˜¯æ¡ç”¨ä¸€å€‹ç‰¹å®šç”¨é€”çš„éæ­¸ç¥ç¶“ç¶²è·¯ï¼ˆRecurrent neural networkï¼ŒRNNï¼‰æ¨¡å‹ï¼Œåœ¨è©²æ¡†æ¶åœ–ä¸Šï¼Œé€éè¼¸å…¥æ–‡å­—çš„éå¢ç·¨è¼¯å‹•ä½œï¼Œä¾†è¨ˆç®—è¼¸å‡ºå€¼ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œè©²æ¡†æ¶åœ–å› ç‚ºéˆæ´»çš„ç‰¹æ€§ï¼Œå¯ä»¥æ“·å–å¤šå€‹èªæ„ä»»å‹™ï¼ŒSLINGçš„èªæ„å‰–æå™¨åªç”¨äº†è¼¸å…¥è©å¥ä¾†è¨“ç·´ï¼Œæ²’æœ‰æ¡ç”¨é¡å¤–çš„ç”Ÿæˆçš„æ¨™è¨»ï¼Œåƒæ˜¯èªå¥ç›¸ä¾æ€§åˆ†æç”¢ç”Ÿçš„æ¨™è¨»ã€‚
    > 
    > ![](https://s4.itho.me/sites/default/files/images/sling.png)
    > 
    > å¤§éƒ¨åˆ†çš„è‡ªç„¶èªè¨€ç†è§£ç³»çµ±éƒ½æ˜¯æ¡ç”¨ä¸€ç¨®åˆ†ææµç¨‹ï¼Œå¾è©æ€§æ¨™è¨» ï¼ˆPart-of-speech taggingï¼‰ï¼Œåˆ°é€éèªå¥ç›¸ä¾æ€§åˆ†æï¼ˆDependency parsingï¼‰ä¾†è¨ˆç®—è¼¸å…¥çš„æ–‡å­—èªæ„ã€‚é€™ç¨®æ¨¡å‹è¼ƒå®¹æ˜“å°‡ä¸åŒçš„æ–¹æéšæ®µæ¨¡çµ„åŒ–ï¼Œä½†æ˜¯å¾€å¾€ä¹Ÿå°è‡´ä¸€å€‹å•é¡Œï¼Œä¸€æ—¦ç”¢ç”ŸéŒ¯èª¤å°‡æœƒå½±éŸ¿æ•´å€‹æ¨¡å‹çš„é æ¸¬ã€‚
    > 
    > SLINGè¼¸å‡ºçš„èªæ„æ¡†æ¶åœ–å¯ä»¥ç›´æ¥æ“·å–ä½¿ç”¨è€…æ„Ÿèˆˆè¶£çš„èªæ„æ¨™ç¤ºï¼ˆSemantic annotationï¼‰ï¼Œä¹Ÿèƒ½é¿å…ç³»çµ±æµç¨‹ä¸­çš„è¨­è¨ˆç¼ºé™·ï¼Œé‚„èƒ½é¿å…ä¸å¿…è¦çš„è¨ˆç®—ã€‚
    > 
    > èˆ‰ä¾‹ä¾†èªªï¼Œå‚³çµ±çš„è‡ªç„¶èªè¨€ç†è§£ç³»çµ±æœƒå…ˆåŸ·è¡Œèªå¥ç›¸ä¾æ€§åˆ†æçš„å·¥ä½œï¼Œæœ€å¾Œæ‰æœƒåŸ·è¡ŒæŒ‡ä»£æ¶ˆè§£ï¼ˆCoreference resolutionï¼‰ï¼ŒæŒ‡ä»£æ¶ˆè§£æ˜¯å°‡æŒ‡å®šä»£åè©é‚„åŸç‚ºè¢«æ›¿æ›çš„åè©ï¼Œä¾†é¿å…é‡è¦çš„å­—è©å› è¢«æ›¿æ›ç‚ºæŒ‡å®šä»£åè©ï¼Œè€Œåœ¨è¨ˆç®—æ¬Šé‡æ™‚é™ä½çš„å•é¡Œï¼Œå¦‚æœèªå¥ç›¸ä¾æ€§åˆ†æéç¨‹è‹¥æœ‰éŒ¯èª¤ï¼Œå°‡æœƒé€£å¸¶å½±éŸ¿æœ€çµ‚è¼¸å‡ºçš„çµæœã€‚
    > 
    > ### èªæ„æ¡†æ¶å‰–æçš„æ©Ÿåˆ¶
    > 
    > èªæ„æ¡†æ¶ä»£è¡¨èªå¥çš„æ„ç¾©ï¼Œä¹Ÿæ˜¯ä¸€å€‹æè¿°ï¼Œæ¯å€‹æè¿°éƒ½è¢«ç¨±ç‚ºä¸€å€‹æ¡†æ¶ï¼Œè©²æ¡†æ¶å¯è¢«è¦–ç‚ºçŸ¥è­˜æˆ–æ˜¯æ„ç¾©çš„å–®å…ƒï¼Œä¹ŸåŒ…å«äº†èˆ‡å…¶ç›¸é—œçš„æ¦‚å¿µæˆ–æ˜¯æ¡†æ¶çš„ç›¸äº’é—œä¿‚ã€‚SLINGå°‡æ¯å€‹æ¡†æ¶çµ„ç¹”æˆä¸€å€‹Slotçš„æ¸…å–®ï¼Œæ¯å€‹Slotéƒ½æœ‰è‡ªå·±çš„è§’è‰²æˆ–æ˜¯åç¨±ï¼Œä»¥åŠä»£è¡¨çš„å€¼ï¼Œè©²ä»£è¡¨å€¼å¯ä»¥æ˜¯å€‹å­—è©çš„åŸæ„ï¼Œæˆ–æ˜¯èˆ‡å…¶ä»–æ¡†æ¶çš„é€£çµã€‚
    > 
    > ä¾‹å¦‚ï¼ŒMany people now claim to have predicted Black Mondayé€™å¥è©±ï¼ŒSLINGå…ˆè¾¨è­˜èªå¥çš„å¯¦é«”ã€æ¸¬é‡å€¼å’Œå…¶ä»–æ¦‚å¿µï¼Œå¯¦é«”åƒæ˜¯äººç‰©ã€åœ°é»ï¼Œäº‹ä»¶ï¼Œæ¸¬é‡å€¼åƒæ˜¯æ™‚é–“ã€è·é›¢ï¼Œå…¶ä»–æ¦‚å¿µå‰‡åŒ…å«å‹•è©ï¼Œæ¥è‘—ï¼Œå°‡é€™äº›è¾¨è­˜å‡ºä¾†çš„å­—è©åˆ†é¡åˆ°æ­£ç¢ºçš„èªæ„è§’è‰²ï¼Œç•¶ä½œè¼¸å…¥å€¼ï¼Œå› æ­¤ï¼ŒSLINGæœƒå…ˆå°‡peopleè¦–ç‚ºäººç‰©æ¡†æ¶ã€predictedæ˜¯å‹•è©é¡åˆ¥æ¡†æ¶ã€Black Mondayæ˜¯äº‹ä»¶æ¡†æ¶ï¼Œpredictedé€™å€‹å‹•è©è¡¨ç¤ºç‚ºPREDICT-01æ¡†æ¶ï¼ŒPREDICT-01æ¡†æ¶èˆ‡é æ¸¬çš„ä¸»è©Slotæœ‰ç›¸äº’é—œä¿‚ï¼Œå› æ­¤ï¼ŒPREDICT-01èˆ‡PERSONæ¡†æ¶é€£æ¥ï¼Œé™¤æ­¤ä¹‹å¤–ï¼ŒPREDICT-01æ¡†æ¶ä¹Ÿèˆ‡è¢«é æ¸¬çš„å—è©æœ‰ç›¸äº’é—œä¿‚ï¼Œèˆ‡Black Mondayçš„EVENTæ¡†æ¶é€£æ¥ã€‚
    > 
    > ![](https://s4.itho.me/sites/default/files/images/%E7%AF%84%E4%BE%8B.PNG)
    > 
    > Googleç ”ç©¶åœ˜éšŠèªç‚ºï¼ŒSLINGé€éèªæ„æ¡†æ¶ï¼Œä¾†è¨“ç·´ä¸¦å„ªåŒ–éæ­¸ç¥ç¶“ç¶²è·¯ã€‚ç¥ç¶“ç¶²è·¯åœ¨éš±è—å±¤ä¸­å­¸ç¿’åˆ°çš„çŸ¥è­˜ï¼Œå¯ä»¥å–ä»£äº†äººå·¥æ¨™è¨»ç‰¹å¾µã€‚
    > 
    > è©²èªæ„å‰–æå™¨çš„è¼¸å…¥æ˜¯ä»¥é›™å‘é•·çŸ­è¨˜æ†¶å–®å…ƒï¼ˆBi-directional LSTMsï¼‰æ¼”ç®—æ³•ç‚ºåŸºç¤çš„è½‰æ›èªæ„æ¡†æ¶å‰–ææ–¹æ³•ï¼Œä½¿ç”¨Transition Based Recurrent Unit (TBRU)ä¾†è¼¸å‡ºï¼Œçµåˆæˆä¸€å€‹è¨“ç·´éçš„æ¨¡å‹ï¼Œåªéœ€è¦æ–‡å­—æ¨™è¨»ç•¶ä½œè¼¸å…¥ï¼Œç¶“éè½‰æ›ç³»çµ±ï¼Œè¼¸å‡ºèªæ„æ¡†æ¶åœ–å½¢ï¼Œä¸éœ€è¦ä¸­é–“ç”¢ç”Ÿçš„æ¨™è¨»ï¼ˆIntervening symbolic representationï¼‰ã€‚
    > 
    > è¼¸å‡ºå±¤çš„æ–‡å­—åœ¨è¼¸å‡ºå¾Œï¼Œé‚„æœƒç¶“éè½‰æ›ç³»çµ±ï¼ˆTransition systemï¼‰ï¼Œå†é‡æ–°é€²å…¥è¼¸å…¥å±¤ï¼Œå…¶ä¸­ï¼Œè½‰æ›ç³»çµ±çš„ä¸€é …é—œéµæ©Ÿåˆ¶æ˜¯æ¡ç”¨äº†å›ºå®šå¤§å°çš„æ¡†æ¶ä¾†è¨˜éŒ„å­—è©å°ä¸Šä¸‹æ–‡é æ¸¬çš„é‡è¦ç¨‹åº¦ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œè©²æ¡†æ¶æ˜¯ç”¨ä¾†è¡¨ç¤ºæœ€è¿‘æåŠï¼Œæˆ–æ˜¯åœ¨èªå¥ä¸­è¢«å¢å¼·çš„é—œéµå­—ã€‚Googleç ”ç©¶åœ˜éšŠç™¼ç¾ï¼Œé€éé€™å€‹ç°¡å–®çš„æ©Ÿåˆ¶ï¼Œåœ¨æ“·å–å¤§é‡èªæ„æ¡†æ¶çš„é—œè¯ä¸Šï¼Œæ•ˆç‡æå‡æœ‰éå¸¸å¤šã€‚
    > 
    > ç›®å‰ï¼ŒGoogleçš„ç ”ç©¶åœ˜éšŠè¡¨ç¤ºï¼ŒSLINGæ˜¯ç ”ç©¶èªæ„å‰–æçš„å¯¦é©—ï¼ŒGoogleå·²åœ¨Githubå°‡SLINGé–‹æºé‡‹å‡ºï¼Œæä¾›é–‹ç™¼äººå“¡é å…ˆè¨“ç·´å®Œæˆçš„èªæ„å‰–ææ¨¡å‹ï¼Œå¯æ‡‰ç”¨æ–¼çŸ¥è­˜èƒå–ã€è§£æè¤‡é›œå¼•ç”¨ï¼ˆResolving complex referencesï¼‰ï¼Œä»¥åŠå°è©±ç†è§£ç­‰å·¥ä½œï¼Œæœªä¾†ï¼ŒGoogleå°‡æœƒæŒçºŒæ“´å¢SLINGçš„åŠŸèƒ½ã€‚

## æ–‡æ³•æ”¹éŒ¯

### DeepFix

- [DeepFix: Fixing Common C Language Errors by Deep Learning - 13921](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14603/13921)




## auto tagging

- [memray/seq2seq-keyphrase](https://github.com/memray/seq2seq-keyphrase)

- [udibr/headlines: Automatically generate headlines to short articles](https://github.com/udibr/headlines)

- [fudannlp16/KeyPhrase-Extraction](https://github.com/fudannlp16/KeyPhrase-Extraction)

- [Tensorflowï¼šåŸºäºLSTMè½»æ¾ç”Ÿæˆå„ç§å¤è¯— - CSDNåšå®¢](https://blog.csdn.net/meyh0x5vdtk48p2/article/details/78987402)

- [[ä»£ç ]åŸºäºRNNçš„æ–‡æœ¬ç”Ÿæˆç®—æ³• - CSDNåšå®¢](https://blog.csdn.net/clayanddev/article/details/53955850)

- [How can I use machine learning to propose tags for content? - Quora](https://www.quora.com/How-can-I-use-machine-learning-to-propose-tags-for-content)

- [Deep Learning for Text Understanding from Scratch](https://www.kdnuggets.com/2015/03/deep-learning-text-understanding-from-scratch.html)

- [neural network - Keyword/phrase extraction from Text using Deep Learning libraries - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/10077/keyword-phrase-extraction-from-text-using-deep-learning-libraries)

- [attardi/deepnl: Deep Learning for Natural Language Processing](https://github.com/attardi/deepnl)

- [Intro to text classification with Keras: automatically tagging Stack Overflow posts | Google Cloud Big Data and Machine Learning Blog  |  Google Cloud](https://cloud.google.com/blog/big-data/2017/10/intro-to-text-classification-with-keras-automatically-tagging-stack-overflow-posts)

- [snkim/AutomaticKeyphraseExtraction: Data for Automatic Keyphrase Extraction Task](https://github.com/snkim/AutomaticKeyphraseExtraction)

- [lvsh/keywordfinder: Automatic keyword extraction - no alchemy required!](https://github.com/lvsh/keywordfinder)

- [Natural Language Toolkit â€” NLTK 3.2.5 documentation](http://www.nltk.org/)

- [nlp - How to auto-tag content, algorithms and suggestions needed - Stack Overflow](https://stackoverflow.com/questions/6039238/how-to-auto-tag-content-algorithms-and-suggestions-needed)




## Chinese Character Embeddings

- [å¹²è´§|NLPé¢†åŸŸä¸­æ–‡vsè‹±æ–‡æœ‰ä»€ä¹ˆå¼‚åŒç‚¹ï¼Œä¸­æ–‡NLPæœ‰ä»€ä¹ˆç‹¬ç‰¹çš„åœ°æ–¹?_æœç‹ç§‘æŠ€_æœç‹ç½‘](http://www.sohu.com/a/138840703_642762)

- [Leonard-Xu/CWE](https://github.com/Leonard-Xu/CWE)




## Entity extraction

- [Entity extraction using Deep Learning â€“ Towards Data Science](https://towardsdatascience.com/entity-extraction-using-deep-learning-8014acac6bb8)

- [Sequence Tagging with Tensorflow](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)

- [dlnd-other/embeddings at master Â· dkarunakaran/dlnd-other](https://github.com/dkarunakaran/dlnd-other/tree/master/embeddings)

- [Pretrained Character Embeddings for Deep Learning and Automatic Text Generation](http://minimaxir.com/2017/04/char-embeddings/)

- [Introduction to Conditional Random Fields](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)





## Entropy

- [æœ€å¤§ç†µæ¨¡å‹ (MaxEnt) å¯¦è¸çŸ­å­—è©åˆ†é¡ â€“ PyLadies Taiwan â€“ Medium](https://medium.com/pyladies-taiwan/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B-maxent-%E5%AF%A6%E8%B8%90%E7%9F%AD%E5%AD%97%E8%A9%9E%E5%88%86%E9%A1%9E-b925665d9082)




## Models
- [DeepMindä¸¨æ·±åº¦å­¸ç¿’æœ€æ–°ç”Ÿæˆè¨˜æ†¶æ¨¡å‹ï¼Œé è¶…RNNçš„GTMM](http://www.bigdatafinance.tw/index.php/tech/557-deepmind-rnn-gtmm)

### RNN+CNN

- [Representing Language with Recurrent and Convolutional Layers: An Authorship Attribution Example](https://hergott.github.io/language-representation-rnn-cnn/)




## æ–°è

### èªéŸ³åŠ©ç†

- [å€‹äººèªéŸ³åŠ©ç†æ™‚ä»£å·²ç¶“ä¾†è‡¨ï¼ - EE Times Taiwan é›»å­å·¥ç¨‹å°ˆè¼¯ç¶²](https://www.eettaiwan.com/news/article/20180416NT31-season-voice-based-personal-assistants?utm_source=EETT%20Article%20Alert&utm_medium=Email&utm_campaign=2018-04-17)

    > éç¨‹å’ŒåŸç†
    > 
    > ä½œç‚ºä¸€åé–‹ç™¼è€…å’Œè¨­è¨ˆå¸«ï¼Œè¦å……ä»½ä½¿ç”¨é€™é …æŠ€è¡“ï¼Œé‡è¦çš„æ˜¯ç­è§£å¦‚ä¸‹çš„å®Œæ•´å‘½ä»¤äº’å‹•éç¨‹ï¼š
    > 
    > * è™›æ“¬åŠ©ç†ä½¿ç”¨ä¸€å€‹è§¸ç™¼è©(å¦‚â€˜Ok Googleâ€™ã€â€˜Hey Siriâ€™)ä¾†ã€Œå–šé†’ã€ï¼Œä»¥ç¢ºä¿å®ƒåªåœ¨å‘½ä»¤ä¸‹é”æ™‚æ‰åŸ·è¡Œã€‚
    > * éŸ³è¨Šè¢«è¨˜éŒ„åœ¨è¨­å‚™ä¸Šï¼Œç¶“éå£“ç¸®ä¸¦é€éWi-Fiå‚³è¼¸åˆ°é›²ç«¯ã€‚é€šå¸¸æœƒæ¡ç”¨é™å™ªæ¼”ç®—æ³•ä¾†è¨˜éŒ„éŸ³è¨Šï¼Œä»¥ä¾¿é›²ç«¯ã€Œå¤§è…¦ã€æ›´å®¹æ˜“ç†è§£ç”¨æˆ¶çš„å‘½ä»¤ã€‚
    > * ä½¿ç”¨å°ˆæœ‰çš„ã€ŒèªéŸ³è½‰æ–‡æœ¬ã€(voice-to-text)å¹³å°å°‡éŸ³è¨Šè½‰æ›æˆæ–‡æœ¬å‘½ä»¤ã€‚é€éæŒ‡å®šçš„é »ç‡å°é¡æ¯”è¨Šè™Ÿé€²è¡Œæ¡æ¨£ï¼Œå°‡é¡æ¯”è²æ³¢è½‰æ›ç‚ºæ•¸ä½è³‡æ–™ã€‚åˆ†ææ•¸ä½è³‡æ–™ä»¥ç¢ºå®šè‹±èªéŸ³ç´ (â€˜bbâ€™ã€â€˜ooâ€™ã€â€˜shâ€™ç­‰)çš„å‡ºç¾ä½ç½®ã€‚ ä¸€æ—¦è¾¨è­˜åˆ¥å‡ºéŸ³ç´ ï¼Œå°±ä½¿ç”¨çµ±è¨ˆå»ºæ¨¡æ¼”ç®—æ³•(å¦‚Hidden Markhovæ¨¡å‹)ä¾†ç¢ºå®šç‰¹å®šå–®è©çš„å¯èƒ½æ€§ã€‚
    > * ä½¿ç”¨è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†è™•ç†æ–‡æœ¬ä»¥ç¢ºå®šæ‰€éœ€çš„æ“ä½œã€‚ è©²æ¼”ç®—æ³•é¦–å…ˆä½¿ç”¨è©æ€§æ¨™è¨»ä¾†ç¢ºå®šå“ªäº›è©æ˜¯å½¢å®¹è©ã€å‹•è©å’Œåè©ç­‰ï¼Œç„¶å¾Œå°‡é€™ç¨®æ¨™è¨˜èˆ‡çµ±è¨ˆæ©Ÿå™¨å­¸ç¿’æ¨¡å‹ç›¸çµåˆèµ·ä¾†ï¼Œæ¨æ–·å¥å­çš„å«ç¾©ã€‚
    > * å¦‚æœå‘½ä»¤æ“ä½œéœ€è¦é€²ä¸€æ­¥çš„æœå°‹ï¼Œç³»çµ±å°‡ç«‹å³é€²è¡Œæœå°‹ã€‚ä¾‹å¦‚ï¼Œã€Œå˜¿ï¼Siriï¼Œä»€éº¼æ˜¯Snapdragonè¡Œå‹•å¹³å°ï¼Ÿã€å°‡è§¸ç™¼ç¶²éš›ç¶²è·¯æœå°‹ï¼Œä¸¦è¿”å›æ‰€å¾—åˆ°çš„è³‡è¨Šã€‚å¦‚æœè©²å‘½ä»¤é¡ä¼¼æ–¼ã€ŒOk Googleï¼Œå‚³ç°¡è¨Šçµ¦åª½åª½ã€ï¼Œé‚£éº¼å‘½ä»¤è³‡æ–™(æ“ä½œï¼šç™¼é€ç°¡è¨Šï¼›æ”¶ä»¶äººï¼šåª½åª½)å°±æœƒè¢«ç›´æ¥å‚³é€åˆ°è™›æ“¬åŠ©ç†ã€‚
    > 
    > 

### å¯«ç¨¿æ©Ÿå™¨äºº

- [ç™¾åº¦AIå¼€æ”¾å¹³å°-å…¨çƒé¢†å…ˆçš„äººå·¥æ™ºèƒ½æœåŠ¡å¹³å°](https://ai.baidu.com/support/news?action=detail&id=140)

- [åªå¥½è·Ÿè‘—æŠ„äº†ï¼PTT å‰µä¸–ç¥çš„ AI Labs ç‚ºã€Œè¨˜è€…å¿«æŠ„ã€æ‰“é€ å¯«ç¨¿æ©Ÿå™¨äºº | TechNews ç§‘æŠ€æ–°å ±](http://technews.tw/2017/08/09/ai-labs-is-using-ai-to-cover-ptt-news/)


## æ©Ÿå™¨ç¿»è­¯

### seq2seq

- [ä»Encoderåˆ°Decoderå®ç°Seq2Seqæ¨¡å‹](https://zhuanlan.zhihu.com/p/27608348)

- [Neural Machine Translation (seq2seq) Tutorial  |  TensorFlow](https://www.tensorflow.org/tutorials/seq2seq)

- [æ·±åº¦å­¦ä¹ ç¬”è®°â€”â€”Word2vecå’ŒDoc2vecè®­ç»ƒå®ä¾‹ä»¥åŠå‚æ•°è§£è¯» - CSDNåšå®¢](https://blog.csdn.net/mpk_no1/article/details/72510655)

- [åºåˆ—åˆ°åºåˆ—çš„è¯­è¨€ç¿»è¯‘æ¨¡å‹ä»£ç (tensorflow)è§£æ - grt1ståšå®¢](https://www.grt1st.cn/posts/seq2seq-code/)

- [ç”¨seq2seq with attentionå®ç°ä¸­æ–‡æ­Œè¯ç”Ÿæˆ](https://zhuanlan.zhihu.com/p/25280463)

- [tensorflowä»£ç å…¨è§£æ -3- seq2seq è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬ - ç®€ä¹¦](https://www.jianshu.com/p/9766b317ffa4)

- [å¾é›¶é–‹å§‹çš„ Sequence to Sequence | é›·å¾·éº¥çš„è—æ›¸é–£](https://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/)

- [æ•™é›»è…¦å¯«ä½œï¼šAIçƒè©•â€”â€”Seq2seqæ¨¡å‹æ‡‰ç”¨ç­†è¨˜(PyTorch + Python3) â€“ Yi-Hsiang Kao â€“ Medium](https://medium.com/@gau820827/%E6%95%99%E9%9B%BB%E8%85%A6%E5%AF%AB%E4%BD%9C-ai%E7%90%83%E8%A9%95-seq2seq%E6%A8%A1%E5%9E%8B%E6%87%89%E7%94%A8%E7%AD%86%E8%A8%98-pytorch-python3-31e853573dd0)



### attention model

- [tensorflow/nmt: TensorFlow Neural Machine Translation Tutorial](https://github.com/tensorflow/nmt)

- [Attention Is All You Needï¼šåŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ©Ÿå™¨ç¿»è­¯æ¨¡å‹ â€“ Youngmi huang â€“ Medium](https://medium.com/@cyeninesky3/attention-is-all-you-need-%E5%9F%BA%E6%96%BC%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%E7%9A%84%E6%A9%9F%E5%99%A8%E7%BF%BB%E8%AD%AF%E6%A8%A1%E5%9E%8B-dcc12d251449)

- [[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- [Kyubyong/transformer: A TensorFlow Implementation of the Transformer: Attention Is All You Need](https://github.com/Kyubyong/transformer)

- [(4 å°ç§ä¿¡ / 23 æ¡æ¶ˆæ¯)å¦‚ä½•ç†è§£è°·æ­Œå›¢é˜Ÿçš„æœºå™¨ç¿»è¯‘æ–°ä½œã€ŠAttention is all you needã€‹ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/61077555)

- [ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰ - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://kexue.fm/archives/4765)








### UNSUPERVISED MACHINE TRANSLATION 

- [ã€ŠUNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLYã€‹é˜…è¯»ç¬”è®°](https://zhuanlan.zhihu.com/p/32375955)

- [ã€Scienceã€‘ç„¡ç›£ç£å¼æ©Ÿå™¨ç¿»è­¯ï¼Œä¸éœ€è¦äººé¡å¹²é å’Œå¹³è¡Œæ–‡æœ¬ - å£¹è®€](https://read01.com/Rnzx84N.html)

    > é€™å…©ç¯‡ä½¿ç”¨éå¸¸ç›¸ä¼¼çš„æ–¹æ³•çš„æ–°è«–æ–‡ä¹Ÿå¯ä»¥åœ¨å¥å­å±¤é¢é€²è¡Œç¿»è­¯ã€‚å®ƒå€‘éƒ½ä½¿ç”¨å…©ç¨®è¨“ç·´ç­–ç•¥ï¼Œç¨±ç‚ºåå‘ç¿»è­¯å’Œå»å™ªï¼ˆBack translation and Denoisingï¼‰ã€‚åœ¨åå‘ç¿»è­¯ä¸­ï¼Œå…ˆæŠŠä¸€ç¨®èªè¨€çš„å¥å­å¤§è‡´ç¿»è­¯æˆå¦ä¸€ç¨®èªè¨€ï¼Œç„¶å¾Œå†ç¿»è­¯å›åŸä¾†çš„èªè¨€ã€‚å¦‚æœç¿»è­¯å¾Œçš„å¥å­èˆ‡æœ€åˆçš„å¥å­ä¸ä¸€è‡´ï¼Œå‰‡èª¿æ•´ç¥ç¶“ç¶²çµ¡å†æ¬¡ç¿»è­¯ï¼Œç›´åˆ°è®Šå¾—è¶Šä¾†è¶Šæ¥è¿‘ã€‚
    > 
    > å»å™ªèˆ‡åå‘ç¿»è­¯é¡ä¼¼ï¼Œä½†ä¸æ˜¯å¾ä¸€ç¨®èªè¨€åˆ°å¦ä¸€ç¨®èªè¨€ç„¶å¾Œå†å›ä¾†ï¼Œè€Œæ˜¯å¾ä¸€ç¨®èªè¨€ï¼ˆé€šéé‡æ–°æ’åˆ—æˆ–åˆªé™¤å–®è©ï¼‰ä¸­æ·»åŠ å™ªè²ï¼Œä¸¦å˜—è©¦å°‡å…¶ç¿»è­¯å›æœ€é–‹å§‹çš„èªè¨€ã€‚é€™äº›æ–¹æ³•çš„çµ„åˆï¼Œèƒ½å¤ æ•™çµ¦ç¶²çµ¡æ›´æ·±å±¤æ¬¡çš„èªè¨€çµæ§‹ã€‚
    > 
    > ä½†æ˜¯å…©ç¨®æŠ€è¡“ä¹‹é–“é‚„æ˜¯æœ‰è‘—ç´°å¾®çš„å·®ç•°ã€‚ UPVçš„ç³»çµ±åœ¨è¨“ç·´æœŸé–“æ›´é »ç¹åœ°é€²è¡Œåå‘ç¿»è­¯ã€‚ç”±ä½æ–¼è³“å¤•æ³•å°¼äºå·åŒ¹èŒ²å ¡çš„ Facebook è¨ˆç®—æ©Ÿç§‘å­¸å®¶Guillaume Lampleå’Œåˆä½œè€…å‰µå»ºçš„å¦ä¸€å€‹ç³»çµ±åœ¨ç¿»è­¯éç¨‹ä¸­å‰‡å¢åŠ äº†ä¸€å€‹é¡å¤–çš„æ­¥é©Ÿã€‚åœ¨å°‡ä¸€å€‹èªè¨€è§£ç¢¼ç‚ºå¦ä¸€ç¨®èªè¨€ä¹‹å‰ï¼Œé€™å…©å€‹ç³»çµ±éƒ½å°‡å…¶å¾ä¸€ç¨®èªè¨€ç·¨ç¢¼ç‚ºæ›´æŠ½è±¡çš„è¡¨ç¤ºï¼Œä½†Facebookç³»çµ±çš„ç ”ç©¶å“¡èªç‚ºï¼Œå…¶ç³»çµ±çš„ã€Œä¸­é–“èªè¨€ã€æ˜¯çœŸæ­£æŠ½è±¡çš„ã€‚ Artetxeå’ŒLampleéƒ½è¡¨ç¤ºï¼Œä»–å€‘å¯ä»¥é€šéæ‡‰ç”¨å°æ–¹è«–æ–‡çš„æŠ€å·§ä¾†æ”¹å–„çµæœã€‚
    > 
    > å…©ç¯‡è«–æ–‡ä¹‹é–“å”¯ä¸€å¯ä»¥ç›´æ¥æ¯”è¼ƒçš„çµæœæ˜¯å¾ä»¥åŒ…å«äº†3000è¬å¥å­çš„è‹±æ³•æ–‡æœ¬è³‡æ–™åº«ä¸­é€²è¡Œçš„ç¿»è­¯ï¼Œå…©å€‹ç³»çµ±éƒ½åœ¨é›™èªè©•ä¼°æ›¿è£œè©•åˆ†ï¼ˆç”¨ä¾†è¡¡é‡ç¿»è­¯çš„æº–ç¢ºæ€§ï¼‰ä¸Šçš„å¾—åˆ†éƒ½åœ¨15åˆ†å·¦å³ ã€‚é€™å€‹æ•¸å­—é‚„æ¯”ä¸ä¸Šè°·æ­Œç¿»è­¯ã€‚è°·æ­Œç¿»è­¯ä½¿ç”¨æœ‰ç›£ç£çš„æ–¹æ³•ï¼Œåœ¨åŒé¡æ¸¬è©¦ä¸Šçš„å¾—åˆ†æ˜¯40å¤šå·¦å³ï¼Œäººé¡æ°´å¹³æ˜¯50åˆ†å·¦å³ã€‚ä½†æ˜¯ï¼Œé€™äº›æ–¹æ³•éƒ½æ¯”è©å°è©çš„ç¿»è­¯è¦å¥½ã€‚


### CipherGAN

- [æ— å¹³è¡Œæ–‡æœ¬ç…§æ ·ç ´è§£å¯†ç ï¼ŒCipherGANæœ‰æœ›æå‡æœºå™¨ç¿»è¯‘æ°´å¹³](https://zhuanlan.zhihu.com/p/33672256)

    > é’ˆå¯¹CipherGANå¯ä»¥ä½¿ç”¨éå¹³è¡Œæ–‡æœ¬ä½œè¾“å…¥çš„ç‰¹ç‚¹ï¼ŒGomezåœ¨æ¥å—Newsweekå¤–åª’é‡‡è®¿çš„æ—¶å€™ï¼Œä¹Ÿæåˆ°äº†ï¼Œâ€œå¯†ç ç ´è¯‘çš„æ¨¡å‹æ€è·¯ä¹Ÿèƒ½è¿ç§»åˆ°éç›‘ç£å­¦ä¹ çš„ç¿»è¯‘ä¸Šã€‚â€
    > 
    > å› ä¸ºè¯­è¨€ç¿»è¯‘å¸¸é¢ä¸´çš„éš¾é¢˜æ˜¯ï¼Œç¼ºä¹è¶³å¤Ÿçš„å¹³è¡Œè¯­æ–™ã€‚
    > 
    > æ­£å¥½å’Œéé…å¯¹æ˜æ–‡å¯†æ–‡çš„å¯†ç ç ´è¯‘è¿‡ç¨‹å¾ˆç›¸ä¼¼ã€‚
    > 
    > 


## é–±è®€ç†è§£

### SQuAD

- [æœºå™¨è¿™æ¬¡å‡»è´¥äººä¹‹åï¼Œäº‰è®ºä¸€ç›´æ²¡å¹³æ¯ | SQuADé£äº‘](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247493419&idx=1&sn=73425fec04482f14f6b9b7316e425e63&chksm=e8d05059dfa7d94fc1457a36d4f62cb1b8a057ce18388fbad448aa6b53f4dbb1299cfd697724&scene=21#wechat_redirect)


    > SQuADè¢«ç§°ä¸ºè¡Œä¸šå…¬è®¤çš„æœºå™¨é˜…è¯»ç†è§£é¡¶çº§æ°´å¹³æµ‹è¯•ï¼Œå¯ä»¥ç†è§£ä¸ºæœºå™¨é˜…è¯»ç†è§£é¢†åŸŸçš„ImageNetã€‚å®ƒä»¬åŒæ ·å‡ºè‡ªæ–¯å¦ç¦ï¼ŒåŒæ ·æ˜¯ä¸€ä¸ªæ•°æ®é›†ï¼Œæ­é…ä¸€ä¸ªç«äº‰æ¿€çƒˆçš„ç«èµ›ã€‚
    > 
    > è¿™ä¸ªç«èµ›åŸºäºSQuADé—®ç­”æ•°æ®é›†ï¼Œè€ƒå¯Ÿä¸¤ä¸ªæŒ‡æ ‡ï¼šEMå’ŒF1ã€‚
    > 
    > EMæ˜¯æŒ‡ç²¾ç¡®åŒ¹é…ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹ç»™å‡ºçš„ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆä¸€æ¨¡ä¸€æ ·ï¼›F1ï¼Œæ˜¯æ ¹æ®æ¨¡å‹ç»™å‡ºçš„ç­”æ¡ˆå’Œæ ‡å‡†ç­”æ¡ˆä¹‹é—´çš„é‡åˆåº¦è®¡ç®—å‡ºæ¥çš„ï¼Œä¹Ÿå°±æ˜¯ç»“åˆäº†å¬å›ç‡å’Œç²¾ç¡®ç‡ã€‚
    > 
    > ç›®å‰é˜¿é‡Œã€å¾®è½¯å›¢é˜Ÿå¹¶åˆ—ç¬¬ä¸€ï¼Œå…¶ä¸­EMå¾—åˆ†å¾®è½¯ï¼ˆr-net+èåˆæ¨¡å‹ï¼‰æ›´é«˜ï¼ŒF1å¾—åˆ†é˜¿é‡Œï¼ˆSLQA+èåˆæ¨¡å‹ï¼‰æ›´é«˜ã€‚ä½†æ˜¯ä»–ä»¬åœ¨EMæˆç»©ä¸Šéƒ½å‡»è´¥äº†â€œäººç±»è¡¨ç°â€ã€‚
    > 
    > ---
    > 
    > 2016å¹´ï¼Œæ–¯å¦ç¦å¤§å­¦ä»ç»´åŸºç™¾ç§‘ä¸Šéšæœºé€‰å–äº†536ç¯‡æ–‡ç« ï¼Œéšåé‡‡ç”¨ä¼—åŒ…çš„æ–¹å¼ï¼Œç”±äººç±»é˜…è¯»è¿™äº›æ–‡ç« åï¼Œæå‡ºé—®é¢˜å¹¶äººå·¥æ ‡æ³¨å‡ºç­”æ¡ˆï¼Œæ„æˆäº†åŒ…å«10ä¸‡å¤šä¸ªé—®é¢˜çš„é˜…è¯»ç†è§£æ•°æ®é›†SQuADã€‚
    > 
    > å¯¹äºè¿™æ ·ä¸€ä¸ªæ•°æ®é›†ï¼Œä»¥è‰²åˆ—å·´ä¼Šå…°å¤§å­¦çš„è‘—åNLPç ”ç©¶è€…Yoav Goldbergçš„è¯„ä»·æ˜¯å¤ªå±€é™ï¼ˆrestrictedï¼‰äº†ã€‚
    > 
    > 
    > æ—©åœ¨å¥½å‡ ä¸ªæœˆä¹‹å‰ï¼ŒAIåœ¨SQuADä¸Šæ¥è¿‘äººç±»å¾—åˆ†çš„æ—¶å€™ï¼ŒGoldbergå°±ä¸“é—¨å†™äº†ä¸ªPPTï¼ŒæŠŠSQuADæ‰¹åˆ¤äº†ä¸€ç•ªã€‚
    > 
    > ![](http://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtD6fFYcb6DzyJFYv9qXbIXrhnSnicBCVRib5QEQ9QvplO5Jb1gicibv4xfnK4VOlxMTBImGVvcnuAlibPQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1)
    > 
    > ä»–åˆ—ä¸¾äº†SQuADçš„ä¸‰å¤§ä¸è¶³ï¼š
    > 
    > -   å—é™äºå¯ä»¥é€‰æ‹©spanæ¥å›ç­”çš„é—®é¢˜ï¼›
    > 
    > -   éœ€è¦åœ¨ç»™å®šçš„æ®µè½é‡Œå¯»æ‰¾ç­”æ¡ˆï¼›
    > 
    > -   æ®µè½é‡Œä¿è¯æœ‰ç­”æ¡ˆã€‚
    > 
    > å¯¹äºè¿™äº›ä¸è¶³ï¼ŒDeepMindå‰ä¸ä¹…å‘å¸ƒçš„[NarrativeQAè®ºæ–‡](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247492423&idx=2&sn=c02f90e09a49c5c41d7aac3472573b94&chksm=e8d05435dfa7dd235aeb66aae46bc619f3cb6940d88b5d00b34204c3e38757f48480de2c6f6f&scene=21#wechat_redirect)åšäº†æ›´è¯¦ç»†çš„è¯´æ˜ã€‚
    > 
    > ---
    > 
    > ä»–ä»¬è®¤ä¸ºï¼Œç”±äºSQuADé—®é¢˜çš„ç­”æ¡ˆå¿…é¡»æ˜¯ç»™å®šæ®µè½ä¸­çš„å†…å®¹ï¼Œè¿™å°±å¯¼è‡´å¾ˆå¤šè¯„ä¼°é˜…è¯»ç†è§£èƒ½åŠ›åº”è¯¥ç”¨åˆ°çš„åˆæƒ…åˆç†çš„é—®é¢˜ï¼Œæ ¹æœ¬æ²¡æ³•é—®ã€‚
    > 
    > åŒæ—¶ï¼Œè¿™ç§ç®€å•çš„ç­”æ¡ˆé€šè¿‡æ–‡æ¡£è¡¨é¢çš„ä¿¡å·å°±èƒ½æå–å‡ºæ¥ï¼Œå¯¹äºæ— æ³•ç”¨æ–‡ä¸­çŸ­è¯­æ¥å›ç­”ã€æˆ–è€…éœ€è¦ç”¨æ–‡ä¸­å‡ ä¸ªä¸è¿ç»­çŸ­è¯­æ¥å›ç­”çš„é—®é¢˜ï¼ŒSQuADè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ— æ³•æ³›åŒ–ã€‚
    > 
    > å¦å¤–ï¼ŒSQuADè™½ç„¶é—®é¢˜å¾ˆå¤šï¼Œä½†å…¶å®ç”¨åˆ°çš„æ–‡ç« åˆå°‘åˆçŸ­ï¼Œè¿™å°±é™åˆ¶äº†æ•´ä¸ªæ•°æ®é›†è¯æ±‡å’Œè¯é¢˜çš„å¤šæ ·æ€§ã€‚
    > 
    > å› æ­¤ï¼ŒSQuADä¸Šè¡¨ç°ä¸é”™çš„æ¨¡å‹ï¼Œå¦‚æœè¦ç”¨åˆ°æ›´å¤æ‚çš„é—®é¢˜ä¸Šï¼Œå¯æ‰©å±•æ€§å’Œé€‚ç”¨æ€§éƒ½å¾ˆæˆé—®é¢˜ã€‚
    > 

## æ–‡ç« åˆ†é¡å™¨


### åƒåœ¾éƒµä»¶åµæ¸¬

- [How To Build a Simple Spam-Detecting Machine Learning Classifier](https://hackernoon.com/how-to-build-a-simple-spam-detecting-machine-learning-classifier-4471fe6b816e)

- [Spam detection using neural networks in Python â€“ Emergent // Future â€“ Medium](https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272)


- [Spam Classification | Kaggle](https://www.kaggle.com/benvozza/spam-classification/notebook)

- [[1606.01042] Machine Learning for E-mail Spam Filtering: Review,Techniques and Trends](https://arxiv.org/abs/1606.01042)



#### dataset

- [UCI Machine Learning Repository: Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/Spambase)

- [SMS Spam Collection Dataset | Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/kernels)

- [Fraudulent E-mail Corpus | Kaggle](https://www.kaggle.com/rtatman/fraudulent-email-corpus/data)



## æ©Ÿå™¨æ‘˜è¦


### 2017


- [Salesforce research](https://einstein.ai/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)

    - [[1705.04304] A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/abs/1705.04304)


## æ–‡å­—ç”Ÿæˆ

- [Neural text generation â€“ Phrasee â€“ Medium](https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b)

### google smart compose

- [æœªä¾†Emailå°‡é æ¸¬ä½ çš„å¿ƒæ€è‡ªå‹•å¯«å®Œï¼ŸGoogleå¤§è…¦é¦–å¸­å·¥ç¨‹å¸«åœ¨å®˜æ–¹éƒ¨è½æ ¼è©³ç´°ä»‹ç´¹äº†åŸç† - INSIDE ç¡¬å¡çš„ç¶²è·¯è¶¨å‹¢è§€å¯Ÿ](https://www.inside.com.tw/2018/06/20/smart-compose)

    - [Google AI Blog: Smart Compose: Using Neural Networks to Help Write Emails](https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html)


    > Google çš„æ–¹æ³•æ˜¯åŒ…å«åˆ©ç”¨é¡å¤–èªå¢ƒçš„ä¸€å€‹æ–¹æ³•ï¼Œè©²æ–¹æ³•æ˜¯å°‡å•é¡Œè½‰æ›æˆä¸€å€‹åºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰çš„æ©Ÿå™¨ç¿»è­¯ä»»å‹™ï¼Œå…¶ä¸­æºåºåˆ—æ˜¯éƒµä»¶ä¸»é¡Œå’Œä¸Šå°éƒµä»¶æ­£æ–‡ï¼ˆå‡è¨­å­˜åœ¨ä¸Šå°éƒµä»¶ï¼‰çš„ä¸²è¯ï¼Œä½¿ç”¨è€…æ­£åœ¨å¯«çš„éƒµä»¶æ˜¯ç›®æ¨™åºåˆ—ã€‚å„˜ç®¡è©²æ–¹æ³•åœ¨é æ¸¬å“è³ªä¸Šè¡¨ç¾è‰¯å¥½ï¼Œä½†å®ƒçš„å»¶é²è¦æ¯” Google åš´è‹›çš„å»¶é²æ¨™æº–è¶…å‡ºäº†å¥½å¹¾å€‹é‡ç´š
    > 
    > ç‚ºäº†æé«˜é æ¸¬å“è³ªï¼Œ Google å°‡ä¸€å€‹ RNN-LM ç¥ç¶“ç¶²è·¯èˆ‡ä¸€å€‹ BoW æ¨¡å‹çµåˆèµ·ä¾†ï¼Œçµåˆå¾Œçš„æ¨¡å‹åœ¨é€Ÿåº¦ä¸Šæ¯” seq2seq æ¨¡å‹è¦å¿«ï¼Œä¸”åªè¼•å¾®çŠ§ç‰²äº†é æ¸¬å“è³ªã€‚åœ¨è©²æ··åˆæ¼”ç®—æ³•ä¸­ï¼Œ Google é€šéæŠŠè©åµŒå¥—å€‘å¹³å‡åˆ†é…åœ¨æ¯å€‹å€åŸŸå…§ï¼Œä¾†å°éƒµä»¶ä¸»é¡Œå’Œæ­¤å‰çš„éƒµä»¶å…§å®¹é€²è¡Œç·¨ç¢¼ã€‚éš¨å¾Œ Google å°‡é€™äº›å¹³å‡åˆ†é…å¾Œçš„åµŒå¥—é€£æ¥åœ¨ä¸€èµ·ï¼Œä¸¦åœ¨æ¯æ¬¡åŸ·è¡Œè§£ç¢¼æ­¥é©Ÿæ™‚å°‡å®ƒå€‘æä¾›çµ¦ç›®æ¨™åºåˆ— RNN-LMï¼Œéç¨‹å¦‚ä¸‹é¢çš„æ¨¡å‹åœ–è§£ã€‚
    > 
    >  Smart Compose RNN-LM æ¨¡å‹æ¶æ§‹ã€‚å°‡éƒµä»¶ä¸»é¡Œå’Œæ­¤å‰éƒµä»¶è¨Šæ¯é€²è¡Œç·¨ç¢¼ï¼Œæ¡ç”¨çš„æ–¹æ³•æ˜¯å°‡å®ƒå€‘çš„è©åµŒå¥—å¹³å‡åˆ†é…åœ¨æ¯ä¸€å€‹å€åŸŸå…§ã€‚éš¨å¾Œï¼Œå¹³å‡å¾Œçš„åµŒå¥—æœƒåœ¨æ¯æ¬¡åŸ·è¡Œè§£ç¢¼æ­¥é©Ÿæ™‚æä¾›çµ¦ç›®æ¨™åºåˆ— RNN-LMã€‚
    > 
    > ![](https://i0.wp.com/www.inside.com.tw/wp-content/uploads/2018/06/model3.png?resize=640%2C236)
    > 




