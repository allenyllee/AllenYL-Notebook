# 機器學習理論1-1_基礎知識

[toc]
<!-- toc --> 

# Reference

- [Machine Learning Foundations](https://www.slideshare.net/albertycchen/machine-learning-foundations-87730305)

- [When not to use deep learning](http://www.kdnuggets.com/2017/07/when-not-use-deep-learning.html#.WXYzsXf1Hgg.linkedin)

- [10 Famous Machine Learning Experts - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/10-famous-machine-learning-experts)

- [21 Great Articles and Tutorials on Time Series - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/21-great-articles-and-tutorials-on-time-series?utm_content=buffer13856&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)

- [How to Train a Final Machine Learning Model - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/how-to-train-a-final-machine-learning-model)

- [The AI Revolution: Why Deep Learning Is Suddenly Changing Your Life](http://fortune.com/ai-artificial-intelligence-deep-machine-learning/)

- [25 Timeless Data Science Articles - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/25-timeless-data-science-articles?utm_content=buffer6dcc9&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)

- [The major advancements in Deep Learning in 2016 - Tryolabs Blog](https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/)

- [So you are interested in deep learning · fast.ai](http://www.fast.ai/2016/12/19/favorite-posts/)

- [50 things I learned at NIPS 2016 – Ought](https://blog.ought.com/nips-2016-875bb8fadb8c)

- [How to Detect if Numbers are Random or Not - AnalyticBridge](https://www.analyticbridge.datasciencecentral.com/profiles/blogs/mysterious-sequences-that-look-random-with-surprising-properties)

- [標題黨太嚇人？這篇文章會告訴你DeepMind關係推理網絡的真實面貌 - 幫趣](http://bangqu.com/79N1q2.html)

- [DeepMind’s Relational Networks — Demystified – Hacker Noon](https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643)

- [Software 2.0 – Andrej Karpathy – Medium](https://medium.com/@karpathy/software-2-0-a64152b37c35)

- [The Emergence of Modular Deep Learning – Intuition Machine – Medium](https://medium.com/intuitionmachine/the-end-of-monolithic-deep-learning-86937c86bc1f)

- [Decoupled Neural Interfaces Using Synthetic Gradients | DeepMind](https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/)

- [Why AlphaGo Zero is a Quantum Leap Forward in Deep Learning](https://medium.com/intuitionmachine/the-strange-loop-in-alphago-zeros-self-play-6e3274fcdd9f)

- [Is Deep Learning Innovation Just Due to Brute Force?](https://medium.com/intuitionmachine/the-brute-force-method-of-deep-learning-innovation-58b497323ae5)

- [Is Deep Learning “Software 2.0”? – Intuition Machine – Medium](https://medium.com/intuitionmachine/is-deep-learning-software-2-0-cc7ad46b138f)


- [打響新年第一炮，Gary Marcus提出對深度學習的系統性批判 - 幫趣](http://bangqu.com/p8Nr93.html)

- [有哪些职业容易被人工智能替代，又有哪些行业不易被人工智能替代？ - 知乎](https://www.zhihu.com/question/61829257/answer/231679440)


- [[科普]如何使用高大上的方法调参数 - 知乎专栏](https://zhuanlan.zhihu.com/p/27555858)


- [如果机器人有了思想，我们该怎么办？ - 知乎专栏](https://zhuanlan.zhihu.com/p/28372529)


- [面试官如何判断面试者的机器学习水平？ - 知乎](https://www.zhihu.com/question/62482926/answer/210531386)


- [机器学习中如何做单元测试(Unit Test)来检测模型稳定性？ - 知乎](https://www.zhihu.com/question/20225527/answer/209958746)

- [如何评价多伦多大学新建的向量学院 (Vector Institute)？对人工智能领域会有何影响? - 知乎](https://www.zhihu.com/question/57768703/answer/209013477)


- [反欺诈(Fraud Detection)中所用到的机器学习模型有哪些？ - 知乎](https://www.zhihu.com/question/30508773/answer/205831957)


- [带你读机器学习经典(一): An Introduction to Statistical Learning (Chapter 1&2) - 知乎专栏](https://zhuanlan.zhihu.com/p/27556007)


- [带你读机器学习经典(二): An Introduction to Statistical Learning (Chapter 3.1 线性回归) - 知乎专栏](https://zhuanlan.zhihu.com/p/27609785)


- [随机森林是否需要后剪枝？sklearn为什么没有实现这个功能，是否有人实现了这个功能？ - 知乎](https://www.zhihu.com/question/59826974/answer/175520871)

- [K-means聚类算法如何应对数据的噪音和离散特征处理的问题？ - 知乎](https://www.zhihu.com/question/60868444/answer/188816384)

- [试玩人脸识别 - 知乎专栏](https://zhuanlan.zhihu.com/p/27275307)

- [R 語言使用者的 Python 學習筆記 系列文章列表 - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天](http://ithelp.ithome.com.tw/users/20103511/ironman/1077)

- [[第 25 天] 機器學習（5）整體學習(Ensemble learning) - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天](http://ithelp.ithome.com.tw/articles/10187452)


- [一文读懂深度学习与机器学习的差异 - 技术翻译 - 开源中国社区](https://www.oschina.net/translate/deep-learning-vs-machine-learning?lang=chs&page=1#)

- [Comparison between Deep Learning & Machine Learning](https://www.analyticsvidhya.com/blog/2017/04/comparison-between-deep-learning-machine-learning/?utm_source=dzone&utm_medium=social)

# 大師觀點


## [Yann LeCun - Quora](https://www.quora.com/profile/Yann-LeCun)


- [Is industry demand for ML researchers mostly limited to giants like Facebook and the occasional startup, or is it more ubiquitous? - Quora](https://www.quora.com/Is-industry-demand-for-ML-researchers-mostly-limited-to-giants-like-Facebook-and-the-occasional-startup-or-is-it-more-ubiquitous)

    > For ML engineers, it’s completely ubiquitous. Everyone wants to hire ML engineers.
    > 
    > For researchers, only a small number of large and prominent companies have real research labs: Facebook, Google, IBM, Microsoft, Adobe and a few others.


- [Is getting a masters or a PhD necessary to get into top AI/ML research groups like FAIR or DeepMind?](https://www.quora.com/Is-getting-a-masters-or-a-PhD-necessary-to-get-into-top-AI-ML-research-groups-like-FAIR-or-DeepMind)

    > There are 6 types of positions at FAIR:
    > 
    > * Research Scientist: you need a PhD, a couple years of experience in research (e.g. as a postdoc) and a good publication record. It’s a pretty high bar.
    > * Research Engineer: you need a Master with some exposure to ML/AI in your previous studies or job. Most of these positions are relatively junior, but there are a few senior. About 25 to 30% of people at FAIR are research engineers.
    > * Postdoc: it’s a 1 or 2 year limited-term research position, generally directly after your PhD.
    > * PhD student: in our Paris lab, we can take a small number of PhD students under what’s called a CIFRE status. This is a special thing in France that allows PhD students to spend most of their time in an industry research lab, co-advised by a researcher in the company and a professor in a university.
    > * Intern: we take summer interns, and sometimes interns during the academic year. Almost all of them are in PhD programs. In continental Europe where people do undergrad + 2 year Master + 3 year PhD, we take some interns between their Master and PhD.
    > 
    > Look here for a list of FAIR member and their backgrounds: Researchers
    > 


- [What is the primary focus of Facebook AI research? - Quora](https://www.quora.com/What-is-the-primary-focus-of-Facebook-AI-research)

    > Solving intelligence and building truly intelligent machines.
    > 
    > We are working on getting learning machine to model their environment, to remember, to reason, and to plan.
    > 
    > For this, we use video games (we have hooked up the Unreal 3D game engine tot Torch, the deep learning environment), and various real and virtual environments.
    > 
    > We also work on applications of AI in image and video understanding, text understanding, dialog systems, language translation, speech recognition, text generation, and other more esoteric domains.



- [Is deep learning a bubble nowadays? Everybody tries to use it in different fields extensively, without knowing what they do. - Quora](https://www.quora.com/Is-deep-learning-a-bubble-nowadays-Everybody-tries-to-use-it-in-different-fields-extensively-without-knowing-what-they-do)

    > There is a lot of hype around AI and deep learning at the moment. Hype is bad because it creates heightened expectations and causes disappointment when these expectations are not met. This is partly what caused “AI winters” in the past.    
    > 
    > So, if you see some egregious hype, call it for what it is. I try to do that whenever I can. 
    > 
    > Startups have a huge incentive to hype what they can do because they need to attract investments or customers. It’s not because a company attracts investments that it’s not hyped: a number of AI companies that have attracted large investments are nothing more than empty hype machines.    
    > 
    > That said, deep learning produces real results and is at the root of a real industry that makes money today. The promises of it in the near future are very exciting (even without the hype) in areas like self-driving cars, medical imaging, personalized medicine, content filtering/ranking, etc.
    > 



- [What are the likely AI advancements that we will see between now and 2027? - Quora](https://www.quora.com/What-are-the-likely-AI-advancements-that-we-will-see-between-now-and-2027)

    > There is a number of areas on which people are working hard and making promising advances:
    > 
    > * deep learning combined with reasoning and planning
    > * deep model-based reinforcement learning (which involved unsupervised predictive learning)
    > * recurrent neural nets augmented with differentiable memory modules (e.g. Memory Networks:
    > - Memory Networks (FAIR): https://scholar.google.com/citat...
    > - Stack-Augmented RNN (FAIR): Google Scholar Citations
    > - Neural Turing Machine (DeepMind): [1410.5401] Neural Turing Machines
    > - End-toEnd MemNN (FAIR/NYU): https://scholar.google.com/citat...
    > - and the flurry of follow-up papers.
    > * generative/predictive models trained with adversarial training
    > * “differentiable programming”: this is the idea of viewing a program (or a circuit) as a graph of differentiable modules that can be trained with backprop. This points towards the possibility of not just learning to recognize patterns (as with feed-forward neural nets) but to produce algorithms (with loops, recursion, subroutines, etc). There are a few papers on this from DeepMind, FAIR and others, but it’s rather preliminary at the moment.
    > * Hierarchical planning and hierarchical reinforcement learning: this is the problem of learning decompose a complex task into simpler subtasks. It seems like a requirement for intelligent systems.
    > * Learning predictive models of the world in an unsupervised fashion (e.g. video prediction)
    > 
    > If significant progress is made along these directions in the next few years, we might see the emergence of considerably more intelligent AI agents for dialog systems, question-answering, adaptive robot control and planning, etc.
    > 
    > A big challenge is to devise unsupervised/predictive learning methods that would allow very large-scale neural nets to “learn how the world works” by watching videos, reading textbooks, etc, without requiring explicit human-annotated data.
    > 
    > This may eventually lead to machines that have learned enough about the world that we see them as having “common sense”.
    > 
    > It may take 5 years, 10 years, 20 years, or more. We don’t really know.



- [What are your recommendations for self-studying machine learning? - Quora](https://www.quora.com/What-are-your-recommendations-for-self-studying-machine-learning)

    > There is tons of on-line material, tutorials and courses on ML, including Coursera lectures.
    > 
    > I’ll respond more specifically for deep learning. You can get a broad idea of deep what deep learning is about through tutorial lectures that are available from the Web. Most notably:
    > 
    > an overview paper in Nature by myself, Yoshua Bengio and Geoff Hinton with lots of pointers to the literature: https://scholar.google.com/citat...
    > The Deep Learning textbook by Goodfellow, Bengio and Courville: Deep Learning
    > A recent series of 8 lectures on deep learning that I gave at Collège de France in Paris. The lectures were taught in French and later dubbed in English:
    > French version: Accueil
    > English version: Home
    > the Coursera course on neural nets by Geoff Hinton (starting to be a bit dated).
    > the lectures from the 2012 IPAM Summer School on Deep Learning: Graduate Summer School: Deep Learning, Feature Learning (Schedule) - IPAM
    > my 2015 course on Deep Learning at NYU: deeplearning2015:schedule | CILVR Lab @ NYU (unfortunately, the videos of the lectures had to be taken down due to stupid legal reasons, but the slides are there). I’m teaching this course again in the Spring of 2017.
    > The 2015 deep learning summer school: Deep Learning Summer School, Montreal 2015
    > Various tutorials generally centered on using a particular software platform, like Torch, TensorFlow or Theano.



- [What’s your advice for undergraduate student who aspires to be a research scientist in deep learning or related field one day?](https://www.quora.com/What%E2%80%99s-your-advice-for-undergraduate-student-who-aspires-to-be-a-research-scientist-in-deep-learning-or-related-field-one-day)

    - [大神Yann LeCun亲授：如何自学深度学习技术并少走弯路 | 雷锋网](https://www.leiphone.com/news/201611/cWf2B23wdy6XLa21.html)

    > (0) take all the continuous math and physics class you can possibly take. If you have the choice between “iOS programming” and “quantum mechanics”, take “quantum mechanics”. In any case, take Calc I, Calc II, Calc III, Linear Algebra, Probability and Statistics, and as many physics courses as you can. But make sure you learn to program.
    > (1) Take an AI-related problem you are passionate about.
    > (2) think about it on your own
    > (3) once you have formed your own idea of it, start reading the literature on the problem
    > (4) you will find that (a) your ideas were probably a bit naive but (b) your view of the problem is slightly different from what was done before.
    > (5) Find a professor in your school that can help you make your ideas concrete. It might be difficult. Professors are busy and don’t have much time for undergrads. The ones with the most free time are the very junior, the very senior, and the ones who are not very active in research.
    > (6) If you don’ find a professor with spare time, hook up with a postdoc or PhD student in his/her lab.
    > (7) ask the professor if you can attend his/her lab meetings and seminars or sit in his/her class.
    > (8) Before you graduate, try to write a paper about your research or release a piece of open source code.
    > (9) Now apply to PhD programs. Forget about the “ranking” of the school for now. Find a reputable professor who works on topics that you are interested in. Pick a person whose papers you like or admire.
    > (10) Apply to several PhD programs in the schools of the above-mentioned professors and mention in your letter that you’d like to work with that professor but would be open to work with others.
    > (11) ask your undergrad professor to write a recommendation letter for you. It’s maximally efficient if your undergrad professor is known by your favorite PhD advisor.
    > (12) if you don’t get accepted in one of your favorite PhD programs, get a job at Facebook or Google and try to get a gig as an engineer assisting research scientists at FAIR or Google Brain.
    > (13) publish a papers with the research scientists in question. Then re-apply to PhD programs and ask the FAIR or Google scientists you work with to write a recommendation letter for you.
    > 
    > ---
    > Why is physics important?
    > 
    > 
    > Because physics has invented a lot of mathematical methods to model the real world. Foe example, the mathematics of BayesIan inference is essentially identical to statistical mechanics. Backpropagation can be shown to be a simple application of Lagrangian methods developed in classical mechanics. The forward algorithm in graphical models is a path integral (widely used in quantum mechanics). Physics teaches you about the use of Fourier transforms (the root of Heisenberg uncertainty principle), vector files as gradients of a potential function, maximum entropy principle, partition functions, Monte Carlo methods, annealing,, Gibbs Boltzmann distributions, dynamical systems, chaos, etc....
    > 

## [微调 - 知乎](https://www.zhihu.com/people/breaknever/activities)

- [未來 3 至 5 年，哪個方向的機器學習人才最缺？ | TechNews 科技新報](https://technews.tw/2017/09/11/what-kind-of-people-does-machine-learning-need/)

    > 1. 基本功
    > 
    >    說到底機器學習還是需要一定的專業知識，這可以透過學校學習或自學完成。但有沒有必要通曉數學，擅長最佳化呢？我的看法是不需要，大前提是要了解基本的數學統計知識即可，更多討論可看我在「阿薩姆：如何看待『機器學習不需要數學，很多算法封裝好了，調個包就行』這種說法？」的答案。最低程度我建議掌握五個小方向，對於現在和未來幾年內的業界夠用了。再次重申，我對演算法的看法是大部分人不要造輪子、不要造輪子、不要造輪子！只要理解自己在做什麼，知道選什麼模型，直接呼叫 API 和現成的工具包就好了。
    > 
    >    回歸模型（Regression）。學校課程其實講更多分類，但事實上回歸才是業界最常見的模型。比如產品定價或預測產品的銷量都需要回歸模型。現階段比較流行的回歸方法是以數為模型的 xgboost，預測效果很好，還可以自動排序變數重要性。傳統的線性回歸（一元和多元）也還會繼續流行下去，因為良好的可解釋性和低運算成本。如何掌握回歸模型？建議閱讀《Introduction to Statistical Learning》的 2-7 章，並看一下 R 裡 xgboost 的 package 介紹。
    > 
    >    分類模型（Classification）。老生常談，但應該對現在流行並繼續流行下去的模型有深刻了解。舉例，隨機森林（Random Forests）和支援向量機（SVM）都屬於現在業界常用的演算法。可能很多人想不到的是，邏輯回歸（Logistic Regression）這個常見於大街小巷每本教科書的經典老演算法，依然占據業界半壁江山。這個部分建議看李航《統計學習算法》，挑著看相對應的那幾章即可。
    > 
    >    神經網路（Neural Networks）。我沒有把神經網路歸結到分類算法還是因為現在太紅了，有必要學習了解一下。隨著硬體能力的持續增長和資料集愈豐富，神經網路在中小企業的發揮之處肯定有。三、五年內，這個可能會發生。但有人會問，神經網路內容那麼多，比如架構，比如正則化，比如權重起始化技巧和觸發函數選擇，我們該學到什麼程度呢？我的建議還是抓住經典，掌握基本的三套網路：a. 普通的 ANN。b. 處理影像的 CNN。c. 處理文字和語音的 RNN（LSTM）。對每個基本網路只要了解經典的處理方式即可，具體可參照《深度學習》的 6~10 章和吳恩達的 Deep Learning 網路課程。
    > 
    >    資料壓縮／可視化（Data Compression & Visualization）。業界常見的就是先可視化資料，比如這兩年很紅的流形學習（manifold learning）就和可視化有很大的關係。業界認為做可視化是磨刀不誤砍柴工，把高維資料壓縮到 2 維或 3 維，可很快看到一些有意思的事，能節省大量時間。學習可視化可以使用現成的工具，如 Qlik Sense 和 Tableau，也可用 Python 的 Sklearn 和 Matplotlib。
    > 
    >    無監督學習和半監督學習（Unsupervised & Semi-supervised Learning）。業界另一個特點就是大量資料缺失，大部分情況都沒有標籤。以最常見的反詐騙為例，有標籤的資料非常少。所以一般都需要使用大量的無監督或半監督學習，來利用有限的標籤學習。多說一句，強化學習在大部分企業使用基本為 0，估計未來很長一陣子可能都不會有特別廣泛的應用。
    > 
    >    基本功的意義是當你面對具體問題時，很清楚可用什麼武器對付。上面介紹的很多工具都有幾十年歷史，依然歷久彌新。所以 3~5 年的跨度來看，這些工具依然非常有用，甚至像 CNN 和 LSTM 之類的深度學習演算法還在繼續發展。無論你還在學校或已工作，掌握這些基本技術，都可以透過自學在幾個月到一兩年內完成。
    > 
    > 
    > 2. 祕密武器
    > 
    > 
    >    已有工作／研究經驗的朋友，要試著利用自己的工作經歷。舉例，不要做機器學習裡最擅長投資的人，而要做金融領域中最擅長機器學習的專家，這才是你的價值主張（value proposition）。最重要的是，機器學習的基本功沒有大家想的那麼高不可攀，沒有必要放棄自己的專業全職轉行，沉沒成本太高。透過跨領域完全可做到曲線救國，化劣勢為優勢，你們可能比只懂機器學習的人有更大的產業價值。
    > 
    >    舉幾個我身邊的例子。一個朋友是做傳統軟體工程研究，前年他和我商量如何使用機器學習以 GitHub 上的 commit 歷史來辨識 bug，這就是一個結合領域的好知識。如果你本身是金融出身，在你補足基本功同時，就可以把機器學習交叉用於你擅長的領域，做策略研究，我已經聽說無數個「宣稱」使用機器學習實現交易策略的案例。雖不可盡信，但對特定領域的深刻理解往往就是捅破窗戶的最後一層紙，只理解模型但不了解資料和背後的意義，導致很多機器學習模型只停留在好看卻不實用的階段。
    > 
    > 
    > 3. 彈藥補給
    > 
    >    沒有什麼不會改變，這個時代的科技更新速度很快。從深度學習開始發力到現在也不過短短十年，所以沒有人知道下一個紅的是什麼。以深度學習為例，這兩年非常紅的對抗生成網路（GAN）、多目標學習（multi-lable learning）、遷移學習（transfer learning）都還在飛速發展。關於深度學習為什麼有良好泛化能力的理論猜想文章，在最新的 NIPS 聽說也收錄了好幾篇。這都說明了沒什麼產業可以靠吃老本瀟灑下去，我們需要追新的熱點。但機器學習的範圍和領域真的很廣，上面所說的都還是有監督的深度學習，無監督的神經網路和深度強化學習也是現在火熱的研究領域。所以我的建議是盡量關注、學習了解已成熟和已有實例的新熱點，不要凡熱點必追。
    > 


- [如何看待「机器学习不需要数学，很多算法封装好了，调个包就行」这种说法？ - 知乎](https://www.zhihu.com/question/60064269/answer/172305599)

    > 我认为：大部分机器学习从业者不需要过度的把时间精力放在数学上，而该用于熟悉不同算法的应用场景和掌握一些调参技巧。好的数学基础可以使你的模型简洁高效，但绝非必要的先决条件。
    > 
    > 
    > 
    > 原因如下：
    > 
    > 1. 即使你有了一定的数学功底，还是不知道怎么调参或者进行优化。这话说的虽然有点自暴自弃，但扪心自问在座的各位，当你发现accuracy不好、loss很高、模型已经overfitting了，你唰唰唰列列公式玩玩矩阵就知道问题出在哪里了吗？不一定。诚然，懂得更多的统计原理可以帮助推测问题出在了哪里，你可能换了一个loss function或者加了新的regularizer，但结果不一定会更好:(
    > 
    >    数学基础之于机器学习从业者很像debugger之于码农，它给了你方向，但不能保证你一定可以解决问题。那怎么能解决问题？只有经验经验经验，别无他法，有时候甚至靠的是直觉。数学基础是好的内功基础，但你调包调多了，其实也慢慢能抓到一些感觉，不必看不起“调包侠”。
    > 
    > 2. 工业界可以应用的模型是很有限的，可调的参数也是有限的。工业界选择模型非常看重可解释性，效率，以及和整个系统的整合能力。举例，在我的工作中，大部分时间都在使用Regression和Decision Tree相关的算法（如 Random Forests）。是因为这两个算法最好么？不，恰恰是因为这两个算法稳定及高效，而且容易解释。对于这样的模型，你即使数学能力很强，能调整的参数也是有限的。根据网上的例子和经验，大量的工程师可以在数学基础稍弱的情况下做到很好的效果。
    > 
    > 3. 数学/统计知识已经成了既得利益者刻意为外来者建立的一道壁垒。不知道大家有多少人是从事过ML研究的。我个人的观察是做出成绩的ML研究人员是有限的，科班出身的researcher更是远远无法工业界的空缺。所以大家没有必要担心会被转行者抢了饭碗，也没有必要刻意鼓吹一定要懂矩阵，凸优化，等数学知识才配做机器学习。大家都是出来卖的，不必互相为难。说来惭愧，在工作中我常常跟老板说这个人不能用，你要用我这种科班出身的人，但我内心是不赞同的。
    > 
    >    每当我看到知乎上有人问机器学习怎么入门，结果大家立马推荐第一本就看PRML和Statitical Learning以及一大堆公开课和数学课的时候，我的内心是崩溃的。各位答主的目标是把所有人都吓回去还是秀一下优越感？
    > 
    > 4. 理论模型和实际应用分的是两块不同的蛋糕。承接第2,3点，做理论研究的发力于突破，提出新的模型或者优化方法，做应用的致力于把模型应用于数据上，攫取商业价值。这两者不存在利益冲突，做理论的人有自带正统光环的优势，所以更该显得大度一些。只有“调包”的人越来越多，这个行业才会繁荣，因为证明技术落了地，可以带来实际价值。
    > 
    > 5. 行业的发展趋势是降低工具的使用难度，这让我们不必反复造轮子。亚马、逊谷歌、微软等各大平台都开放了他们的机器学习工具。以前人们还需要自己写各种模型，好一些的调一下sklearn，但现在Azure ML Studio已经方便到零代码了。年初的时候，我试了一下ML studio，简直方便的可怕，完全是图形拖动连接就可以建立模型，那一刻我仿似看到了自己即将失业。
    > 
    > 6. 文艺一点说，我们需要更包容的心态，切勿文人相轻。想要接触了解一门学科，应该先有兴趣，才有探索的积极性。就像我们第一次看到Hello word出现的样子，很多刚入行的人第一次看到机器学习能解决实际问题时，会产生浓厚的兴趣。
    > 


- [未来的人工智能有哪些商业模式？ - 知乎](https://www.zhihu.com/question/41848628/answer/221088507)

    > 3. 人工智能咨询与定制服务(AI Consulting and Customized Service)
    > 
    >    根据我自己的观察和分析，AI咨询和定制服务是未来很有潜力的模型。简单来说，就是根据企业/客户的需求进行定制化的人工智能解决方案。在现阶段，人工智能方案对于大部分企业来说还是“奢侈品”，甚至有些超前。但在不久的未来随着技术进一步成熟以及概念得到普及，价格和门槛也会下降，越来越多的中小型企业也可以负担并愿意进行人工智能升级。
    > 
    >    和创业公司不同，这个商业模型不要求高精尖技术或是在某个领域的突破，但通用的AI平台也无法完成客户定制的需求。这就是为什么这样的商业服务可能有前景 - 它和前两种商业模型有交集但并不重叠。
    > 
    >    这样的商业模型主要给客户提供两种服务：
    > 
    >    成熟的专利AI应用。举例，我们为A银行安装了一个我们开发并拥有专利的人工智能风控模型，在进行数据替换后还可以卖给B、C、D银行或者相似行业。银行可以使用我们的微调后的模型，但我们可以将原始模型进行无限次转卖。
    >    
    >    客户定制化服务。举例，A客户要求我们为它们独家定制服务，服务的归属权归客户所有，我们无权转卖，仅为客户进行维护升级。当然，这种服务的价格肯定较高。
    > 
    >    同时提供两种收费模式：
    > 
    >    一次性收费/升级费用(one-time purchase)。和其他软件产品一样，客户可以一次性买断服务的使用权。但并不建议这个模式，因为AI产品有较大的不稳定性，随着数据的变化模型可能失效。
    >    
    >    订阅服务(subscription based)。正因为AI产品需要常常升级，机器学习模型也需要重新训练，订阅服务更适合AI类产品。客户可以按月付费，得到相应的维护和升级服务。
    > 
    >    这样的商业模型还可以搭配主动式的营销手段。因为AI产品的本质是通过数据解决问题，据我所知很多企业现在已经和客户签署了“数据保留协议”，即AI产品供应商可以在特定范围内使用客户的数据进行其他活动。这样的协议有两个好处:
    > 
    >    精准营销(Customized Recommendation)。因为我们有权使用客户A的数据，根据分析其数据，我们可以个性化推荐适合客户A的其他产品。甚至我们可以使用客户A的数据为其免费定制一个概念产品。免费其实是一种营销手段，德勤的数据分析部门给客户50小时的免费时长来感受它们的产品。
    >    
    >    数据整合(Data Integration & Enrichment)。假设客户A、B、C和D都允许我们保留并使用其数据，那么我们可以进行整合并获得行业级别的数据，从而开发出更加智能的产品。
    > 
    >    在这个数据为王的时代，拥有客户的数据并提供定制化服务有非常强的客户黏性。总结一下，销售成熟的AI产品+适量的定制，留住客户的数据，并提供后续的维护和支持就是我觉得很有潜力的新型AI领域商业模型。
    > 
    >    从市场竞争角度来说，这个商业模型既不需要高精技术，也不大需要基础平台或者高额的固定投资，甚至还可以使用文中介绍的创业公司和科技巨头的服务。但根据经济学原理，低门槛，充分竞争的市场代表从长期来看不会有暴利存在。
    > 
    >    但如果能在早期拥有足够多的行业数据，数据优势将会使你的企业走在其他人之前。或许，是时候入场了...
    > 


- [有哪些职业容易被人工智能替代，又有哪些行业不易被人工智能替代？ - 知乎](https://www.zhihu.com/question/61829257/answer/231679440)

    > 所以在我看来，一个行业/职业达到被人工智能取代至少需要满足以下三个条件:
    > 
    > 1. 结构化的数据和良好的数据积累
    > 2. 清晰明确的任务定义
    > 3. 可接受的回报周期和高利润率
    > 
    > 因此总结来看，满足以上条件的暂时有:
    > 
    > 医疗：疾病检索、早期预测、手术机器人(勉强把机器人算作人工智能)
    > 翻译/律师：作为收入比较丰厚的行业，中低端的工作会被人工智能代替
    > 金融：作为数据结构化比较好且资金丰厚投入好的行业，越来越多的金融分析师都迎来了失业
    > 驾驶：其实运输行业是大家不大了解的体量巨大回报丰厚的行业，这也是为什么无人驾驶被那么多公司追捧
    > 高危行业：高危行业约等于高回报率，如矿洞机器人和高空作业机器人。
    > 

## 吳恩達

- [刚刚，吴恩达讲了干货满满的一节全新AI课，全程手写板书 - 量子位 - 知乎专栏](https://zhuanlan.zhihu.com/p/29504856?utm_medium=social&utm_source=ZHShareTargetIDMore)

    > 做AI产品要注意什么？
    > 
    > 有一个很有意思的趋势，是AI的崛起正改变着公司间竞争的基础。
    > 公司的壁垒不再是算法，而是数据。
    > 当我建立一家新公司，会特地设计一个循环：
    > 
    > 先为算法收集足够的数据，这样就能推出产品，然后通过这个产品来获取用户，用户会提供更多的数据……
    > 有了这个循环之后，对手就很难追赶你。
    > 
    > 这方面有一个很明显的例子：搜索公司。搜索公司有着大量的数据，显示如果用户搜了这个词，就会倾向于点哪个链接。
    > 我很清楚该如何构建搜索算法，但是如果没有大型搜索公司那样的数据集，简直难以想象一个小团队如何构建一个同样优秀的搜索引擎。这些数据资产就是最好的壁垒。
    > 
    > 工程师们还需要清楚这一点：
    > 
    > AI的范围，比监督学习广泛得多。我认为人们平时所说的AI，其实包含了好几类工具：比如机器学习、图模型、规划算法、知识表示（知识图谱）。
    > 人们的关注点集中在机器学习和深度学习，很大程度上是因为其他工具的发展速度很平稳。
    > 如果我现在建立一个AI团队，做AI项目，很多时候应该用图模型，有时应该用知识图谱，但是最大的机遇还是在于机器学习，这才是几年来发展最快、出现突破的领域。
    > 
    > 接下来我要和大家分享一下我看问题的框架。
    > 
    > 计算机，或者说算法是怎样知道该做什么的呢？它有两个知识来源，一是数据，二是人工（human engineering）。
    > 要解决不同的问题，该用的方法也不同。
    > 比如说在线广告，我们有那么多的数据，不需要太多的人工，深度学习算法就能学得很好。
    > 
    > 但是在医疗领域，数据量就很少，可能只有几百个样例，这时就需要大量的人工，比如说用图模型来引入人类知识。
    > 也有一些领域，我们有一定数量的数据，但同时也需要人工来做特征工程。
    > 
    > 当然，还要谈一谈工程师如何学习。
    > 
    > 很多工程师想要进入AI领域，很多人会去上在线课程，但是有一个学习途径被严重忽视了：读论文，重现其中的研究。
    > 当你读了足够多的论文，实现了足够多的算法，它们都会内化成你的知识和想法。
    > 
    > 要培养机器学习工程师，我推荐的流程是：上（deeplearning.ai的）机器学习课程来打基础，然后读论文并复现其中的结果，另外，还要通过参加人工智能的会议来巩固自己的基础。
    > 
    > 
    > △ 再擦一块白板（×3）
    > 怎样成为真正的AI公司？
    > 
    > 我接下来要分享的这个观点，可能是我今天所讲的最重要的一件事。
    > 从大约20年、25年前开始，我们开始看见互联网时代崛起，互联网成为一个重要的东西。
    > 我从那个时代学到了一件重要的事：
    > 
    > 商场 + 网站 ≠ 互联网公司
    > 
    > 我认识一家大型零售公司的CIO，有一次CEO对他说：我们在网上卖东西，亚马逊也在网上卖东西，我们是一样的。
    > 不是的。
    > 
    > 互联网公司是如何定义的呢？不是看你有没有网站，而是看做不做A/B测试、能不能快速迭代、是否由工程师和产品经理来做决策。
    > 
    > 这才是互联网公司的精髓。
    > 
    > 现在我们经常听人说“AI公司”。在AI时代，我们同样要知道：
    > 
    > 传统科技公司 + 机器学习/神经网络 ≠ AI公司（全场笑）
    > 
    > 公司里有几个人在用神经网络，并不能让你们成为一家AI公司，要有更深层的变化。
    > 20年前，我并不知道A/B测试对互联网公司来说有多重要。现在，我在想AI公司的核心是什么。
    > 
    > 我认为，AI公司倾向于策略性地获取数据。我曾经用过这样一种做法：在一个地区发布产品，为了在另一个地区发布产品而获取数据，这个产品又是为了在下一个地区发布产品来获取数据用的，如此循环。而所有产品加起来，都是为了获取数据驱动一个更大的目标。
    > 像Google和百度这样的大型AI公司，都有着非常复杂的策略，为几年后做好了准备。
    > 
    > 第二点是比较战术性的，你可能现在就可以开始施行：AI公司通常有统一的数据仓库。
    > 
    > 很多公司有很多数据仓库，很分散，如果工程师想把这些数据放在一起来做点什么，可能需要和50个不同的人来沟通。
    > 所以我认为建立一个统一的数据仓库，所有的数据都存储在一起是一种很好的策略。
    > 另外，普遍的自动化和新的职位描述也是AI公司的重要特征。
    > 
    > 比如说在移动互联网时代，产品经理在设计交互App的时候可能会画个线框图：
    > 然后工程师去实现它，整个流程很容易理清楚。
    > 
    > 但是假设在AI时代，我们要做一个聊天机器人，这时候如果产品经理画个线框图说：这是头像，这是聊天气泡，并不能解决问题。
    > 聊天气泡长什么样不重要，我需要知道的是，这个聊天机器人要说什么话。线框图对聊天机器人项目来说没什么用。
    > 如果一个产品经理画了个无人车的线框图，说“我们要做个这个”，更是没什么用。（全场笑）
    > 
    > 在AI公司里，产品经理在和工程师沟通的时候，需要学会运用数据，要求精确的反馈。

## 知乎

- [深度学习-程序员的学习路径 - 知乎专栏](https://zhuanlan.zhihu.com/p/29349938)

    > 数学基础。如果你去读深度学习的论文，会发现数学对于DL非常重要，线性代数、概率论、甚至微积分都有用武之地。这些知识都还给学校了怎么办？难道要把所有这些课程再学一遍？大可不必。只要把DL需要用到的部分好好复习一下就好。这里推荐《Deep Learning》这本权威著作的第一部分，详述了机器学习需要的数学基础，另外也讲了机器学习领域的很多基本概念。
    > 
    > 一本入门教材。虽然上面提到的《Deep Learning》是公认的DL最权威教材，但是一般人会觉得过于艰深，包含太多数学方面的论证。如果你对数学公式不太感冒，建议不要用这本书入门。我推荐一本《Hands on Machine Learning》。不要被书的名字欺骗了，本书的内容其实一点也不初级，讲述的很全面深入。但是语言非常流畅，容易读懂。这本书分为两部分，第一部分讲述了机器学习的各种传统算法，第二部分才是深度学习的内容。传统算法的学习很有必要，一方面帮助我们理解ML发展的脉络，另一方面，很多传统算法其实并没有被淘汰，比如RandomForest这种基于决策树的算法，在结构化数据的挖掘方面非常有效。
    > 
    > 一个入门课程。这里我提供两个选项：
    > 
    > 第一推荐Andrew Ng刚刚上线的Deeplearning.ai。Andrew Ng的课，品质应该不会差，而且估计他会加入很多最前沿的研究。这门课刚刚开课，应该会持续几个月的时间。所以要做好长期学习的准备。
    > 
    > 第二个是我上过的fast.ai。这门课的讲师也是个牛人，曾负责Kaggle平台的研发。这个课程更加注重实践，在讲解概念的同时，用Keras+Jupyter Notebook直接演示模型的训练。总共只有7节课，每课时两个小时，但是每节课的Notes里面附带大量的参考资料，需要花很多时间去自学、消化。这门课现在开始了第二学期，讲DL最前沿的研究进展，也是7节课。另外，需要习惯一下讲师的澳洲英语。
    > 
    > 选一个方向。深度学习可应用的领域很多，我们需要选择其中一个深入研究。方向包括：计算机视觉（图像、视频处理，主要用CNN）、自然语言处理NLP（包括文本、语音处理，序列数据往往需要RNN）、增强学习（用在机器人、自动驾驶等方面），此外对于生成模型（GAN、VAE等等）的研究也是一个热点。
    > 
    > 读一些论文。在选定方向以后，我们可以去读一下这个方向的经典论文。说是经典，但是深度学习这个方向真正爆发也就是最近几年的事情，所以很多东西其实都是前两年的论文结果。推荐一个Awesome Deep Learning Papers的论文列表，个人觉得整理得不错，有参考价值。DL这个领域进展特别快，前几年的研究结果可能早已经被超越了，所以读论文要保持开放心态。不过，论文有的时候真的很啰嗦，幸好有网友总结了一下：aleju/papers，先读这个总结，带着问题再去读论文效果好很多。
    > 
    > 选一个框架。DL现在有很多非常成熟的框架了，每个科技公司都有自己的一套东西，Google系的TensorFlow现在似乎风头更劲一些。他们之间的比较可以看看这篇文章。我觉得这个跟编程语言之争一样，没必要太纠结，选一个自己喜欢的就好。各个框架训练出来的模型一定可以互相转换的。
    > 
    > 动手做一些深度学习项目。网上有很多开放的数据集，可以拿来做训练，先做些简单的，比如MNIST，IMDB影评的情感分析等。然后可以去Kaggle上做一些以前的的竞赛项目，比如Cats VS Dogs一类的。如果你足够厉害，可以参加当前的Kaggle挑战，说不定顺便赢个几十万美金呢：）
    > 
    > 最后，关注一下DL最新的研究动向。这方面的媒体非常多，公众号、知乎专栏，一搜一大把，还有很多科技博客也是频繁更新。我推荐一个newsletter：import AI，很多人都觉得不错，一周一次，读起来也不会有太大负担。


- [有哪些你看了以后大呼过瘾的数据分析书？ - 知乎](https://www.zhihu.com/question/60241622/answer/174310709)

    > 推荐两本真正意义上的神书，一本是《Pattern Recognition and Machine Learning》（PRML），一本是《Deep Learning》（Ian Goodfellow和Yoshua Bengio写的那本），绝对符合题主“看了以后大呼过瘾”的标准。这两本书都是属于“大而全”的类型，两本书都是从头到尾讲清楚了一个领域的细节。PRML是传统机器学习，《Deep Learning》是讲的最近几年兴起的深度学习。
    > 
    > 说了那么多，其实我想表达的是：与其对很多模型一知半解，不如真正学懂一个模型，这样其实是节省时间的。原因在于当你搞懂一个模型后，就算你忘记了某些细节，再次查看资料也可以很快回忆起来。而当你不懂一个模型时，每次回忆都要从零开始，来来回回其实浪费了很多时间。而PRML和《Deep Learning》就是让你真正理解机器学习算法的最佳途径之一，强烈推荐。



# 機器學習分類

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > ![](https://pic2.zhimg.com/80/v2-2f2b4f460b5490102c6901c9350895f7_hd.jpg)
    > 
    > 比喻
    > ==
    > 
    > 例如学英语。有监督学习是先读几篇中英文对照的文章，从而学会阅读纯英语文章。无监督学习是直接阅读大量纯英文文章，当数量达到一定程度，虽然不能完全理解文章，但也会发现一些词组的固定搭配，句式等等。
    > 
    > 举例
    > ==
    > 
    > -   有监督学习：学认字
    > 
    > -   无监督学习：自动聚类
    > 
    > -   增强学习：学下棋

## 有監督學習

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > **有监督学习**是机器学习任务的一种。它从有标记的训练数据中推导出预测函数。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话：**给定数据，预测标签**。
    > 
    > -   有标签
    > 
    > -   直接反馈
    > 
    > -   预测未来结果
    > 
    > ---
    > 算法:
    > 
    > -   分类
    > 
    > -   回归
    > 

- [【講講科普】 當你有了三個孩子他們分別叫監督式學習、非監督式學習與強化式學習 – Jason Kuan – Medium](https://medium.com/@capillaryj/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%95%B6%E4%BD%A0%E6%9C%89%E4%BA%86%E4%B8%89%E5%80%8B%E5%AD%A9%E5%AD%90%E4%BB%96%E5%80%91%E5%88%86%E5%88%A5%E5%8F%AB%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E8%88%87%E5%BC%B7%E5%8C%96%E5%BC%8F%E5%AD%B8%E7%BF%92-1caa26c7da87)

    > 時機:
    > 監督式學習: 資料已有標記，運用已標記資料來做訓練。





## 無監督學習

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > **无监督学习**是机器学习任务的一种。它从无标记的训练数据中推断结论。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话：**给定数据，寻找隐藏的结构**。
    > 
    > -   无标签
    > 
    > -   无反馈
    > 
    > -   寻找隐藏的结构
    > 
    > ---
    > 算法:
    > 
    > -   聚类
    > 
    > -   降维
    > 

- [【講講科普】 當你有了三個孩子他們分別叫監督式學習、非監督式學習與強化式學習 – Jason Kuan – Medium](https://medium.com/@capillaryj/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%95%B6%E4%BD%A0%E6%9C%89%E4%BA%86%E4%B8%89%E5%80%8B%E5%AD%A9%E5%AD%90%E4%BB%96%E5%80%91%E5%88%86%E5%88%A5%E5%8F%AB%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E8%88%87%E5%BC%B7%E5%8C%96%E5%BC%8F%E5%AD%B8%E7%BF%92-1caa26c7da87)

    > 時機:
    > 非監督式學習: 資料沒有標記，從中找出擁有相同特徵的資料群。


## 強化學習

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > **强化学习**是机器学习的另一个领域。它关注的是软件代理如何在一个环境中采取行动以便最大化某种累积的回报。一句话：**给定数据，学习如何选择一系列行动，以最大化长期收益**。
    > 
    > -   决策流程
    > 
    > -   激励系统
    > 
    > -   学习一系列的行动
    > 
    > ---
    > 算法:
    > 
    > -   马尔可夫决策过程
    > 
    > -   动态规划

- [【講講科普】 當你有了三個孩子他們分別叫監督式學習、非監督式學習與強化式學習 – Jason Kuan – Medium](https://medium.com/@capillaryj/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%95%B6%E4%BD%A0%E6%9C%89%E4%BA%86%E4%B8%89%E5%80%8B%E5%AD%A9%E5%AD%90%E4%BB%96%E5%80%91%E5%88%86%E5%88%A5%E5%8F%AB%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E8%88%87%E5%BC%B7%E5%8C%96%E5%BC%8F%E5%AD%B8%E7%BF%92-1caa26c7da87)

    > 時機:
    > 強化式學習: 可能手上沒有任何資料，直接讓模型執行，再將執行結果反饋回去做訓練。



# 模型複雜度

## Balancing Bias and Variance to Control Errors in Machine Learning

- [Balancing Bias and Variance to Control Errors in Machine Learning](https://towardsdatascience.com/balancing-bias-and-variance-to-control-errors-in-machine-learning-16ced95724db)

    > Suppose we are observing a *response variable Y* (qualitative or quantitative) and *input variable X* having p number of features or columns (X1, X2.....Xp) and we assume there is relation between them. This relation can be expressed as
    > 
    > > **Y = f(X) + e**
    > > 
    > 
    > ***even if we make a 100% accurate estimate of f(X), our model won't be error free, this is known as irreducible error***(e in the above equation).
    > 
    > In other terms, the irreducible error can be seen as information that X cannot provide about Y. ***The quantity e may contain unmeasured variables that are useful in predicting Y***: since we don't measure them, f cannot use them for its prediction. ***The quantity e may also contain unmeasurable variation******.*** For example, the risk of an adverse reaction might vary for a given patient on a given day, depending on manufacturing variation in the drug itself or the patient's general feeling of well-being on that day.
    > 
    > the error they introduce is not reducible as generally they are not present in the training data. Nothing that we can do about it.
    > 
    > ---
    > 
    > #### Model Complexity
    > 
    > The complexity of a relation, f(X), between input and response variables, is an important factor to consider while learning from a dataset. **A simple relation is easy to interpret.** For example a linear model would look like this
    > 
    > > **Y ≈ β0 + β1X1 + β2X2 + ...+ βpXp**
    > > 
    > 
    > for example it may be quadratic, circular, etc. These models are **more flexible** as they fit data points more closely can take different forms. Generally such methods result in a higher accuracy. ***But this flexibility comes at the cost of interpretability, as a complex relation is harder to interpret.***
    > 
    > Choosing a flexible model, does not always guarantee high accuracy. It happens because our flexible statistical learning procedure is working too hard to ﬁnd patterns in the training data, and ***may be picking up some patterns that are just caused by random chance rather than by true properties***
    > 
    > This phenomenon is also known as **overfitting**.
    > 
    > ---
    > 
    > #### Quality of fit
    > 
    > the most commonly-used measure in regression setting is the mean squared error (MSE),
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*mXRQkeM6d-9xA0YXHKhe4Q.png)
    > 
    > The expected test MSE, for a given value x0, can always be decomposed into the sum of three fundamental quantities: the variance of f(x0), the squared bias of f(x0) and the variance of the error terms e. Where, e is the irreducible error, about which we discusses earlier. So, lets see more about bias and variance.
    > 
    > #### Bias
    > 
    > Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.
    > 
    > #### **Variance**
    > 
    > Variance refers to the amount by which your estimate of f(X) would change if we estimated it using a diﬀerent training data set.
    > 
    > ideally the estimate for f(X) should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in f(X).
    > 
    > #### General Rule
    > 
    > A general rule is that, ***as a statistical method tries to match data points more closely or when a more flexible method is used, the bias reduces, but variance increases.***
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*TwEC9bcoXthzg_sFWgsFlA.png)
    > 
    > Credit : An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/900/1*cSUMKHGKbgU2InrPr6WQkA.png)
    > 
    > Credit : ISLR by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
    > 
    > This is a graph showing test MSE(red curve), bias(green curve) and variance(yellow curve), with respect to flexibility of chosen method, for a particular dataset. The point of lowest MSE makes an interesting point about the error forms bias and variance. It shows that with *increase in flexibility, bias decreases more rapidly than variance increases. After some point there is no more decrease in bias but variance starts increasing rapidly due to overfitting.*
    > 
    > ---
    > 
    > #### Bias-Variance Trade off
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*k_D4-U7c3Tf8hJRpaOZoBQ.png)
    > 
    > Credit : An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
    > 
    > As described earlier, **in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias****.**
    > 
    > The challenge lies in ﬁnding a method for which both the variance and the squared bias are low.
    > 
    > 


# 模型有效性驗證

## 為何需要validation?

- 首先，我們必須知道，test set是用來評估模型推廣到沒看過的data的能力。因此，我們不應該根據test set的結果來調整模型使用的參數。(因為這種修正可能只是對test set的overfitting，面對真正沒看過的data，卻不見得有效。就好像我們已經偷看了答案，看到答案出現很多CCABB，就回去修正我們猜答案的方法一樣。換到另外一張考卷，這種猜法就行不通了。)

- 透過validation set，我們從已知的data中取出一部分來當作調整模型參數的依據，避免根據test set的結果去做調整。(就好像我們拿一些考古題出來，用我們的方法來猜考古題答案，猜得不好的可以做修正，直到找到一個猜得最好的方法，再用他來猜新考題的答案。)

- 但是，根據validation set做的修正，還是有可能只是對valitation set的overfitting。因此，我們必須要增加validation set的變異性。(如果從頭到尾只用一個validation set，就好像每次都看到考古題裡出現CCABB，就把答案背下來。所以，藉由把考古題重新打亂，切成K份，每次只用其中一份做validation，不重複使用，如此做K次，增加變異性，就可避免背答案的狀況。這也就是K-fold Cross Validation的由來。最後選擇K次平均錯誤較低的模型參數，作為最終的模型。)

- 但是，如果data set 是imbalanced，模型就會很容易overfitting到某個答案上去，因此需要將各個答案的數量做平均分配。(例如，我們發現考古題中大量出現C，其他選項ABD都很少，即使做成K-fold 還是大部分只有C。因此，我們將每個fold中的答案比例重新分配，讓ABCD平均出現，這樣每個fold的變異性增加了，就減少背答案的可能。這就是Stratified K-Fold Cross Validation)

- 如果data 數量已經很少，還要拿去做validation就很不經濟了。所以，假設data數量為n，令k=n，將data切成k份，因此每個fold只有一個data，每次只有一個fold=一個data做validation，這就是leave-one-out。(因為考古題數量稀少，所以我們每次只挑一題出來，用我們的模型來猜答案，選擇平均錯誤較低的模型參數，作為最終的模型。)



## 訓練集/測試集/驗證集

- [What are training set, validation set and test set?- - 杰 - C++博客](http://www.cppblog.com/guijie/archive/2008/07/29/57407.html)

    > 这三个名词在机器学习领域的文章中极其常见，但很多人对他们的概念并不是特别清楚，尤其是后两个经常被人混用。Ripley, B.D（1996）在他的经典专著Pattern Recognition and Neural Networks中给出了这三个词的定义。
    > 
    > **Training set**: A set of examples used for learning, which is to fit the parameters [i.e., weights] of the classifier.
    > 
    > **Validation set**: A set of examples used to tune the parameters [i.e., architecture, not weights] of a classifier, for example to choose the number of hidden units in a neural network.
    > 
    > **Test set**: A set of examples used only to assess the performance [generalization] of a fully specified classifier.
    > 
    > 显然，**training set**是用来训练模型或确定模型参数的，如ANN中权值等； **validation set**是用来做模型选择（model selection），即做模型的最终优化及确定的，如ANN的结构；而 **test set**则纯粹是为了测试已经训练好的模型的推广能力。当然，test set这并不能保证模型的正确性，他只是说相似的数据用此模型会得出相似的结果。但实际应用中，一般只将数据集分成两类，即training set 和test set，大多数文章并不涉及validation set。
    > 
    > Ripley还谈到了Why separate test and validation sets?
    > 
    > 1. The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model.
    > 
    > 2. After assessing the final model with the test set, YOU MUST NOT tune the model any further.
    > 

- [神经网络焦点问题的解决方案_小虫子_新浪博客](http://blog.sina.com.cn/s/blog_4d2f6cf201000cjx.html)

    > 1.【讨论】如何选择训练集和测试集数据？
    > 
    > 一般需要将样本分成独立的三部分训练集（train set），验证集（validation set)和测试集（test set）。其中训练集用来估计模型，验证集用来确定网络结构或者控制模型复杂程度的参数，而测试集则检验最终选择最优的模型的性能如何。一个典型的划分是训练集占总样本的50％，而其它各占25％，三部分都是从样本中随机抽取。
    > 
    > 样本少的时候，上面的划分就不合适了。常用的是留少部分做测试集。然后对其余N个样本采用K折交叉验证法。就是将样本打乱，然后均匀分成K份，轮流选择其中K－1份训练，剩余的一份做验证，计算预测误差平方和，最后把K次的预测误差平方和再做平均作为选择最优模型结构的依据。特别的K取N，就是留一法（leave one out）。
    > 

## Validation 方法

- [Cross-Validation in Machine Learning – Towards Data Science](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)

    > you just can't fit the model to your training data and hope it would accurately work for the real data it has never seen before. ***You need some kind of assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.***
    > 
    > ---
    > 
    > #### Validation
    > 
    > Generally, an error estimation for the model is made after training, better known as evaluation of residuals. In this process, a numerical estimate of the difference in predicted and original responses is done, also called the training error. However, *this only gives us an idea about how well our model does on data used to train it*.
    > 
    > So, the ***problem with this evaluation technique is that it does not give an indication of how well the learner will generalize to an independent/ unseen data set***. **Getting this idea about our model is known as Cross Validation.**
    > 
    > ---
    > 
    > #### Holdout Method
    > 
    > *removing a part of the training data and using it to get predictions from the model trained on rest of the data**.*
    > 
    > also known as the holdout method.
    > 
    > ***it still suffers from issues of high variance***. ***This is because it is not certain which data points will end up in the validation set and the result might be entirely different for different sets.***
    > 
    > ---
    > 
    > #### ***K-Fold Cross Validation***
    > 
    > 
    > removing a part of it for validation poses a problem of underfitting. By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias.
    > 
    > In **K Fold cross validation**, the data is divided into k subsets. Now the holdout method is repeated k times, such that ***each time, one of the k subsets is used as the test set/ validation set and the other k-1 subsets are put together to form a training set***. The *error estimation is averaged over all k trials to get total effectiveness of our model*. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set *k-1* times. ***This* *****significantly reduces bias as we are using most of the data for fitting******, and* *****also significantly reduces variance as most of the data is also being used in validation set******.*** Interchanging the training and test sets also adds to the effectiveness of this method. **As a general rule and empirical evidence, K = 5 or 10 is generally preferred**, but nothing's fixed and it can take any value.
    > 
    > ---
    > 
    > #### Stratified K-Fold Cross Validation
    > 
    > *In some cases, there may be a large imbalance in the response variables*. For example, in dataset concerning price of houses, there might be large number of houses having high price. Or in case of classification, there might be several times more negative samples than positive samples. For such problems, ***a slight variation in the K Fold cross validation technique is made,* *****such that each fold contains approximately the same percentage of samples of each target class as the complete set******, or in case of prediction problems,* *****the mean response value is approximately equal in all the folds******.*** This variation is also known as **Stratified K Fold**.
    > 
    > ---
    > 
    > Above explained validation techniques are also referred to as Non-exhaustive cross validation methods.
    > 
    > method explained below, also called Exhaustive Methods, that computes all possible ways the data can be split into training and test sets.
    > 
    > ---
    > 
    > #### Leave-P-Out Cross Validation
    > 
    > This approach leaves p data points out of training data, i.e. if there are n data points in the original sample then, n-p samples are used to train the model and p points are used as the validation set.
    > 
    > *This method is exhaustive in the sense that it needs to train and validate the model for all possible combinations, and* *for moderately large p, it can become computationally infeasible**.*
    > 
    > 
    > **A particular case of this method is** **when p = 1. This is known as Leave one out cross validation.** This method is generally preferred over the previous one because ***it does not suffer from the intensive computation, as number of possible combinations is equal to number of data points in original sample or n.***
    > 
    > ---
    > 
    > Cross Validation is a very useful technique for assessing the effectiveness of your model, particularly in cases where you need to mitigate overfitting. It is also of use in determining the hyper parameters of your model, in the sense that which parameters will result in lowest test error. 



### Holdout Method

- 從 training data 中取出一部分當作 validation data， validation data取出後不再放回去做training
- 此法的問題在於容易導致較高的變異(variation)，因為我們只sample出一部分做validation，但不同sample做validation的差異可能很大，模型訓練時並沒有考慮到這點，最終產出的模型對不同的資料預測結果可能差異很大。


### K-Fold Cross Validation

- 將 training data 分割成K份，每次training 只取K-1份，用剩下的一份作validation，每一份都要拿出來做validation，做完一輪共K次。因此，每個資料點只會有一次成為validation set，而有k-1次成為training set。而模型對這個dataset的error由K次validation error的平均來估計。

- 此法可以顯著降低偏差(bias)，因為我們充份使用了大部份資料來做training；同時，也會顯著降低變異(variation)，因為我們用了大部份資料來做驗證。

- 一般來說，K=5 或 10 是比較合適的


### Stratified K-Fold Cross Validation

- 當數據不平衡(imbalance)時，將K-Fold中的每個Fold的組成調整成平均分配每個目標分類的樣本數，所以對每個Fold來說，分類結果的數量都幾乎相等。


### Leave-P-Out Cross Validation


- 留下p個資料點作驗證，其他n-p做訓練，重複各種可能的組合，共Cn取p次。
- 由於嘗試所有分割的可能，又稱為耗盡法(Exhaustive Methods)。
- 當p很大時，計算量會暴增而變得不可行。
- 因此通常取p=1，又稱為留一法。又可看成 k=n 時的k-Fold。
- 留一法(LOO)通常會造成高變異(variance)，因為每次都用n-1個data做training，跟用全部n個data做training是幾乎相同的，無法有效減少變異。
- 所以一般來說，用 K=5 或 10 的K-Fold會比留一法好



## 不平衡數據(imbalanced data)

- [Classification when 80% of my training set is of one class.：MachineLearning](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)

- [In classification, how do you handle an unbalanced training set? - Quora](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)



- [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)

    > ### 1) Can You Collect More Data?
    > 
    > You might think it's silly, but collecting more data is almost always overlooked.
    > 
    > Can you collect more data? Take a second and think about whether you are able to gather more data on your problem.
    > 
    > A larger dataset might expose a different and perhaps more balanced perspective on the classes.
    > 
    > More examples of minor classes may be useful later when we look at resampling your dataset.
    > 
    > ### 2) Try Changing Your Performance Metric
    > 
    > Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.
    > 
    > There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes.
    > 
    > I give more advice on selecting different performance measures in my post "[Classification Accuracy is Not Enough: More Performance Measures You Can Use](http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)".
    > 
    > In that post I look at an imbalanced dataset that characterizes the recurrence of breast cancer in patients.
    > 
    > From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:
    > 
    > -   **Confusion Matrix**: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).
    > -   **Precision**: A measure of a classifiers exactness.
    > -   **Recall**: A measure of a classifiers completeness
    > -   **F1 Score (or F-score)**: A weighted average of precision and recall.
    > 
    > I would also advice you to take a look at the following:
    > 
    > -   **Kappa (or [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))**: Classification accuracy normalized by the imbalance of the classes in the data.
    > -   **ROC Curves**: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.
    > 
    > You can learn a lot more about using ROC Curves to compare classification accuracy in our post "[Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)".
    > 
    > Still not sure? Start with kappa, it will give you a better idea of what is going on than classification accuracy.
    > 
    > ### 3) Try Resampling Your Dataset
    > 
    > You can change the dataset that you use to build your predictive model to have more balanced data.
    > 
    > This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:
    > 
    > 1.  You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or
    > 2.  You can delete instances from the over-represented class, called under-sampling.
    > 
    > These approaches are often very easy to implement and fast to run. They are an excellent starting point.
    > 
    > In fact, I would advise you to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred accuracy measures.
    > 
    > You can learn a little more in the the Wikipedia article titled "[Oversampling and undersampling in data analysis](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)".
    > 
    > #### Some Rules of Thumb
    > 
    > -   Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)
    > -   Consider testing over-sampling when you don't have a lot of data (tens of thousands of records or less)
    > -   Consider testing random and non-random (e.g. stratified) sampling schemes.
    > -   Consider testing different resampled ratios (e.g. you don't have to target a 1:1 ratio in a binary classification problem, try other ratios)
    > 
    > ### 4) Try Generate Synthetic Samples
    > 
    > A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.
    > 
    > You could sample them empirically within your dataset or you could use a method like Naive Bayes that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.
    > 
    > There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique.
    > 
    > As its name suggests, SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.
    > 
    > Learn more about SMOTE, see the original 2002 paper titled "[SMOTE: Synthetic Minority Over-sampling Technique](http://www.jair.org/papers/paper953.html)".
    > 
    > There are a number of implementations of the SMOTE algorithm, for example:
    > 
    > -   In Python, take a look at the "[UnbalancedDataset](https://github.com/fmfn/UnbalancedDataset)" module. It provides a number of implementations of SMOTE as well as various other resampling techniques that you could try.
    > -   In R, the [DMwR package](https://cran.r-project.org/web/packages/DMwR/index.html) provides an implementation of SMOTE.
    > -   In Weka, you can use the [SMOTE supervised filter](http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html).
    > 
    > ### 5) Try Different Algorithms
    > 
    > As always, I strongly advice you to not use your favorite algorithm on every problem. You should at least be spot-checking a variety of different types of algorithms on a given problem.
    > 
    > For more on spot-checking algorithms, see my post "Why you should be Spot-Checking Algorithms on your Machine Learning Problems".
    > 
    > That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed.
    > 
    > If in doubt, try a few popular decision tree algorithms like C4.5, C5.0, CART, and Random Forest.
    > 
    > For some example R code using decision trees, see my post titled "[Non-Linear Classification in R with Decision Trees](http://machinelearningmastery.com/non-linear-classification-in-r-with-decision-trees/)".
    > 
    > For an example of using CART in Python and scikit-learn, see my post titled "[Get Your Hands Dirty With Scikit-Learn Now](http://machinelearningmastery.com/get-your-hands-dirty-with-scikit-learn-now/)".
    > 
    > ### 6) Try Penalized Models
    > 
    > You can use the same algorithms but give them a different perspective on the problem.
    > 
    > Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.
    > 
    > Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA.
    > 
    > It is also possible to have generic frameworks for penalized models. For example, Weka has a [CostSensitiveClassifier](http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html) that can wrap any classifier and apply a custom penalty matrix for miss classification.
    > 
    > Using penalization is desirable if you are locked into a specific algorithm and are unable to resample or you're getting poor results. It provides yet another way to "balance" the classes. Setting up the penalty matrix can be complex. You will very likely have to try a variety of penalty schemes and see what works best for your problem.
    > 
    > ### 7) Try a Different Perspective
    > 
    > There are fields of study dedicated to imbalanced datasets. They have their own algorithms, measures and terminology.
    > 
    > Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas.
    > 
    > Two you might like to consider are **anomaly detection** and **change detection**.
    > 
    > [Anomaly detection](https://en.wikipedia.org/wiki/Anomaly_detection) is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by it's sequence of system calls. The events are rare and when compared to normal operation.
    > 
    > This shift in thinking considers the minor class as the outliers class which might help you think of new ways to separate and classify samples.
    > 
    > [Change detection](https://en.wikipedia.org/wiki/Change_detection) is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.
    > 
    > Both of these shifts take a more real-time stance to the classification problem that might give you some new ways of thinking about your problem and maybe some more techniques to try.
    > 
    > ### 8) Try Getting Creative
    > 
    > Really climb inside your problem and think about how to break it down into smaller problems that are more tractable.
    > 
    > For inspiration, take a look at the very creative answers on Quora in response to the question "[In classification, how do you handle an unbalanced training set?](http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)"
    > 
    > For example:
    > 
    > > Decompose your larger class into smaller number of other classes...
    > >
    > > ...use a One Class Classifier... (e.g. treat like outlier detection)
    > >
    > > ...resampling the unbalanced training set into not one balanced set, but several. Running an ensemble of classifiers on these sets could produce a much better result than one classifier alone
    > 
    > These are just a few of some interesting and creative ideas you could try.
    > 
    > For more ideas, check out these comments on the reddit post "[Classification when 80% of my training set is of one class](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)".
    > 
    > Pick a Method and Take Action
    > -----------------------------
    > 
    > You do not need to be an algorithm wizard or a statistician to build accurate and reliable models from imbalanced datasets.
    > 
    > We have covered a number of techniques that you can use to model an imbalanced dataset.
    > 
    > Hopefully there are one or two that you can take off the shelf and apply immediately, for example changing your accuracy metric and resampling your dataset. Both are fast and will have an impact straight away.
    > 
    > ***Which method are you going to try?***
    > 
    > A Final Word, Start Small
    > -------------------------
    > 
    > Remember that we cannot know which approach is going to best serve you and the dataset you are working on.
    > 
    > You can use some expert heuristics to pick this method or that, but in the end, the best advice I can give you is to "become the scientist" and empirically test each method and select the one that gives you the best results.
    > 
    > Start small and build upon what you learn.
    > 
    > Want More? Further Reading...
    > ---------------------------
    > 
    > There are resources on class imbalance if you know where to look, but they are few and far between.
    > 
    > I've looked and the following are what I think are the cream of the crop. If you'd like to dive deeper into some of the academic literature on dealing with class imbalance, check out some of the links below.
    > 
    > ### Books
    > 
    > -   [Imbalanced Learning: Foundations, Algorithms, and Applications](http://www.amazon.com/dp/1118074629?tag=inspiredalgor-20)
    > 
    > ### Papers
    > 
    > -   [Data Mining for Imbalanced Datasets: An Overview](http://link.springer.com/chapter/10.1007/978-0-387-09823-4_45)
    > -   [Learning from Imbalanced Data](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5128907)
    > -   [Addressing the Curse of Imbalanced Training Sets: One-Sided Selection](http://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf) (PDF)
    > -   [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](http://dl.acm.org/citation.cfm?id=1007735)
    > 

# 模型評估方法 Accuracy Metrics

## Accuracy

||predicted 0|predicted 1|
|--|--|--|
|Actual 0|True Negative |False Positive|
|Actual 1| False Negative | True Positive |

$$ Accuracy = \frac{All\ True\ Predicted}{ All\ Possible}\\
(做出正確判定的機率)\\
= \frac{\#TN+\#TP}{\#TP+\#FP+\#TN+\#FN}
$$

## Precision and Recall

$$
Precision = \frac{Predicted\ 1\ and\ Actual\ 1}{Predicted\ 1} \\(判定為貓，真的為貓的機率)\\
= \frac{\#TP}{\#TP+\#FP}
$$

$$ 
Recall = \frac{Predicted\ 1\ and\ Actual\ 1}{Actual\ 1} \\(把全部的貓找回來的機率)\\
= \frac{\#TP}{\#TP+\#FN} 
$$

## F1 Score
$$
F_1 = \frac{2}{ \frac{1}{recall} + \frac{1}{precision}}\\
(recall\ 和\ precision\ 的調和平均)\\
= 2 * \frac{precision * recall}{precision + recall}
$$ 

- [如何理解与应用调和平均数？ - LIQiNG的回答 - 知乎](https://www.zhihu.com/question/23096098/answer/340657629)

    调和平均数，强调了较小值的重要性；在机器学习中。召回率为R, 准确率为P。使用他们对算法的评估，这两个值通常情况下相互制约。为了更加方便的评价算法的好坏。于是引入了F1值。F1为准确率P和召回率R的调和平均数。为什么F1使用调和平均数，而不是数字平均数。举个例子：当R 接近于1, P 接近于 0 时。采用调和平均数的F1值接近于0；而如果采用算数平均数F1的值为0.5；显然采用调和平均数能更好的评估算法的性能。等效于评价R和P的整体效果

## ROC Curves and Area Under the Curve(AUC)

<iframe width="560" height="315" src="https://www.youtube.com/embed/OAl6eAyP-yo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

- [机器学习和统计里面的auc怎么理解？ - 现在几点了的回答 - 知乎](https://www.zhihu.com/question/39840928/answer/83576302)

    >从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC



## Matthew's Correlation Coefficient
* 如果 sample size 有 unbalanced 的現象可以利用此 accuacy metric 去測量預測精準度
$$
MCC= \frac{TP*TN - FP*FN}{\sqrt{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}}
$$



# 過擬合 Overfitting


## Regularization

- [Machine Learning學習日記 — Coursera篇 (Week 3.4):The – Pandora123 – Medium](https://medium.com/@ken90242/machine-learning%E5%AD%B8%E7%BF%92%E6%97%A5%E8%A8%98-coursera%E7%AF%87-week-3-4-the-c05b8ba3b36f)

    > ### 1\. The Problem of Overfitting
    > 
    > > What is "overfitting" ?
    > 
    > (1) Overfitting的意思就是太過追求參數完美預測出訓練數據的結果，反而導致實際預測效果不佳
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*2J4X673RGHSwkUo04xdoFw.png)
    > 
    > > high variance可以理解為其有著過多的 variable
    > 
    > (2) 跟overfit相反的狀況：underfit，代表在訓練數據中也有著高預測誤差的問題
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*HzNe5B2wm7RtrNYpOEBjMw.png)
    > 
    > > high bias可以理解為其會過度依賴其截距(θ0)
    > 
    > (3) 比較適當的Hypothesis會長的像：
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*a8Z12cA6A8QP3WxGuwMiNA.png)
    > 
    > 上述都是以Linear Regression為例，下面將以Logistic Regression為例：
    > 
    > Underfit：
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*-LBSsXiBgYuOGO6zqB6qqg.png)
    > 
    > Overfit：
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*-lqUQLwLMeYDJOzqFQhMAQ.png)
    > 
    > 而比較剛好的狀況會是
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*2cZgxduxw7VyGcPrLfywdA.png)
    > 
    > Andrew以一段話談論Overfitting：
    > 
    > > If we have too many features, the learned hypothesis may fit the training set very well (J(θ)=0), but fail to generalize to new examples(Predictions on new examples)
    > 
    > 那我們要怎麼解決Overfitting的問題呢？
    > 
    > 有幾種做法：
    > 
    > 1\. 降低features的數量：人工選擇、model selection algorithm
    > 
    > 2\. Regularization：維持現有的features，但是降低部分不重要feature的影響力。這對於有著許多feature的hypothesis很有幫助
    > 
    > ---
    > 
    > ### 2\. Cost Function
    > 
    > 下面的例子：左方為適當的模型，右方為Overfitting
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*ca3r2frg--jgL2U3A9f1Eg.png)
    > 
    > 我們可以發現主要的問題是在加了θ3跟θ4之後出現了overfitting的問題
    > 
    > 那假設我們將θ3跟θ4的影響降到最低呢(讓其逼近於0)？
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*5aO_R8Lg9QXHaakLMgHIKg.png)
    > 
    > 就像是我們改寫J(θ)，多加上1000*(θ3²)和1000*(θ4²)
    > 
    > 上述的1000可以隨時替換成一個極大的數值。
    > 
    > 而模型自然會為了將J(θ)降到最低，而使θ3跟θ4降至最低，形成下面的粉紅色線構成的模型
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*yfvqviVHVAkXYiJfBg_40w.png)
    > 
    > 以下將舉例說明如何使用Regularization來改善Overfitting的問題
    > 
    > 假設我們要預測房價，其包含了很多個features
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*vOCGnq-TPsO4e6f4QZ2nkQ.png)
    > 
    > 而我們要做的就是在J(θ)後面加上一個多項式：
    > 
    > 這個 λ 其實跟上面例子的數字1000是一樣的意思，它正式的名稱是regularization parameter
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*wKIxmqs4kw5iBooHUlfIxg.png)
    > 
    > > 按照慣例，這個多項式不會加上截距項θ0
    > 
    > 這會大幅改善overfitting的問題
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*9tj5EEzEHyDYvM5EvAcYUA.png)
    > 
    > > 可能會有人好奇 λ 到底代表什麼意思？
    > 
    > > λ代表的其實是我們對於預測誤差跟正規項的取捨
    > 
    > > 當今天 λ越大，模型會比較不重視預測落差反而是極力地想要壓低所有θ值的大小
    > 
    > 就像是我們如果把 λ 設成10¹⁰的話，所有θ值都會趨近於0，最後形成一條直線的hypothesis
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*6TaOolLvrclx8e0gjntXoQ.png)
    > 
    > > 而若是 λ 越小甚至趨近於0，可能對改善overfitting就沒什麼太大的幫助了
    > 
    > > 我在學習正規化的時候，對於為什麼只是加上個正規多項式就可以改善overfitting感到非常疑惑
    > 
    > > 畢竟我們根本不知道要降低哪一個 θ值(feature)的影響力啊
    > 
    > > 就這樣直接一視同仁的一起打壓所有的θ值到底為什麼會有效？
    > 
    > > 這是我想到的答案(沒有證實過)：的確是一視同仁的打壓所有的 θ值，但是若是今天θ值只要設定到某幾個數字的話可以使預測誤差降到非常低的話，**那麼模型如果為了貪小便宜的將那幾個 θ值給壓低反而造成了預測誤差的大幅上升，使得J(θ)不降反升的反效果的話，會非常得不償失**，因此模型將會斟酌不要降低那些重要的 θ值
    > 
    > > 上述的假設都建立在今天 λ 設立得宜的情況以及有足夠的資料來support預測落差的下降，說服模型不要把重要的 θ值降低
    > > 


- [Regularization in Machine Learning – Towards Data Science](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)

    > A simple relation for linear regression looks like this. Here Y represents the learned relation and *β represents the coefficient estimates for different variables or predictors(X).*
    > 
    > ***Y ≈ β0 + β1X1 + β2X2 + ...+ βpXp***
    > 
    > The fitting procedure involves a loss function, known as residual sum of squares or RSS. The coefficients are chosen, such that they minimize this loss function.
    > 
    > ![](https://cdn-images-1.medium.com/max/900/1*DY3-IaGcHjjLg7oYXx1O3A.png)
    > 
    > Now, this will adjust the coefficients based on your training data. *If there is noise in the training data, then the estimated coefficients won't generalize well to the future data.* *This is where regularization comes in and shrinks or regularizes these learned estimates towards zero.*
    > 
    > 
    > #### Ridge Regression
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*CiqZ8lhwxi5c4d1nV24w4g.png)
    > 
    > Above image shows ridge regression, where the ***RSS is modified by adding the shrinkage quantity.*** Now, the coefficients are estimated by minimizing this function. Here, ***λ is the tuning parameter that decides how much we want to penalize the flexibility of our model.***
    > 
    > ---
    > 
    > *When λ = 0, the penalty term has no eﬀect*, and the estimates produced by ridge regression will be equal to least squares. However, ***as λ→∞, the impact of the shrinkage penalty grows, and the ridge regression coeﬃcient estimates will approach zero***. As can be seen, selecting a good value of λ is critical. Cross validation comes in handy for this purpose. The coefficient estimates produced by this method are ***also known as the L2 norm***.
    > 
    > ***The coefficients that are produced by the standard least squares method are scale equivariant***, i.e. if we multiply each input by c then the corresponding coefficients are scaled by a factor of 1/c. Therefore, regardless of how the predictor is scaled, the multiplication of predictor and coefficient(Xjβj) remains the same. ***However, this is not the case with ridge regression, and therefore, we need to standardize the predictors or bring the predictors to the same scale before performing ridge regression***. The formula used to do this is given below.
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*6KRAdbf-CApFPR7gASZaSA.png)
    > 
    > #### Lasso
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*tHJ4sSPYV0bDr8xxEdiwXA.png)
    > 
    > Lasso is another variation, in which the above function is minimized. Its clear that ***this variation differs from ridge regression only in penalizing the high coefficients***. It uses |βj|(modulus)instead of squares of β, as its penalty. In statistics, this is** *known as the L1 norm***.
    > 
    > ---
    > 
    > ***Consider their are 2 parameters in a given problem***. Then according to above formulation, the ***ridge regression is expressed by β1² + β2² ≤ s***. This implies that *ridge regression coefficients have the smallest RSS(loss function) for all points that lie within the circle given by β1² + β2² ≤ s.*
    > 
    > Similarly, ***for lasso, the equation becomes,|β1|+|β2|≤ s***. This implies that *lasso coefficients have the smallest RSS(loss function) for all points that lie within the diamond given by |β1|+|β2|≤ s.*
    > 
    > ---
    > 
    > The image below describes these equations.
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*XC-8tHoMxrO3ogHKylRfRA.png)
    > 
    > Credit : An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
    > 
    > ***The above image shows the constraint functions(green areas), for lasso(left) and ridge regression(right), along with contours for RSS(red ellipse)***. Points on the ellipse share the value of RSS. For a very large value of s, the green regions will contain the center of the ellipse, making coefficient estimates of both regression techniques, equal to the least squares estimates. But, this is not the case in the above image. In this case, the lasso and ridge regression coefficient estimates are given by the ﬁrst point at which an ellipse contacts the constraint region. ***Since ridge regression has a circular constraint with no sharp points,* *****this intersection will not generally occur on an axis, and so the ridge regression coeﬃcient estimates will be exclusively non-zero******.*** ***However,* *****the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coeﬃcients will equal zero******.*** In higher dimensions(where parameters are much more than 2), many of the coeﬃcient estimates may equal zero simultaneously.
    > 
    > ---
    > 
    > **Therefore, the lasso method also performs variable selection and is said to yield sparse models.**
    > 
    > ---
    > 
    > #### What does Regularization achieve?
    > 
    > ***Regularization, significantly reduces the variance of the model, without substantial increase in its bias***.
    > 
    > As the value of λ rises, it reduces the value of coefficients and thus reducing the variance.
    > 
    > But after certain value, the model starts loosing important properties, giving rise to bias in the model and thus underfitting.
    > 
    > 

    ---

    > 總結:
    > 
    > L2 norm 就是把所有係數取平方和乘上λ後最小化
    > ![](https://cdn-images-1.medium.com/max/1200/1*CiqZ8lhwxi5c4d1nV24w4g.png)
    > 
    > L1 norm 就是把所有係數取絕對值乘上λ後最小化
    > ![](https://cdn-images-1.medium.com/max/1200/1*tHJ4sSPYV0bDr8xxEdiwXA.png)
    > 
    > 
    > 以兩個係數β1,β2為例，
    > 
    > L2 norm 可以寫成β1^2 + β2^2 <=s，因此是個圓形區域，參數收斂到最後會碰到圓形區域的邊界，因此通常兩個係數都不會是0
    > L1 norm 可以寫成|β1| + |β2| <=s，因此是個菱形區域，參數收斂到最後會碰到菱形的尖點上，由於此點在座標軸上，因此會產生其他座標軸係數為0的效果，可用來篩選不重要的特徵。
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*XC-8tHoMxrO3ogHKylRfRA.png)
    > 
    > 
    > Regularization 正則化可以顯著降低參數的 variance，而不增加bias；然而，當λ超過某個數值後，就會變成underfitting。因此需要慎選λ。而 cross validation 就是用來做這件事。
    > [name=Ya-Lun Li]
    > 




## 測試集與過擬合

- [機器學習5年大躍進，可能是個錯覺 - 幫趣](http://bangqu.com/Zwd41J.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > 這項研究，就是加州大學伯克利分校和MIT的幾名科學家在arXiv上公開的一篇論文：Do CIFAR-10 Classifiers Generalize to CIFAR-10?。
    > 
    > 解釋一下，這個看似詭異的問題——「CIFAR-10分類器能否泛化到CIFAR-10？」，針對的是當今深度學習研究的一個大缺陷：
    > 
    > 看起來成績不錯的深度學習模型，在現實世界中不見得管用。因爲很多模型和訓練方法取得的好成績，都來自對於那些著名基準驗證集的過擬合。


- [常用測試集帶來過擬合？你真的能控制自己不根據測試集調參嗎 - 幫趣](http://bangqu.com/G53jP8.html#utm_source=Facebook_PicSee&utm_medium=Social)

    > 儘管對比新模型與之前模型的結果是非常自然的想法，但很明顯當前的研究方法論削弱了一個關鍵假設：分類器與測試集是獨立的。這種不匹配帶來了一種顯而易見的危險，研究社區可能會輕易設計出只在特定測試集上性能良好，但無法泛化至新數據的模型 [1]。
    > 
    > ---
    > 
    > 該研究分爲三步：
    > 
    > 1. 首先，研究者創建一個新的測試集，將新測試集的子類別分佈與原始 CIFAR-10 數據集進行仔細匹配。
    > 
    > 2. 在收集了大約 2000 張新圖像之後，研究者在新測試集上評估 30 個圖像分類模型的性能。結果顯示出兩個重要現象。一方面，從原始測試集到新測試集的模型準確率顯著下降。例如，VGG 和 ResNet 架構 [7, 18] 的準確率從 93% 下降至新測試集上的 85%。另一方面，研究者發現在已有測試集上的性能可以高度預測新測試集上的性能。即使在 CIFAR-10 上的微小改進通常也能遷移至留出數據。
    > 
    > 3. 受原始準確率和新準確率之間差異的影響，第三步研究了多個解釋這一差距的假設。一種自然的猜想是重新調整標準超參數能夠彌補部分差距，但是研究者發現該舉措的影響不大，僅能帶來大約 0.6% 的改進。儘管該實驗和未來實驗可以解釋準確率損失，但差距依然存在。
    > 
    > 總之，研究者的結果使得當前機器學習領域的進展意味不明。適應 CIFAR-10 測試集的努力已經持續多年，模型表現的測試集適應性並沒有太大提升。頂級模型仍然是近期出現的使用 Cutout 正則化的 Shake-Shake 網絡 [3, 4]。此外，該模型比標準 ResNet 的優勢從 4% 上升至新測試集上的 8%。這說明當前對測試集進行長時間「攻擊」的研究方法具有驚人的抗過擬合能力。
    > 
    > 但是該研究結果令人對當前分類器的魯棒性產生質疑。儘管新數據集僅有微小的分佈變化，但廣泛使用的模型的分類準確率卻顯著下降。例如，前面提到的 VGG 和 ResNet 架構，其準確率損失相當於模型在 CIFAR-10 上的多年進展 [9]。注意該實驗中引入的分佈變化不是對抗性的，也不是不同數據源的結果。因此即使在良性設置中，分佈變化也對當前模型的真正泛化能力帶來了嚴峻挑戰。
    > 
    > 



## 深度神經網路與過擬合

- [Tomaso Poggio深度學習理論：深度網絡「過擬合缺失」的本質 - 幫趣](http://bangqu.com/qA6W5k.html)

    > 本文是 DeepMind 創始人 Demis Hassabis 和 Mobileye 創始人 Amnon Shashua 的導師、MIT 教授 Tomaso Poggio 的深度學習理論系列的第三部分，分析深度神經網絡的泛化能力。該系列前兩部分討論了深度神經網絡的表徵和優化問題，機器之心之前對整個理論系列進行了簡要總結。在深度網絡的實際應用中，通常會添加顯性（如權重衰減）或隱性（如早停）正則化來避免過擬合，但這並非必要，尤其是在分類任務中。在本文中，Poggio 討論了深度神經網絡的過擬合缺失問題，即在參數數量遠遠超過訓練樣本數的情況下模型也具備良好的泛化能力。其中特別強調了經驗損失和分類誤差之間的差別，證明深度網絡每一層的權重矩陣可收斂至極小范數解，並得出深度網絡的泛化能力取決於多種因素的互相影響，包括損失函數定義、任務類型、數據集類型等。
    > 
    > **1 引言**
    > 
    > 過去幾年來，深度學習在許多機器學習應用領域都取得了極大的成功。然而，我們對深度學習的理論理解以及開發原理的改進能力上都有所落後。如今對深度學習令人滿意的理論描述正在形成。這涵蓋以下問題：1）深度網絡的表徵能力；2）經驗風險的優化；3）泛化------當網絡過參數化（overparametrized）時，即使缺失顯性的正則化，爲什麼期望誤差沒有增加？
    > 
    > 本論文解決了第三個問題，也就是非過擬合難題，這在最近的多篇論文中都有提到。論文 [1] 和 [7] 展示了線性網絡的泛化特性可被擴展到 DNN 中從而解決該難題，這兩篇論文中的泛化即用梯度下降訓練的帶有特定指數損失的線性網絡收斂到最大間隔解，提供隱性的正則化。本論文還展示了同樣的理論可以預測經驗風險的不同零最小值（zero minimizer）的泛化。
    > 
    > **2 過擬合難題**
    > 
    > 經典的學習理論將學習系統的泛化行爲描述爲訓練樣本數 n 的函數。從這個角度看，DNN 的行爲和期望一致：更多訓練數據帶來更小的測試誤差，如圖 1a 所示。該學習曲線的其他方面似乎不夠直觀，但也很容易解釋。例如即使在訓練誤差爲零時，測試誤差也會隨着 n 的增加而減小（正如 [1] 中所指出的那樣，因爲被報告的是分類誤差，而不是訓練過程中被最小化的風險，如交叉熵）。看起來 DNN 展示出了泛化能力，從技術角度上可定義爲：隨着 n → ∞，訓練誤差收斂至期望誤差。圖 1 表明對於正常和隨機標籤，模型隨 n 的增加的泛化能力變化。這與之前研究的結果一致（如 [8]），與 [9] 的穩定性結果尤其一致。注意泛化的這一特性並不尋常：很多算法（如 K 最近鄰算法）並不具備該保證。
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k15318036171099JP52.png)
    > 
    > *圖 1：不同數量訓練樣本下的泛化。（a）在 CIFAR 數據集上的泛化誤差。（b）在隨機標籤的 CIFAR 數據集上的泛化誤差。該深度神經網絡是通過最小化交叉熵損失訓練的，並且是一個 5 層卷積網絡（即沒有池化），每個隱藏層有 16 個通道。ReLU 被用做層之間的非線性函數。最終的架構有大約 1 萬個參數。圖中每一個點使用批大小爲 100 的 SGD 並訓練 70 個 epoch 而得出，訓練過程沒有使用數據增強和正則化。*
    > 
    > 泛化的這一特性雖然重要，但在這裏也只是學術上很重要。現在深度網絡典型的過參數化真正難題（即本論文的重點）是在缺乏正則化的情況下出現明顯缺乏過擬合的現象。從隨機標註數據中獲得零訓練誤差的同樣網絡（圖 1b）很顯然展示出了大容量，但並未展示出在不改變多層架構的情況下，每一層神經元數量增加時期望誤差會有所增加（圖 2a）。具體來說，當參數數量增加並超過訓練集大小時，未經正則化的分類誤差在測試集上的結果並未變差。
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k15318036187892u56K.png)
    > 
    > *圖 2：在 CIFAR-10 中的期望誤差，橫軸爲神經元數量。該 DNN 與圖 1 中的 DNN 一樣。（a）期望誤差與參數數量增加之間的相關性。（b）交叉熵風險與參數數量增加之間的相關性。期望風險中出現部分「過擬合」，儘管該指數損失函數的特點略微有些誇大。該過擬合很小，因爲 SGD 收斂至每一層具備最小弗羅貝尼烏斯範數（Frobenius norm）的網絡。因此，當參數數量增加時，這裏的期望分類誤差不會增加，因爲分類誤差比損失具備更強的魯棒性（見附錄 9）。*
    > 
    > 我們應該明確參數數量只是過參數化的粗略表徵。實驗設置詳見第 6 章。
    > 
    > **5 深度網絡的非線性動態**
    > 
    > **5.3 主要問題**
    > 
    > 把所有引理合在一起，就得到了
    > 
    > **定理 3**：給定一個指數損失函數和非線性分割的訓練數據，即對於訓練集中的所有 x_n，∃f(W; x_n) 服從 y_n*f(W; x_n) > 0，獲得零分類誤差。以下特性展示了漸近平衡（asymptotic equilibrium）：
    > 
    > 1.  GD 引入的梯度流從拓撲學角度來看等於線性化流；
    > 
    > 2.  解是每一層權重矩陣的局部極小弗羅貝尼烏斯範數解。
    > 
    > 在平方損失的情況下分析結果相同，但是由於線性化動態只在零初始條件下收斂至極小范數，因此該定理的最終表述「解是局部極小範數解」僅適用於線性網絡，如核機器，而不適用於深度網絡。因此在非線性的情況下，平方損失和指數損失之間的差別變得非常顯著。對其原因的直觀理解見圖 3。對於全局零最小值附近的深度網絡，平方損失的「地形圖」通常有很多零特徵值，且在很多方向上是平坦的。但是，對於交叉熵和其他指數損失而言，經驗誤差山谷有一個很小的向下的坡度，在||w||無限大時趨近於零（詳見圖 3）。
    > 
    > 在補充材料中，研究者展示了通過懲罰項 λ 寫出 W_k = ρ_k*V_k，並使 ||V_k||^2 = 1，來考慮相關動態，從而展示初始條件的獨立性以及早停和正則化的等效性。
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k153180362114695b7b.png)
    > 
    > *圖 3：具備參數 w_1 和 w_2 的平方損失函數（左）。極小值具備一個退化 Hessian（特徵值爲零）。如文中所述，它表示在零最小值的小近鄰區域的「一般」情況，零最小值具備很多零特徵值，和針對非線性多層網絡的 Hessian 的一些正特徵值。收斂處的全局最小值附近的交叉熵風險圖示如右圖所示。隨着||w|| → ∞，山谷坡度稍微向下。在多層網絡中，損失函數可能表面是分形的，具備很多退化全局最小值，每個都類似於這裏展示的兩個最小值的多維度版本。*
    > 
    > **5.4 爲什麼分類比較不容易過擬合**
    > 
    > 由於這個解是線性化系統的極小範數解，因此我們期望，對於低噪聲數據集，與交叉熵最小化相關的分類誤差中幾乎很少或沒有過擬合。注意：交叉熵作爲損失函數的情況中，梯度下降在線性分離數據上可收斂至局部極大間隔解（local max-margin solution），起點可以是任意點（原因是非零斜率，如圖 3 所示）。因此，對於期望分類誤差，過擬合可能根本就不會發生，如圖 2 所示。通常相關損失中的過擬合很小，至少在幾乎無噪聲的數據情況下是這樣，因爲該解是局部極大間隔解，即圍繞極小值的線性化系統的僞逆。近期結果（Corollary 2.1 in [10]）證明具備 RELU 激活函數的深度網絡的 hinge-loss 的梯度最小值具備大的間隔，前提是數據是可分離的。這個結果與研究者將 [1] 中針對指數損失的結果擴展至非線性網絡的結果一致。注意：目前本論文研究者沒有對期望誤差的性質做出任何聲明。不同的零最小值可能具備不同的期望誤差，儘管通常這在 SGD 的類似初始化場景中很少出現。本文研究者在另一篇論文中討論了本文提出的方法或許可以預測與每個經驗最小值相關的期望誤差。
    > 
    > 總之，本研究結果表明多層深度網絡的行爲在分類中類似於線性模型。更準確來說，在分類任務中，通過最小化指數損失，可確保全局最小值具備局部極大間隔。因此動態系統理論爲非過擬合的核心問題提供了合理的解釋，如圖 2 所示。主要結果是：接近經驗損失的零極小值，非線性流的解繼承線性化流的極小範數特性，因爲這些流拓撲共軛。損失中的過擬合可以通過正則化來顯性（如通過權重衰減）或隱性（通過早停）地控制。分類誤差中的過擬合可以被避免，這要取決於數據集類型，其中漸近解是與特定極小值相關的極大間隔解（對於交叉熵損失來說）。
    > 
    > **6 實驗**
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k1531803622387653TL.png)
    > 
    > *圖 4：使用平方損失在特徵空間中對線性網絡進行訓練和測試（即 y = WΦ(X)），退化 Hessian 如圖 3 所示。目標函數是一個 sine 函數 f(x) = sin(2πfx)，在區間 [-1, 1] 上 frequency f = 4。訓練數據點有 9 個，而測試數據點的數量是 100。第一對圖中，特徵矩陣 φ(X) 是多項式的，degree 爲 39。第一對圖中的數據點根據 Chebyshev 節點機制進行採樣，以加速訓練，使訓練誤差達到零。訓練使用完整梯度下降進行，步長 0.2，進行了 10, 000, 000 次迭代。每 120, 000 次迭代後權重受到一定的擾動，每一次擾動後梯度下降被允許收斂至零訓練誤差（機器準確率的最高點）。通過使用均值 0 和標準差 0.45 增加高斯噪聲，進而擾動權重。在第 5, 000, 000 次迭代時擾動停止。第二張圖展示了權重的 L_2 範數。注意訓練重複了 29 次，圖中報告了平均訓練和測試誤差，以及權重的平均範數。第二對圖中，特徵矩陣 φ(X) 是多項式的，degree 爲 30。訓練使用完整梯度下降進行，步長 0.2，進行了 250, 000 次迭代。第四張圖展示了權重的 L_2 範數。注意：訓練重複了 30 次，圖中報告了平均訓練和測試誤差，以及權重的平均範數。該實驗中權重沒有遭到擾動。*
    > 
    > **7 解決過擬合難題**
    > 
    > 本研究的分析結果顯示深度網絡與線性模型類似，儘管它們可能過擬合期望風險，但不經常過擬合低噪聲數據集的分類誤差。這遵循線性網絡梯度下降的特性，即風險的隱性正則化和對應的分類間隔最大化。在深度網絡的實際應用中，通常會添加顯性正則化（如權重衰減）和其他正則化技術（如虛擬算例），而且這通常是有益的，雖然並非必要，尤其是在分類任務中。
    > 
    > 如前所述，平方損失與指數損失不同。在平方損失情況中，具備任意小的 λ 的正則化（沒有噪聲的情況下）保留梯度系統的雙曲率，以收斂至解。但是，解的範數依賴於軌跡，且無法確保一定會是線性化引入的參數中的局部極小範數解（在非線性網絡中）。在沒有正則化的情況下，可確保線性網絡（而不是深度非線性網絡）收斂至極小范數解。在指數損失線性網絡和非線性網絡的情況下，可獲得雙曲梯度流。因此可確保該解是不依賴初始條件的極大間隔解。對於線性網絡（包括核機器），存在一個極大間隔解。在深度非線性網絡中，存在多個極大間隔解，每個對應一個全局最小值。在某種程度上來說，本研究的分析結果顯示了正則化主要提供了動態系統的雙曲率。在條件良好的線性系統中，即使 λ → 0，這種結果也是對的，因此內插核機器的通用情況是在無噪聲數據情況下無需正則化（即條件數依賴於 x 數據的分割，因此 y 標籤與噪聲無關，詳見 [19]）。在深度網絡中，也會出現這種情況，不過只適用於指數損失，而非平方損失。
    > 
    > 結論就是深度學習沒什麼神奇，在泛化方面深度學習需要的理論與經典線性網絡沒什麼不同，泛化本身指收斂至期望誤差，尤其是在過參數化時出現了過擬合缺失的情況。本研究分析通過將線性網絡的特性（如 [1] 強調的那些）應用到深度網絡，解釋了深度網絡泛化方面的難題，即不會過擬合期望分類誤差。
    > 
    > **8 討論**
    > 
    > 當然，構建對深度網絡性能有用的量化邊界仍然是一個開放性問題，因爲它是非常常見的情形，即使是對於簡單的僅包含一個隱藏層的網絡，如 SVM。本論文研究者主要的成果是圖 2 所展示的令人費解的行爲可以通過經典理論得到定性解釋。
    > 
    > 該領域存在很多開放性問題。儘管本文解釋了過擬合的缺失，即期望誤差對參數數量增加的容錯，但是本文並未解釋爲什麼深度網絡泛化得這麼好。也就是說，本論文解釋了爲什麼在參數數量增加並超過訓練數據數量時，圖 2 中的測試分類誤差沒有變差，但沒有解釋爲什麼測試誤差這麼低。
    > 
    > 基於 [20]、[18]、[16]、[10]，研究者猜測該問題的答案包含在以下深度學習理論框架內：
    > 
    > -   不同於淺層網絡，深度網絡能逼近層級局部函數類，且不招致維數災難（[21, 20]）。
    > 
    > -   經由 SGD 選擇，過參數化的深度網絡有很大概率會產生很多全局退化，或者大部分退化，以及「平滑」極小值（[16]）。
    > 
    > -   過參數化，可能會產生預期風險的過擬合。因爲梯度下降方法獲得的間隔最大化，過參數化也能避免過擬合低噪聲數據集的分類誤差。
    > 
    > 根據這一框架，淺層網絡與深度網絡之間的主要區別在於，基於特定任務的組織結構，兩種網絡從數據中學習較好表徵的能力，或者說是逼近能力。不同於淺層網絡，深度局部網絡特別是卷積網絡，能夠避免逼近層級局部合成函數類時的維度災難（curse of dimensionality）。這意味着對於這類函數，深度局部網絡可以表徵一種適當的假設類，其允許可實現的設置，即以最小容量實現零逼近誤差。
    > 
    > **論文：Theory IIIb: Generalization in Deep Networks**
    > 
    > ![](http://i2.bangqu.com/j/news/20180717/qA6W5k1531803623697vY5a7.png)
    > 
    > 論文鏈接：<https://arxiv.org/abs/1806.11379>
    > 
    > **摘要：**深度神經網絡（DNN）的主要問題圍繞着「過擬合」的明顯缺失，本論文將其定義如下：當神經元數量或梯度下降迭代次數增加時期望誤差卻沒有變差。鑑於 DNN 擬合隨機標註數據的大容量和顯性正則化的缺失，這實在令人驚訝。近期 Srebro 等人的研究結果爲二分類線性網絡中的該問題提供瞭解決方案。他們證明損失函數（如 logistic、交叉熵和指數損失）最小化可在線性分離數據集上漸進、「緩慢」地收斂到最大間隔解，而不管初始條件如何。本論文中我們證明了對於非線性多層 DNN 在經驗損失最小值接近零的情況下也有類似的結果。指數損失的結果也是如此，不過不適用於平方損失。具體來說，我們證明深度網絡每一層的權重矩陣可收斂至極小范數解，達到比例因子（在獨立案例下）。我們對動態系統的分析對應多層網絡的梯度下降，這展示了一種對經驗損失的不同零最小值泛化性能的簡單排序標準。




- [深度神經網絡爲什麼不易過擬合？傅里葉分析發現固有頻譜偏差 - 幫趣](http://bangqu.com/5424Y1.html)
    - [[1806.08734] On the Spectral Bias of Deep Neural Networks](https://arxiv.org/abs/1806.08734)

    > 1.  本文展示了對於參數 θ 的任意有限值來說，深度神經網絡（DNN）的 ReLU 函數的一個特定的頻率分量（k）的量級至少以 O(1/k^2 ) 的速率衰減，並且網絡的寬度和深度分別以多項式和指數的級別幫助其捕獲更高的頻率；因此，高頻分量的大小會更小（DNN 更容易趨向於光滑）。其結果是，對深度神經網絡（DNN）進行有限步訓練使其更趨向於表示如上面所描述的函數。
    > 
    > 3.  作爲這一理論的附帶結果，研究者揭示了（有限權重）深度神經網絡在學習類似狄拉克 delta 函數（單位脈衝函數）峯函數的理論極限。這是因爲它的傅立葉變換的量級是一個常值函數（因此所有的頻率都有相同的振幅）。並且如上文中所討論的，深度神經網絡（DNN）無法學習對這樣的函數建模，因爲它們的傅立葉係數必須至少以 1/k^2 的速率衰減（儘管增加寬度和深度可以分別以多項式級和指數級別幫助其捕獲更高的頻率）。
    > 
    > 5.  研究者指出，如果在低維流形上定義數據-目標函數的映射，深度神經網絡（DNN）可以利用流形的幾何結構來對函數取近似，這些函數沿着流形（其函數的頻率分量相對於其輸入空間較低）具有高頻分量。
    > 
    > 7.  通過分析實驗表明，對於一個在 CIFAR-10 數據集上訓練的深度神經網絡（DNN）來說，存在幾乎線性的路徑能夠連接所有的對抗性樣本，它們被分類成一個特定的類（比如「貓」）。對於所有真正類別爲「貓」的訓練樣本，所有的樣本也沿着這條路徑被分類成同一個類別------「貓」。研究者進一步展示了對於在 CIFAR-10 數據集上訓練的深度神經網絡（DNN）來說，所有同一個類別中的訓練樣本也通過同樣的方式連接起來。
    > 
    > 9.  實驗表明，與帶有高頻分量的函數相對應的深度神經網絡（DNN）在參數空間中所佔的體積更小。
    > 
    > ![](http://i2.bangqu.com/j/news/20180715/5424Y115316560033654hX78.png)*圖 2：展示訓練期間（y 軸）頻譜演變（x 軸）的熱圖。顏色代表測量出的在相應的頻率上網絡頻譜的幅值，其值用相同的頻率的目標幅值進行了歸一化操作。此圖說明，儘管更高頻率的訓練數據具有 g 的振幅，深度神經網絡仍然優先訓練低頻數據。*
    > 
    > ![](http://i2.bangqu.com/j/news/20180715/5424Y11531656003972AfN5s.png)*圖 3: 一個深度爲 D、寬度爲 W，權重修剪 K=0.1 的網絡被訓練去預測一個 delta 峯（所有頻率的振幅都相同）。在圖（a）和圖（b）中，y 軸對應於不斷增加的訓練迭代次數（向上遞增），x 軸則代表頻域（右圖）和輸入域（左圖）。更亮的顏色表示數值更大。此圖說明，根據理論所闡述的，寬度和深度分別以多項式和指數級幫助網絡捕獲高頻分量。這一點在輸入域和頻域上都可以看出來（注：64^4=8^8）。更多的圖片請參見附錄（圖 11）。*
    > 
    > ![](http://i2.bangqu.com/j/news/20180715/5424Y115316560043688X739.png)*圖 5: 在圖 3 中所使用的 delta 峯數據集上，一個深度爲 D 層、寬度爲 W 個單元的網絡的所有權重的譜範數（y 軸）與訓練過程中迭代次數（x 軸）的關係圖。*
    > 
    > 對於矩陣值權重，它們的譜範數是通過估計由 10 次冪迭代得到的特徵向量的特徵值計算而來。對於向量值權重，則僅使用了 L2 範數。此圖說明，隨着神經網絡通過學習去擬合更大的頻率，神經網絡權值的譜範數也增大，從而鬆弛頻譜的邊界![](http://i2.bangqu.com/j/news/20180715/5424Y1153165600501439q4U.png)
    > 
    > ![](http://i2.bangqu.com/j/news/20180715/5424Y11531656005232cEkz5.png)*圖 6: 在圖（a）和圖（b）中，左圖：L=0 瓣（虛線圓）；右圖：L=20 瓣（由 20 瓣組成的虛線花）定義了數據的流形。*
    > 
    > 對於這兩個流形，我們沿着流形定義了一個頻率爲 k Hz 的正弦信號，並將它二值化，得到一個 0/1 的目標（點的顏色）。對於每種情況，研究者訓練了一個 6 層深的 ReLU 網絡，將數據樣本從流形映射到它相應的目標上。填充的顏色表示預測出的類，等高線表示該網絡經過 sigmoid 函數處理的對數 logits 的絕對值。此圖說明，對應較大的 L 的流形，即使在兩種流形沿着流形的目標頻率相同時，也能使深度神經網絡在其域空間學習到更光滑的函數。可以看到，網絡會學習利用 L 值較大的流形的幾何結構去學習關於其輸入空間的低頻函數。這個結論在另一個實驗中得到了證實。
    > 
    > ![](http://i2.bangqu.com/j/news/20180715/5424Y11531656005954b5U49.png)*圖 8: 用於預測定義在一個 L 瓣的流形（y 軸）上的給定頻率（x 軸）的二值化正弦波的訓練分類準確率的熱圖。此圖說明，如果目標信號的頻率較低或數據定義在一個具有更大的 L 的流形上，固定大小的網絡的準確率越高。後者的結果表明，隨着流形中瓣數的增加，在一個流形上學習一個高頻目標就變得更容易。*
    > 
    > ![](http://i2.bangqu.com/j/news/20180715/5424Y115316560064227H7c2.png)*圖 9: 每一行都展示了圖像空間中的一條路徑，從右至左顯示了從對抗性樣本變爲一個真實訓練圖像的過程。*


# Loss function



## MSE 均方誤差(L2 損失)

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529651503728.jpg)
    > 
    > 均方誤差 (MSE) 是最常用的回歸損失函數，計算方法是求預測值與真實值之間距離的平方和，公式如圖。
    > 
    > 下圖是 MSE 函數的圖像，其中目標值是 100，預測值的範圍從 -10000 到 10000，Y 軸代表的 MSE 取值範圍是從 0 到正無窮，並且在預測值為 100 處達到最小。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650984294.jpg)

## MAE 平均絕對值誤差（也稱 L1 損失）

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650940657.jpg)
    > 
    > 平均絕對誤差（MAE）是另一種用於回歸模型的損失函數。MAE 是目標值和預測值之差的絕對值之和。其只衡量了預測值誤差的平均模長，而不考慮方向，取值範圍也是從 0 到正無窮（如果考慮方向，則是殘差/誤差的總和——平均偏差（MBE））。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650862540.jpg)
    > 
    > ---
    > 
    > MSE 對誤差取了平方（令 e=真實值-預測值），因此若 e>1，則 MSE 會進一步增大誤差。如果數據中存在異常點，那麼 e 值就會很大，而 e²則會遠大於|e|。
    > 
    > 因此，相對於使用 MAE 計算損失，使用 MSE 的模型會賦予異常點更大的權重。在第二個例子中，用 RMSE 計算損失的模型會以犧牲了其他樣本的誤差為代價，朝著減小異常點誤差的方向更新。然而這就會降低模型的整體性能。
    > 
    > 如果訓練數據被異常點所污染，那麼 MAE 損失就更好用（比如，在訓練數據中存在大量錯誤的反例和正例標記，但是在測試集中沒有這個問題）。
    > 
    > 直觀上可以這樣理解：如果我們最小化 MSE 來對所有的樣本點只給出一個預測值，那麼這個值一定是所有目標值的平均值。但如果是最小化 MAE，那麼這個值，則會是所有樣本點目標值的中位數。眾所周知，對異常值而言，中位數比均值更加魯棒，因此 MAE 對於異常值也比 MSE 更穩定。
    > 
    > 然而 MAE 存在一個嚴重的問題（特別是對於神經網絡）：更新的梯度始終相同，也就是說，即使對於很小的損失值，梯度也很大。這樣不利於模型的學習。為了解決這個缺陷，我們可以使用變化的學習率，在損失接近最小值時降低學習率。
    > 
    > 而 MSE 在這種情況下的表現就很好，即便使用固定的學習率也可以有效收斂。MSE 損失的梯度隨損失增大而增大，而損失趨於 0 時則會減小。這使得在訓練結束時，使用 MSE 模型的結果會更精確。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650753085.jpg)
    > 

## Huber 

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > Huber 損失對數據中的異常點沒有平方誤差損失那麼敏感。它在 0 也可微分。本質上，Huber 損失是絕對誤差，只是在誤差很小時，就變為平方誤差。誤差降到多小時變為二次誤差由超參數 δ（delta）來控制。當 Huber 損失在 \[0-δ,0+δ\] 之間時，等價為 MSE，而在 \[-∞,δ\] 和 \[δ,+∞\] 時為 MAE。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650718886.jpg)
    > 
    > 這裡超參數 delta 的選擇非常重要，因為這決定了你對與異常點的定義。當殘差大於 delta，應當採用 L1（對較大的異常值不那麼敏感）來最小化，而殘差小於超參數，則用 L2 來最小化。
    > 
    > **為何要使用 Huber 損失？**
    > 
    > 使用 MAE 訓練神經網絡最大的一個問題就是不變的大梯度，這可能導致在使用梯度下降快要結束時，錯過了最小點。而對於 MSE，梯度會隨著損失的減小而減小，使結果更加精確。
    > 
    > 在這種情況下，Huber 損失就非常有用。它會由於梯度的減小而落在最小值附近。比起 MSE，它對異常點更加魯棒。因此，Huber 損失結合了 MSE 和 MAE 的優點。但是，Huber 損失的問題是我們可能需要不斷調整超參數 delta。


## Log-Cosh

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > Log-cosh 是另一種應用於回歸問題中的，且比 L2 更平滑的的損失函數。它的計算方式是預測誤差的雙曲餘弦的對數。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650678423.jpg)
    > 
    > 優點：對於較小的 x，log(cosh(x)) 近似等於 (x^2)/2，對於較大的 x，近似等於 abs(x)-log(2)。這意味著’logcosh’ 基本類似於均方誤差，但不易受到異常點的影響。它具有 Huber 損失所有的優點，但不同於 Huber 損失的是，Log-cosh 二階處處可微。
    > 
    > 為什麼需要二階導數？許多機器學習模型如 XGBoost，就是採用牛頓法來尋找最優點。而牛頓法就需要求解二階導數（Hessian）。因此對於諸如 XGBoost 這類機器學習框架，損失函數的二階可微是很有必要的。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650587995.jpg)
    > 
    > 但 Log-cosh 損失也並非完美，其仍存在某些問題。比如誤差很大的話，一階梯度和 Hessian 會變成定值，這就導致 XGBoost 出現缺少分裂點的情況。

## Quantile

- [機器學習大神最常用的 5 個回歸損失函數，你知道幾個？ | TechOrange](https://buzzorange.com/techorange/2018/06/22/computer-learning-5-tips/)

    > 當我們更關注區間預測而不僅是點預測時，分位數損失函數就很有用。使用最小二乘回歸進行區間預測，基於的假設是殘差（y-y_hat）是獨立變量，且方差保持不變。
    > 
    > 一旦違背了這條假設，那麼線性回歸模型就不成立。但是我們也不能因此就認為使用非線性函數或基於樹的模型更好，而放棄將線性回歸模型作為基線方法。這時，分位數損失和分位數回歸就派上用場了，因為即便對於具有變化方差或非正態分佈的殘差，基於分位數損失的回歸也能給出合理的預測區間。
    > 
    > 下面讓我們看一個實際的例子，以便更好地理解基於分位數損失的回歸是如何對異方差數據起作用的。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650532097.jpg)
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650491449.jpg)
    > 
    > ---
    > 
    > 如何選取合適的分位值取決於我們對正誤差和反誤差的重視程度。損失函數通過分位值（γ）對高估和低估給予不同的懲罰。例如，當分位數損失函數 γ=0.25 時，對高估的懲罰更大，使得預測值略低於中值。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650454121.jpg)
    > 
    > γ 是所需的分位數，其值介於 0 和 1 之間。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650428251.jpg)
    > 
    > 這個損失函數也可以在神經網絡或基於樹的模型中計算預測區間。以下是用 Sklearn 實現梯度提升樹回歸模型的示例。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650390909.jpg)
    > 
    > 上圖表明：在 sklearn 庫的梯度提升回歸中使用分位數損失可以得到 90％ 的預測區間。其中上限為 γ=0.95，下限為 γ=0.05。
    > 
    > ---
    > 
    > **對比研究**
    > 
    > 為了證明上述所有損失函數的特點，讓我們來一起看一個對比研究。首先，我們建立了一個從 sinc（x）函數中採樣得到的數據集，並引入了兩項人為噪聲：高斯噪聲分量 ε〜N（0，σ2）和脈衝噪聲分量 ξ〜Bern（p）。
    > 
    > 加入脈衝噪聲是為了說明模型的魯棒效果。以下是使用不同損失函數擬合 GBM 回歸器的結果。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/06/1529650322786.jpg)
    > 
    > 連續損失函數：（A）MSE 損失函數；（B）MAE 損失函數；（C）Huber 損失函數；（D）分位數損失函數。將一個平滑的 GBM 擬合成有噪聲的 sinc（x）數據的示例：（E）原始 sinc（x）函數；（F）具有 MSE 和 MAE 損失的平滑 GBM；（G）具有 Huber 損失的平滑 GBM ，且δ={4,2,1}；（H）具有分位數損失的平滑的 GBM，且α={0.5,0.1,0.9}。
    > 
    > 仿真對比的一些觀察結果：
    > 
    > *   MAE 損失模型的預測結果受脈衝噪聲的影響較小，而 MSE 損失函數的預測結果受此影響略有偏移。
    > *   Huber 損失模型預測結果對所選超參數不敏感。
    > *   分位數損失模型在合適的置信水平下能給出很好的估計。
    > 



## Cross Entropy

- [A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)



