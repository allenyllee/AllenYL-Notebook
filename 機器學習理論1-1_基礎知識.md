# 機器學習理論1-1_基礎知識

[toc]
<!-- toc --> 

# Reference

- [Machine Learning Foundations](https://www.slideshare.net/albertycchen/machine-learning-foundations-87730305)

- [When not to use deep learning](http://www.kdnuggets.com/2017/07/when-not-use-deep-learning.html#.WXYzsXf1Hgg.linkedin)

- [10 Famous Machine Learning Experts - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/10-famous-machine-learning-experts)

- [21 Great Articles and Tutorials on Time Series - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/21-great-articles-and-tutorials-on-time-series?utm_content=buffer13856&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)

- [How to Train a Final Machine Learning Model - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/how-to-train-a-final-machine-learning-model)

- [The AI Revolution: Why Deep Learning Is Suddenly Changing Your Life](http://fortune.com/ai-artificial-intelligence-deep-machine-learning/)

- [25 Timeless Data Science Articles - Data Science Central](http://www.datasciencecentral.com/profiles/blogs/25-timeless-data-science-articles?utm_content=buffer6dcc9&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)

- [The major advancements in Deep Learning in 2016 - Tryolabs Blog](https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/)

- [So you are interested in deep learning · fast.ai](http://www.fast.ai/2016/12/19/favorite-posts/)

- [50 things I learned at NIPS 2016 – Ought](https://blog.ought.com/nips-2016-875bb8fadb8c)

- [How to Detect if Numbers are Random or Not - AnalyticBridge](https://www.analyticbridge.datasciencecentral.com/profiles/blogs/mysterious-sequences-that-look-random-with-surprising-properties)

- [標題黨太嚇人？這篇文章會告訴你DeepMind關係推理網絡的真實面貌 - 幫趣](http://bangqu.com/79N1q2.html)

- [DeepMind’s Relational Networks — Demystified – Hacker Noon](https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643)

- [Software 2.0 – Andrej Karpathy – Medium](https://medium.com/@karpathy/software-2-0-a64152b37c35)

- [The Emergence of Modular Deep Learning – Intuition Machine – Medium](https://medium.com/intuitionmachine/the-end-of-monolithic-deep-learning-86937c86bc1f)

- [Decoupled Neural Interfaces Using Synthetic Gradients | DeepMind](https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/)

- [Why AlphaGo Zero is a Quantum Leap Forward in Deep Learning](https://medium.com/intuitionmachine/the-strange-loop-in-alphago-zeros-self-play-6e3274fcdd9f)

- [Is Deep Learning Innovation Just Due to Brute Force?](https://medium.com/intuitionmachine/the-brute-force-method-of-deep-learning-innovation-58b497323ae5)

- [Is Deep Learning “Software 2.0”? – Intuition Machine – Medium](https://medium.com/intuitionmachine/is-deep-learning-software-2-0-cc7ad46b138f)


- [打響新年第一炮，Gary Marcus提出對深度學習的系統性批判 - 幫趣](http://bangqu.com/p8Nr93.html)

- [有哪些职业容易被人工智能替代，又有哪些行业不易被人工智能替代？ - 知乎](https://www.zhihu.com/question/61829257/answer/231679440)


- [[科普]如何使用高大上的方法调参数 - 知乎专栏](https://zhuanlan.zhihu.com/p/27555858)


- [如果机器人有了思想，我们该怎么办？ - 知乎专栏](https://zhuanlan.zhihu.com/p/28372529)


- [面试官如何判断面试者的机器学习水平？ - 知乎](https://www.zhihu.com/question/62482926/answer/210531386)


- [机器学习中如何做单元测试(Unit Test)来检测模型稳定性？ - 知乎](https://www.zhihu.com/question/20225527/answer/209958746)

- [如何评价多伦多大学新建的向量学院 (Vector Institute)？对人工智能领域会有何影响? - 知乎](https://www.zhihu.com/question/57768703/answer/209013477)


- [反欺诈(Fraud Detection)中所用到的机器学习模型有哪些？ - 知乎](https://www.zhihu.com/question/30508773/answer/205831957)


- [带你读机器学习经典(一): An Introduction to Statistical Learning (Chapter 1&2) - 知乎专栏](https://zhuanlan.zhihu.com/p/27556007)


- [带你读机器学习经典(二): An Introduction to Statistical Learning (Chapter 3.1 线性回归) - 知乎专栏](https://zhuanlan.zhihu.com/p/27609785)


- [随机森林是否需要后剪枝？sklearn为什么没有实现这个功能，是否有人实现了这个功能？ - 知乎](https://www.zhihu.com/question/59826974/answer/175520871)

- [K-means聚类算法如何应对数据的噪音和离散特征处理的问题？ - 知乎](https://www.zhihu.com/question/60868444/answer/188816384)

- [试玩人脸识别 - 知乎专栏](https://zhuanlan.zhihu.com/p/27275307)

- [R 語言使用者的 Python 學習筆記 系列文章列表 - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天](http://ithelp.ithome.com.tw/users/20103511/ironman/1077)

- [[第 25 天] 機器學習（5）整體學習(Ensemble learning) - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天](http://ithelp.ithome.com.tw/articles/10187452)


- [一文读懂深度学习与机器学习的差异 - 技术翻译 - 开源中国社区](https://www.oschina.net/translate/deep-learning-vs-machine-learning?lang=chs&page=1#)

- [Comparison between Deep Learning & Machine Learning](https://www.analyticsvidhya.com/blog/2017/04/comparison-between-deep-learning-machine-learning/?utm_source=dzone&utm_medium=social)

# 大師觀點


## [Yann LeCun - Quora](https://www.quora.com/profile/Yann-LeCun)


- [Is industry demand for ML researchers mostly limited to giants like Facebook and the occasional startup, or is it more ubiquitous? - Quora](https://www.quora.com/Is-industry-demand-for-ML-researchers-mostly-limited-to-giants-like-Facebook-and-the-occasional-startup-or-is-it-more-ubiquitous)

    > For ML engineers, it’s completely ubiquitous. Everyone wants to hire ML engineers.
    > 
    > For researchers, only a small number of large and prominent companies have real research labs: Facebook, Google, IBM, Microsoft, Adobe and a few others.


- [Is getting a masters or a PhD necessary to get into top AI/ML research groups like FAIR or DeepMind?](https://www.quora.com/Is-getting-a-masters-or-a-PhD-necessary-to-get-into-top-AI-ML-research-groups-like-FAIR-or-DeepMind)

    > There are 6 types of positions at FAIR:
    > 
    > * Research Scientist: you need a PhD, a couple years of experience in research (e.g. as a postdoc) and a good publication record. It’s a pretty high bar.
    > * Research Engineer: you need a Master with some exposure to ML/AI in your previous studies or job. Most of these positions are relatively junior, but there are a few senior. About 25 to 30% of people at FAIR are research engineers.
    > * Postdoc: it’s a 1 or 2 year limited-term research position, generally directly after your PhD.
    > * PhD student: in our Paris lab, we can take a small number of PhD students under what’s called a CIFRE status. This is a special thing in France that allows PhD students to spend most of their time in an industry research lab, co-advised by a researcher in the company and a professor in a university.
    > * Intern: we take summer interns, and sometimes interns during the academic year. Almost all of them are in PhD programs. In continental Europe where people do undergrad + 2 year Master + 3 year PhD, we take some interns between their Master and PhD.
    > 
    > Look here for a list of FAIR member and their backgrounds: Researchers
    > 


- [What is the primary focus of Facebook AI research? - Quora](https://www.quora.com/What-is-the-primary-focus-of-Facebook-AI-research)

    > Solving intelligence and building truly intelligent machines.
    > 
    > We are working on getting learning machine to model their environment, to remember, to reason, and to plan.
    > 
    > For this, we use video games (we have hooked up the Unreal 3D game engine tot Torch, the deep learning environment), and various real and virtual environments.
    > 
    > We also work on applications of AI in image and video understanding, text understanding, dialog systems, language translation, speech recognition, text generation, and other more esoteric domains.



- [Is deep learning a bubble nowadays? Everybody tries to use it in different fields extensively, without knowing what they do. - Quora](https://www.quora.com/Is-deep-learning-a-bubble-nowadays-Everybody-tries-to-use-it-in-different-fields-extensively-without-knowing-what-they-do)

    > There is a lot of hype around AI and deep learning at the moment. Hype is bad because it creates heightened expectations and causes disappointment when these expectations are not met. This is partly what caused “AI winters” in the past.    
    > 
    > So, if you see some egregious hype, call it for what it is. I try to do that whenever I can. 
    > 
    > Startups have a huge incentive to hype what they can do because they need to attract investments or customers. It’s not because a company attracts investments that it’s not hyped: a number of AI companies that have attracted large investments are nothing more than empty hype machines.    
    > 
    > That said, deep learning produces real results and is at the root of a real industry that makes money today. The promises of it in the near future are very exciting (even without the hype) in areas like self-driving cars, medical imaging, personalized medicine, content filtering/ranking, etc.
    > 



- [What are the likely AI advancements that we will see between now and 2027? - Quora](https://www.quora.com/What-are-the-likely-AI-advancements-that-we-will-see-between-now-and-2027)

    > There is a number of areas on which people are working hard and making promising advances:
    > 
    > * deep learning combined with reasoning and planning
    > * deep model-based reinforcement learning (which involved unsupervised predictive learning)
    > * recurrent neural nets augmented with differentiable memory modules (e.g. Memory Networks:
    > - Memory Networks (FAIR): https://scholar.google.com/citat...
    > - Stack-Augmented RNN (FAIR): Google Scholar Citations
    > - Neural Turing Machine (DeepMind): [1410.5401] Neural Turing Machines
    > - End-toEnd MemNN (FAIR/NYU): https://scholar.google.com/citat...
    > - and the flurry of follow-up papers.
    > * generative/predictive models trained with adversarial training
    > * “differentiable programming”: this is the idea of viewing a program (or a circuit) as a graph of differentiable modules that can be trained with backprop. This points towards the possibility of not just learning to recognize patterns (as with feed-forward neural nets) but to produce algorithms (with loops, recursion, subroutines, etc). There are a few papers on this from DeepMind, FAIR and others, but it’s rather preliminary at the moment.
    > * Hierarchical planning and hierarchical reinforcement learning: this is the problem of learning decompose a complex task into simpler subtasks. It seems like a requirement for intelligent systems.
    > * Learning predictive models of the world in an unsupervised fashion (e.g. video prediction)
    > 
    > If significant progress is made along these directions in the next few years, we might see the emergence of considerably more intelligent AI agents for dialog systems, question-answering, adaptive robot control and planning, etc.
    > 
    > A big challenge is to devise unsupervised/predictive learning methods that would allow very large-scale neural nets to “learn how the world works” by watching videos, reading textbooks, etc, without requiring explicit human-annotated data.
    > 
    > This may eventually lead to machines that have learned enough about the world that we see them as having “common sense”.
    > 
    > It may take 5 years, 10 years, 20 years, or more. We don’t really know.



- [What are your recommendations for self-studying machine learning? - Quora](https://www.quora.com/What-are-your-recommendations-for-self-studying-machine-learning)

    > There is tons of on-line material, tutorials and courses on ML, including Coursera lectures.
    > 
    > I’ll respond more specifically for deep learning. You can get a broad idea of deep what deep learning is about through tutorial lectures that are available from the Web. Most notably:
    > 
    > an overview paper in Nature by myself, Yoshua Bengio and Geoff Hinton with lots of pointers to the literature: https://scholar.google.com/citat...
    > The Deep Learning textbook by Goodfellow, Bengio and Courville: Deep Learning
    > A recent series of 8 lectures on deep learning that I gave at Collège de France in Paris. The lectures were taught in French and later dubbed in English:
    > French version: Accueil
    > English version: Home
    > the Coursera course on neural nets by Geoff Hinton (starting to be a bit dated).
    > the lectures from the 2012 IPAM Summer School on Deep Learning: Graduate Summer School: Deep Learning, Feature Learning (Schedule) - IPAM
    > my 2015 course on Deep Learning at NYU: deeplearning2015:schedule | CILVR Lab @ NYU (unfortunately, the videos of the lectures had to be taken down due to stupid legal reasons, but the slides are there). I’m teaching this course again in the Spring of 2017.
    > The 2015 deep learning summer school: Deep Learning Summer School, Montreal 2015
    > Various tutorials generally centered on using a particular software platform, like Torch, TensorFlow or Theano.



- [What’s your advice for undergraduate student who aspires to be a research scientist in deep learning or related field one day?](https://www.quora.com/What%E2%80%99s-your-advice-for-undergraduate-student-who-aspires-to-be-a-research-scientist-in-deep-learning-or-related-field-one-day)

    - [大神Yann LeCun亲授：如何自学深度学习技术并少走弯路 | 雷锋网](https://www.leiphone.com/news/201611/cWf2B23wdy6XLa21.html)

    > (0) take all the continuous math and physics class you can possibly take. If you have the choice between “iOS programming” and “quantum mechanics”, take “quantum mechanics”. In any case, take Calc I, Calc II, Calc III, Linear Algebra, Probability and Statistics, and as many physics courses as you can. But make sure you learn to program.
    > (1) Take an AI-related problem you are passionate about.
    > (2) think about it on your own
    > (3) once you have formed your own idea of it, start reading the literature on the problem
    > (4) you will find that (a) your ideas were probably a bit naive but (b) your view of the problem is slightly different from what was done before.
    > (5) Find a professor in your school that can help you make your ideas concrete. It might be difficult. Professors are busy and don’t have much time for undergrads. The ones with the most free time are the very junior, the very senior, and the ones who are not very active in research.
    > (6) If you don’ find a professor with spare time, hook up with a postdoc or PhD student in his/her lab.
    > (7) ask the professor if you can attend his/her lab meetings and seminars or sit in his/her class.
    > (8) Before you graduate, try to write a paper about your research or release a piece of open source code.
    > (9) Now apply to PhD programs. Forget about the “ranking” of the school for now. Find a reputable professor who works on topics that you are interested in. Pick a person whose papers you like or admire.
    > (10) Apply to several PhD programs in the schools of the above-mentioned professors and mention in your letter that you’d like to work with that professor but would be open to work with others.
    > (11) ask your undergrad professor to write a recommendation letter for you. It’s maximally efficient if your undergrad professor is known by your favorite PhD advisor.
    > (12) if you don’t get accepted in one of your favorite PhD programs, get a job at Facebook or Google and try to get a gig as an engineer assisting research scientists at FAIR or Google Brain.
    > (13) publish a papers with the research scientists in question. Then re-apply to PhD programs and ask the FAIR or Google scientists you work with to write a recommendation letter for you.
    > 
    > ---
    > Why is physics important?
    > 
    > 
    > Because physics has invented a lot of mathematical methods to model the real world. Foe example, the mathematics of BayesIan inference is essentially identical to statistical mechanics. Backpropagation can be shown to be a simple application of Lagrangian methods developed in classical mechanics. The forward algorithm in graphical models is a path integral (widely used in quantum mechanics). Physics teaches you about the use of Fourier transforms (the root of Heisenberg uncertainty principle), vector files as gradients of a potential function, maximum entropy principle, partition functions, Monte Carlo methods, annealing,, Gibbs Boltzmann distributions, dynamical systems, chaos, etc....
    > 

## [微调 - 知乎](https://www.zhihu.com/people/breaknever/activities)

- [未來 3 至 5 年，哪個方向的機器學習人才最缺？ | TechNews 科技新報](https://technews.tw/2017/09/11/what-kind-of-people-does-machine-learning-need/)

    > 1. 基本功
    > 
    >    說到底機器學習還是需要一定的專業知識，這可以透過學校學習或自學完成。但有沒有必要通曉數學，擅長最佳化呢？我的看法是不需要，大前提是要了解基本的數學統計知識即可，更多討論可看我在「阿薩姆：如何看待『機器學習不需要數學，很多算法封裝好了，調個包就行』這種說法？」的答案。最低程度我建議掌握五個小方向，對於現在和未來幾年內的業界夠用了。再次重申，我對演算法的看法是大部分人不要造輪子、不要造輪子、不要造輪子！只要理解自己在做什麼，知道選什麼模型，直接呼叫 API 和現成的工具包就好了。
    > 
    >    回歸模型（Regression）。學校課程其實講更多分類，但事實上回歸才是業界最常見的模型。比如產品定價或預測產品的銷量都需要回歸模型。現階段比較流行的回歸方法是以數為模型的 xgboost，預測效果很好，還可以自動排序變數重要性。傳統的線性回歸（一元和多元）也還會繼續流行下去，因為良好的可解釋性和低運算成本。如何掌握回歸模型？建議閱讀《Introduction to Statistical Learning》的 2-7 章，並看一下 R 裡 xgboost 的 package 介紹。
    > 
    >    分類模型（Classification）。老生常談，但應該對現在流行並繼續流行下去的模型有深刻了解。舉例，隨機森林（Random Forests）和支援向量機（SVM）都屬於現在業界常用的演算法。可能很多人想不到的是，邏輯回歸（Logistic Regression）這個常見於大街小巷每本教科書的經典老演算法，依然占據業界半壁江山。這個部分建議看李航《統計學習算法》，挑著看相對應的那幾章即可。
    > 
    >    神經網路（Neural Networks）。我沒有把神經網路歸結到分類算法還是因為現在太紅了，有必要學習了解一下。隨著硬體能力的持續增長和資料集愈豐富，神經網路在中小企業的發揮之處肯定有。三、五年內，這個可能會發生。但有人會問，神經網路內容那麼多，比如架構，比如正則化，比如權重起始化技巧和觸發函數選擇，我們該學到什麼程度呢？我的建議還是抓住經典，掌握基本的三套網路：a. 普通的 ANN。b. 處理影像的 CNN。c. 處理文字和語音的 RNN（LSTM）。對每個基本網路只要了解經典的處理方式即可，具體可參照《深度學習》的 6~10 章和吳恩達的 Deep Learning 網路課程。
    > 
    >    資料壓縮／可視化（Data Compression & Visualization）。業界常見的就是先可視化資料，比如這兩年很紅的流形學習（manifold learning）就和可視化有很大的關係。業界認為做可視化是磨刀不誤砍柴工，把高維資料壓縮到 2 維或 3 維，可很快看到一些有意思的事，能節省大量時間。學習可視化可以使用現成的工具，如 Qlik Sense 和 Tableau，也可用 Python 的 Sklearn 和 Matplotlib。
    > 
    >    無監督學習和半監督學習（Unsupervised & Semi-supervised Learning）。業界另一個特點就是大量資料缺失，大部分情況都沒有標籤。以最常見的反詐騙為例，有標籤的資料非常少。所以一般都需要使用大量的無監督或半監督學習，來利用有限的標籤學習。多說一句，強化學習在大部分企業使用基本為 0，估計未來很長一陣子可能都不會有特別廣泛的應用。
    > 
    >    基本功的意義是當你面對具體問題時，很清楚可用什麼武器對付。上面介紹的很多工具都有幾十年歷史，依然歷久彌新。所以 3~5 年的跨度來看，這些工具依然非常有用，甚至像 CNN 和 LSTM 之類的深度學習演算法還在繼續發展。無論你還在學校或已工作，掌握這些基本技術，都可以透過自學在幾個月到一兩年內完成。
    > 
    > 
    > 2. 祕密武器
    > 
    > 
    >    已有工作／研究經驗的朋友，要試著利用自己的工作經歷。舉例，不要做機器學習裡最擅長投資的人，而要做金融領域中最擅長機器學習的專家，這才是你的價值主張（value proposition）。最重要的是，機器學習的基本功沒有大家想的那麼高不可攀，沒有必要放棄自己的專業全職轉行，沉沒成本太高。透過跨領域完全可做到曲線救國，化劣勢為優勢，你們可能比只懂機器學習的人有更大的產業價值。
    > 
    >    舉幾個我身邊的例子。一個朋友是做傳統軟體工程研究，前年他和我商量如何使用機器學習以 GitHub 上的 commit 歷史來辨識 bug，這就是一個結合領域的好知識。如果你本身是金融出身，在你補足基本功同時，就可以把機器學習交叉用於你擅長的領域，做策略研究，我已經聽說無數個「宣稱」使用機器學習實現交易策略的案例。雖不可盡信，但對特定領域的深刻理解往往就是捅破窗戶的最後一層紙，只理解模型但不了解資料和背後的意義，導致很多機器學習模型只停留在好看卻不實用的階段。
    > 
    > 
    > 3. 彈藥補給
    > 
    >    沒有什麼不會改變，這個時代的科技更新速度很快。從深度學習開始發力到現在也不過短短十年，所以沒有人知道下一個紅的是什麼。以深度學習為例，這兩年非常紅的對抗生成網路（GAN）、多目標學習（multi-lable learning）、遷移學習（transfer learning）都還在飛速發展。關於深度學習為什麼有良好泛化能力的理論猜想文章，在最新的 NIPS 聽說也收錄了好幾篇。這都說明了沒什麼產業可以靠吃老本瀟灑下去，我們需要追新的熱點。但機器學習的範圍和領域真的很廣，上面所說的都還是有監督的深度學習，無監督的神經網路和深度強化學習也是現在火熱的研究領域。所以我的建議是盡量關注、學習了解已成熟和已有實例的新熱點，不要凡熱點必追。
    > 


- [如何看待「机器学习不需要数学，很多算法封装好了，调个包就行」这种说法？ - 知乎](https://www.zhihu.com/question/60064269/answer/172305599)

    > 我认为：大部分机器学习从业者不需要过度的把时间精力放在数学上，而该用于熟悉不同算法的应用场景和掌握一些调参技巧。好的数学基础可以使你的模型简洁高效，但绝非必要的先决条件。
    > 
    > 
    > 
    > 原因如下：
    > 
    > 1. 即使你有了一定的数学功底，还是不知道怎么调参或者进行优化。这话说的虽然有点自暴自弃，但扪心自问在座的各位，当你发现accuracy不好、loss很高、模型已经overfitting了，你唰唰唰列列公式玩玩矩阵就知道问题出在哪里了吗？不一定。诚然，懂得更多的统计原理可以帮助推测问题出在了哪里，你可能换了一个loss function或者加了新的regularizer，但结果不一定会更好:(
    > 
    >    数学基础之于机器学习从业者很像debugger之于码农，它给了你方向，但不能保证你一定可以解决问题。那怎么能解决问题？只有经验经验经验，别无他法，有时候甚至靠的是直觉。数学基础是好的内功基础，但你调包调多了，其实也慢慢能抓到一些感觉，不必看不起“调包侠”。
    > 
    > 2. 工业界可以应用的模型是很有限的，可调的参数也是有限的。工业界选择模型非常看重可解释性，效率，以及和整个系统的整合能力。举例，在我的工作中，大部分时间都在使用Regression和Decision Tree相关的算法（如 Random Forests）。是因为这两个算法最好么？不，恰恰是因为这两个算法稳定及高效，而且容易解释。对于这样的模型，你即使数学能力很强，能调整的参数也是有限的。根据网上的例子和经验，大量的工程师可以在数学基础稍弱的情况下做到很好的效果。
    > 
    > 3. 数学/统计知识已经成了既得利益者刻意为外来者建立的一道壁垒。不知道大家有多少人是从事过ML研究的。我个人的观察是做出成绩的ML研究人员是有限的，科班出身的researcher更是远远无法工业界的空缺。所以大家没有必要担心会被转行者抢了饭碗，也没有必要刻意鼓吹一定要懂矩阵，凸优化，等数学知识才配做机器学习。大家都是出来卖的，不必互相为难。说来惭愧，在工作中我常常跟老板说这个人不能用，你要用我这种科班出身的人，但我内心是不赞同的。
    > 
    >    每当我看到知乎上有人问机器学习怎么入门，结果大家立马推荐第一本就看PRML和Statitical Learning以及一大堆公开课和数学课的时候，我的内心是崩溃的。各位答主的目标是把所有人都吓回去还是秀一下优越感？
    > 
    > 4. 理论模型和实际应用分的是两块不同的蛋糕。承接第2,3点，做理论研究的发力于突破，提出新的模型或者优化方法，做应用的致力于把模型应用于数据上，攫取商业价值。这两者不存在利益冲突，做理论的人有自带正统光环的优势，所以更该显得大度一些。只有“调包”的人越来越多，这个行业才会繁荣，因为证明技术落了地，可以带来实际价值。
    > 
    > 5. 行业的发展趋势是降低工具的使用难度，这让我们不必反复造轮子。亚马、逊谷歌、微软等各大平台都开放了他们的机器学习工具。以前人们还需要自己写各种模型，好一些的调一下sklearn，但现在Azure ML Studio已经方便到零代码了。年初的时候，我试了一下ML studio，简直方便的可怕，完全是图形拖动连接就可以建立模型，那一刻我仿似看到了自己即将失业。
    > 
    > 6. 文艺一点说，我们需要更包容的心态，切勿文人相轻。想要接触了解一门学科，应该先有兴趣，才有探索的积极性。就像我们第一次看到Hello word出现的样子，很多刚入行的人第一次看到机器学习能解决实际问题时，会产生浓厚的兴趣。
    > 


- [未来的人工智能有哪些商业模式？ - 知乎](https://www.zhihu.com/question/41848628/answer/221088507)

    > 3. 人工智能咨询与定制服务(AI Consulting and Customized Service)
    > 
    >    根据我自己的观察和分析，AI咨询和定制服务是未来很有潜力的模型。简单来说，就是根据企业/客户的需求进行定制化的人工智能解决方案。在现阶段，人工智能方案对于大部分企业来说还是“奢侈品”，甚至有些超前。但在不久的未来随着技术进一步成熟以及概念得到普及，价格和门槛也会下降，越来越多的中小型企业也可以负担并愿意进行人工智能升级。
    > 
    >    和创业公司不同，这个商业模型不要求高精尖技术或是在某个领域的突破，但通用的AI平台也无法完成客户定制的需求。这就是为什么这样的商业服务可能有前景 - 它和前两种商业模型有交集但并不重叠。
    > 
    >    这样的商业模型主要给客户提供两种服务：
    > 
    >    成熟的专利AI应用。举例，我们为A银行安装了一个我们开发并拥有专利的人工智能风控模型，在进行数据替换后还可以卖给B、C、D银行或者相似行业。银行可以使用我们的微调后的模型，但我们可以将原始模型进行无限次转卖。
    >    
    >    客户定制化服务。举例，A客户要求我们为它们独家定制服务，服务的归属权归客户所有，我们无权转卖，仅为客户进行维护升级。当然，这种服务的价格肯定较高。
    > 
    >    同时提供两种收费模式：
    > 
    >    一次性收费/升级费用(one-time purchase)。和其他软件产品一样，客户可以一次性买断服务的使用权。但并不建议这个模式，因为AI产品有较大的不稳定性，随着数据的变化模型可能失效。
    >    
    >    订阅服务(subscription based)。正因为AI产品需要常常升级，机器学习模型也需要重新训练，订阅服务更适合AI类产品。客户可以按月付费，得到相应的维护和升级服务。
    > 
    >    这样的商业模型还可以搭配主动式的营销手段。因为AI产品的本质是通过数据解决问题，据我所知很多企业现在已经和客户签署了“数据保留协议”，即AI产品供应商可以在特定范围内使用客户的数据进行其他活动。这样的协议有两个好处:
    > 
    >    精准营销(Customized Recommendation)。因为我们有权使用客户A的数据，根据分析其数据，我们可以个性化推荐适合客户A的其他产品。甚至我们可以使用客户A的数据为其免费定制一个概念产品。免费其实是一种营销手段，德勤的数据分析部门给客户50小时的免费时长来感受它们的产品。
    >    
    >    数据整合(Data Integration & Enrichment)。假设客户A、B、C和D都允许我们保留并使用其数据，那么我们可以进行整合并获得行业级别的数据，从而开发出更加智能的产品。
    > 
    >    在这个数据为王的时代，拥有客户的数据并提供定制化服务有非常强的客户黏性。总结一下，销售成熟的AI产品+适量的定制，留住客户的数据，并提供后续的维护和支持就是我觉得很有潜力的新型AI领域商业模型。
    > 
    >    从市场竞争角度来说，这个商业模型既不需要高精技术，也不大需要基础平台或者高额的固定投资，甚至还可以使用文中介绍的创业公司和科技巨头的服务。但根据经济学原理，低门槛，充分竞争的市场代表从长期来看不会有暴利存在。
    > 
    >    但如果能在早期拥有足够多的行业数据，数据优势将会使你的企业走在其他人之前。或许，是时候入场了...
    > 


- [有哪些职业容易被人工智能替代，又有哪些行业不易被人工智能替代？ - 知乎](https://www.zhihu.com/question/61829257/answer/231679440)

    > 所以在我看来，一个行业/职业达到被人工智能取代至少需要满足以下三个条件:
    > 
    > 1. 结构化的数据和良好的数据积累
    > 2. 清晰明确的任务定义
    > 3. 可接受的回报周期和高利润率
    > 
    > 因此总结来看，满足以上条件的暂时有:
    > 
    > 医疗：疾病检索、早期预测、手术机器人(勉强把机器人算作人工智能)
    > 翻译/律师：作为收入比较丰厚的行业，中低端的工作会被人工智能代替
    > 金融：作为数据结构化比较好且资金丰厚投入好的行业，越来越多的金融分析师都迎来了失业
    > 驾驶：其实运输行业是大家不大了解的体量巨大回报丰厚的行业，这也是为什么无人驾驶被那么多公司追捧
    > 高危行业：高危行业约等于高回报率，如矿洞机器人和高空作业机器人。
    > 

## 吳恩達

- [刚刚，吴恩达讲了干货满满的一节全新AI课，全程手写板书 - 量子位 - 知乎专栏](https://zhuanlan.zhihu.com/p/29504856?utm_medium=social&utm_source=ZHShareTargetIDMore)

    > 做AI产品要注意什么？
    > 
    > 有一个很有意思的趋势，是AI的崛起正改变着公司间竞争的基础。
    > 公司的壁垒不再是算法，而是数据。
    > 当我建立一家新公司，会特地设计一个循环：
    > 
    > 先为算法收集足够的数据，这样就能推出产品，然后通过这个产品来获取用户，用户会提供更多的数据……
    > 有了这个循环之后，对手就很难追赶你。
    > 
    > 这方面有一个很明显的例子：搜索公司。搜索公司有着大量的数据，显示如果用户搜了这个词，就会倾向于点哪个链接。
    > 我很清楚该如何构建搜索算法，但是如果没有大型搜索公司那样的数据集，简直难以想象一个小团队如何构建一个同样优秀的搜索引擎。这些数据资产就是最好的壁垒。
    > 
    > 工程师们还需要清楚这一点：
    > 
    > AI的范围，比监督学习广泛得多。我认为人们平时所说的AI，其实包含了好几类工具：比如机器学习、图模型、规划算法、知识表示（知识图谱）。
    > 人们的关注点集中在机器学习和深度学习，很大程度上是因为其他工具的发展速度很平稳。
    > 如果我现在建立一个AI团队，做AI项目，很多时候应该用图模型，有时应该用知识图谱，但是最大的机遇还是在于机器学习，这才是几年来发展最快、出现突破的领域。
    > 
    > 接下来我要和大家分享一下我看问题的框架。
    > 
    > 计算机，或者说算法是怎样知道该做什么的呢？它有两个知识来源，一是数据，二是人工（human engineering）。
    > 要解决不同的问题，该用的方法也不同。
    > 比如说在线广告，我们有那么多的数据，不需要太多的人工，深度学习算法就能学得很好。
    > 
    > 但是在医疗领域，数据量就很少，可能只有几百个样例，这时就需要大量的人工，比如说用图模型来引入人类知识。
    > 也有一些领域，我们有一定数量的数据，但同时也需要人工来做特征工程。
    > 
    > 当然，还要谈一谈工程师如何学习。
    > 
    > 很多工程师想要进入AI领域，很多人会去上在线课程，但是有一个学习途径被严重忽视了：读论文，重现其中的研究。
    > 当你读了足够多的论文，实现了足够多的算法，它们都会内化成你的知识和想法。
    > 
    > 要培养机器学习工程师，我推荐的流程是：上（deeplearning.ai的）机器学习课程来打基础，然后读论文并复现其中的结果，另外，还要通过参加人工智能的会议来巩固自己的基础。
    > 
    > 
    > △ 再擦一块白板（×3）
    > 怎样成为真正的AI公司？
    > 
    > 我接下来要分享的这个观点，可能是我今天所讲的最重要的一件事。
    > 从大约20年、25年前开始，我们开始看见互联网时代崛起，互联网成为一个重要的东西。
    > 我从那个时代学到了一件重要的事：
    > 
    > 商场 + 网站 ≠ 互联网公司
    > 
    > 我认识一家大型零售公司的CIO，有一次CEO对他说：我们在网上卖东西，亚马逊也在网上卖东西，我们是一样的。
    > 不是的。
    > 
    > 互联网公司是如何定义的呢？不是看你有没有网站，而是看做不做A/B测试、能不能快速迭代、是否由工程师和产品经理来做决策。
    > 
    > 这才是互联网公司的精髓。
    > 
    > 现在我们经常听人说“AI公司”。在AI时代，我们同样要知道：
    > 
    > 传统科技公司 + 机器学习/神经网络 ≠ AI公司（全场笑）
    > 
    > 公司里有几个人在用神经网络，并不能让你们成为一家AI公司，要有更深层的变化。
    > 20年前，我并不知道A/B测试对互联网公司来说有多重要。现在，我在想AI公司的核心是什么。
    > 
    > 我认为，AI公司倾向于策略性地获取数据。我曾经用过这样一种做法：在一个地区发布产品，为了在另一个地区发布产品而获取数据，这个产品又是为了在下一个地区发布产品来获取数据用的，如此循环。而所有产品加起来，都是为了获取数据驱动一个更大的目标。
    > 像Google和百度这样的大型AI公司，都有着非常复杂的策略，为几年后做好了准备。
    > 
    > 第二点是比较战术性的，你可能现在就可以开始施行：AI公司通常有统一的数据仓库。
    > 
    > 很多公司有很多数据仓库，很分散，如果工程师想把这些数据放在一起来做点什么，可能需要和50个不同的人来沟通。
    > 所以我认为建立一个统一的数据仓库，所有的数据都存储在一起是一种很好的策略。
    > 另外，普遍的自动化和新的职位描述也是AI公司的重要特征。
    > 
    > 比如说在移动互联网时代，产品经理在设计交互App的时候可能会画个线框图：
    > 然后工程师去实现它，整个流程很容易理清楚。
    > 
    > 但是假设在AI时代，我们要做一个聊天机器人，这时候如果产品经理画个线框图说：这是头像，这是聊天气泡，并不能解决问题。
    > 聊天气泡长什么样不重要，我需要知道的是，这个聊天机器人要说什么话。线框图对聊天机器人项目来说没什么用。
    > 如果一个产品经理画了个无人车的线框图，说“我们要做个这个”，更是没什么用。（全场笑）
    > 
    > 在AI公司里，产品经理在和工程师沟通的时候，需要学会运用数据，要求精确的反馈。

### 公司如何導入 AI ( 人工智慧 )

- [[閱讀筆記] 公司如何導入 AI ( 人工智慧 ) | Soft & Share](https://softnshare.com/read-company-ai-transform/)

    > 最近Landing AI CEO、Coursera 共同創辦人吳恩達( Andrew Ng )提供了 AI Transformation Playbook 供市值美金5億以上的公司參考。以他輔導 Google 、矽谷和世界知名公司導入 AI 的經驗，提供如何將 AI  融入公司既有的業務、創造價值的劇本。
    > 
    > 整個劇本是:
    > 
    >     先從對公司有意義但不影響主要盈利的專案入手，創造飛輪效應，獲得認同也提升進一步導入的動力。如 Andrew Ng 先做 Google 語音辨識的深度學習，獲得成功後，進行 Google Maps 的深度學習，再度成功後才去與 Google 主要盈利的廣告團隊合作進行這方面的 AI 轉化。
    >         ⭐️ 可找外部 AI 專家與內部產業專家合作一起導入 AI
    >         ⭐️ 從技術上已可行的專案著手
    >         ⭐️ 專案需有清楚定義可測量的目標，且可創造商業價值
    >     長期而言，建立公司內的 AI  團隊，直屬 CEO 或 CTO/CIO/CDO 跨不同事業單位做公司整體的 AI  轉型專案。
    >         ⭐️ 建立支援全公司的 AI 能量， 如提供全公司資訊服務一樣
    >         ⭐️ 執行跨組織序列的 AI 專案，這些專案要能提供一系列的價值
    >         ⭐️在招聘與留用上發展一致的標準 ( 含工作職位、流程 )
    >         ⭐️發展屬於全公司可跨部門共用的平台，如和 CTO/CIO/CDO 合作開發統一的資料倉儲標準
    >     就不同角色提供 AI 培訓
    >         ⭐️找外部培訓或公司內自製課程在花費與時效上不是很可行，建議運用既有的線上教育訓練，依個人不同的需求線上學習
    >         ⭐️ 找內部 AI 專業人才分享其個人的經驗，以激發大家學習的興趣
    >         ⭐️ 高級主管、事業單位主管(將合作執行 AI 專案)和 AI 工程師分別需要受不同的教育訓練( 請看原文內容)，相對訓練時數分別約 4+小時、12+小時、100+小時。
    >     發展 AI 連貫戰略，建立競爭力護城河資產，依產品獲得更多使用者與有用資料的 AI 良性循環( Virtuous circle of AI )
    >          ⭐️ 以連貫戰略建立多個難以超越的資產，讓競爭者難以同時發展替代方案。
    >         ⭐️ 利用 AI 發展特殊產業或特殊情況下獨特的優勢 (而不是與 Google 這種泛 AI 的公司競爭)
    >         ⭐️ 安排由產品->帶來更多使用者->帶來更多資料以幫助深度學習的良性循環( 如 Google 和 Baidu 都提供許多免費產品已收集資訊 )
    >         💥 沒有經過 AI 專家衡量資料是否有價值前不要為了獲得資料而做大規模的投資，如採購某家公司
    >         ⭐️利用 AI 建立網路效應與平台優勢，包含更低的花費、更高的價值、更機動的服務
    >     建立對內( AI 人才與人力資源、既有員工）對外( 客戶、使用者、政府法令、投資者) 等的溝通計畫，以確保公司能順利運用 AI 轉型成功
    >         ⭐️ 對政府法令 : 尤其在自駕車與健保領域，要能提供可信與吸引人的 AI 能對產業或社會創造什麼價值與好處。
    >         ⭐️ 對客戶/使用者 : 宣傳運用 AI 後創造的好處與產品計畫
    >         ⭐️ 對人才/人力資源 : 展示公司內展示公司內令人興奮與有意義的 AI 專案結果讓人才有興趣參與
    >         ⭐️ 對內部既有員工 : 讓員工瞭解 AI 並消除其對 AI 導入的恐懼
    > 
    > 轉型為 AI 公司不是一般公司 + AI 的技術就可稱為 AI 公司，而是一家公司真的因為應用了AI 脫胎換骨表現優異。
    > 
    > 要能成功轉型為 AI 公司，你必須 :
    > 
    >     有效應用內外資源有系統地執行多個能產出價值的 AI 專案
    >     對 AI 有足夠的知識，識別選出並執行對企業有價值的 AI 專案
    >     在策略上走入以 AI 為運行電力的成功模式
    > 
    > 一般 AI 轉型計畫月需 2~3 年，不過你會在 6~12 個月看到初期的成果。 投資在 AI 轉型，將幫助你的企業獲得很大的優勢，大幅超越你的競爭者。
    > 


## 知乎

- [深度学习-程序员的学习路径 - 知乎专栏](https://zhuanlan.zhihu.com/p/29349938)

    > 数学基础。如果你去读深度学习的论文，会发现数学对于DL非常重要，线性代数、概率论、甚至微积分都有用武之地。这些知识都还给学校了怎么办？难道要把所有这些课程再学一遍？大可不必。只要把DL需要用到的部分好好复习一下就好。这里推荐《Deep Learning》这本权威著作的第一部分，详述了机器学习需要的数学基础，另外也讲了机器学习领域的很多基本概念。
    > 
    > 一本入门教材。虽然上面提到的《Deep Learning》是公认的DL最权威教材，但是一般人会觉得过于艰深，包含太多数学方面的论证。如果你对数学公式不太感冒，建议不要用这本书入门。我推荐一本《Hands on Machine Learning》。不要被书的名字欺骗了，本书的内容其实一点也不初级，讲述的很全面深入。但是语言非常流畅，容易读懂。这本书分为两部分，第一部分讲述了机器学习的各种传统算法，第二部分才是深度学习的内容。传统算法的学习很有必要，一方面帮助我们理解ML发展的脉络，另一方面，很多传统算法其实并没有被淘汰，比如RandomForest这种基于决策树的算法，在结构化数据的挖掘方面非常有效。
    > 
    > 一个入门课程。这里我提供两个选项：
    > 
    > 第一推荐Andrew Ng刚刚上线的Deeplearning.ai。Andrew Ng的课，品质应该不会差，而且估计他会加入很多最前沿的研究。这门课刚刚开课，应该会持续几个月的时间。所以要做好长期学习的准备。
    > 
    > 第二个是我上过的fast.ai。这门课的讲师也是个牛人，曾负责Kaggle平台的研发。这个课程更加注重实践，在讲解概念的同时，用Keras+Jupyter Notebook直接演示模型的训练。总共只有7节课，每课时两个小时，但是每节课的Notes里面附带大量的参考资料，需要花很多时间去自学、消化。这门课现在开始了第二学期，讲DL最前沿的研究进展，也是7节课。另外，需要习惯一下讲师的澳洲英语。
    > 
    > 选一个方向。深度学习可应用的领域很多，我们需要选择其中一个深入研究。方向包括：计算机视觉（图像、视频处理，主要用CNN）、自然语言处理NLP（包括文本、语音处理，序列数据往往需要RNN）、增强学习（用在机器人、自动驾驶等方面），此外对于生成模型（GAN、VAE等等）的研究也是一个热点。
    > 
    > 读一些论文。在选定方向以后，我们可以去读一下这个方向的经典论文。说是经典，但是深度学习这个方向真正爆发也就是最近几年的事情，所以很多东西其实都是前两年的论文结果。推荐一个Awesome Deep Learning Papers的论文列表，个人觉得整理得不错，有参考价值。DL这个领域进展特别快，前几年的研究结果可能早已经被超越了，所以读论文要保持开放心态。不过，论文有的时候真的很啰嗦，幸好有网友总结了一下：aleju/papers，先读这个总结，带着问题再去读论文效果好很多。
    > 
    > 选一个框架。DL现在有很多非常成熟的框架了，每个科技公司都有自己的一套东西，Google系的TensorFlow现在似乎风头更劲一些。他们之间的比较可以看看这篇文章。我觉得这个跟编程语言之争一样，没必要太纠结，选一个自己喜欢的就好。各个框架训练出来的模型一定可以互相转换的。
    > 
    > 动手做一些深度学习项目。网上有很多开放的数据集，可以拿来做训练，先做些简单的，比如MNIST，IMDB影评的情感分析等。然后可以去Kaggle上做一些以前的的竞赛项目，比如Cats VS Dogs一类的。如果你足够厉害，可以参加当前的Kaggle挑战，说不定顺便赢个几十万美金呢：）
    > 
    > 最后，关注一下DL最新的研究动向。这方面的媒体非常多，公众号、知乎专栏，一搜一大把，还有很多科技博客也是频繁更新。我推荐一个newsletter：import AI，很多人都觉得不错，一周一次，读起来也不会有太大负担。


- [有哪些你看了以后大呼过瘾的数据分析书？ - 知乎](https://www.zhihu.com/question/60241622/answer/174310709)

    > 推荐两本真正意义上的神书，一本是《Pattern Recognition and Machine Learning》（PRML），一本是《Deep Learning》（Ian Goodfellow和Yoshua Bengio写的那本），绝对符合题主“看了以后大呼过瘾”的标准。这两本书都是属于“大而全”的类型，两本书都是从头到尾讲清楚了一个领域的细节。PRML是传统机器学习，《Deep Learning》是讲的最近几年兴起的深度学习。
    > 
    > 说了那么多，其实我想表达的是：与其对很多模型一知半解，不如真正学懂一个模型，这样其实是节省时间的。原因在于当你搞懂一个模型后，就算你忘记了某些细节，再次查看资料也可以很快回忆起来。而当你不懂一个模型时，每次回忆都要从零开始，来来回回其实浪费了很多时间。而PRML和《Deep Learning》就是让你真正理解机器学习算法的最佳途径之一，强烈推荐。


## Your AI skills are worth less than you think – Ryszard Szopa

- [Your AI skills are worth less than you think – Ryszard Szopa – Medium](https://medium.com/@szopa/your-ai-skills-are-worth-less-than-you-think-e4b5640adb4f?fbclid=IwAR3sNglZqSaOg8h8YicpDRVk5wkoE01U6Tr8fI-P-kkWz9vvgMyI8ehZd1Q)

    > I want to point out is that the accuracy of the "worse" model trained on 40 thousand samples is **better** than of the "better" model at 30 thousand samples!
    > 
    > ----
    > 
    > You shouldn't rely solely on your AI skills, as they are depreciating due to larger market trends. Building AI models may be very interesting, but what really matters is having better data than the competition. Maintaining a competitive advantage is hard, especially if you encounter a competitor who is richer than you, which is very likely to happen if your AI idea takes off. You should aim to create a scalable data collection process which is hard to reproduce by your competition. AI is well suited to disrupt industries which rely on the cognitive work of low qualified humans, as it allows to automate this work.
    > 



# 新聞

## 這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎 - 幫趣

- [這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎 - 幫趣](http://bangqu.com/7F8o81.html)


    > 如果要我把 2018 年機器學習進展的主要亮點總結成幾個標題，那就是：
    > 
    > -   **對 AI 的大肆宣揚和恐慌逐漸冷卻；**
    > 
    > -   **更多地關注具體問題，如公平性、可解釋性或因果性；**
    > 
    > -   **深度學習將持續下去，並且在實踐中不僅僅可以用來對圖像分類（特別是對 NLP 也非常有用）；**
    > 
    > -   **AI 框架方面的競爭正在升溫，如果你不想默默無聞，你最好發佈一些自己的框架。**
    > 
    > 讓我們來看看具體是怎麼樣的。
    > 
    > 如果說 2017 年對人工智能的大肆宣揚和恐慌都達到了極點，那麼 2018 年似乎就是我們開始冷靜下來的一年。雖然有些人一直在散佈他們對於人工智能的恐懼情緒，但與此同時，媒體和其他人似乎已經同意這樣的觀點，即雖然自動駕駛和類似的技術正在向我們走來，但它們不會在明天發生。儘管如此，仍然有聲音認爲我們應該規範人工智能，而不是集中於控制其結果。
    > 
    > 不過，很高興地看到，今年的人們關注的重點似乎已經轉移到了可以處理的更具體的問題上。例如，關於公平性的討論很多，而且關於這個話題的會議不止 1 次（例如 FATML 和 ACM FAT），甚至還有一些在線課程也在是關於這個話題的，比如 Google 教人們[認識機器學習中的公平性](https://www.blog.google/technology/ai/new-course-teach-people-about-fairness-machine-learning/amp/)的課程。
    > 
    > ![這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎](http://i2.bangqu.com/lf1/news/20181230/5c22e69016107.png)
    > 
    > 順着這條思路，今年已經深入討論的其他問題有可解釋性、解釋性和因果性。因果關係似乎回到了聚光燈下，這源於 Judea Pearl 的《The Book of Why》的出版。作者不僅決定寫他的第一本「普通人能夠讀懂」的書，而且他還利用 Twitter 來推廣關於因果關係的討論。事實上，甚至大衆媒體都認爲這是對現有 AI 方法的「挑戰」（例如，你可以看看《[機器學習的先驅如何成爲最尖銳的批評者之一](https://www.theatlantic.com/technology/archive/2018/05/machine-learning-is-stuck-on-asking-why/560675/?single_page=true)》這篇文章。實際上，甚至 ACM Recsys 大會上的最佳論文獎也授予了一篇相關論文，該論文討論瞭如何嵌入因果關係的問題（參見《[Causal Embeddings for Recommendations](https://recsys.acm.org/recsys18/session-2/)》）。儘管如此，許多作家認爲因果關係在某種程度上只是一種理論，我們應該關注更具體的問題。
    > 
    > ![這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎](http://i2.bangqu.com/lf1/news/20181230/5c22e6c0baf43.png)
    > 
    > Judea Pearl 的書
    > 
    > 作爲最通用的 AI 範例，深度學習仍然存在很多問題，很顯然，深度學習不會止步於當前，就它所能提供的東西而言，它還遠遠沒有達到高峯。更具體地說，今年深度學習方法在從語言到醫療保健等不同於計算機視覺的領域取得了前所未有的成功。
    > 
    > 事實上，今年深度學習最有趣的進展可能出現在 NLP 領域。如果要我選出今年最令人印象深刻的 AI 應用程序，它們都將是 NLP 應用程序（並且都來自於 Google）。第一個是 Google 發佈的超級有用的 smart compose，第二個是他們的 Duplex 對話系統。
    > 
    > ![這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎](http://i2.bangqu.com/lf1/news/20181230/5c22e6f5627e3.png)
    > 
    > 使用語言模型的想法加速了這些進步，這種想法在今年 Fast.ai 的 UMLFit 課程裏面普及。隨後我們看到了其他改進的方法，比如 Allen 的 ELMO，Open AI』s transformers，或最近擊敗了很多 SOTA 的 Google 的 BERT。這些模型被描述爲「Imagenet moment for NLP」，因爲它們提供了隨時可用的預訓練好的通用的模型，這些模型還可以針對特定任務進行微調。除了語言模型之外，還有許多其他有趣的進步，比如 Facebook 的多語言嵌入。值得注意的是，我們還看到這些和其他方法被集成到更通用的 NLP 框架（比如 AllenNLP 或 Zalando） 的 FLAIR 中的速度非常快。
    > 
    > ![這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎](http://i2.bangqu.com/lf1/news/20181230/5c22e70e98dfa.png)
    > 
    > 說到框架，今年的「人工智能框架之戰」已經升溫。令人驚訝的是，Pytorch 1.0 發佈之初，它就似乎正在趕上 TensorFlow。雖然在生產中 Pytorch 的使用情況仍然不太理想，但它在可用性、文檔和教育方面的進展速度似乎比 Tensor Flow 要快。有趣的是，選擇 Pytorch 作爲實現 Fast.ai 庫的框架可能起到了很大的作用。也就是說，Google 意識到了這一切，並且在框架中把 Keras 作爲高級模塊。最終，我們都能夠接觸到這些偉大的資源，並從中受益。
    > 
    > ![這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎](http://i2.bangqu.com/lf1/news/20181230/5c22e89bd5c62.png)
    > 
    > pytorch 與 tensorflow 搜索趨勢
    > 
    > 有趣的是，另一個在框架方面看到許多有趣的進展的領域是增強學習。雖然我不認爲 RL 的研究進展與前幾年一樣令人印象深刻（只有最近 DeepMind 的 Impala 浮現在腦海中），但是令人驚訝的是，在一年之內，我們看到了所有主要的 AI 玩家都發布了 RL 框架。Google 發佈了用於研究的 Dopamine 框架，而 Deepmind（隸屬於 Google）發佈了有一定競爭力的 TRFL 框架。Facebook 不甘落後，發佈了 Horizon，微軟也發佈了 TextWorld，而 TextWorld 在基於文本的訓練上更加專業。希望所有這些開源框架在能夠讓我們在 2019 年看到 RL 的進步。
    > 
    > 我很高興看到最近 Google 在 Tensor Flow 上發佈了 TFRank。Ranking 是一個非常重要的 ML 應用程序，它可能應該得到更多的重視。
    > 
    > 在數據改善領域，也有非常有趣的進展。例如，今年 Google 發佈了 auto-augment，這是一種深度增強學習方法，用於自動增強訓練數據。一個更極端的想法是用合成數據訓練 DL 模型。這種想法已經被在實踐中嘗試了一段時間，並且被許多人視爲 AI 未來的關鍵所在。NVidia 在他們的論文《[Training Deep Learning with Synthetic Data](https://arxiv.org/abs/1804.06516)》中提出了新穎的想法。在我們「[Learning from the Experts](https://arxiv.org/abs/1804.08033)」這篇文章中，我們還展示瞭如何使用專家系統生成合成數據，這些合成數據即使在與真實數據結合之後也能用於訓練 DL 系統。
    > 
    > ![這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎](http://i2.bangqu.com/lf1/news/20181230/5c22e8c31cc0a.png)
    > 
    > 至於人工智能領域的更多基本突破，可能由於我個人偏好的原因，並沒有看到太多。我不完全同意 Hinton 的說法，他認爲這個領域缺乏創新性是由於這個領域有「幾個老傢伙和一大堆年輕人」，儘管科學界有一種趨勢，那就是在研究者會在晚年取得突破性進展。在我看來，當前缺乏突破的主要原因是，現有方法和它們的變體仍然有許多有趣的實際應用，因此很難冒險採用可能並不實用的方法。更重要的是，這個領域的大部分研究都是由大公司贊助的。有一篇有趣的論文《[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/pdf/1803.01271.pdf)》，雖然它高度依賴過去的經驗，並且使用了已知的方法，但是它打開了發現新方法的大門，因爲它證明了通常被認爲是最優的方法實際上並不是最優的。顯然，我不同意 Bored Yann LeCun 的觀點，他認爲卷積網絡是最終的「主算法」，而 RNN 不是。即便是在序列建模領域，研究空間也很大！另一篇值得研究的論文是最近 NeurIPS 最佳論文「[Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)」，它挑戰了 DL 中的一些基本問題，包括層本身的概念。
    > 
    > ![這是一份 2018 年 AI/ML 領域年度進展總結，你準備好了嗎](http://i2.bangqu.com/lf1/news/20181230/5c22e8e5da972.png)
    > 

## 2019年，Alexa將會走向何方

- [2019年，Alexa將會走向何方？ - 幫趣](http://bangqu.com/hqY24a.html)

    > 今年IEEE語言技術峯會上，亞馬遜展示了專門針對Alexa NLU的數據表示方案。數據顯示，在一些關鍵任務的技能選擇上，以及在數千種技能中，該方案將技能選擇錯誤率降低了40%。在Alexa的NLU系統中，用戶語言經過了更加細粒度的分類。
    > 
    > 首先，對話領域或者對話主題的分類，例如，音樂、天氣。其次，根據潛在的意圖，或者用戶所希望的對話分類。比如，音樂領域中，可能是搜索、播放、下載等指令。最後，根據詞語位置類型進行分類。例如，播放AA唱的XX歌曲，AA屬於歌手名字，XX屬於歌曲名字。
    > 
    > 亞馬遜數據表示方案通過領域、意圖、位置三個方面形成了一個比較自然的層次結構。通過一系列語言位置，將語言串聯起來來界定用戶意圖，一系列的意圖構成了域。亞馬遜已經訓練了覆蓋17個域在內的24.6萬個語言神經網絡。該網絡首先生成一個位置表示（ 注：slot representation），然後生成意圖表示（intent representation），最後產生域表示（domain representation）。
    > 
    > 在訓練期間，神經網絡需要評估怎樣準確地對域分類，其目的在於表達（ 注：representation）而不是分類（classification）。評估有效地執行了表示的層次結構，即確保語言位置和意圖不會丟失域所必須的任何信息。網絡輸入時，首先會通過一個「去詞彙化器」，即用一個特定的語言位置值代替，例如，播放Drake的Nice for What，變爲播放歌手的歌曲。這個過程由單獨的NLU系統處理。網絡分類的目的在於分類表示的最佳方法，而不是進行分類。
    > 
    > ![2019年，Alexa將會走向何方？](http://i2.bangqu.com/lf1/news/20181224/5c20b6a79d8fa.png)
    > 
    > 注：架構圖，如何產生意圖，聚合意圖，產生域表示
    > 
    > 去詞彙化的語句傳遞進入嵌入層，該層採用現成的嵌入網絡。網絡將單詞轉換成固定長度的向量---數字串。比如，在高緯空間中的空間座標，將有相似意義的單詞聚集在一起。特定的詞語通過去詞彙化器，由網絡以簡單的標準嵌入，但語言位置的理解會有所不同。通過訓練表示網絡。算法對訓練數據進行梳理，以識別每個語言位置採用的可能值。比如，天氣領域天氣狀況相關的語言位置，可能包括風、暴雨、雪、暴雪等等。
    > 
    > 具有相似詞語含義的嵌入詞彼此空間位置接近，平均嵌入層的幾個相關詞彙可以捕獲其空間位置的接近性。在訓練以前，去詞彙化的位置被簡單的嵌入，作爲平均的可能值。訓練過程中，可以修改嵌入網絡的設置，根據語言位置、意圖、域的特性情況進行調整，基本原則仍爲對向量進行分組。
    > 
    > 去詞彙化話語嵌入後傳遞到雙向長短期記憶網絡。長短期記憶LSTMs按順序處理數據，並在其之前的輸出中，處理給定的輸出因子。LSTM在NLU中被廣泛使用，因爲它可以根據在句子中的位置來學習解釋單詞。融合LSTM(bi-LSTM)是處理從前到後和從後到前相同輸入序列的一種LSTM。
    > 
    > bi-LSTM的輸出是一個向量，用作意圖表示。意圖向量通過單個網絡層，該網絡層產生域表示。爲了評估表示方案，亞馬遜將編碼輸入到兩種技術選擇系統中。當使用原始文本作爲輸入時，系統準確率爲90%，亞馬遜則將準確率提高到94%。
    > 
    > 爲了證明其表示成功依賴於分類類別的分層嵌套，將設計的三個不同系統進行比較，通過融合LSTM編碼的去詞彙化輸入學習域和意圖嵌入。三個系統顯示原始文本的改進，均不能匹配分層系統。「從本質講，通過深度學習，亞馬遜對大量領域進行了建模，並將學習轉移到新的領域或者新的技能。」Rohit Prasad說。
    > 
    > 最近，亞馬遜推出了遷移學習，該項目屬於亞馬遜未來戰略的一部分。機器學習的改進最直接的影響就是使得系統錯誤率較去年減少25%。此外，今年12月，亞馬遜啓動了機器的自學習，系統可以聯繫上下文線索進行修正。Rohit Prasad舉例說，用戶對Echo說玩XM Chill請求失敗時，可以通過說播放Sirius 53頻道繼續收聽。對於Alexa而言，XM Chill和Sirius 53頻道的意義是相同且獨立的。「從隱藏式反饋中學習。」
    > 

## 【2018 AI 發展總整理】帶你了解 AI 的 5 大領域進展，各種開放原始碼任你取用

- [【2018 AI 發展總整理】帶你了解 AI 的 5 大領域進展，各種開放原始碼任你取用 | TechOrange](https://buzzorange.com/techorange/2018/12/22/ai-breakthrough-in-2018/)

    > 2018 ，仍是 AI 領域激動人心的一年。
    > 
    > 這一年成為 NLP（Natural Language Processing，自然語言處理）研究的分水嶺，各種突破接連不斷； CV（Computer Vision，電腦視覺）領域同樣精采紛呈，與四年前相比 GAN（Generative Adversarial Network，生成對抗網路）生成的假臉逼真到讓人不敢相信；新工具、新框架的出現，也讓這個領域的明天特別讓人期待。
    > 
    > 近日， Analytics Vidhya 發佈了一份 2018 人工智慧技術總結與 2019 趨勢預測報告，原文作者 PRANAV DAR 。量子位在保留這個報告架構的基礎上，對內容進行了重新編輯和補充。
    > 
    > 這份報告總結和整理了全年主要 AI 技術領域的重大進展，同時也給出了相關的資源位址，以便大家更好的使用、查詢。
    > 
    > 報告共涉及了五個主要部分：\
    > 自然語言處理（NLP）\
    > 電腦視覺（CV）\
    > 工具和庫\
    > 強化學習（RL）\
    > AI 道德
    > 
    > 下面，我們就逐一來盤點和展望。
    > 
    > 自然語言處理（NLP）
    > -----------
    > 
    > 2018 年在 NLP 歷史上的特殊地位，已經毋庸置疑。
    > 
    > 這份報告認為，這一年正是 NLP 的分水嶺。 2018 年裡， NLP 領域的突破接連不斷： ULMFiT 、 ELMo 、最近大熱的 BERT。
    > 
    > 遷移學習成了 NLP 進展的重要推動力。從一個預訓練模型開始，不斷去適應新的數據，帶來了無盡的潛力，甚至有「NLP 領域的 ImageNet 時代已經到來」一說。
    > 
    > ■ ULMFiT\
    > 這個縮寫，代表「通用語言模型的微調」，出自 ACL 2018 論文： Universal Language Model Fine-tuning for Text Classification。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/1-3.png)
    > 
    > 正是這篇論文，打響了今年 NLP 遷移學習狂歡的第一槍。
    > 
    > 論文兩名作者一是 Fast.ai 創始人 Jeremy Howard ，在遷移學習上經驗豐富；一是自然語言處理方向的博士生 Sebastian Ruder ，他的 NLP 博客幾乎所有同行都在讀。
    > 
    > 兩個人的專長綜合起來，就有了 ULMFiT 。想要搞定一項 NLP 任務，不再需要從 0 開始訓練模型，拿來 ULMFiT ，用少量數據微調一下，它就可以在新任務上實現更好的性能。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/2-3.png)
    > 
    > 他們的方法，在六項文本分類任務上超越了之前最先進的模型。
    > 
    > 詳細的說明可以讀他們的 [論文](https://arxiv.org/abs/1801.06146)\
    > Fast.ai [網站](http://nlp.fast.ai/category/classification.html) 上放出了訓練腳本、模型
    > 
    > ■ ELMo\
    > 這個名字，當然不是指《芝麻街》裡那個角色，而是「語言模型的詞嵌入」，出自艾倫人工智慧研究院和華盛頓大學的論文 Deep contextualized word representations ， NLP 頂會 NAACL HLT 2018 的優秀論文之一。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/3-1.png)
    > 
    > ELMo 用語言模型（language model）來獲取詞嵌入，同時也把詞語所處句、段的語境考慮進來。
    > 
    > 這種語境化的詞語表示，能夠體現一個詞在語法語義用法上的複雜特徵，也能體現它在不同語境下如何變化。
    > 
    > 當然， ELMo 也在試驗中展示出了強大功效。把 ELMo 用到已有的 NLP 模型上，能夠帶來各種任務上的性能提升。比如在機器問答數據集 SQuAD 上，用 ELMo 能讓此前最厲害的模型成績在提高 4.7 個百分點。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/4-1.png)
    > 
    > [這裡](https://allennlp.org/elmo) 有 ELMo 的更多介紹和資源
    > 
    > ■ BERT\
    > 說 BERT 是 2018 年最火的 NLP 模型，一點也不為過，它甚至被稱為 NLP 新時代的開端。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/5.png)
    > 
    > 它由 Google 推出，全稱是 Bidirectional Encoder Representations from Transformers ，意思是來自 Transformer 的雙向編碼器表示，也是一種預訓練語言表示的方法。
    > 
    > 從性能上來看，沒有哪個模型能與 BERT 一戰。它在 11 項 NLP 任務上都取得了最頂尖成績，到現在， SQuAD 2.0 前 10 名只有一個不是 BERT 變體：
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/6-1.png)
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/7-1.png)
    > 
    > 如果你還沒有讀過 BERT 的 [論文](https://arxiv.org/abs/1810.04805) ，真的應該在 2018 年結束前補完這一課。\
    > 另外，Google 官方開源了 [訓練代碼和預訓練模型](https://github.com/google-research/bert) 。\
    > 如果你是 PyTorch 黨，也不怕。這裡還有官方推薦的 PyTorch [重實現和轉換腳本](https://github.com/huggingface/pytorch-pretrained-BERT) 。
    > 
    > ■ PyText\
    > BERT 之後， NLP 圈在 2018 年還能收穫什麼驚喜？答案是，一款新工具。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/8.png)
    > 
    > 就在上週末， Facebook 開源了自家工程師們一直在用的 NLP 建模框架 PyText 。這個框架，每天要為 Facebook 旗下各種應用處理超過 10 億次 NLP 任務，是一個工業級的工具包。
    > 
    > （Facebook 開源新 NLP 框 架：簡化部署流程，大規模應用也 OK）
    > 
    > PyText 基於 PyTorch ，能夠加速從研究到應用的進度，從模型的研究到完整實施只需要幾天時間。框架裡還包含了一些預訓練模型，可以直接拿來處理文本分類、序列標註等任務。
    > 
    > 想試試？開源地址 [在此](https://github.com/facebookresearch/pytext)
    > 
    > ■ Duplex\
    > 如果前面這些研究對你來說都太抽象的話， Duplex 則是 NLP 進展的最生動例證。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/9-1.png)
    > 
    > 名字有點陌生？不過這個產品你一定聽說過，它就是 Google 在 2018 年 I/O 開發者大會上展示的「打電話 AI」。
    > 
    > 它能主動打電話給美髮店、餐館預約服務，全程流暢交流，簡直以假亂真。 Google 董事長 John Hennessy 後來稱之為「非凡的突破」，還說：「在預約領域，這個 AI 已經通過了圖靈測試」。
    > 
    > Duplex 在多輪對話中表現出的理解能力、合成語音的自然程度，都是 NLP 目前水平的體現。
    > 
    > ■ 2019 年展望\
    > NLP 在 2019 年會怎麼樣？我們借用一下 ULMFiT 作者 Sebastian Ruder 的展望：
    > 
    > 預訓練語言模型嵌入將無處不在：不用預訓練模型，從頭開始訓練達到頂尖水平的模型，將十分罕見。
    > 
    > 能編碼專業信息的預訓練表示將會出現，這是語言模型嵌入的一種補充。到時候，我們就能根據任務需要，把不同類型的預訓練表示結合起來。
    > 
    > 在多語言應用、跨語言模型上，將有更多研究。特別是在跨語言詞嵌入的基礎上，深度預訓練跨語言表示將會出現。
    > 
    > 電腦視覺（CV）
    > --------
    > 
    > 今年，無論是圖象還是影片方向都有大量新研究問世，有三大研究曾在 CV 圈掀起了集體波瀾。
    > 
    > ■ BigGAN\
    > 今年 9 月，當搭載 BigGAN 的雙盲評審中的 ICLR 2019 論文現身，行家們就沸騰了：簡直看不出這是 GAN 自己生成的。
    > 
    > 在電腦圖像研究史上， BigGAN 的效果比前人進步了一大截。比如在 ImageNet 上進行 128 × 128 分辨率的訓練後，它的 Inception Score（IS）得分 166.3 ，是之前最佳得分 52.52 的 3 倍。
    > 
    > 除了搞定 128 × 128 小圖之外， BigGAN 還能直接在 256 × 256 、 512 × 512 的 ImageNet 數據上訓練，生成更讓人信服的樣本。
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/10.png)
    > 
    > 在論文中研究人員揭秘， BigGAN 的驚人效果背後，真的付出了金錢的代價，最多要用 512 個 TPU 訓練，費用可達 11 萬美元，合人民幣 76 萬元（約新台幣 330 萬元）。
    > 
    > 不止是模型參數多，訓練規模也是有 GAN 以來最大的。它的參數是前人的 2 -- 4 倍，批次大小是前人的 8 倍。
    > 
    > [研究論文](https://openreview.net/pdf?id=B1xsqj09Fm)
    > 
    > 延伸閲讀：\
    > [驚！史上最佳 GAN 現身，超真實 AI 假照片，行家們都沸騰了](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247505181&idx=1&sn=2e817ec2ae918fd85c0ebcf85e810997&chksm=e8d0626fdfa7eb799021a16584bef14fb56bcda155dcee2d12b612e77ac18c53d6faedcd5aaa&scene=21#wechat_redirect)\
    > [訓練史上最佳 GAN 用了 512 塊 TPU，一作自述：這不是算法進步，是算力進步](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247505758&idx=3&sn=f6586a89960f54e89e72c42d2b65397e&chksm=e8d0602cdfa7e93aebef859f3e546b6df596af6a6ec0526518b95b7d6b223fb877596e1286da&scene=21#wechat_redirect)\
    > [史上最強 GAN：訓練費 10 萬起，現在免費體驗，畫風鬼畜又逼真](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247507950&idx=1&sn=5f6ad3faa1a7a9dada286911f549bbd5&chksm=e8d0689cdfa7e18a6daa6101172ce6c69c4b97da57f20175e1b2365c4c9f3f08ad4b1a3501ca&scene=21#wechat_redirect)
    > 
    > ■ Fast.ai 18 分鐘訓練整個 ImageNet\
    > 在完整的 ImageNet 上訓練一個模型需要多久？各大公司不斷下血本刷新著記錄。
    > 
    > 不過，也有不那麼燒計算資源的平民版。
    > 
    > 今年 8 月，在線深度學習課程 Fast.ai 的創始人 Jeremy Howard 和自己的學生，用租來的亞馬遜 AWS 的雲端運算資源， 18 分鐘在 ImageNet 上將圖像分類模型訓練到了 93% 的準確率。
    > 
    > 前前後後， Fast.ai 團隊只用了 16 個 AWS 雲實例，每個實例搭載 8 塊英偉達 V100 GPU ，結果比 Google 用 TPU Pod 在斯坦福 DAWNBench 測試上達到的速度還要快 40%。
    > 
    > 這樣拔群的成績，成本價只需要 40 美元（約 1200 元新台幣）， Fast.ai 在博客中將其稱作人人可實現。
    > 
    > [Fast.ai 博客介紹](https://www.fast.ai/2018/08/10/fastai-diu-imagenet/)
    > 
    > 延伸閲讀：\
    > [40 美元 18 分鐘訓練整個 ImageNet！人人可實現](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247502384&idx=2&sn=e5f0517baca5e5d63d26de66071d6e98&chksm=e8d07d42dfa7f4548c5547bf30bf84020a8f2cb4fd648905b2afeac9fbf67b60ff1ae7c1323e&scene=21#wechat_redirect)\
    > [224 秒！ImageNet 上訓練 ResNet-50 最佳戰績出爐，索尼下血本破紀錄](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247508021&idx=2&sn=8d84bb079dd38d526c114dc4af916ee6&chksm=e8d01747dfa79e51ab40c216cdba293fd72aa71aa90b77472411d02c6f7bac1b2e222657a43d&scene=21#wechat_redirect)
    > 
    > ■ vid2vid 技術\
    > 今年 8 月，英偉達和 MIT 的研究團隊高出一個超逼真高解析影片生成 AI。
    > 
    > 只要一幅動態的語義地圖，就可獲得和真實世界幾乎一模一樣的影片。換句話說，只要把你心中的場景勾勒出來，無需實拍，電影級的影片就可以自動 P 出來：
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/11.gif)
    > 
    > 除了街景，人臉也可生成：
    > 
    > ![](https://buzzorange.com/techorange/wp-content/uploads/sites/2/2018/12/12.gif)
    > 
    > 這背後的 vid2vid 技術，是一種在生成對抗性學習框架下的新方法：精心設計的生成器和鑒別器架構，再加上時空對抗目標。
    > 
    > 這種方法可以在分割蒙版、素描草圖、人體姿勢等多種輸入格式上，實現高分辨率、逼真、時間相干的影片效果。
    > 
    > 好消息， vid2vid 現已被英偉達開源。
    > 
    > [研究論文](https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf)\
    > [GitHub 地址](https://github.com/NVIDIA/vid2vid)
    > 
    > 延伸閲讀：\
    > [真實到可怕！英偉達 MIT 造出馬良的神筆](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247502757&idx=1&sn=8603dbc1b19f33b59095c5647d3d2fc3&chksm=e8d07cd7dfa7f5c1a01323f0daccd292ddf6d9db8b59bf75c93f1f16dbca8e4233030315fc3d&scene=21#wechat_redirect)\
    > [一文看盡深度學習這半年](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247509236&idx=2&sn=c1086db22ac1c62659b8cc4d47b5946a&chksm=e8d01386dfa79a90315d5daff027ce8dab97837478e3d02dd28b9f40a496a7d47020fc7eb45e&scene=21#wechat_redirect)
    > 
    > ■ 2019 趨勢展望\
    > Analytics Vidhya 預計，明年在電腦視覺領域，對現有方法的改進和增強的研究可能多於創造新方法。
    > 
    > 在美國，政府對無人機的限令可能會稍微「鬆綁」，開放程度可能增加。而今年火熱的自監督學習明年可能會應用到更多研究中。
    > 
    > Analytics Vidhya 對視覺領域也有一些期待，目前來看，在 CVPR 和 ICML 等國際頂會上公佈最新研究成果，在工業界的應用情況還不樂觀。他希望在 2019 年，能看到更多的研究在實際場景中落地。
    > 
    > Analytics Vidhya 預計，視覺問答（Visual Question Answering，VQA）技術和視覺對話系統可能會在各種實際應用中首次亮相。
    > 
    > 工具和框架
    > -----
    > 
    > 哪種工具最好？哪個框架代表了未來？這都是一個個能永遠爭論下去的話題。
    > 
    > 沒有異議的是，不管爭辯的結果是什麼，我們都需要掌握和瞭解最新的工具，否則就有可能被行業所拋棄。
    > 
    > 今年，機器學習領域的工具和框架仍在快速的發展，下面就是這方面的總結和展望。
    > 
    > ■ PyTorch 1.0
    > 
    > 根據 10 月 GitHub 發佈的 2018 年度報告， PyTorch 在增長最快的開源項目排行上，名列第二。也是唯一入圍的深度學習框架。
    > 
    > 作為谷歌 TensorFlow 最大的「勁敵」， PyTorch 其實是一個新兵， 2017 年 1 月 19 日才正式發佈。 2018 年 5 月， PyTorch 和 Caffe2 整合，成為新一代 PyTorch 1.0 ，競爭力更進一步。
    > 
    > 相較而言， PyTorch 速度快而且非常靈活，在 GitHub 上有越來越多的開碼都採用了 PyTorch 框架。可以預見，明年 PyTorch 會更加普及。
    > 
    > 至於 PyTorch 和 TensorFlow 怎麼選擇？在我們之前發過的一篇報導裡，不少大老站 PyTorch。
    > 
    > 實際上，兩個框架越來越像。前 Google Brain 深度學習研究員 Denny Britz 認為，大多數情況下，選擇哪一個深度學習框架，其實影響沒那麼大。
    > 
    > [PyTorch 官網](https://pytorch.org/)
    > 
    > 延伸閲讀：\
    > [PyTorch 還是 TensorFlow？這有一份新手指南](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247488154&idx=1&sn=88e8f47568c170b78563c89e0246c319&chksm=e8d3a5e8dfa42cfe8bb906c21465a691359c1622095edad1c2748cb00d9aabb0be94f6e04759&scene=21#wechat_redirect)\
    > [嘗鮮 PyTorch 1.0 必備伴侶](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247505364&idx=1&sn=388b4924f44c52da7ef4f1fc0f8e8789&chksm=e8d062a6dfa7ebb0d580d0f9b07f5d64368155f4ad59feb526449bfda6575cbfad4aa49d2c3b&scene=21#wechat_redirect)\
    > [TensorFlow 王位不保？ICLR 投稿論文 PyTorch 出鏡率快要反超了](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247505393&idx=2&sn=73258548a04648f664cdbecb846b1708&chksm=e8d06283dfa7eb956b6fff10c77f6d850495685130f86f6043d79cbfd8389244c6f2751e81d0&scene=21#wechat_redirect)
    > 
    > ■ AutoML\
    > 很多人將 AutoML 稱為深度學習的新方式，認為它改變了整個系統。有了 AutoML ，我們就不再需要設計複雜的深度學習網絡。
    > 
    > 今年 1 月 17 日，谷歌推出 Cloud AutoML 服務，把自家的 AutoML 技術通過雲平台對外發佈，即便你不懂機器學習，也能訓練出一個定製化的機器學習模型。
    > 
    > 不過 AutoML 並不是谷歌的專利。過去幾年，很多公司都在涉足這個領域，比方國外有 RapidMiner 、 KNIME 、 DataRobot 和 H2O.ai 等等。
    > 
    > 除了這些公司的產品，還有一個開源庫要介紹給大家： Auto Keras
    > 
    > 這是一個用於執行 AutoML 任務的開源庫，意在讓更多人即便沒有人工智慧的專家背景，也能搞定機器學習這件事。
    > 
    > 這個庫的作者是美國德州農工大學（Texas A&M University）助理教授胡俠和他的兩名博士生：金海峰、 Qingquan Song 。 Auto Keras 直擊谷歌 AutoML 的三大缺陷：
    > 
    > 第一，得付錢。\
    > 第二，因為在雲端上，還得配置 Docker 容器和 Kubernetes 。\
    > 第三，服務商 Google 保證不了你的數據安全和隱私。
    > 
    > [官網](https://autokeras.com/)\
    > [GitHub](https://github.com/jhfjhfj1/autokeras)
    > 
    > 延伸閲讀：\
    > [一文看懂深度學習新王者「AutoML」](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247503016&idx=1&sn=5521830e7a201d1893be62aa8975a032&chksm=e8d07bdadfa7f2cc45c9d784b73691826f9d41d4fe713586f63ce0325c61c94202a4e9526939&scene=21#wechat_redirect)\
    > [開源的"谷歌 AutoML 殺手"來了](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247502064&idx=3&sn=526c0eb5f9520ed1584873a5af48b08f&chksm=e8d07f82dfa7f6942c06298653808fa722f3359c0407de492de8a4b0fd0af861f17f1fe35d3f&scene=21#wechat_redirect)\
    > [谷歌放大招！全自動訓練 AI 無需寫代碼，全靠剛發佈的 Cloud AutoML](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247493350&idx=1&sn=76db3f988ae1c373c50ebe6814a276ea&chksm=e8d05194dfa7d88257ae1dc188aa68ace7fc8f9dc90ab3da93152a973d4509441eccff1a368d&scene=21#wechat_redirect)
    > 
    > ■ TensorFlow.js\
    > 今年 3 月底的 TensorFlow 開發者會峰會 2018 上， TensorFlow.js 正式發佈。
    > 
    > 這是一個面向 JavaScript 開發者的機器學習框架，可以完全在瀏覽器中定義和訓練模型，也能導入離線訓練的 TensorFlow 和 Keras 模型進行預測，還對 WebGL 實現無縫支持。
    > 
    > 在瀏覽器中使用 TensorFlow.js 可以擴展更多的應用場景，包括展開交互式的機器學習、所有數據都保存在客戶端的情況等。
    > 
    > 實際上，這個新發佈的 TensorFlow.js ，就是基於之前的 deeplearn.js ，只不過被整合進 TensorFlow 之中。
    > 
    > 谷歌還給了幾個 TensorFlow.js 的應用案例。比如借用你的攝影機，來玩經典遊戲：吃豆人（Pac-Man）。
    > 
    > [官網](https://js.tensorflow.org/)
    > 
    > 延伸閲讀：\
    > [有筆記本就能玩的體感遊戲！TensorFlow.js 實現體感格鬥教程](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247507369&idx=3&sn=2ceeb8c3c8e7e24abc16b6f68bee78ff&chksm=e8d06adbdfa7e3cdc42728fd800c06b7b24fead53528a552f1eb801e58d27079a887392ae01a&scene=21#wechat_redirect)\
    > [谷歌 AI 魔鏡：看你手舞足蹈，就召喚出 8 萬幅照片學你跳](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247501485&idx=3&sn=985f4000b9679f181868ca1d3a7216dc&chksm=e8d071dfdfa7f8c950a82df7f49610475c9b06886a66fe3040d0a20bb0b9c552ed28048abd85&scene=21#wechat_redirect)\
    > [我不是偷拍的變態，只是在找表情包的本尊](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247498027&idx=4&sn=3725d210bc763de85c6b63f1ad1b9285&chksm=e8d04e59dfa7c74f6c1383fe9b388d1ac39cdf3de6e277544082cd3bf252c3b7d8e377f7f26d&scene=21#wechat_redirect)
    > 
    > ■ 2019 趨勢展望\
    > 在工具這個主題中，最受關注的就是 AutoML 。因為這是一個真正會改變遊戲規則的核心技術。在此，引用 H2O.ai 的大神 Marios Michailidis（KazAnova）對明年 AutoML 領域的展望：
    > 
    > 以智慧可視化、提供洞見等方式，幫助描述和理解數據\
    > 為數據集發現、構建、提取更好的特徵\
    > 快速構建更強大、更智能的預測模型\
    > 通過機器學習可解釋性，彌補黑盒建模帶來的差距\
    > 推動這些模型的產生
    > 
    > 強化學習（RL）
    > --------
    > 
    > 強化學習還有很長的路要走。
    > 
    > 除了偶爾成為頭條新聞之外，目前強化學習領域還缺乏真正的突破。強化學習的研究非常依賴數學，而且還沒有形成真正的產業應用。
    > 
    > 希望明年可以看到更多 RL 的實際用例。現在我每個月都會特別關注一下強化學習的進展，以期看到未來可能會有什麼大事發生。
    > 
    > ■ OpenAI 的強化學習入門教程\
    > 全無機器學習基礎的人類，現在也可以迅速上手強化學習。
    > 
    > 11 月初， OpenAI 發佈了強化學習入門教程： Spinning Up 。從一套重要概念，到一系列關鍵演算法實現代碼，再到熱身練習，每一步都以清晰簡明為上，全程站在初學者角度。
    > 
    > 團隊表示，目前還沒有一套比較通用的強化學習教材， RL 領域只有一小撮人進得去。這樣的狀態要改變啊！因為強化學習真的很有用。
    > 
    > [教程入口](https://spinningup.openai.com/en/latest/index.html)\
    > [GitHub 傳送門](https://github.com/openai/spinningup)
    > 
    > 延伸閲讀：\
    > [強化學習如何入門？看這篇文章就夠了](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247498702&idx=3&sn=766997fc7ecd5d1e68e051870490ea08&chksm=e8d04cbcdfa7c5aa3e18a80caaa063c6013e2ce36df74b120dcff37baabda669de3409968e7a&scene=21#wechat_redirect)\
    > [人人能上手：OpenAI 發射初學者友好的強化學習教程 | 代碼簡約易懂](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247507733&idx=3&sn=1160f715dbfb405345a3c41750d06193&chksm=e8d06867dfa7e17187116e9d5e88ed040e99c2027dcbcc790807281acae6b8ce8f934a5acca6&scene=21#wechat_redirect)\
    > [強化學習算法 Q-learning 入門：教電腦玩"抓住芝士"小遊戲](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247488154&idx=4&sn=05485435b9410f046011a5ac95e943a6&chksm=e8d3a5e8dfa42cfec3d7aecbe98f8c4da7ce7d1d59ce153d7ab9306ac09af64e0eba1c73523c&scene=21#wechat_redirect)
    > 
    > ■ 谷歌的強化學習新框架「多巴胺」\
    > Dopamine（多巴胺），這是谷歌今年 8 月發佈的強化學習開源框架，基於 TensorFlow 。
    > 
    > 新框架在設計時就秉承著清晰簡潔的理念，所以代碼相對緊湊，大約是 15 個 Python 文件，基於 Arcade Learning Environment（ALE）基準，整合了 DQN 、 C51 、  Rainbow agent 精簡版和 ICML 2018 上的 Implicit Quantile Networks 。
    > 
    > 為了讓研究人員能快速比較自己的想法和已有的方法，該框架提供了 DQN 、 C51 、  Rainbow agent 精簡版和 Implicit Quantile Networks 的玩 ALE 基準下的那 60 個雅達利遊戲的完整訓練數據。
    > 
    > 另外，還有一組 Dopamine 的教學 colab 。
    > 
    > [Dopamine 谷歌博客](https://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html)\
    > [Dopamine github 下載](https://github.com/google/dopamine/tree/master/docs#downloads)\
    > [colabs](https://github.com/google/dopamine/blob/master/dopamine/colab/README.md)\
    > [遊戲訓練可視化網頁](https://google.github.io/dopamine/baselines/plots.html)
    > 
    > ■ 2019 趨勢展望\
    > DataHack Summit 2018 發言人、 ArxivInsights 創始人 Xander Steenbrugge ，也是一名強化學習專家，以下是來自他的總結和展望。
    > 
    > 1、由於輔助學習任務越來越多，增加了稀疏的外在獎勵，樣本的複雜性將繼續提高。在非常稀疏的獎勵環境中，效果非常好。
    > 
    > 2、正因如此，直接在物理世界訓練將越來越可行，替代當前大多先在虛擬環境中訓練的方法。我預測 2019 年，會出現第一個只由深度學習訓練，沒有人工參與而且表現出色的機器人 demo 出現。
    > 
    > 3、在 DeepMind 把 AlphaGo 的故事延續到生物領域之後（AlphaFold），我相信強化學習將逐步在學術領域外創造實際的商業價值。例如新藥探索、電子晶片架構優化、車輛等等。
    > 
    > 4、強化學習會有一個明顯的轉變，以前在訓練數據上測試智能體的行為將不再視為「允許」。泛化指標將成為核心，就像監督學習一樣。
    > 



# 機器學習之數學基礎

## Probably approximately correct learning(PAC learnable)

- [Probably approximately correct learning - Wikiwand](https://www.wikiwand.com/en/Probably_approximately_correct_learning)

    > probably approximately correct learning (PAC learning) is a framework for mathematical analysis of machine learning.
    > 
    > ---
    > 
    > The goal is that, with high probability (the "probably" part), the selected function will have low generalization error (the "approximately correct" part).
    > 
    > ---
    > 
    > An important innovation of the PAC framework is the introduction of computational complexity theory concepts to machine learning.
    > 
    > ---
    > 
    > with probability of at least ![1-\delta ](https://wikimedia.org/api/rest_v1/media/math/render/svg/e7fc18b68a939b8f9eb465e354a64164a1202901), ![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) outputs a hypothesis ![h\in C](https://wikimedia.org/api/rest_v1/media/math/render/svg/8f1a1a2ca51565d176f6fae32d3a4528ca50bca4) that has an average error less than or equal to ![\epsilon ](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3837cad72483d97bcdde49c85d3b7b859fb3fd2) on ![X](https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab) with the same distribution ![D](https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6). Further if the above statement for algorithm ![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is true for every concept ![c\in C](https://wikimedia.org/api/rest_v1/media/math/render/svg/447d3982c94c23d6b6d01c90da81a6125aa26567) and for every distribution ![D](https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6) over ![X](https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab), and for all ![{\displaystyle 0<\epsilon ,\delta <1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a947df177dd2845e5e7af154cf5694f2cff9e129) then ![C](https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029) is (efficiently) **PAC learnable** (or *distribution-free PAC learnable*). We can also say that ![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is a **PAC learning algorithm** for ![C](https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029).
    > 
    > ---
    > 
    > Under some regularity conditions these three conditions are equivalent:
    > 
    > 1.  The concept class *C* is **PAC learnable**.
    > 2.  The [VC dimension](https://www.wikiwand.com/en/VC_dimension "VC dimension") of *C* is finite.
    > 3.  *C* is a uniform [Glivenko-Cantelli class](https://www.wikiwand.com/en/Glivenko-Cantelli_class).
    > 


## Shannon entropy/cross entropy/KL divergence

- [Shannon entropy in the context of machine learning and AI](https://medium.com/swlh/shannon-entropy-in-the-context-of-machine-learning-and-ai-24aee2709e32)

    > 
    > Shannon entropy is used as a measure for information content of probability distributions. It is named after the father of information theory, [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) (1916--2001).
    > 
    > #### Self-information
    > 
    >  Consequently, the self-information is larger than zero. If the coin is biased towards always landing on tails (instead of heads), we again end up with zero entropy, or surprise. The maximum amount of surprise is obtained in the case of an unbiased coin where the chances of landing on heads or tails are both 50%, as this is the situation where the outcome of the coin toss is least predictable.
    >  
    >  One additional and crucial property, however, is additivity of independent events; the self-information of two subsequent coin tosses should be twice the self-information of a single coin toss. This makes sense for independent variables, as the amount of surprise, or unpredictability, becomes twice larger in this case. Formally, we need I(p_i-p_j) = I(p_i)+I(p_j) for independent events x_i and x_j. The function fulfilling all of these requirements is the negative logarithm,
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*M61jMpKOcTRJTRMiFFDwxQ@2x.png)
    > 
    > Figure 1 shows a plot of I(p).
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*SyomXV4fILTT-Hc5uXurbg@2x.png)
    > 
    > Figure 1. The self-information function I(p). Low probabilities are associated with high self-information, and vice versa.
    > 
    >  Let's walk through the simple calculations for three subsequent coin tosses. There are 2^3 = 8 possible outcomes, and the probability of any particular outcome is 0.5^3 = 0.125. Therefore, the self-information of this experiment is I(0.125) = -log_2(0.125) = 3. We need 3 bits to represent all possible outcomes, so the self-information of any particular sequence of three coin tosses equals 3.0.
    > 
    > We can also calculate the self-information of a continuous random variable. Figure 2 shows three different pdfs and their corresponding information content. The Dirac delta in 2a corresponds to the strong bias, zero entropy case of a biased coin that always lands on the same side. An infinitely high information content would be associated wherever p(x) = 0. However, this is hypothetical since these events will never actually occur due to zero probability. The Gaussian pdf in Figure 2B is analogous to the biased coin that tends to fall on the same side often, but not always. Lastly, Figure 2C depicts a uniform pdf, with associated uniform information content, analogous to our unbiased coin toss.
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*USFoTUyFmiEtXTmLEtRG1w.png)
    > 
    > Figure 2. Three different probability densities p on [-3,+3] and their self-information I(p). (A) Dirac delta function (completely deterministic) (B) Gaussian with μ = 0,σ = 0.5 (biased towards x = 0) (C.) Uniform distribution.
    > 
    > #### Entropy
    > 
    > So far we have only discussed self-information. For the case of a fair coin, self-information actually equals Shannon entropy, because all outcomes are equal in probability. In general, however, Shannon entropy is the average self-information (expected value) over all possible values of X,
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*pXcsUyiIsPJDU_ruWkgpBg@2x.png)
    > 
    > where b is the base of the logarithm. Above we used b = 2, other common choices are b = 10 as well as Euler's number e, but the choice doesn't matter much, since logarithms of varying bases are related by a constant. We will assume base 2 from here on and omit b.
    > 
    > 
    > Shannon entropy generalizes to the continuous domain, where it is referred to as differential entropy ([some restrictions apply](https://en.wikipedia.org/wiki/Differential_entropy) which shall not be discussed here). For a continuous random variable x and probability density function p(x), Shannon entropy is defined as follows,
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*o4T0pgqjCYoxkvZpQ4j9bg@2x.png)
    > 
    > 
    > #### Cross entropy
    > 
    > Cross entropy is a mathematical tool for comparing two probability distributions p and q. It is similar to entropy, but instead of the expectation of log( p) under p, we compute the expectation of log(q) under p,
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*uA6LE5g6egD9iBPksvu6QA@2x.png)
    > 
    > In the language of information theory, this quantity gives us the average number of bits required to code an event from q if we use the "wrong" coding scheme q instead of p. In machine learning, it is a very useful measure for the similarity of probability distributions and serves as a loss function (more details below).
    > 
    > ### Uses in machine learning
    > 
    > You may wonder, at this point, how entropy is relevant in machine learning. So let's look at a some specific areas next.
    > 
    > #### Bayesian learning
    > 
    > In a Bayesian setting, often a prior distribution is assumed that has a broad pdf, reflecting the uncertainty in the value of the random variable before an observation is made. When data comes in, the entropy reduces and causes the posterior to form peaks around the likely values of the parameters.
    > 
    > #### Decision tree learning
    > 
    > In decision tree learning, entropy is used to build the tree. Constructing a decision tree starts at the root node, by splitting the data set S into a number of subsets according to all possible values of the "best" attribute, i.e., the one that minimizes the (combined) entropy in the resulting subsets. This procedure is repeated recursively until there are no more attributes left to split. This prodecude is called [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm).
    > 
    > #### Classification
    > 
    > Cross entropy is the basis of the standard loss function for logistic regression and neural networks, in both binomial and multinomial classification scenarios. Usually, p is used for the true (or empirical) distribution (i.e., the distribution of the training set), and q is the distribution described by a model. Let's look at binary logistic regression as an example. The two classes are labeled 0 and 1, and the logistic model assigns the probabilities q_(y=1) = ŷ and q_(y=0) = 1 - ŷ to each input x. This can be concisely written as q ∈ {ŷ, 1 - ŷ}. Even though the empirical labels p are always exactly 0 or 1, the same notation is used here, p ∈ {y, 1 - y}, so don't get confused. Using this notation, the cross entropy between empirical and estimated distribution for a single sample is
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*Ayisu1Zs2t_7tPBEyyAgGg@2x.png)
    > 
    > When used as a loss function, the average of all cross entropies from all N samples is used,
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*3LkITokhU5tKkivVn2ftdw@2x.png)
    > 
    > #### KL divergence
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*faNf4hDTT9KMCCrI29PIfg@2x.png)
    > 
    > While cross entropy measures the average total bits needed to code an event from p when a coding scheme optimized for q is used, while the KL divergence gives the number of additional bits needed when the optimal coding scheme for q is used, instead of the optimal coding scheme for p. From this we can see that in the context of machine learning, where p is fixed, cross entropy and KL divergence are related by a constant additive term, so for the purpose of optimization they are equivalent. 
    > 
    > 

## What is the difference Cross-entropy and KL divergence?

- [kullback leibler - What is the difference Cross-entropy and KL divergence? - Cross Validated](https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence)

    > You will need some conditions for claiming the equivalence of minimizing cross entropy to minimizing KL divergence. I will put your question under the context of classification problems using cross entropy as loss functions.
    > 
    > Let us first recall that entropy is used to measure the uncertainty of a system, which is defined as
    > 
    > $$ 
    > S(v)=-\sum_ip(v_i)\log p(v_i), 
    > $$
    > 
    >  for $p(v_i)$ as the probabilities of different states $v_i$ of the system. From an information theory point of view, $S(v)$ is the amount of information is needed for removing the uncertainty.
    > 
    > For instance, the event A `I will die eventually` is almost certain (maybe we can solving the aging problem for word `almost`), therefore it has low entropy which requires only the information of `the aging problem cannot be solved` to make it certain. However, the event B `The president will die in 50 years` is much more uncertain than A, thus it needs more information to remove the uncertainties.
    > 
    > Now look at the definition of KL divergence between events A and B
    > 
    > $$ D_{KL}(A\parallel B) = \sum_ip_A(v_i)\log p_A(v_i) - p_A(v_i)\log p_B(v_i), $$
    > 
    >  where the first term of the right hand side is the entropy of event A, the second term can be interpreted as the expectation of event B in terms of event A. And the 
    > 
    > $D_{KL}$ describes how different B is from A from the perspective of A.
    > 
    > To relate cross entropy to entropy and KL divergence, we formalize the cross entropy in terms of events A and B as
    > 
    > $$ H(A, B) = -\sum_ip_A(v_i)\log p_B(v_i). $$
    > 
    >  From the definitions, we can easily see
    > 
    > $$ H(A, B) = D_{KL}(A\parallel B)+S_A. $$
    > 
    >  If $S_A$ is a constant, then minimizing $H(A, B)$ is equivalent to minimizing $D_{KL}(A\parallel B)$.
    > 
    > A further question follows naturally as how the entropy can be a constant. In a machine learning task, we start with a dataset (denoted as $P(\mathcal D)$) which represent the problem to be solved, and the learning purpose is to make the model estimated distribution (denoted as $P(model)$) as close as possible to true distribution of the problem (denoted as $P(truth)$). $P(truth)$ is unknown and represented by $P(\mathcal D)$. Therefore in an ideal world, we expect
    > 
    > $$\begin{equation} P(model)\approx P(\mathcal D) \approx P(truth) \end{equation}$$
    > 
    >  and minimize $D_{KL}(P(\mathcal D)\parallel P(model))$. And luckily, in practice $\mathcal D$ is given, which means its entropy $S(D)$ is fixed as a constant.

## KL散度（Kullback-Leibler Divergence）

- [KL散度（Kullback-Leibler Divergence） | ZHANG RONG](https://zr9558.com/2015/11/17/kullback-leibler-divergence/)

    > ### **简单例子：**
    > 
    > 比如有四个类别，一个方法 P 得到四个类别的概率分别是0.1，0.2，0.3，0.4。另一种方法 Q（或者说是事实情况）是得到四个类别的概率分别是0.4，0.3，0.2，0.1,那么这两个分布的 KL 散度就是
    > 
    > ![D_{KL}(P||Q)](https://s0.wp.com/latex.php?latex=D_%7BKL%7D%28P%7C%7CQ%29&bg=ffffff&fg=2b2b2b&s=0 "D_{KL}(P||Q)")
    > 
    > ![=0.1*\log_{2}(0.1/0.4)](https://s0.wp.com/latex.php?latex=%3D0.1%2A%5Clog_%7B2%7D%280.1%2F0.4%29&bg=ffffff&fg=2b2b2b&s=0) + ![0.2*\log_{2}(0.2/0.3)](https://s0.wp.com/latex.php?latex=0.2%2A%5Clog_%7B2%7D%280.2%2F0.3%29&bg=ffffff&fg=2b2b2b&s=0 "0.2*\log_{2}(0.2/0.3)") + ![0.3*\log_{2}(0.3/0.2)](https://s0.wp.com/latex.php?latex=0.3%2A%5Clog_%7B2%7D%280.3%2F0.2%29&bg=ffffff&fg=2b2b2b&s=0 "0.3*\log_{2}(0.3/0.2)") + ![0.4*\log_{2}(0.4/0.1).](https://s0.wp.com/latex.php?latex=0.4%2A%5Clog_%7B2%7D%280.4%2F0.1%29.&bg=ffffff&fg=2b2b2b&s=0)
    > 
    > ### 案例分析：
    > 
    > 在实际的工作中，我们通常会有某个网页或者app被用户点击或者播放的数据，同时，我们会拥有用户画像（personas），此刻我们关心的问题就是如何计算网页里面的某个新闻或者app里面的某个专辑受到不同维度的用户画像的喜好程度。比如，如何判断该新闻是受到各个年龄层的用户的喜爱还是只是受到某个年龄层的用户的喜爱？如何判断某个电台节目是受到大众的喜爱还是更加受到男性或者女性的喜爱？此时，怎么计算物品被用户的喜欢程度就成为了一个重要的关键。
    > 
    > 通常的方案就是，比方说：某个电台专辑被66.7%的男性用户播放过，被33.3%的女性用户播放过，那么该物品是不是真的就是更加受到男性用户的青睐呢？答案是否定的。如果使用这款app的用户的男女比例恰好是1：1，那么根据以上数据，该电台专辑显然更加受到男性用户的喜爱。但是，实际的情况，使用这款app的用户的男女比例不完全是1：1。如果男性用户：女性用户=2：1的话，如果某个电台专辑被66.7%的男性用户播放过，被33.3%的女性用户播放过，那么说明该专辑深受男女喜欢，两者对该专辑的喜好程度是一样的。因为基础的男女比例是2：1，该专辑被男女播放的比例是2：1，所以，该专辑对男女的喜爱程度恰好是1：1。
    > 
    > 那么如何计算该专辑的喜好程度呢？首先我们会使用KL散度来描述这件事情。
    > 
    > ---
    > 所以假設P是基礎的男女分布，Q是電台的男女分布， 當 P=Q 時 DKL(P||Q)=0，當 P,Q 差異越大，DKL的值也越大，表示男女的喜好程度差異越大。[name=Ya-Lun Li]





# 機器學習分類

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > ![](https://pic2.zhimg.com/80/v2-2f2b4f460b5490102c6901c9350895f7_hd.jpg)
    > 
    > 比喻
    > ==
    > 
    > 例如学英语。有监督学习是先读几篇中英文对照的文章，从而学会阅读纯英语文章。无监督学习是直接阅读大量纯英文文章，当数量达到一定程度，虽然不能完全理解文章，但也会发现一些词组的固定搭配，句式等等。
    > 
    > 举例
    > ==
    > 
    > -   有监督学习：学认字
    > 
    > -   无监督学习：自动聚类
    > 
    > -   增强学习：学下棋

## 有監督學習

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > **有监督学习**是机器学习任务的一种。它从有标记的训练数据中推导出预测函数。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话：**给定数据，预测标签**。
    > 
    > -   有标签
    > 
    > -   直接反馈
    > 
    > -   预测未来结果
    > 
    > ---
    > 算法:
    > 
    > -   分类
    > 
    > -   回归
    > 

- [【講講科普】 當你有了三個孩子他們分別叫監督式學習、非監督式學習與強化式學習 – Jason Kuan – Medium](https://medium.com/@capillaryj/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%95%B6%E4%BD%A0%E6%9C%89%E4%BA%86%E4%B8%89%E5%80%8B%E5%AD%A9%E5%AD%90%E4%BB%96%E5%80%91%E5%88%86%E5%88%A5%E5%8F%AB%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E8%88%87%E5%BC%B7%E5%8C%96%E5%BC%8F%E5%AD%B8%E7%BF%92-1caa26c7da87)

    > 時機:
    > 監督式學習: 資料已有標記，運用已標記資料來做訓練。





## 無監督學習

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > **无监督学习**是机器学习任务的一种。它从无标记的训练数据中推断结论。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话：**给定数据，寻找隐藏的结构**。
    > 
    > -   无标签
    > 
    > -   无反馈
    > 
    > -   寻找隐藏的结构
    > 
    > ---
    > 算法:
    > 
    > -   聚类
    > 
    > -   降维
    > 

- [【講講科普】 當你有了三個孩子他們分別叫監督式學習、非監督式學習與強化式學習 – Jason Kuan – Medium](https://medium.com/@capillaryj/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%95%B6%E4%BD%A0%E6%9C%89%E4%BA%86%E4%B8%89%E5%80%8B%E5%AD%A9%E5%AD%90%E4%BB%96%E5%80%91%E5%88%86%E5%88%A5%E5%8F%AB%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E8%88%87%E5%BC%B7%E5%8C%96%E5%BC%8F%E5%AD%B8%E7%BF%92-1caa26c7da87)

    > 時機:
    > 非監督式學習: 資料沒有標記，從中找出擁有相同特徵的資料群。


## 強化學習

- [有监督学习、无监督学习以及强化学习 - 知乎](https://zhuanlan.zhihu.com/p/26304729)

    > **强化学习**是机器学习的另一个领域。它关注的是软件代理如何在一个环境中采取行动以便最大化某种累积的回报。一句话：**给定数据，学习如何选择一系列行动，以最大化长期收益**。
    > 
    > -   决策流程
    > 
    > -   激励系统
    > 
    > -   学习一系列的行动
    > 
    > ---
    > 算法:
    > 
    > -   马尔可夫决策过程
    > 
    > -   动态规划

- [【講講科普】 當你有了三個孩子他們分別叫監督式學習、非監督式學習與強化式學習 – Jason Kuan – Medium](https://medium.com/@capillaryj/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%95%B6%E4%BD%A0%E6%9C%89%E4%BA%86%E4%B8%89%E5%80%8B%E5%AD%A9%E5%AD%90%E4%BB%96%E5%80%91%E5%88%86%E5%88%A5%E5%8F%AB%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E8%88%87%E5%BC%B7%E5%8C%96%E5%BC%8F%E5%AD%B8%E7%BF%92-1caa26c7da87)

    > 時機:
    > 強化式學習: 可能手上沒有任何資料，直接讓模型執行，再將執行結果反饋回去做訓練。



# 模型複雜度

## Balancing Bias and Variance to Control Errors in Machine Learning

- [Balancing Bias and Variance to Control Errors in Machine Learning](https://towardsdatascience.com/balancing-bias-and-variance-to-control-errors-in-machine-learning-16ced95724db)

    > Suppose we are observing a *response variable Y* (qualitative or quantitative) and *input variable X* having p number of features or columns (X1, X2.....Xp) and we assume there is relation between them. This relation can be expressed as
    > 
    > > **Y = f(X) + e**
    > > 
    > 
    > ***even if we make a 100% accurate estimate of f(X), our model won't be error free, this is known as irreducible error***(e in the above equation).
    > 
    > In other terms, the irreducible error can be seen as information that X cannot provide about Y. ***The quantity e may contain unmeasured variables that are useful in predicting Y***: since we don't measure them, f cannot use them for its prediction. ***The quantity e may also contain unmeasurable variation******.*** For example, the risk of an adverse reaction might vary for a given patient on a given day, depending on manufacturing variation in the drug itself or the patient's general feeling of well-being on that day.
    > 
    > the error they introduce is not reducible as generally they are not present in the training data. Nothing that we can do about it.
    > 
    > ---
    > 
    > #### Model Complexity
    > 
    > The complexity of a relation, f(X), between input and response variables, is an important factor to consider while learning from a dataset. **A simple relation is easy to interpret.** For example a linear model would look like this
    > 
    > > **Y ≈ β0 + β1X1 + β2X2 + ...+ βpXp**
    > > 
    > 
    > for example it may be quadratic, circular, etc. These models are **more flexible** as they fit data points more closely can take different forms. Generally such methods result in a higher accuracy. ***But this flexibility comes at the cost of interpretability, as a complex relation is harder to interpret.***
    > 
    > Choosing a flexible model, does not always guarantee high accuracy. It happens because our flexible statistical learning procedure is working too hard to ﬁnd patterns in the training data, and ***may be picking up some patterns that are just caused by random chance rather than by true properties***
    > 
    > This phenomenon is also known as **overfitting**.
    > 
    > ---
    > 
    > #### Quality of fit
    > 
    > the most commonly-used measure in regression setting is the mean squared error (MSE),
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*mXRQkeM6d-9xA0YXHKhe4Q.png)
    > 
    > The expected test MSE, for a given value x0, can always be decomposed into the sum of three fundamental quantities: the variance of f(x0), the squared bias of f(x0) and the variance of the error terms e. Where, e is the irreducible error, about which we discusses earlier. So, lets see more about bias and variance.
    > 
    > #### Bias
    > 
    > Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.
    > 
    > #### **Variance**
    > 
    > Variance refers to the amount by which your estimate of f(X) would change if we estimated it using a diﬀerent training data set.
    > 
    > ideally the estimate for f(X) should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in f(X).
    > 
    > #### General Rule
    > 
    > A general rule is that, ***as a statistical method tries to match data points more closely or when a more flexible method is used, the bias reduces, but variance increases.***
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*TwEC9bcoXthzg_sFWgsFlA.png)
    > 
    > Credit : An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
    > 
    > 
    > 
    > ![](https://cdn-images-1.medium.com/max/900/1*cSUMKHGKbgU2InrPr6WQkA.png)
    > 
    > Credit : ISLR by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
    > 
    > This is a graph showing test MSE(red curve), bias(green curve) and variance(yellow curve), with respect to flexibility of chosen method, for a particular dataset. The point of lowest MSE makes an interesting point about the error forms bias and variance. It shows that with *increase in flexibility, bias decreases more rapidly than variance increases. After some point there is no more decrease in bias but variance starts increasing rapidly due to overfitting.*
    > 
    > ---
    > 
    > #### Bias-Variance Trade off
    > 
    > ![](https://cdn-images-1.medium.com/max/1200/1*k_D4-U7c3Tf8hJRpaOZoBQ.png)
    > 
    > Credit : An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani
    > 
    > As described earlier, **in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias****.**
    > 
    > The challenge lies in ﬁnding a method for which both the variance and the squared bias are low.
    > 
    > 

## 維度災難(The Curse of Dimensionality in Classification)


- [The Curse of Dimensionality in Classification](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)
- https://hyp.is/go?url=http%3A%2F%2Fwww.visiondummy.com%2F2014%2F04%2Fcurse-dimensionality-affect-classification%2F

    > 
    > Introduction
    > ------------
    > 
    > In this article, we will discuss the so called 'Curse of Dimensionality', and explain why it is important when designing a classifier. In the following sections I will provide an intuitive explanation of this concept, illustrated by a clear example of overfitting due to the curse of dimensionality.
    > 
    > Consider an example in which we have a set of images, each of which depicts either a cat or a dog. We would like to create a classifier that is able to distinguish dogs from cats automatically. To do so, we first need to think about a descriptor for each object class that can be expressed by numbers, such that a mathematical algorithm, i.e. a classifier, can use these numbers to recognize the object. We could for instance argue that cats and dogs generally differ in color. A possible descriptor that discriminates these two classes could then consist of three number; the average red color, the average green color and the average blue color of the image under consideration. A simple linear classifier for instance, could combine these features linearly to decide on the class label:
    > 
    > > If 0.5*red + 0.3*green + 0.2*blue > 0.6 : return cat;\
    > > else return dog;
    > 
    > However, these three color-describing numbers, called features, will obviously not suffice to obtain a perfect classification. Therefore, we could decide to add some features that describe the texture of the image, for instance by calculating the average edge or gradient intensity in both the X and Y direction. We now have 5 features that, in combination, could possibly be used by a classification algorithm to distinguish cats from dogs.
    > 
    > To obtain an even more accurate classification, we could add more features, based on color or texture histograms, statistical moments, etc. Maybe we can obtain a perfect classification by carefully defining a few hundred of these features? The answer to this question might sound a bit counter-intuitive: *no we can not!*. In fact, after a certain point, increasing the dimensionality of the problem by adding new features would actually degrade the performance of our classifier. This is illustrated by figure 1, and is often referred to as 'The Curse of Dimensionality'.
    > 
    > [![Feature dimensionality versus classifier performance](http://www.visiondummy.com/wp-content/uploads/2014/04/dimensionality_vs_performance.png "Feature dimensionality versus classifier performance")](http://www.visiondummy.com/wp-content/uploads/2014/04/dimensionality_vs_performance.png)
    > 
    > **Figure 1.** As the dimensionality increases, the classifier's performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance.
    > 
    > In the next sections we will review why the above is true, and how the curse of dimensionality can be avoided.
    > 
    > The curse of dimensionality and overfitting
    > -------------------------------------------
    > 
    > In the earlier introduced example of cats and dogs, let's assume there are an infinite number of cats and dogs living on our planet. However, due to our limited time and processing power, we were only able to obtain 10 pictures of cats and dogs. The end-goal in classification is then to train a classifier based on these 10 training instances, that is able to correctly classify the infinite number of dog and cat instances which we do not have any information about.
    > 
    > Now let's use a simple linear classifier and try to obtain a perfect classification. We can start by a single feature, e.g. the average 'red' color in the image:
    > 
    > [![A 1D classification problem](http://www.visiondummy.com/wp-content/uploads/2014/04/1Dproblem.png "A 1D classification problem")](http://www.visiondummy.com/wp-content/uploads/2014/04/1Dproblem.png)
    > 
    > **Figure 2.** A single feature does not result in a perfect separation of our training data.
    > 
    > Figure 2 shows that we do not obtain a perfect classification result if only a single feature is used. Therefore, we might decide to add another feature, e.g. the average 'green' color in the image:
    > 
    > [![2D classification problem](http://www.visiondummy.com/wp-content/uploads/2014/04/2Dproblem.png "2D classification problem")](http://www.visiondummy.com/wp-content/uploads/2014/04/2Dproblem.png)
    > 
    > **Figure 3.** Adding a second feature still does not result in a linearly separable classification problem: No single line can separate all cats from all dogs in this example.
    > 
    > Finally we decide to add a third feature, e.g. the average 'blue' color in the image, yielding a three-dimensional feature space:
    > 
    > [![3D classification problem](http://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem.png "3D classification problem")](http://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem.png)
    > 
    > **Figure 4.** Adding a third feature results in a linearly separable classification problem in our example. A plane exists that perfectly separates dogs from cats.
    > 
    > In the three-dimensional feature space, we can now find a plane that perfectly separates dogs from cats. This means that a linear combination of the three features can be used to obtain perfect classification results on our training data of 10 images:
    > 
    > [![Linearly separable classification problem](http://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem_separated.png "Linearly separable classification problem")](http://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem_separated.png)
    > 
    > **Figure 5.** The more features we use, the higher the likelihood that we can successfully separate the classes perfectly.
    > 
    > The above illustrations might seem to suggest that increasing the number of features until perfect classification results are obtained is the best way to train a classifier, whereas in the introduction, illustrated by figure 1, we argued that this is not the case. However, note how the density of the training samples decreased exponentially when we increased the dimensionality of the problem.
    > 
    > In the 1D case (figure 2), 10 training instances covered the complete 1D feature space, the width of which was 5 unit intervals. Therefore, in the 1D case, the sample density was 10/5=2 samples/interval. In the 2D case however (figure 3), we still had 10 training instances at our disposal, which now cover a 2D feature space with an area of 5×5=25 unit squares. Therefore, in the 2D case, the sample density was 10/25 = 0.4 samples/interval. Finally, in the 3D case, the 10 samples had to cover a feature space volume of 5x5x5=125 unit cubes. Therefore, in the 3D case, the sample density was 10/125 = 0.08 samples/interval.
    > 
    > If we would keep adding features, the dimensionality of the feature space grows, and becomes sparser and sparser. Due to this sparsity, it becomes much more easy to find a separable hyperplane because the likelihood that a training sample lies on the wrong side of the best hyperplane becomes infinitely small when the number of features becomes infinitely large. However, if we project the highly dimensional classification result back to a lower dimensional space, a serious problem associated with this approach becomes evident:
    > 
    > [![Overfitting](http://www.visiondummy.com/wp-content/uploads/2014/04/overfitting.png "Overfitting")](http://www.visiondummy.com/wp-content/uploads/2014/04/overfitting.png)
    > 
    > **Figure 6.** Using too many features results in overfitting. The classifier starts learning exceptions that are specific to the training data and do not generalize well when new data is encountered.
    > 
    > Figure 6 shows the 3D classification results, projected onto a 2D feature space. Whereas the data was linearly separable in the 3D space, this is not the case in a lower dimensional feature space. In fact, adding the third dimension to obtain perfect classification results, simply corresponds to using a complicated non-linear classifier in the lower dimensional feature space. As a result, the classifier learns the appearance of specific instances and exceptions of our training dataset. Because of this, the resulting classifier would fail on real-world data, consisting of an infinite amount of unseen cats and dogs that often do not adhere to these exceptions.
    > 
    > This concept is called overfitting and is a direct result of the curse of dimensionality. Figure 7 shows the result of a linear classifier that has been trained using only 2 features instead of 3:
    > 
    > [![Linear classifier](http://www.visiondummy.com/wp-content/uploads/2014/04/no_overfitting.png "Linear classifier")](http://www.visiondummy.com/wp-content/uploads/2014/04/no_overfitting.png)
    > 
    > **Figure 7.** Although the training data is not classified perfectly, this classifier achieves better results on unseen data than the one from figure 5.
    > 
    > Although the simple linear classifier with decision boundaries shown by figure 7 seems to perform worse than the non-linear classifier in figure 5, this simple classifier generalizes much better to unseen data because it did not learn specific exceptions that were only in our training data by coincidence. In other words, by using less features, the curse of dimensionality was avoided such that the classifier did not overfit the training data.
    > 
    > Figure 8 illustrates the above in a different manner. Let's say we want to train a classifier using only a single feature whose value ranges from 0 to 1. Let's assume that this feature is unique for each cat and dog. If we want our training data to cover 20% of this range, then the amount of training data needed is 20% of the complete population of cats and dogs. Now, if we add another feature, resulting in a 2D feature space, things change; To cover 20% of the 2D feature range, we now need to obtain 45% of the complete population of cats and dogs in each dimension (0.45^2 = 0.2). In the 3D case this gets even worse: to cover 20% of the 3D feature range, we need to obtain 58% of the population in each dimension (0.58^3 = 0.2).
    > 
    > [![The amount of training data grows exponentially with the number of dimensions](http://www.visiondummy.com/wp-content/uploads/2014/04/curseofdimensionality.png)](http://www.visiondummy.com/wp-content/uploads/2014/04/curseofdimensionality.png)
    > 
    > **Figure 8.** The amount of training data needed to cover 20% of the feature range grows exponentially with the number of dimensions.
    > 
    > In other words, if the amount of available training data is fixed, then overfitting occurs if we keep adding dimensions. On the other hand, if we keep adding dimensions, the amount of training data needs to grow exponentially fast to maintain the same coverage and to avoid overfitting.
    > 
    > In the above example, we showed that the curse of dimensionality introduces sparseness of the training data. The more features we use, the more sparse the data becomes such that accurate estimation of the classifier's parameters (i.e. its decision boundaries) becomes more difficult. Another effect of the curse of dimensionality, is that this sparseness is not uniformly distributed over the search space. In fact, data around the origin (at the center of the hypercube) is much more sparse than data in the corners of the search space. This can be understood as follows:
    > 
    > Imagine a unit square that represents the 2D feature space. The average of the feature space is the center of this unit square, and all points within unit distance from this center, are inside a unit circle that inscribes the unit square. The training samples that do not fall within this unit circle are closer to the corners of the search space than to its center. These samples are difficult to classify because their feature values greatly differs (e.g. samples in opposite corners of the unit square). Therefore, classification is easier if most samples fall inside the inscribed unit circle, illustrated by figure 9:
    > 
    > [![Features at unit distance from their average fall inside a unit circle](http://www.visiondummy.com/wp-content/uploads/2014/04/inscribed_circle.png "Features at unit distance from their average fall inside a unit circle")](http://www.visiondummy.com/wp-content/uploads/2014/04/inscribed_circle.png)
    > 
    > **Figure 9.** Training samples that fall outside the unit circle are in the corners of the feature space and are more difficult to classify than samples near the center of the feature space.
    > 
    > An interesting question is now how the volume of the circle (hypersphere) changes relative to the volume of the square (hypercube) when we increase the dimensionality of the feature space. The volume of a unit hypercube of dimension d is always 1^d = 1. The [volume of the inscribing hypersphere](https://en.wikipedia.org/wiki/Volume_of_an_n-ball "Volume of a hypersphere") of dimension d and with radius 0.5 can be calculated as:
    > 
    > (1)  ![\begin{equation*} V(d) = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2} + 1)}0.5^d. \end{equation*}](http://www.visiondummy.com/wp-content/ql-cache/quicklatex.com-3472e58fe7837e68dc4f98a8516cc5bc_l3.png "Rendered by QuickLaTeX.com")
    > 
    > Figure 10 shows how the volume of this hypersphere changes when the dimensionality increases:
    > 
    > [![The volume of the hypersphere tends to zero as the dimensionality increases](http://www.visiondummy.com/wp-content/uploads/2014/04/hypersphere.png "Volume of a hypersphere versus the number of dimensions")](http://www.visiondummy.com/wp-content/uploads/2014/04/hypersphere.png)
    > 
    > **Figure 10.** The volume of the hypersphere tends to zero as the dimensionality increases.
    > 
    > This shows that the volume of the hypersphere tends to zero as the dimensionality tends to infinity, whereas the volume of the surrounding hypercube remains constant. This surprising and rather counter-intuitive observation partially explains the problems associated with the curse of dimensionality in classification: In high dimensional spaces, most of the training data resides in the corners of the hypercube defining the feature space. As mentioned before, instances in the corners of the feature space are much more difficult to classify than instances around the centroid of the hypersphere. This is illustrated by figure 11, which shows a 2D unit square, a 3D unit cube, and a creative visualization of an 8D hypercube which has 2^8 = 256 corners:
    > 
    > [![Highly dimensional feature spaces are sparse around their origin](http://www.visiondummy.com/wp-content/uploads/2014/04/sparseness.png "Highly dimensional feature spaces are sparse around their origin")](http://www.visiondummy.com/wp-content/uploads/2014/04/sparseness.png)
    > 
    > **Figure 11.** As the dimensionality increases, a larger percentage of the training data resides in the corners of the feature space.
    > 
    > For an 8-dimensional hypercube, about 98% of the data is concentrated in its 256 corners. As a result, when the dimensionality of the feature space goes to infinity, the ratio of the difference in minimum and maximum Euclidean distance from sample point to the centroid, and the minimum distance itself, tends to zero:
    > 
    > (2)  ![\begin{equation*} \lim_{d \to \infty} \frac{\operatorname{dist}_{\max} - \operatorname{dist}_{\min}}{\operatorname{dist}_{\min}} \to 0 \end{equation*}](http://www.visiondummy.com/wp-content/ql-cache/quicklatex.com-7ffb60f75669300ffbcf8768471ca99d_l3.png "Rendered by QuickLaTeX.com")
    > 
    > Therefore, distance measures start losing their effectiveness to measure dissimilarity in highly dimensional spaces. Since classifiers depend on these distance measures (e.g. Euclidean distance, Mahalanobis distance, Manhattan distance), classification is often easier in lower-dimensional spaces where less features are used to describe the object of interest. Similarly, Gaussian likelihoods become flat and heavy tailed distributions in high dimensional spaces, such that the ratio of the difference between the minimum and maximum likelihood and the minimum likelihood itself tends to zero.
    > 
    > How to avoid the curse of dimensionality?
    > -----------------------------------------
    > 
    > Figure 1 showed that the performance of a classifier decreases when the dimensionality of the problem becomes too large. The question then is what 'too large' means, and how overfitting can be avoided. Regrettably there is no fixed rule that defines how many feature should be used in a classification problem. In fact, this depends on the amount of training data available, the complexity of the decision boundaries, and the type of classifier used.
    > 
    > If the theoretical infinite number of training samples would be available, the curse of dimensionality does not apply and we could simply use an infinite number of features to obtain perfect classification. The smaller the size of the training data, the less features should be used. If N training samples suffice to cover a 1D feature space of unit interval size, then N^2 samples are needed to cover a 2D feature space with the same density, and N^3 samples are needed in a 3D feature space. In other words, the number of training instances needed grows exponentially with the number of dimensions used.
    > 
    > Furthermore, classifiers that tend to model non-linear decision boundaries very accurately (e.g. neural networks, KNN classifiers, decision trees) do not generalize well and are prone to overfitting. Therefore, the dimensionality should be kept relatively low when these classifiers are used. If a classifier is used that generalizes easily (e.g. naive Bayesian, linear classifier), then the number of used features can be higher since the classifier itself is less expressive. Figure 6 showed that using a simple classifier model in a high dimensional space corresponds to using a complex classifier model in a lower dimensional space.
    > 
    > Therefore, overfitting occurs both when estimating relatively few parameters in a highly dimensional space, and when estimating a lot of parameters in a lower dimensional space. As an example, consider a [Gaussian density function](http://www.visiondummy.com/2014/03/divide-variance-n-1/ "Why divide the sample variance by N-1?"), parameterized by its mean and covariance matrix. Let's say we operate in a 3D space, such that the covariance matrix is a 3×3 symmetric matrix consisting of 6 unique elements (3 variances on the diagonal and 3 covariances off-diagonal). Together with the 3D mean of the distribution this means that we need to estimate 9 parameters based on our training data, to obtain the Gaussian density that represent the likelihood of our data. In the 1D case, only 2 parameters need to be estimated (mean and variance), whereas in the 2D case 5 parameters are needed (2D mean, two variances and a covariance). Again we can see that the number of parameters to be estimated grows quadratic with the number of dimensions.
    > 
    > [In an earlier article](http://www.visiondummy.com/2014/03/divide-variance-n-1/ "Why divide the sample variance by N-1?") we showed that the variance of a parameter estimate increases if the number of parameters to be estimated increases (and if the bias of the estimate and the amount of training data are kept constant). This means that the quality of our parameter estimates decreases if the dimensionality goes up, due to the increase of variance. An increase of classifier variance corresponds to overfitting.
    > 
    > Another interesting question is which features should be used. Given a set of N features; how do we select an optimal subset of M features such that M<N? One approach would be to search for the optimum in the curve shown by figure 1. Since it is often intractable to train and test classifiers for all possible combinations of all features, several methods exist that try to find this optimum in different manners. These methods are called [feature selection algorithms](https://en.wikipedia.org/wiki/Feature_selection "Feature selection") and often employ heuristics (greedy methods, best-first methods, etc.) to locate the optimal number and combination of features.
    > 
    > Another approach would be to replace the set of N features by a set of M features, each of which is a combination of the original feature values. Algorithms that try to find the optimal linear or non-linear combination of original features to reduce the dimensionality of the final problem are called [Feature Extraction methods](https://en.wikipedia.org/wiki/Feature_extraction "Feature extraction"). A well known dimensionality reduction technique that yields uncorrelated, linear combinations of the original N features is [Principal Component Analysis (PCA)](http://www.visiondummy.com/2014/05/feature-extraction-using-pca/ "Feature extraction using PCA"). PCA tries to find a linear subspace of lower dimensionality, such that the largest variance of the original data is kept. However, note that the largest variance of the data not necessarily represents the most discriminative information.
    > 
    > Finally, an invaluable technique used to detect and avoid overfitting during classifier training is [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics) "Cross Validation"). Cross validation approaches split the original training data into one or more training subsets. During classifier training, one subset is used to test the accuracy and precision of the resulting classifier, while the others are used for parameter estimation. If the classification results on the subsets used for training greatly differ from the results on the subset used for testing, overfitting is in play. Several types of cross-validation such as k-fold cross-validation and leave-one-out cross-validation can be used if only a limited amount of training data is available.
    > 
    > Conclusion
    > ----------
    > 
    > In this article we discussed the importance of feature selection, feature extraction, and cross-validation, in order to avoid overfitting due to the curse of dimensionality. Using a simple example, we reviewed an important effect of the curse of dimensionality in classifier training, namely overfitting.













