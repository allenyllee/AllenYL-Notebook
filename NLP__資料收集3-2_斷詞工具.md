# NLP__資料收集3-2_斷詞工具

[toc]
<!-- toc --> 

# 中文分詞算法

## 最小熵原理分詞(無監督)

- [最小熵原理（一）：无监督学习的原理 - 科学空间|Scientific Spaces](https://kexue.fm/archives/5448)


    > ### 无意中的邂逅 
    > 
    > 前段时间看了一篇关于[无监督句法分析的文章](http://www.52nlp.cn/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E7%94%9F%E8%AF%AD%E6%96%99%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E8%AF%AD%E6%B3%95%E8%A7%84%E5%88%99%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95)，继而从它的参考文献中发现了论文[《Redundancy Reduction as a Strategy for Unsupervised Learning》](https://kexue.fm/usr/uploads/2018/04/3543773848.pdf)，这篇论文介绍了如何从去掉空格的英文文章中将英文单词复原。对应到中文，这不就是词库构建吗？于是饶有兴致地细读了一番，发现论文思路清晰、理论完整、结果漂亮，让人赏心悦目。
    > 
    > 尽管现在看来，这篇论文的价值不是很大，甚至其结果可能已经被很多人学习过了，但是要注意：这是一篇1993年的论文！在PC机还没有流行的年代，就做出了如此前瞻性的研究。虽然如今深度学习流行，NLP任务越做越复杂，这确实是一大进步，但是我们对NLP原理的真正了解，还不一定超过几十年前的前辈们多少。
    > 
    > 这篇论文是通过“去冗余”（Redundancy Reduction）来实现无监督地构建词库的，从信息论的角度来看，“去冗余”就是信息熵的最小化。无监督句法分析那篇文章也指出“信息熵最小化是无监督的NLP的唯一可行的方案”。我进而学习了一些相关资料，并且结合自己的理解思考了一番，发现这个评论确实是耐人寻味。我觉得，不仅仅是NLP，信息熵最小化很可能是所有无监督学习的根本。
    > 
    > ### 何为最小熵原理？ 
    > 
    > 读者或许已经听说过[最大熵原理](https://kexue.fm/archives/3552#%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86)和[最大熵模型](https://kexue.fm/archives/3567#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B)，现在这个最小熵原理又是什么？它跟最大熵不矛盾吗？
    > 
    > 我们知道，熵是不确定性的度量，最大熵原理的意思就是说我们在对结果进行推测时，要承认我们的无知，所以要最大化不确定性，以得到最客观的结果。而对于最小熵原理，我们有两个理解角度：
    > 
    > > 1、直观的理解：文明的演化过程总是一个探索和发现的过程，经过我们的努力，越多越多的东西从不确定变成了确定，熵逐渐地趋于最小化。因此，要从一堆原始数据中发现隐含的规律（把文明重演出来），就要在这个规律是否有助于降低总体的信息熵，因为这代表了文明演化的方向，这就是“最小熵原理”。
    > > 
    > > 2、稍严谨的理解：“知识”有一个固有信息熵，代表它的本质信息量。但在我们彻底理解它之前，总会有未知的因素，这使得我们在表达它的时候带有冗余，所以按照我们当前的理解去估算信息熵，得到的事实上是固有信息熵的上界，而信息熵最小化意味着我们要想办法降低这个上界，也就意味着减少了未知，逼近固有信息熵。
    > 
    > 于是，我沿着“最小熵原理”这条路，重新整理了前人的工作，并做了一些新的拓展，最终就有了这些文字。读者将会慢慢看到，最小熵原理能用一种极具解释性和启发性的方法来导出丰富的结果来。
    > 
    > ## 语言的信息 
    > 
    > 让我们从考究语言的信息熵开始，慢慢进入这个最小熵原理的世界～
    > 
    > ### 信息熵=学习成本 
    > 
    > 从[《“熵”不起：从熵、最大熵原理到最大熵模型（一）》](https://kexue.fm/archives/3534/)我们可以知道，一个对象的信息熵是正比于它的概率的负对数的，也就是  
    > 
    > $$I(c)\sim -\log p_c\tag{1.1}$$  
    > 如果认为中文的基本单位是字，那么中文就是字的组合，$p_c$就意味着对应字的概率，而$-\log p_c$就是该字的信息量。我们可以通过大量的语料估算每个汉字的平均信息：  
    > $$\mathcal{H}_c = -\sum_{c\in\text{汉字}} p_c\log p_c\tag{1.2}$$  
    > 如果
    > 
    > $\log$是以2为底的话，那么根据网上流传的数据，这个值约是9.65比特（我自己统计了一些文章，得到的数值约为9.5，两者相当）。类似地，英文中每个字母的平均信息约是4.03比特。
    > 
    > 这个数字意味着什么呢？一般可以认为，我们接收或记忆信息的速度是固定的，那么这个信息量的大小事实上也就相当于我们接收这个信息所需要的时间（或者所花费的精力，等等），从而也可以说这个数字意味着我们学习这个东西的难度（记忆负荷）。比如，假设我们每秒只能接收1比特的信息，那么按字来记忆一篇800字文章，就需要
    > 
    > $9.65\times 800$秒的时间。
    > 
    > ### 中英文孰优孰劣？ 
    > 
    > 既然中文单字信息熵是9.65，而英文字母信息熵才4.03，这是不是意味着英文是一种更高效的表达呢？
    > 
    > 显然不能这么武断，**难道背英语作文一定比背诵汉语作文要容易么？**
    > 
    > 比如800字的中文作文翻译成英文的话，也许就有500词了，平均每个英文4个字母，那么信息量就是
    > 
    > $4.03\times 500\times 4 \approx 9.65\times 800$，可见它们是相当的。换句话说，比较不同文字单元的信息量是没有意义的，有意义的是信息总量，也就是说描述同样的意思时谁更简练。
    > 
    > 当两句话的意思一样时，这个“意思”的固有信息量是不变的，但用不同语言表达时，就不可避免引入“冗余”，所以不同语言表达起来的信息量不一样，这个信息量其实就相当于记忆负荷，越累赘的语言信息量越大，记忆负荷越大。就好比教同样的课程，有的老师教得清晰明了，学生轻松地懂了，有的老师教得哆里哆嗦，学生也学得很痛苦。同样是一节课程，本质上它们的知识量是一样的，而教得差的老师，表明授课过程中带来了过多的无关信息量，这就是“冗余”，所以我们要想办法“去冗余”。
    > 
    > 上述中英文的估计结果相当，表明中英文都经过长期的优化，双方都大致达到了一个比较优化的状态，并没有谁明显优于谁的说法。
    > 
    > ## 套路之路 
    > 
    > 注意，上面的估算中，我们强调了“按字来记忆”，也许我们太熟悉中文了，没意识到这意味着什么，事实上这代表了一种很机械的记忆方式，而我们事实上不是这样做的。
    > 
    > ### 念经也有学问 
    > 
    > 回顾我们小时候背诵古诗、文言文的情景，刚开始我们是完全不理解、囫囵吞枣地背诵，也就是每个字都认识、串起来就不知道什么含义的那种，这就是所谓的“按字来阅读”了。很显然，这样的记忆难度是很大的。后来，我们也慢慢去揣摩古文的成文规律了，逐渐能理解一些古诗或文言文的含义了，背诵难度就会降下来。到了高中，我们还会学习到文言文中“宾语前置”、“定语后置”之类的语法规律，这对我们记忆和理解文言文都是很有帮助的。
    > 
    > 重点来了！
    > 
    > 从古文的例子我们就能够感受到，像念经一样逐字背诵是很困难的，组词理解后就容易些，如果能找到一些语法规律，就更加容易记忆和理解了。但是我们接收（记忆）信息的速度还是固定的，这也就意味着分词、语法这些步骤，降低了语言的信息量，从而降低了我们的学习成本！
    > 
    > 再细想一下，其实不单单是语言，我们学习任何东西都是这样的，如果只有少数的内容要学习，那么我们强行记住就行了，但学习的东西比较多时，我们就试图找出其中的“套路”，比如中国象棋中就分开局、中局、残局，每种局面都有很多“定式”，用来降低初学者的学习难度，而且也是在复杂局面中应变的基础；再好比我们有《孙子兵法》、《三十六计》，这些都是“套路大全”。通过挖掘“套路”来减轻逐一记忆的负担，“套路”的出现就是一个减少信息量的过程。
    > 
    > 说到底，念经念多了，也能发现经文的套路了。
    > 
    > ### 以不变应万变 
    > 
    > 一言以蔽之，我们接收信息的速度是固定的，所以加快我们的学习进度的唯一方法就是降低学习目标的冗余信息量，所谓“去芜存菁”，这就是NLP中的最小熵原理了，也就是一开始所提到的“去冗余”，我们可以理解为是“省去没必要的学习成本”。
    > 
    > 事实上，一个高效的学习过程必定能体现出这个思想，同样地，教师也是根据这个思想来设计他们的教学计划的。在教学的时候，教师更倾向于讲授那些“通解通法”（哪怕步骤多一点），也不会选择每一题都讲一种独特的、巧妙的解法；在准备高考时，我们会努力摸索各种出题套路、解题套路。这些都是通过挖掘“套路”来降低信息熵、从而降低学习成本的过程，“套路”就是“去冗余”的方法。
    > 
    > “套路”即“定式”，有了足够多的套路，我们才能以不变应万变。所谓“万变不离其宗”，这个“宗”，也就是套路了吧。当“套路”过多时，我们又会进一步寻找“套套路”——即套路的套路，来减轻我们记忆套路的负担，这是一个层层递进的过程。看来，将个体现象上升为套路，正是人类的智能的体现呢～
    > 
    > 好了，空话不宜多说，接下来我们就正式走上修炼套路的旅途。




- [最小熵原理（二）：“当机立断”之词库构建 - 科学空间|Scientific Spaces](https://kexue.fm/archives/5476)

    > ## 为什么需要词语 
    > 
    > 从上一篇文章可以看到，假设我们根本不懂中文，那么我们一开始会将中文看成是一系列“字”随机组合的字符串，但是慢慢地我们会发现上下文是有联系的，它并不是“字”的随机组合，它应该是“套路”的随机组合。于是为了减轻我们的记忆成本，我们会去挖掘一些语言的“套路”。第一个“套路”，是相邻的字之间的组合定式，这些组合定式，也就是我们理解的“词”。
    > 
    > ### 平均字信息熵 
    > 
    > 假如有一批语料，我们将它分好词，以词作为中文的单位，那么每个词的信息量是
    > 
    > $-\log p_w$，因此我们就可以计算记忆这批语料所要花费的时间为  
    > $$-\sum_{w\in \text{语料}}\log p_w\tag{2.1}$$  
    > 这里$w\in \text{语料}$是对语料逐词求和，不用去重。如果不分词，按照字来理解，那么需要的时间为  
    > $$-\sum_{c\in \text{语料}}\log p_c\tag{2.2}$$  
    > 根据前一节的理解，分词其实就是为了减轻记忆负担，那么理论上应该有  
    > $$-\sum_{w\in \text{语料}}\log p_w < -\sum_{c\in \text{语料}}\log p_c\tag{2.3}$$  
    > 当然，很多词语是重复出现的，因此我们可以把相同项合并：  
    > $$-\sum_{w\in \text{词表}}N_w\log p_w < -\sum_{c\in \text{字表}}N_c\log p_c\tag{2.4}$$  
    > 其中$N_w,N_c$分别是在语料中统计得到词和字的频数。对式$(2.4)$两边除以语料的总字数，那么右端就是  
    > $$-\sum_{c\in \text{字表}}\frac{N_c}{\text{总字数}}\log p_c = -\sum_{c\in \text{字表}}p_c \log p_c\tag{2.5}$$  
    > 其中$N_c/\text{总字数}=p_c$即为字$c$的频率，所以上式就是每个字的平均信息，而$(2.4)$左端可以变形为  
    > $$\begin{aligned}\mathcal{L} =& -\sum_{w\in \text{词表}}\frac{N_w}{\text{总字数}}\log p_w \\ =& \left(-\sum_{w\in \text{词表}}\frac{N_w}{\text{总词数}}\log p_w\right)\div \left(\frac{\text{总字数}}{\text{总词数}}\right)\\ =&\left(-\sum_{w\in \text{词表}}\frac{N_w}{\text{总词数}}\log p_w\right)\div \left(\frac{\sum\limits_{w\in \text{词表}}N_w l_w}{\text{总词数}}\right)\\ =&\left(-\sum_{w\in \text{词表}}\frac{N_w}{\text{总词数}}\log p_w\right)\div \left(\sum\limits_{w\in \text{词表}}\frac{N_w}{\text{总词数}} l_w\right)\\ =&\frac{\mathcal{H}}{l}\end{aligned}\tag{2.6}$$  
    > 其中$N_w/\text{总词数}=p_w$是词$w$的频率，$l_w$是词$w$的字数，所以$\mathcal{H}$是平均每个词的信息量，$l$是每个词的平均字数：  
    > $$\mathcal{H} = -\sum_{w\in\text{词表}} p_w\log p_w,\quad l=\sum_{w\in\text{词表}} p_w l_w\tag{2.7}$$  
    > 因此
    > 
    > **$\mathcal{L}$实际上是按词来记忆时平均每个字的信息量，是我们要比较和优化的终极目标。我们将总的信息量转化为平均到每个字的信息量，是为了得到一个统一的度量标准。**
    > 
    > ### 丰富你的词汇量 
    > 
    > 是不是真如期望一样，分词有助于降低了学习难度？我简单统计了一下，对微信公众号的文章做一个基本的分词后，
    > 
    > $\mathcal{H}\approx 10.8$比特，每个词平均是1.5个字，也就是$l\approx 1.5$，那么
    > 
    > $\mathcal{L}=7.2$比特，这明显小于逐字记忆的9.65比特。这就说明了在中文里边，按词来记忆确实比按字来记忆要轻松。
    > 
    > **“分词”这个操作，降低了中文的学习难度，这也是“分词”通常都作为中文NLP的第一步的原因。**
    > 
    > 简单来想，对语言进行分词之后，我们需要记忆的词表变大了，但是每个句子的长度变短了，整体来说学习难度是降低了。所以这也就不难理解为什么要学好英语，就得去背单词，因为丰富我们的词汇量能降低学习语言的难度。
    > 
    > ## 套路是如何炼成的 
    > 
    > 反过来，我们也可以由
    > 
    > $\mathcal{L}$的最小化，来指导我们无监督地把词库构建出来。这就是“套路是如何炼成的”这门课的重点内容了。
    > 
    > ### 套路之行，始于局部 
    > 
    > 首先我们将平均字信息熵
    > 
    > $\mathcal{L}$局部化。所谓局部化，是指考察怎样的两个基本元素合并成一个新的元素后，会导致
    > 
    > $\mathcal{L}$降低，这样我们就可以逐步合并以完成熵最小化的目标。这里用“基本元素”来描述，是为了将问题一般化，因为字可以合并为词，词可以合并为词组，等等，这个过程是迭代的，迭代过程中“基本元素”是变化的。
    > 
    > **“局部化”是接下来大部分内容的基础，其推导过程虽然冗长，但都是比较简单的运算，稍加思考即可弄懂。**
    > 
    > 假设目前
    > 
    > $i$的频数为$N_i$，总频数为$N$，那么可以估算$p_i=N_i/N$，假设$i$的字数为$l_i$，那么就可以算出当前的  
    > $$\mathcal{L}=\frac{\mathcal{H}}{l}=\frac{-\sum\limits_i p_i\log p_i}{\sum\limits_i p_i l_i}\tag{2.8}$$  
    > 如果将两个相邻的$a,b$合并成一项呢？假设$(a,b)$的频数为$N_{ab}$，那么在合并前可以估计$p_{ab}=N_{ab}/N$。如果将它们合并为一个“词”来看待，那么总频数实际上是下降了，变为$\tilde{N}=N-N_{ab}$，而$\tilde{N}_a=N_a-N_{ab}$，$\tilde{N}_b = N_b-N_{ab}$，其他频数不变，因此我们就可以重新估计各个频率  
    > $$\begin{aligned}\tilde{p}_{ab}=&\frac{N_{ab}}{\tilde{N}}=\frac{p_{ab}}{1-p_{ab}}\\ \tilde{p}_{a}=&\frac{\tilde{N}_{a}}{\tilde{N}}=\frac{p_a - p_{ab}}{1-p_{ab}},\,\tilde{p}_{b}=\frac{\tilde{N}_{b}}{\tilde{N}}=\frac{p_b - p_{ab}}{1-p_{ab}}\\ \tilde{p}_{i}=&\frac{N_{i}}{\tilde{N}}=\frac{p_i}{1-p_{ab}},\, (i\neq a,b) \end{aligned}\tag{2.9}$$  
    > 于是  
    > $$\begin{aligned}\tilde{\mathcal{H}}=&-\frac{1}{1-p_{ab}}\Bigg\{p_{ab}\log\Big(\frac{p_{ab}}{1-p_{ab}}\Big) + \\ &\qquad \sum_{i=a,b}(p_i - p_{ab})\log\Big(\frac{p_i - p_{ab}}{1-p_{ab}}\Big)+\sum_{i\neq a,b} p_i\log \Big(\frac{p_i}{1-p_{ab}}\Big)\Bigg\}\\ =&\frac{1}{1-p_{ab}}(\mathcal{H}-\mathcal{F}_{ab})\end{aligned}\tag{2.10}$$  
    > 其中  
    > $$\begin{aligned}\mathcal{F}_{ab}= &p_{ab}\log \frac{p_{ab}}{p_a p_b} -(1-p_{ab})\log(1-p_{ab}) \\ &+ \sum_{i=a,b}(p_i-p_{ab})\log\Big(1-\frac{p_{ab}}{p_i}\Big)\end{aligned}\tag{2.11}$$  
    > 而  
    > $$\begin{aligned}\tilde{l}=&\frac{p_{ab}}{1-p_{ab}}(l_a + l_b) + \sum_{i=a,b}\frac{p_i - p_{ab}}{1-p_{ab}}l_i + \sum_{i\neq a,b} \frac{p_i}{1-p_{ab}} l_i\\ =&\frac{l}{1-p_{ab}} \end{aligned}\tag{2.12}$$  
    > 因此  
    > $$\frac{\tilde{\mathcal{H}}}{\tilde{l}}-\frac{\mathcal{H}}{l}=-\frac{\mathcal{F}_{ab}}{l}\tag{2.13}$$  
    > 我们的目的是让$\tilde{\mathcal{H}}/\tilde{l}$变小，所以很明显，一个好的“套路”应该要使得
    > 
    > $\mathcal{F}_{ab} \gg 0$。
    > 
    > ### 简明优美的近似 
    > 
    > $\mathcal{F}_{ab}$的表达式过于复杂，以至于难以发现出规律来，我们可以做一些近似。$p_{ab} \leq p_a, p_b$总是成立的，而很多时候甚至可以认为$p_{ab}\ll p_a,p_b$，这样一来在使用自然对数时就有  
    > $$\begin{aligned}&\ln(1-p_{ab})\approx -p_{ab}\\ &\ln\Big(1-\frac{p_{ab}}{p_i}\Big)\approx -\frac{p_{ab}}{p_i}\end{aligned}\tag{2.14}$$  
    > 因为这个近似的条件是要使用自然对数（$\ln(1+x)\approx x$），所以我们将下面的$\log$全部改为自然对数$\ln$。代入$\mathcal{F}_{ab}$的表达式并去掉$p_{ab}$的二次以上的项，得到  
    > $$\mathcal{F}_{ab}\approx F_{ab}^*=p_{ab} \left(\ln \frac{p_{ab}}{p_{a} p_{b}}-1\right)\tag{2.15}$$  
    > 这个指标就比较简明漂亮了，其中
    > 
    > $PMI(a, b)=\ln\frac{p_{ab}}{p_{a} p_{b}}$我们称之为点互信息。
    > 
    > > 利用泰勒级数，可以得到更一般的展开式为  
    > 
    > $$\mathcal{F}_{ab} = p_{ab} \left(\ln \frac{p_{ab}}{p_{a} p_{b}}-1\right)+\frac{1}{2}\left(\frac{1}{p_a}+\frac{1}{p_b}-1\right)p_{ab}^2+\dots$$  
    > 可见（也可以严格证明）$F_{ab}^*$的近似总是小于$\mathcal{F}_{ab}$的真实值，因此$F_{ab}^* = p_{ab} \left(\ln \frac{p_{ab}}{p_{a} p_{b}}-1\right)\gg 0$其实是
    > 
    > > $\mathcal{F}_{ab}\gg 0$的一个**充分条件**。
    > 
    > 上面讨论的是两个元素的合并，如果是
    > 
    > $k$个基本元素$\boldsymbol{a}=(a_1,\dots,a_k)$的合并，那么同样可以推得  
    > $$\begin{aligned}\mathcal{F}_{\boldsymbol{a}}= &p_{\boldsymbol{a}}\ln \frac{p_{\boldsymbol{a}}}{\prod\limits_{i\in \boldsymbol{a}} p_i} -\big[1-(k-1)p_{\boldsymbol{a}}\big]\ln\big[1-(k-1)p_{\boldsymbol{a}}\big] \\ &\qquad+ \sum_{i\in\boldsymbol{a}}(p_i- p_{\boldsymbol{a}})\ln\Big(1-\frac{ p_{\boldsymbol{a}}}{p_i}\Big)\end{aligned}\tag{2.16}$$  
    > 其近似公式为  
    > 
    > $$\mathcal{F}_{\boldsymbol{a}}\approx \mathcal{F}_{\boldsymbol{a}}^* = p_{\boldsymbol{a}} \left(\ln \frac{p_{\boldsymbol{a}}}{\prod\limits_{i\in\boldsymbol{a}} p_i}-1\right)\tag{2.17}$$
    > 
    > ### 推导尽，词库现 
    > 
    > 现在我们可以看到，要使得目标让
    > 
    > $\tilde{\mathcal{H}}/\tilde{l}$变小，就应该要有$\mathcal{F}_{ab} \gg 0$，而$\mathcal{F}_{ab}$的一个下界近似是$F_{ab}^*$，所以我们可以用$F_{ab}^*\gg 0$来进行确定哪些元素是需要合并，而对应于词库构建的过程，
    > 
    > $F_{ab}^*\gg 0$实际上告诉我们哪些字是需要合并成词的。
    > 
    > 根据
    > 
    > $F_{ab}^*$的特点不难看出，$F_{ab}^* \gg 0$的一个必要的条件是$\ln \frac{p_{ab}}{p_{a} p_{b}} > 1$，也就是说互信息至少要大于1，这个必要条件下，互信息越大越好，
    > 
    > $a,b$的共现频率也越大越好，这就告诉我们要从共现频率和互信息两个角度来判断是否要把元素合并。
    > 
    > 在词库构建这个事情上，还有一个巧妙的招数，那就是对
    > 
    > $F_{ab}^*$的 **“逆运用”** ：式$F_{ab}^*$告诉我们满足$\mathcal{F}_{ab}^* \gg 0$时，两个元素就应该合并。那么反过来， $\mathcal{F}_{ab}^*$ 小于某个
    > 
    > $\theta$时，是不是就应该切分呢？（逆向思维，从确定哪些要“合并”变为确定哪些要“切分”，所谓“当机立断”。）这样的话我们只需要用两个元素合并的公式来指导我们对语料进行一个粗糙的切分，然后对切分结果进行统计筛选，就可以得到很多词了。
    > 
    > 这样，我们就得到一种简单有效的构建词库的算法：
    > 
    > > 1、**统计**：从大量的语料中统计每个字的频率（
    > 
    > $p_a,p_b$），以及统计相邻两字的共现频率(
    > 
    > $p_{ab}$)；
    > 
    > 2、**切分**：分别设定出现频率的阈值
    > 
    > $min\_prob$和互信息的阈值$min\_pmi$，然后在语料中将$p_{ab} < min\_prob$或
    > 
    > $\ln\frac{p_{ab}}{p_a p_b} < min\_pmi$的邻字切开（“当机立断”，相当于一次粗糙的分词）；
    > 
    > 3、**截断**：经过第2步的切分后，统计各个“准词语”的频率
    > 
    > $p_{w'}$，仅保留
    > 
    > > $p_{w'} > min\_prob$部分；
    > 
    > 以前三步得到的词典还是有冗余的，这体现为：直接用这个词典构建一个一元分词模型，那么词典中的有些词永远都不会被分出来，因为那些词可以用它们的子词表示出来，并且子词的概率乘积还大于它本身的概率。比如在实验中，因为“的、主”以及“主、要”的互信息都大于1，所以词库中出现了“的主要”这个“词”，但统计发现
    > 
    > $p(\text{的主要}) < p(\text{的})p(\text{主要})$，那么这个词在一元模型分词的时候是不会分出来的，这个“词”放在词库中只会浪费内存，所以要去掉“的主要”这个词，并且把“的主要”的频数加到“的”和“主要”这两个词上面。
    > 
    > 因此，根据这个原则，我们可以过滤掉一部分的候选词：
    > 
    > > 4、**去冗**：将词典的候选词按字数从多到少排列，然后依次将每个候选词在词库中删除掉，用剩下的词和词频将这个候选词分词，计算原词与子词之间的互信息，根据式
    > 
    > > $(2.17)$，如果互信息大于1，则恢复这个词，否则保持删除，并且更新切分出来的子词的词频。
    > 
    > “去冗”这一步的作用还是很明显的，可以去掉30%甚至50%以上的“不及格”的候选词，以下就是筛选出来的一部分（100个）：
    > 
    > > 的研究, 上的, 的一个, 的方法, 是在, 在中国, 的主要, 的一, 性的, 系统的, 是中国, 发展的, 方面的, 的方式, 的社会, 以上的, 传统的, 化的, 我们的, 的功能, 的需要, 的各种, 中国人, 的形成, 问题的, 为一, 物质的, 名的, 产品的, 的数据, 里的, 组织的, 时期的, 来的, 的结构, 的信息, 式的, 好的, 的一些, 的传统, 文化的, 的标准, 的目标, 主要的, 为中国, 和社会, 人们的, 规定的, 的规定, 的支持, 的最大, 在一个, 主义的, 的价值, 所谓的, 国家和, 的主, 的特征, 量的, 能力的, 在中, 世界的, 系统中, 者的, 的空间, 成立的, 的最高, 的方向, 是不, 一般的, 是通过, 建立的, 在他, 的最后, 后来的, 体的, 的重大, 等方面的, 这里的, 力的, 领域的, 人之, 具体的, 的反应, 度的, 明显的, 面的, 的电子, 的一切, 民族的, 的数量, 的发现, 技术和, 早期的, 的发生, 知识的, 是美国, 项目的, 的优势, 在一
    > 
    > 可以发现，除了“中国人”这个外，其他确实都是我们认识中的“不及格”的词语。
    > 
    > PS：“去冗”这一步需要一些人工规则来对长词进行降权，因为有限的语料对长词的统计是很不充分的，因此长词的概率估计会偏高。而既然是“人工规则”，那就让读者自由发挥了，这里不明确给出。
    > 
    > ### 难道就这么简单？ 
    > 
    > 当然，哪怕是经过上述4步，这个算法看起来还是过于简单，以至于会让人怀疑它的效果。有读者应该会想到，
    > 
    > $\ln\frac{p_{ab}}{p_a p_b} < min\_pmi$不意味着
    > 
    > $\ln\frac{p_{abc}}{p_a p_b p_c} < min\_pmi$呀，也就是相邻两个字的互信息很小，但三个字的互信息可能就大了，只根据两字的互信息来切分是不是过于武断了？
    > 
    > 确实有这样的例子，比如在某些语料中，“共和国”中的“和”、“国”，“林心如”中的“心”、“如”，两个字的互信息是比较小的，但三字的互信息就很大了。然而事实上，落实到中文词库构建这个应用上，这种情况并不多，因为中文跟英文不同，英文基本单位是字母，也就是只有26个，它们两两组合才676个，甚至三三组合也才一万多个，而汉字本身就有数千个，而理论上汉字对（即2-gram）就有数百万个了，所以只需要统计2-gram就能得到相对充分地反映汉字的组合特征。所以如果出现了这种错例，很大可能是因为这些词不是我们输入语料的显著词，说白了，错了就错了呗。
    > 
    > **因此我们基本上不必要讨论三个或三个以上元素合并的公式，用上述的邻字的判别算法即可，在很多情景下它都可以满足我们的需求了。如果要对它进行改进的话，都无法避免引入三阶或更高阶的语言模型，并且还可能需要引入动态规划算法，这会大大降低算法效率。也就是说，改进的思路是有的，但改进的代价会有点大。**
    > 
    > ## 识别第一种套路 
    > 
    > 现在我们可以找出一些自然语言的“套路”——也就是词语了。接下来的问题是，给我们一句话，如何识别出这句话中的套路呢？而对于“词”这个套路，说白了，就是有了词库和词频后如何分词呢？
    > 
    > ### 一元分词新诠释 
    > 
    > 有了前述讨论，其实就很简单了，自始至终，我们找套路就是为了减轻记忆负担，降低语料总的信息量，而我们有  
    > 
    > $$-\sum_{w\in \text{语料}}\ln p_w = -\sum_{w\in \text{句子}\subset \text{语料}}\ln p_w\tag{2.18}$$  
    > 也就是最小化总的信息量等价最小化每个句子的信息量。所以对于一个给定的句子，它的最好的分词方案——假设为$w_1,w_2,\dots,w_n$——那么应该使得该句子的总信息量  
    > 
    > $$-\sum_{k=1}^n \ln p(w_k)=-\ln\prod_{k=1}^n p(w_k)\tag{2.19}$$  
    > 最小，这其实就是一元分词模型——使得每个词的概率乘积最大，但这里我们通过最小熵原理赋予了它新的诠释。
    > 
    > ### 核心的动态规划 
    > 
    > 一元分词模型可以通过动态规划来求解。动态规划是NLP中的核心算法之一，它是求满足马尔可夫假设的图模型的最优路径的方法。
    > 
    > 一般的最优路径是一个复杂度很高的问题，然而由于马尔可夫假设的存在，使得这里存在一个高效的算法，效率为
    > 
    > $\mathscr{O}(n)$，这个算法称之为动态规划，也称为viterbi算法，是由安德鲁·维特比(Andrew Viterbi)于1967年提出。动态规划的其根本思想是：一条最优路径如果拆分为两部分，那么每一部分都是一条（局部）最优路径。
    > 
    > 具体来讲，在最短路径分词中，动态规划就是说：从左往右扫描句子，扫描到当前字时，只保留直到当前字的最优分词结果，以及经过当前字的所有“准最优序列”（防止过早地误切）。
    > 
    > > 比如，对于句子“中外科学名著”，首先扫描“中”，它就是唯一的分词结果，这个结果被保存下来，经过“中”字的词还有“中外”，因此也要保存下来，也就是说，当扫描到第一个字时，临时变量存的是“中”、“中外”两个候选结果。第二步，扫描“外”字，这时我们知道“中外”可以有“中 / 外”分成两字，或者直接就“中外”成一个词两种选择，当然，后者概率更大，因此“中 / 外”的结果不保留，只保留“中外”，但是，经过“外”的词还有“外科”，那么应当保留“中 / 外科”的可能性，也就是说，扫描到第二字时，所存的就是“中外”、“中 / 外科”两个候选结果。
    > > 
    > > 接下来到“科”字，由于“中外 / 科”的概率没有“中 / 外科”概率那么大，因此到这一步最优的分词方案是“中 / 外科”，但是经过“科”字的词语还有“科学”，因此此时还有保留“中外 / 科学”的可能性，所以扫描到第三字是，所存的就是“中 / 外科”和“中外 / 科学”。依此类推，直到最后一字，就可以成功分出“中外 / 科学 / 名著”。整个过程的原则就是：保留至当前字最优的方案，以及不放过经过当前字的所有词（避免过早误切）。
    > 
    > 在此，笔者觉得，动态规划在NLP中的作用是怎么强调都不为过的，不管你面对的是传统机器学习还是深度学习，都必须掌握动态规划算法，不仅要懂得用，还要懂得自己完整写出来（结合Trie树或者AC自动机）。
    > 
    > ## 解码语言之美 
    > 
    > 尽管我们只是初步完成了无监督构建词库这件事情，但含义远不止于此。读者不妨头脑风暴一下，我们前面做的是一件非常神奇的事情：我们并不是人为总结一些机械的规则来构建词库，我们是找到了平均字信息熵这个目标，然后试图最小化它，然后就把词库构建出来了。
    > 
    > **这似乎有点“重演文明”的味道？我们像一个天真的孩童，纯粹在没有目的地自娱自乐，最终却成功地解码了语言的奥秘。**
    > 
    > 做过实验的读者可能又会反驳：你这出来的东西，很多都不是我们认识的“词”呀？笔者认为，只要让模型具有了演化的能力和方向，那么它究竟演化出什么样子的结果来其实是不重要的。就好比分词，我们并不是为了分词而分词，我们分词是为了后面的任务做准备的，既然如此，何必关心分出来的词准不准的？前面都已经说了，分词的目的是为了降低任务难度罢了。我们真正要关心的，是对最后的任务的处理能力。
    > 
    > 听起来有点耍无赖，但事实上这才是真正有人工智能的味道——假如一群真正具有智慧的机器人慢慢地从零开始发展文明，凭什么他发展出来的结果要跟我们的一样呢？
    > 
    > 当然，分词只是第一步，事实上出现误差的本质原因是因为我们只关心了分词，而没有做更复杂的语言结构和语义分析。我们后面还会试图做更多的工作，推导更多的“套路”，读者将会慢慢发现，我们最后得到的结果确实是跟语言学现象吻合的。



- [最小熵原理（三）：“飞象过河”之句模版和语言结构 - 科学空间|Scientific Spaces](https://kexue.fm/archives/5577)


    > ## 语言结构 
    > 
    > 对于大多数人来说，并不会真正知道什么是语法，他们脑海里就只有一些“固定搭配”、“定式”，或者更正式一点可以叫“模版”。大多数情况下，我们是根据模版来说出合理的话来。而不同的人的说话模版可能有所不同，这就是个人的说话风格，甚至是“口头禅”。
    > 
    > ### 句模版 
    > 
    > 比如，“X的Y是什么”就是一个简单的模版，它有一些明确的词语“的”、“是”、“什么”，还有一些占位符X、Y，随便将X和Y用两个名词代进去，得到的就是合乎语法的句子（合不合事实，那是另外一回事了）。这类模版还可以举出很多，“X和Y”、“X的Y”、“X可以Y吗”、“X有哪些Y”、“X是Y还是Z”等等。
    > 
    > [![句模版及其相互嵌套示例](https://kexue.fm/usr/uploads/2018/05/3178766694.png)](https://kexue.fm/usr/uploads/2018/05/3178766694.png "点击查看原图")
    > 
    > 句模版及其相互嵌套示例
    > 
    > 当然，虽然可以抽取尽可能多的模版，但有限的模版是无法覆盖千变万化的语言想象的，所以更重要的是基于模版的嵌套使用。比如“X的Y是什么”这个模版，X可以用模版“A和B”来代替，从而得到“(A和B)的Y是什么”。如此以来，模版相互嵌套，就可以得到相当多句子了。
    > 
    > ### 等价类 
    > 
    > 接着，有了模版“X的Y是什么？”之后，我们怎么知道X和Y分别可以填些什么呢？
    > 
    > 刚才我们说“随便用两个名词”代进去，可是按照我们的思路，到现在为止我们也就只会构建词库，我们连什么是“名词”都不知道，更不知道应该把名词填进去。事实上，我们不需要预先知道什么，我们可以通过大料的语料来抽取每个候选位置的“等价类”，其中X的候选词组成一个词语等价类，Y的候选词也组成一个词语等价类，等等。
    > 
    > [![句模版及等价类的概念](https://kexue.fm/usr/uploads/2018/05/2032573917.png)](https://kexue.fm/usr/uploads/2018/05/2032573917.png "点击查看原图")
    > 
    > 句模版及等价类的概念
    > 
    > **当然，这样的设想是比较理想的，事实上目前我们能获取的生语料情况糟糕得多，但不管怎样，万丈高楼平地起，我们先解决理想情况，实际使用时再去考虑一般情况。**
    > 
    > 下面我们来逐一探究如何从大量的原始语料获取句模版，并考虑如何识别句子中所用到的句模版，甚至挖掘出句子的层次结构。
    > 
    > ## 生成模版 
    > 
    > 事实上，有了前一文的构建词库的经验，事实上就不难构思生成句子模版的算法了。
    > 
    > 在构建词库那里，我们的统计对象是字，现在我们的统计对象是词，此外，词语是由相邻的字组成的，但句子模版却未必是由相邻的词组成的（否则就退化为词或词组），所以我们还要考虑跨词共现，也就是Word2Vec中的Skip Gram模型。
    > 
    > ### 有向无环图 
    > 
    > 有向无环图（Directed Acyclic Graph，DAG）其实是NLP中经常会遇到的一个图论模型。事实上，一元分词模型也可以直接抽象为有向无环图上的最短路径规划问题。而这里的候选模版集构建，也需要用到有向无环图。
    > 
    > 因为考虑了Skip Gram模型，因此我们可以把句子内比较“紧凑”（互信息比较大）的“词对”连接起来，用图论的角度看，这就构成了一个“有向无环图”：
    > 
    > [![句子及Skip Gram关系构成的有向无环图](https://kexue.fm/usr/uploads/2018/05/4049313165.png)](https://kexue.fm/usr/uploads/2018/05/4049313165.png)
    > 
    > 句子及Skip Gram关系构成的有向无环图
    > 
    > 我们直接将图上的所有路径都取出来，如果跨过了相邻节点，那么就插入一个占位符（下面全部用X表示占位符），这样就可以得到候选模版集了。比如从上图中，抽取到的候选模版为：
    > 
    > > 计算机 X 鼠标 X
    > > 
    > > X 的 X 有 什么 X 的 X 呢
    > > 
    > > X 的 X 有 什么 X 呢
    > > 
    > > X 比较 X
    > 
    > ### 算法步骤 
    > 
    > 我们可以把上述流程具体描述如下：
    > 
    > > 1、将语料按句子切分，并分词；
    > > 
    > > 2、选定一个窗口大小window，从语料中统计每个词的频率（
    > 
    > $p_a,p_b$），以及在窗口大小内中任意两词的共现频率(
    > 
    > $p_{ab}$)；
    > 
    > 3、分别设定出现频率的阈值
    > 
    > $min\_prob$和互信息的阈值
    > 
    > $min\_pmi$；
    > 
    > 4、遍历所有句子：  
    >         4.1、对每个句子构建一个图，句子中的词当作图上的点；  
    >         4.2、句子中窗口内的词对
    > 
    > $(a,b)$，如果满足$p_{ab} > min\_prob$和
    > 
    > > $\ln\frac{p_{ab}}{p_a p_b} > min\_pmi$，那么就给图上添加一条“a-->b”的有向边；  
    > >         4.3、找出图上所有的路径（孤立点也算路径），作为候选模版加入统计。
    > > 
    > > 5、统计各个“准模版”的频率，将“准模版”按频率降序排列，取前面部分即可。
    > 
    > **这个算法既可以用来句模版的抽取，也可以简单地用来做词组（短语）的抽取，只需要将window设为1。因此它也就基本上包含了前一文所说的词库构建了，所以上述算法是一个一般化的抽取框架。**
    > 
    > ### 效果演示 
    > 
    > 下面是从[百度知道的问题集](https://kexue.fm/archives/5067)中抽取出来的一些句模版（数字是统计出来的频数，可以忽略）：
    > 
    > > < Template: \[X\] 的 \[X\] > 20199  
    > > < Template: \[X\] 吗 > 9695  
    > > < Template: \[X\] 是 \[X\] > 5358  
    > > < Template: \[X\] 的 > 3979  
    > > < Template: \[X\] 和 \[X\] > 3919  
    > > < Template: 我 \[X\] > 3766  
    > > < Template: \[X\] 有 \[X\] > 3568  
    > > < Template: \[X\] 了 > 2910  
    > > < Template: \[X\] 了 \[X\] > 2702  
    > > < Template: \[X\] 怎么 \[X\] > 2340  
    > > < Template: \[X\] 到 \[X\] > 2254  
    > > < Template: \[X\] 在 \[X\] > 2234  
    > > < Template: \[X\] 我 \[X\] > 2147  
    > > < Template: \[X\] 不 \[X\] > 1708  
    > > < Template: 求 \[X\] > 1547  
    > > < Template: \[X\] 怎么样 > 1371
    > 
    > 注意，事实上“X 的 X”、“X 怎么 X”这种两个占位符夹住一个词的模版是平凡的，它只不过是告诉我们这个词可以插入到句子中使用。因此，为了看出效果，我们排除掉这一类模版，得到：
    > 
    > > < Template: \[X\] 吗 > 9695  
    > > < Template: \[X\] 的 > 3979  
    > > < Template: 我 \[X\] > 3766  
    > > < Template: \[X\] 了 > 2910  
    > > < Template: 求 \[X\] > 1547  
    > > < Template: \[X\] 怎么样 > 1371  
    > > < Template: \[X\] 啊 > 1324  
    > > < Template: 有 \[X\] > 1319  
    > > < Template: \[X\] 怎么办 > 1232  
    > > < Template: 为什么 \[X\] > 1220  
    > > < Template: 请问 \[X\] > 1189  
    > > < Template: \[X\] 呢 > 1099  
    > > < Template: \[X\] 么 > 1003  
    > > < Template: 谢谢 > 997  
    > > < Template: 怎么 \[X\] > 903  
    > > < Template: 在 \[X\] > 894  
    > > < Template: 现在 \[X\] > 874  
    > > < Template: 如何 \[X\] > 867  
    > > < Template: \[X\] 好 > 798  
    > > < Template: \[X\] 是 \[X\] 意思 > 728  
    > > < Template: 是 \[X\] > 727  
    > > < Template: \[X\] 是什么意思 > 721  
    > > < Template: \[X\] 是什么 > 684  
    > > < Template: 怎么办 > 589  
    > > < Template: 有没有 \[X\] > 582  
    > > < Template: \[X\] 多少钱 > 550  
    > > < Template: 从 \[X\] > 526  
    > > < Template: 什么 \[X\] > 522  
    > > < Template: \[X\] 有哪些 > 490  
    > > < Template: \[X\] 是什么 \[X\] > 483
    > 
    > 从结果来看，我们的句模版生成算是确实是有效的。因为这些句模版就有助于我们发现语言的使用规律了。比如：
    > 
    > > 1、“X吗”、“X了”、“X怎么样”这些模版的占位符出现在前面，说明这些词可以放在问句的末尾（我们用到的语料是问句）；
    > > 
    > > 2、“我X”、“求X”、“为什么X”、“请问X”等模版的占位符出现在后面，说明这些词可以放到问句的开头；
    > > 
    > > 3、“谢谢”、“怎么办”这类模版并没有出现占位符，表明它可以单独成句；
    > > 
    > > 4、“X是X意思”、“X是什么X”等模版则反映了语言的一些固定搭配。
    > 
    > 用通用的观点看，这些模版所描述的都是句法级的语言现象。当然，为了不至于跟目前主流的句法分析混淆，我们不妨就称为语言结构规律，或者直接就称为“句模版”。
    > 
    > ## 结构解析 
    > 
    > **跟分词一样，当构建好句子模版后，我们也需要有算法来识别句子中用到了哪些模版，也只有做到了这一步，才有可能从语料中识别出词语的等价类出来。**
    > 
    > 回顾分词算法，分词只是一个句子的切分问题，切分出来的词是没有“洞”（占位符）的，而如果要识别句子中用了哪些模版，这些模版是有“洞”的，并且还可能相互嵌套，这就造成了识别上的困难。然而，一旦我们能够完成这个事情，我们就得到了句子的一个层次结构分解，这是非常有吸引力的目标。
    > 
    > ### 投射性假设 
    > 
    > 为了实现对句子的层次分解，我们首先可以借鉴的是句法分析一般都会使用的“投射性（projective）假设”。
    > 
    > 语言的投射性大概意思是指如果句子可以分为几个“语义块”，那么这些语义块是不交叉的。也就是说，假如第1、2、3个词组成一个语义块、第4、5个词组成一个语义块，这种情况是允许的，而第1、2、4个词组成一个语义块、第3、5个词组成一个语义块，这种情况是不可能的。大多数语言，包括汉语和英语，基本上都满足投射性。
    > 
    > ### 结构假设 
    > 
    > 为了完成句子的层次结构分解，我们需要对句子的组成结构做更完整的假设。受到投射性假设的启发，笔者认为可以将句子的结构做如下假设：
    > 
    > > 1、每个语义块是句子的一个连续子字符串，句子本身也算是一个语义块；
    > > 
    > > 2、每个语义块由一个主的句模版生成，其中句模版的占位符部分也是一个语义块；
    > > 
    > > 3、每个单独的词可以看成是一个平凡的句模版，也可以看成是一个最小粒度的语义块。
    > 
    > 说白了，这三点假设可以归纳为一句话：
    > 
    > > 每个句子是由句模版相互嵌套生成的。
    > 
    > 咋看之下这个假设不够合理，但仔细思考就会发现，这个假设已经足够描述大多数句子的结构了。读者可能有疑虑的是“有没有可能并行地使用两个句模版，而不是嵌套”？答案是：应该不会。因为如果出现这种情况，只需要将“并行”本身视为一个模版就行了，比如将“X和X”也视为一个模版，那么“X和X”这个模版中的两个语义块就是并行的了，甚至它可以与自身嵌套得到“X和(X和X)”描述更多的并行现象。
    > 
    > **也正因为我们对语言结构做了这种假设，所以一旦我们识别出某个句子的最优句模版组合，我们就得到了句子的层次结构——因为根据假设，模版是按照嵌套的方式组合的，嵌套意味着递归，递归就是一种层次树的结构了。**
    > 
    > ### 分解算法 
    > 
    > 有了对句子结构的假设，我们就可以描述句模版识别算法了。首先来重述一下分词算法，一元分词算法的思路为
    > 
    > > 对句子切分成词，使得这些词的概率对数之和最大（信息量之和最小）。
    > 
    > 它还可以换一种表述如下：
    > 
    > > 找一系列的词来不重不漏地覆盖句子中的每个字，使得这些词的概率对数之和最大（信息量之和最小）。
    > 
    > 以往我们会认为分词是对句子进行切分，这种等价的表述则是反过来，要对句子进行覆盖。有了这个逆向思维，就可以提出模版识别算法了：
    > 
    > > 找一系列的句模版来不重、不漏、不交叉地覆盖句子中的每个词，使得这些模版的概率对数之和最大（信息量之和最小）。
    > 
    > 当然，这只是思路，在实现过程中，主要难点是对占位符的处理，也就是说，句子中的每个词既代表这个词本身，也可以代表占位符，这种二重性使得扫描和识别都有困难。而不幸中的万幸是，如果按照上面所假设的语言结构，我们可以转化为一个递归运算：
    > 
    > > 最优的结构分解方案中，主模版下的每个语义块的分解方案也是最优的。
    > 
    > [![句子的层次结构解析，包含了句模版的嵌套调用](https://kexue.fm/usr/uploads/2018/05/2446338822.png)](https://kexue.fm/usr/uploads/2018/05/2446338822.png)
    > 
    > 句子的层次结构解析，包含了句模版的嵌套调用
    > 
    > 因此我们可以得到算法：
    > 
    > > 1、扫描中句子中所有可能出现的模版（通过Trie树结构可以快速扫描）；
    > > 
    > > 2、每种分解方案的得分，等于句子的主模版得分，加上每个语料块的最优分解方案的得分。
    > 
    > ### 结果展示 
    > 
    > 下面是一些简单例子的演示，是通过有限的几个模版进行的分析，可以看到，的确初步实现了句子的层次结构解析。
    > 
    >     +---> (鸡蛋)可以(吃)吗
    >     |     +---> 鸡蛋
    >     |     |     +---> 鸡蛋
    >     |     +---> 可以
    >     |     +---> 吃
    >     |     |     +---> 吃
    >     |     +---> 吗
    >     
    >     +---> (牛肉鸡蛋)可以(吃)吗
    >     |     +---> 牛肉鸡蛋
    >     |     |     +---> 牛肉
    >     |     |     +---> 鸡蛋
    >     |     +---> 可以
    >     |     +---> 吃
    >     |     |     +---> 吃
    >     |     +---> 吗
    >     
    >     +---> (苹果)的(颜色)是(什么)呢
    >     |     +---> 苹果
    >     |     |     +---> 苹果
    >     |     +---> 的
    >     |     +---> 颜色
    >     |     |     +---> 颜色
    >     |     +---> 是
    >     |     +---> 什么
    >     |     |     +---> 什么
    >     |     +---> 呢
    >     
    >     +---> (雪梨和苹果和香蕉)的(颜色)是(什么)呢
    >     |     +---> (雪梨和苹果)和(香蕉)
    >     |     |     +---> (雪梨)和(苹果)
    >     |     |     |     +---> 雪梨
    >     |     |     |     |     +---> 雪梨
    >     |     |     |     +---> 和
    >     |     |     |     +---> 苹果
    >     |     |     |     |     +---> 苹果
    >     |     |     +---> 和
    >     |     |     +---> 香蕉
    >     |     |     |     +---> 香蕉
    >     |     +---> 的
    >     |     +---> 颜色
    >     |     |     +---> 颜色
    >     |     +---> 是
    >     |     +---> 什么
    >     |     |     +---> 什么
    >     |     +---> 呢
    >     
    > 
    > 当然，不能报喜不报忧，也有一些失败的例子：
    > 
    >     +---> (我的美味)的(苹果的颜色)是(什么)呢
    >     |     +---> (我)的(美味)
    >     |     |     +---> 我
    >     |     |     |     +---> 我
    >     |     |     +---> 的
    >     |     |     +---> 美味
    >     |     |     |     +---> 美味
    >     |     +---> 的
    >     |     +---> (苹果)的(颜色)
    >     |     |     +---> 苹果
    >     |     |     |     +---> 苹果
    >     |     |     +---> 的
    >     |     |     +---> 颜色
    >     |     |     |     +---> 颜色
    >     |     +---> 是
    >     |     +---> 什么
    >     |     |     +---> 什么
    >     |     +---> 呢
    >     
    >     +---> (苹果)的(颜色)是(什么的意思是什么)呢
    >     |     +---> 苹果
    >     |     |     +---> 苹果
    >     |     +---> 的
    >     |     +---> 颜色
    >     |     |     +---> 颜色
    >     |     +---> 是
    >     |     +---> (什么)的(意思)是(什么)
    >     |     |     +---> 什么
    >     |     |     |     +---> 什么
    >     |     |     +---> 的
    >     |     |     +---> 意思
    >     |     |     |     +---> 意思
    >     |     |     +---> 是
    >     |     |     +---> 什么
    >     |     |     |     +---> 什么
    >     |     +---> 呢
    >     
    > 
    > 失败的例子我们后面再分析。
    > 
    > ## 文章总结 
    > 
    > 看到一脸懵逼的，有各种话要吐槽的，还请先看到这一节哈～
    > 
    > ### 拼图游戏 
    > 
    > 从词、词组都句模版，我们都像是在玩拼图：拼着拼着发现这两块合在一起效果还行，那么就将它合起来吧。因为将互信息大的项合起来，作为一个整体来看，就有助于降低整体的信息熵，也就能降低整体的学习难度。
    > 
    > 对于句模版，如果在中文的世界里想不通，那么就回顾一下我们在小学、初中时学英语是怎么学过来的吧，那会我们应该学习了很多英语的句模版～
    > 
    > ### 有什么用 
    > 
    > “句模版”算是本文提出的新概念，用它来识别语言结果也算是一种新的尝试。读者不禁要问：这玩意有什么用？
    > 
    > 我想，回答这个问题的最好方式，是引用牛顿的一段话：
    > 
    > > 我自己认为，我好像只是一个在海边玩耍的孩子，不时为捡到比通常更光滑的石子或更美丽的贝壳而欢欣，而展现在我面前的是完全未被探明的真理之海。
    > 
    > 我引用这段话是想表明，做这个探究的最根本原因，**并不是出于某种实用目的，而是为了纯粹地探究自然语言的奥秘。**
    > 
    > 当然，如果与此同时，研究出来的结果能具备一定的应用价值，那就更加完美了。从现在的结果来看，这种应用价值可能是存在的。因为我们在NLP中，面对的句子千变万化，但事实上“句式”却是有限的，这也意味着句模版也是有限的，如果有必要，我们可以对各个句模板的占位符含义进行人工标注，这就能将句模板的结构跟常规的句法描述对应起来了。通过有限的句模版来对句子进行（无限的）分解，能让NLP可面对的场景更加灵活多变一些。
    > 
    > 也许以往的传统自然语言处理中，也出现过类似的东西，但本文所描述的内容纯粹是无监督的结果，并且也有自洽的理论描述，算是一个比较完整的框架，初步的结果也差强人意，因此值得进一步去思考它的应用价值。
    > 
    > ### 艰难前进 
    > 
    > 浏览完这篇文章，读者最大的感觉也许是“一脸懵逼”：能再简化一点吗？
    > 
    > 要回答这个问题，就不得不提到：距离这个系列的上一篇文章已经过了一个多月，这篇文章才正式发出，这似乎有点久了？从形式上看，本文只不过是前文的简单推广：不就是将相邻关联推广到非相邻关联吗？
    > 
    > 的确，形式上确实如此。但为了将这个想法推广至同时具备理论和实用价值，却并不是那么简单和顺畅的事情。比如，在句模版生成时，如何不遗漏地得到所有的候选模版，这便是一个难题；其次，在得到句模版（不管是自动生成还是人工录入）后，如何识别出句子中的句模版，这更加艰难了，不论在理论思考还是编程实现上，都具有相当多的障碍，需要对树结构、递归编程有清晰的把握。我也是陆陆续续调试了半个多月，才算是把整个流程调通了，但估计还不完备。
    > 
    > 所以，你看得一脸懵逼是再正常不过了，我自己做完、写完这篇文章，还感觉很懵呢～
    > 
    > ### 改进思路 
    > 
    > 在结果[结果展示](https://kexue.fm/archives/5577#结果展示)一节中，我们也呈现一些失败的例子。事实上，失败的例子可能还更多。
    > 
    > 我们要从两个角度看待这个事情。一方面，我们有成功的例子，对应纯粹无监督挖掘的探索，我们哪怕只能得到一小部分成功的结果，也是值得高兴的；另外一方面，对于失败的例子，我们需要思考失败的原因，并且考虑解决方案。
    > 
    > 笔者认为，整体的句模版思路是没有问题的，而问题在于我们没有达到真正的语义级别的理解。比如第一个失败的例子，结果是
    > 
    > > (我的美味)的(苹果的颜色)是(什么)呢
    > 
    > 我们能说这个分解完全错吗？显然不是，严格来讲，这种分解在语法上并没有任何错误，只是它不符合语义，不符合我们的常识。因此，并非是句模版的错，而是还不能充分地结合语义来构建句模版。
    > 
    > 回顾目前主流的句法分析工作，不管是有监督的还是无监督的，它们基本上都要结合“词性”来完成句法分析。所以这给我们提供了一个方向：最小熵系列下一步的工作就是要探究词语的聚类问题，以便更好地捕捉词义和语言共性。


## 信息熵分詞法

- [互联网时代的社会语言学：基于SNS的文本数据挖掘 | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/5044)

    > 互联网时代的社会语言学：基于SNS的文本数据挖掘
    > ========================
    > 
    >   今年上半年，我在人人网实习了一段时间，期间得到了很多宝贵的数据，并做了一些还算有意义的事情，在这里和大家一块儿分享。感谢人人网提供的数据与工作环境，感谢赵继承博士、詹卫东老师的支持和建议。在这项工作中，我得到了很多与众人交流的机会，特别感谢 OpenParty 、 TEDxBeijing 提供的平台。本文已发表在了《程序员》杂志，分上下两部分刊于 2012 年 7 月刊和 8 月刊，在此感谢卢鸫翔编辑的辛勤工作。由于众所周知的原因，《程序员》刊出的文章被和谐过（看到后面大家就自动地知道被和谐的内容是什么了），因而我决定把完整版发在 Blog 上，同时与更多的人一同分享。对此感兴趣的朋友可以给我发邮件继续交流。好了，开始说正文吧。
    > 
    >   作为中文系应用语言学专业的学生以及一名数学 Geek ，我非常热衷于用计算的方法去分析汉语资料。汉语是一种独特而神奇的语言。对汉语资料进行自然语言处理时，我们会遇到很多其他语言不会有的困难，比如分词------汉语的词与词之间没有空格，那计算机怎么才知道，"已结婚的和尚未结婚的青年都要实行计划生育"究竟说的是"已／结婚／的／和／尚未／结婚／的／青年"，还是"已／结婚／的／和尚／未／结婚／的／青年"呢？这就是所谓的分词歧义难题。不过，现在很多语言模型已经能比较漂亮地解决这一问题了。但在中文分词领域里，还有一个比分词歧义更令人头疼的东西------未登录词。中文没有首字母大写，专名号也被取消了，这叫计算机如何辨认人名地名之类的东西？更惨的则是机构名、品牌名、专业名词、缩略语、网络新词等等，它们的产生机制似乎完全无规律可寻。最近十年来，中文分词领域都在集中攻克这一难关。自动发现新词成为了关键的环节。
    > 
    >   挖掘新词的传统方法是，先对文本进行分词，然后猜测未能成功匹配的剩余片段就是新词。这似乎陷入了一个怪圈：分词的准确性本身就依赖于词库的完整性，如果词库中根本没有新词，我们又怎么能信任分词结果呢？此时，一种大胆的想法是，首先不依赖于任何已有的词库，仅仅根据词的共同特征，将一段大规模语料中可能成词的文本片段全部提取出来，不管它是新词还是旧词。然后，再把所有抽出来的词和已有词库进行比较，不就能找出新词了吗？有了抽词算法后，我们还能以词为单位做更多有趣的数据挖掘工作。这里，我所选用的语料是人人网 2011 年 12 月前半个月部分用户的状态。非常感谢人人网提供这份极具价值的网络语料。
    > 
    >   要想从一段文本中抽出词来，我们的第一个问题就是，怎样的文本片段才算一个词？大家想到的第一个标准或许是，看这个文本片段出现的次数是否足够多。我们可以把所有出现频数超过某个阈值的片段提取出来，作为该语料中的词汇输出。不过，光是出现频数高还不够，一个经常出现的文本片段有可能不是一个词，而是多个词构成的词组。在人人网用户状态中，"的电影"出现了 389 次，"电影院"只出现了 175 次，然而我们却更倾向于把"电影院"当作一个词，因为直觉上看，"电影"和"院"凝固得更紧一些。
    > 
    >   为了证明"电影院"一词的内部凝固程度确实很高，我们可以计算一下，如果"电影"和"院"真的是各自独立地在文本中随机出现，它俩正好拼到一起的概率会有多小。在整个 2400 万字的数据中，"电影"一共出现了 2774 次，出现的概率约为 0.000113 。"院"字则出现了 4797 次，出现的概率约为 0.0001969 。如果两者之间真的毫无关系，它们恰好拼在了一起的概率就应该是 0.000113 × 0.0001969 ，约为 2.223 × 10^-8^ 次方。但事实上，"电影院"在语料中一共出现了 175 次，出现概率约为 7.183 × 10^-6^ 次方，是预测值的 300 多倍。类似地，统计可得"的"字的出现概率约为 0.0166 ，因而"的"和"电影"随机组合到了一起的理论概率值为 0.0166 × 0.000113 ，约为 1.875 × 10^-6^ ，这与"的电影"出现的真实概率很接近------真实概率约为 1.6 × 10^-5^ 次方，是预测值的 8.5 倍。计算结果表明，"电影院"更可能是一个有意义的搭配，而"的电影"则更像是"的"和"电影"这两个成分偶然拼到一起的。
    > 
    >   当然，作为一个无知识库的抽词程序，我们并不知道"电影院"是"电影"加"院"得来的，也并不知道"的电影"是"的"加上"电影"得来的。错误的切分方法会过高地估计该片段的凝合程度。如果我们把"电影院"看作是"电"加"影院"所得，由此得到的凝合程度会更高一些。因此，为了算出一个文本片段的凝合程度，我们需要枚举它的凝合方式------这个文本片段是由哪两部分组合而来的。令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义"电影院"的凝合程度就是 p(电影院) 与 p(电) - p(影院) 比值和 p(电影院) 与 p(电影) - p(院) 的比值中的较小值，"的电影"的凝合程度则是 p(的电影) 分别除以 p(的) - p(电影) 和 p(的电) - p(影) 所得的商的较小值。
    > 
    >   可以想到，凝合程度最高的文本片段就是诸如"蝙蝠"、"蜘蛛"、"彷徨"、"忐忑"、"玫瑰"之类的词了，这些词里的每一个字几乎总是会和另一个字同时出现，从不在其他场合中使用。
    >   
    >   光看文本片段内部的凝合程度还不够，我们还需要从整体来看它在外部的表现。考虑"被子"和"辈子"这两个片段。我们可以说"买被子"、"盖被子"、"进被子"、"好被子"、"这被子"等等，在"被子"前面加各种字；但"辈子"的用法却非常固定，除了"一辈子"、"这辈子"、"上辈子"、"下辈子"，基本上"辈子"前面不能加别的字了。"辈子"这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，"辈子"并不单独成词，真正成词的其实是"一辈子"、"这辈子"之类的整体。可见，文本片段的自由运用程度也是判断它是否成词的重要标准。如果一个文本片段能够算作一个词的话，它应该能够灵活地出现在各种不同的环境中，具有非常丰富的左邻字集合和右邻字集合。
    > 
    >   "信息熵"是一个非常神奇的概念，它能够反映知道一个事件的结果后平均会给你带来多大的信息量。如果某个结果的发生概率为 p ，当你知道它确实发生了，你得到的信息量就被定义为 -- log(p) 。 p 越小，你得到的信息量就越大。如果一颗骰子的六个面分别是 1 、 1 、 1 、 2 、 2 、 3 ，那么你知道了投掷的结果是 1 时可能并不会那么吃惊，它给你带来的信息量是 -- log(1/2) ，约为 0.693 。知道投掷结果是 2 ，给你带来的信息量则是 -- log(1/3) ≈ 1.0986 。知道投掷结果是 3 ，给你带来的信息量则有 -- log(1/6) ≈ 1.79 。但是，你只有 1/2 的机会得到 0.693 的信息量，只有 1/3 的机会得到 1.0986 的信息量，只有 1/6 的机会得到 1.79 的信息量，因而平均情况下你会得到 0.693/2 + 1.0986/3 + 1.79/6 ≈ 1.0114 的信息量。这个 1.0114 就是那颗骰子的信息熵。现在，假如某颗骰子有 100 个面，其中 99 个面都是 1 ，只有一个面上写的 2 。知道骰子的抛掷结果是 2 会给你带来一个巨大无比的信息量，它等于 -- log(1/100) ，约为 4.605 ；但你只有百分之一的概率获取到这么大的信息量，其他情况下你只能得到 -- log(99/100) ≈ 0.01005 的信息量。平均情况下，你只能获得 0.056 的信息量，这就是这颗骰子的信息熵。再考虑一个最极端的情况：如果一颗骰子的六个面都是 1 ，投掷它不会给你带来任何信息，它的信息熵为 -- log(1) = 0 。什么时候信息熵会更大呢？换句话说，发生了怎样的事件之后，你最想问一下它的结果如何？直觉上看，当然就是那些结果最不确定的事件。没错，信息熵直观地反映了一个事件的结果有多么的随机。
    > 
    >   我们用信息熵来衡量一个文本片段的左邻字集合和右邻字集合有多随机。考虑这么一句话"吃葡萄不吐葡萄皮不吃葡萄倒吐葡萄皮"，"葡萄"一词出现了四次，其中左邻字分别为 {吃, 吐, 吃, 吐} ，右邻字分别为 {不, 皮, 倒, 皮} 。根据公式，"葡萄"一词的左邻字的信息熵为 -- (1/2) - log(1/2) -- (1/2) - log(1/2) ≈ 0.693 ，它的右邻字的信息熵则为 -- (1/2) - log(1/2) -- (1/4) - log(1/4) -- (1/4) - log(1/4) ≈ 1.04 。可见，在这个句子中，"葡萄"一词的右邻字更加丰富一些。
    > 
    >   在人人网用户状态中，"被子"一词一共出现了 956 次，"辈子"一词一共出现了 2330 次，两者的右邻字集合的信息熵分别为 3.87404 和 4.11644 ，数值上非常接近。但"被子"的左邻字用例非常丰富：用得最多的是"晒被子"，它一共出现了 162 次；其次是"的被子"，出现了 85 次；接下来分别是"条被子"、"在被子"、"床被子"，分别出现了 69 次、 64 次和 52 次；当然，还有"叠被子"、"盖被子"、"加被子"、"新被子"、"掀被子"、"收被子"、"薄被子"、"踢被子"、"抢被子"等 100 多种不同的用法构成的长尾⋯⋯所有左邻字的信息熵为 3.67453 。但"辈子"的左邻字就很可怜了， 2330 个"辈子"中有 1276 个是"一辈子"，有 596 个"这辈子"，有 235 个"下辈子"，有 149 个"上辈子"，有 32 个"半辈子"，有 10 个"八辈子"，有 7 个"几辈子"，有 6 个"哪辈子"，以及"n 辈子"、"两辈子"等 13 种更罕见的用法。所有左邻字的信息熵仅为 1.25963 。因而，"辈子"能否成词，明显就有争议了。"下子"则是更典型的例子， 310 个"下子"的用例中有 294 个出自"一下子"， 5 个出自"两下子"， 5 个出自"这下子"，其余的都是只出现过一次的罕见用法。事实上，"下子"的左邻字信息熵仅为 0.294421 ，我们不应该把它看作一个能灵活运用的词。当然，一些文本片段的左邻字没啥问题，右邻字用例却非常贫乏，例如"交响"、"后遗"、"鹅卵"等，把它们看作单独的词似乎也不太合适。我们不妨就把一个文本片段的自由运用程度定义为它的左邻字信息熵和右邻字信息熵中的较小值。
    >   
    >   在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出"巧克"、"俄罗"、"颜六色"、"柴可夫"等实际上是"半个词"的片段；只看自由程度的话，程序则会把"吃了一顿"、"看了一遍"、"睡了一晚"、"去了一趟"中的"了一"提取出来，因为它的左右邻字都太丰富了。
    > 
    >   我们把文本中出现过的所有长度不超过 d 的子串都当作潜在的词（即候选词，其中 d 为自己设定的候选词长度上限，我设定的值为 5 ），再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。为了提高效率，我们可以把语料全文视作一整个字符串，并对该字符串的所有后缀按字典序排序。下表就是对"四是四十是十十四是十四四十是四十"的所有后缀进行排序后的结果。实际上我们只需要在内存中存储这些后缀的前 d + 1 个字，或者更好地，只储存它们在语料中的起始位置。
    > 
    > > 十\
    > > 十十四是十四四十是四十\
    > > 十是十十四是十四四十是四十\
    > > 十是四十\
    > > 十四是十四四十是四十\
    > > 十四四十是四十\
    > > 是十十四是十四四十是四十\
    > > 是十四四十是四十\
    > > 是四十\
    > > 是四十是十十四是十四四十是四十\
    > > 四十\
    > > 四十是十十四是十四四十是四十\
    > > 四十是四十\
    > > 四是十四四十是四十\
    > > 四是四十是十十四是十四四十是四十\
    > > 四四十是四十
    > 
    >   这样的话，相同的候选词便都集中在了一起，从头到尾扫描一遍便能算出各个候选词的频数和右邻字信息熵。将整个语料逆序后重新排列所有的后缀，再扫描一遍后便能统计出每个候选词的左邻字信息熵。另外，有了频数信息后，凝固程度也都很好计算了。这样，我们便得到了一个无需任何知识库的抽词算法，输入一段充分长的文本，这个算法能以大致 O(n - logn) 的效率提取出可能的词来。
    >   对不同的语料进行抽词，并且按这些词的频数从高到低排序。你会发现，不同文本的用词特征是非常明显的。下面是对《西游记》上册的抽词结果：
    > 
    > > 行者、师父、三藏、八戒、大圣、菩萨、悟空、怎么、和尚、唐僧、老孙、溃骸、什么、沙僧、太宗、徒弟、袈裟、妖精、玉帝、今日、兄弟、公主、玄奘、陛下、宝贝、性命、晓得、门外、妖魔、光蕊、观音、花果山、土地、木叉、东土、变化、变做、伯钦、判官、多少、真君、齐天大圣、蟠桃、丞相、魏征、扯住、溃骸澳、抬头、揭谛、言语、猪八戒、兵器、吩咐、安排、叩头、清风、哪吒、左右、美猴王、钉钯、孩儿、女婿、金箍棒、二郎、东西、许多、奈何、人参果、收拾、近前、太保、明月、南海、水帘洞、门首、弼马温、李天王⋯⋯
    > 
    >   《资本论》全文：
    > 
    > > 商品、形式、货币、我们、过程、自己、机器、社会、部分、表现、没有、流通、需要、增加、已经、交换、关系、先令、积累、必须、英国、条件、发展、麻布、儿童、进行、提高、消费、减少、任何、手段、职能、土地、特殊、实际、完全、平均、直接、随着、简单、规律、市场、增长、上衣、决定、什么、制度、最后、支付、许多、虽然、棉纱、形态、棉花、法律、绝对、提供、扩大、独立、世纪、性质、假定、每天、包含、物质、家庭、规模、考察、剥削、经济学、甚至、延长、财富、纺纱、购买、开始、代替、便士、怎样、降低、能够、原料、等价物⋯⋯
    > 
    >   《圣经》全文：
    > 
    > > 以色列、没有、自己、一切、面前、大卫、知道、什么、犹大、祭司、摩西、看见、百姓、吩咐、埃及、听见、弟兄、告诉、基督、已经、先知、扫罗、父亲、雅各、永远、攻击、智慧、荣耀、临到、洁净、离开、怎样、平安、律法、支派、许多、门徒、打发、好像、仇敌、原文作、名叫、巴比伦、今日、首领、旷野、所罗门、约瑟、两个、燔祭、法老、衣服、脱离、二十、公义、审判、十二、亚伯拉罕、石头、聚集、按着、祷告、罪孽、约书亚、事奉、指着、城邑、进入、彼此、建造、保罗、应当、摩押、圣灵、惧怕、应许、如今、帮助、牲畜⋯⋯
    > 
    >   《时间简史》全文：
    > 
    > > 黑洞、必须、非常、任何、膨胀、科学、预言、太阳、观察、定律、运动、事件、奇点、坍缩、问题、模型、方向、区域、知道、开始、辐射、部分、牛顿、产生、夸克、无限、轨道、解释、边界、甚至、自己、类似、描述、最终、旋转、爱因斯坦、绕着、什么、效应、表明、温度、研究、收缩、吸引、按照、完全、增加、开端、基本、计算、结构、上帝、进行、已经、发展、几乎、仍然、足够、影响、初始、科学家、事件视界、第二、改变、历史、世界、包含、准确、证明、导致、需要、应该、至少、刚好、提供、通过、似乎、继续、实验、复杂、伽利略⋯⋯
    > 
    >   哦，对了，还有我最喜欢的，《人民日报》 2000 年 4 月新闻版的抽词结果：
    > 
    > > 发展、我们、经济、主席、江泽民、领导、建设、关系、教育、干部、企业、问题、主义、政治、群众、改革、政府、思想、加强、台湾、地区、北京、总统、世界、记者、代表、民族、组织、历史、访问、原则、努力、管理、今天、技术、市场、世纪、坚持、社会主义、财政、江泽民主席、增长、积极、精神、同志、双方、自己、友好、领导干部、进一步、基础、提高、必须、不断、制度、政策、解决、取得、表示、活动、支持、通过、研究、没有、学习、稳定、举行、欢迎、农村、生活、促进、科技、投资、科学、环境、领域、公司、情况、充分⋯⋯
    > 
    >   当然，我也没有忘记对人人网用户状态进行分析------人人网用户状态中最常出现的词是：
    > 
    > > 哈哈、什么、今天、怎么、现在、可以、知道、喜欢、终于、这样、觉得、因为、如果、感觉、开始、回家、考试、老师、幸福、朋友、时间、发现、东西、快乐、为什么、睡觉、生活、已经、希望、最后、各种、状态、世界、突然、手机、其实、那些、同学、孩子、尼玛、木有、然后、以后、学校、所以、青年、晚安、原来、电话、加油、果然、学习、中国、最近、应该、需要、居然、事情、永远、特别、北京、他妈、伤不起、必须、呵呵、月亮、毕业、问题、谢谢、英语、生日快乐、工作、虽然、讨厌、给力、容易、上课、作业、今晚、继续、努力、有木有、记得⋯⋯
    > 
    >   事实上，程序从人人网的状态数据中一共抽出了大约 1200 个词，里面大多数词也确实都是标准的现代汉语词汇。不过别忘了，我们的目标是新词抽取。将所有抽出来的词与已有词库作对比，于是得到了人人网特有的词汇（同样按频数从高到低排序）：
    > 
    > > 尼玛、伤不起、给力、有木有、挂科、坑爹、神马、淡定、老爸、卧槽、牛逼、肿么、苦逼、无语、微博、六级、高数、选课、悲催、基友、蛋疼、很久、人人网、情何以堪、童鞋、哇咔咔、脑残、吐槽、猥琐、奶茶、我勒个去、刷屏、妹纸、胃疼、飘过、考研、弱爆了、太准了、搞基、忽悠、羡慕嫉妒恨、手贱、柯南、狗血、秒杀、装逼、真特么、碎觉、奥特曼、内牛满面、斗地主、腾讯、灰常、偶遇、拉拉、屌丝、九把刀、高富帅、阿内尔卡、魔兽世界、线代、三国杀、林俊杰、速速、臭美、花痴⋯⋯
    > 
    >   我还想到了更有意思的玩法。为什么不拿每一天状态里的词去和前一天的状态作对比，从而提取出这一天里特有的词呢？这样一来，我们就能从人人网的用户状态中提取出每日热点了！从手里的数据规模看，这是完全有可能的。我选了 12 个比较具有代表性的词，并列出了它们在 2011 年 12 月 13 日的用户状态中出现的频数（左列的数），以及 2011 年 12 月 14 日的用户状态中出现的频数（右列的数）：
    > 
    > | 下雪     | 33   | 92   |
    > |----------|------|------|
    > | 那些年   | 139  | 146  |
    > | 李宇春   | 1    | 4    |
    > | 看见     | 145  | 695  |
    > | 魔兽     | 23   | 20   |
    > | 高数     | 82   | 83   |
    > | 生日快乐 | 235  | 210  |
    > | 今天     | 1416 | 1562 |
    > | 北半球   | 2    | 18   |
    > | 脖子     | 23   | 69   |
    > | 悲伤     | 61   | 33   |
    > | 电磁炉   | 0    | 3    |
    > 
    >   大家可以从直觉上迅速判断出，哪些词可以算作是 12 月 14 日的热词。比方说，"下雪"一词在 12 月 13 日只出现了 33 次，在 12 月 14 日却出现了 92 次，后者是前者的 2.8 倍，这不大可能是巧合，初步判断一定是 12 月 14 日真的有什么地方下雪了。"那些年"在 12 月 14 日的频数确实比 12 月 13 日更多，但相差并不大，我们没有理由认为它是当日的一个热词。
    > 
    >   一个问题摆在了我们面前：我们如何去量化一个词的"当日热度"？第一想法当然是简单地看一看每个词的当日频数和昨日频数之间的倍数关系，不过细想一下你就发现问题了：它不能解决样本过少带来的偶然性。 12 月 14 日"李宇春"一词的出现频数是 12 月 13 日的 4 倍，这超过了"下雪"一词的 2.8 倍，但我们却更愿意相信"李宇春"的现象只是一个偶然。更麻烦的则是"电磁炉"一行， 12 月 14 日的频数是 12 月 13 日的无穷多倍，但显然我们也不能因此就认为"电磁炉"是 12 月 14 日最热的词。
    > 
    >   忽略所有样本过少的词？这似乎也不太好，样本少的词也有可能真的是热词。比如"北半球"一词，虽然它在两天里的频数都很少，但这个 9 倍的关系确实不容忽视。事实上，人眼很容易看出哪些词真的是 12 月 14 日的热词：除了"下雪"以外，"看见"、"北半球"和"脖子"也应该是热词。你或许坚信后三个词异峰突起的背后一定有什么原因（并且迫切地想知道这个原因究竟是什么），但却会果断地把"李宇春"和"电磁炉"这两个"异常"归结为偶然原因。你的直觉是对的------ 2011 年 12 月 14 日发生了极其壮观的双子座流星雨，此乃北半球三大流星雨之一。白天网友们不断转发新闻，因而"北半球"一词热了起来；晚上网友们不断发消息说"看见了"、"又看见了"，"看见"一词的出现频数猛增；最后呢，仰望天空一晚上，脖子终于出毛病了，于是回家路上一个劲儿地发"脖子难受"。
    > 
    >   让计算机也能聪明地排除偶然因素，这是我们在数据挖掘过程中经常遇到的问题。我们经常需要对样本过少的项目进行"平滑"操作，以避免分母过小带来的奇点。这里，我采用的是一个非常容易理解的方法：一个词的样本太少，就给这个词的热度打折扣。为了便于说明，我们选出四个词为例来分析。
    > 
    >   下表截取了前四个词，右边四列分别表示各词在 12 月 13 日出现的频数，在 12 月 14 日出现的频数，在两天里一共出现的总频数，以及后一天的频数所占的比重。第三列数字是前两列数字之和，第四列数字则是第二列数字除以第三列数字的结果。最后一列应该是一个 0 到 1 之间的数，它表明对应的词有多大概率出现在了 12 月 14 日这一天。最后一列可以看作是各词的得分。可以看到，此时"下雪"的得分低于"李宇春"，这是我们不希望看到的结果。"李宇春"的样本太少，我们想以此为缘由把它的得分拖下去。
    > 
    > | 下雪     | 33  | 92  | 125    | 0.736 |
    > |----------|-----|-----|--------|-------|
    > | 那些年   | 139 | 146 | 285    | 0.512 |
    > | 李宇春   | 1   | 4   | 5      | 0.8   |
    > | 看见     | 145 | 695 | 840    | 0.827 |
    > | （平均） |     |     | 313.75 | 0.719 |
    > 
    >   怎么做呢？我们把每个词的得分都和全局平均分取一个加权平均！首先计算出这四个词的平均总频数，为 313.75 ；再计算出这四个词的平均得分，为 0.719 。接下来，我们假设已经有 313.75 个人预先给每个词都打了 0.719 分，换句话说每个词都已经收到了 313.75 次评分，并且所有这 313.75 个评分都是 0.719 分。"下雪"这个词则还有额外的 125 个人评分，其中每个人都给了 0.736 分。因此，"下雪"一词的最终得分就是：
    > 
    > | 下雪 | (0.736 × 125 + 0.719 × 313.75) / (125 + 313.75) ≈ 0.724 |
    > |------|---------------------------------------------------------|
    > 
    >   类似地，其他几个词的得分依次为：
    > 
    > | 那些年 | (0.512 × 285 + 0.719 × 313.75) / (285 + 313.75) ≈ 0.62  |
    > |--------|---------------------------------------------------------|
    > | 李宇春 | (0.8 × 5 + 0.719 × 313.75) / (5 + 313.75) ≈ 0.7202      |
    > | 看见   | (0.827 × 840 + 0.719 × 313.75) / (840 + 313.75) ≈ 0.798 |
    > 
    >   容易看出，此时样本越大的词，就越有能力把最终得分拉向自己本来的得分，样本太小的词，最终得分将会与全局平均分非常接近。经过这么一番调整，"下雪"一词的得分便高于了"李宇春"。实际运用中， 313.75 这个数也可以由你自己来定，定得越高就表明你越在意样本过少带来的负面影响。这种与全局平均取加权平均的思想叫做 Bayesian average ，从上面的若干式子里很容易看出，它实际上是最常见的平滑处理方法之一------分子分母都加上一个常数------的一种特殊形式。
    > 
    >   利用之前的抽词程序抽取出人人网每一天内用户状态所含的词，把它们的频数都与前一天的作对比，再利用刚才的方法加以平滑，便能得出每一天的热词了。我手上的数据是人人网 2011 年 12 月上半月的数据，因此我可以得出从 12 月 2 日到 12 月 15 日的热词（选取每日前 5 名，按得分从高到低）。
    > 
    > > 2011-12-02：第一场雪、北京、金隅、周末、新疆\
    > > 2011-12-03：荷兰、葡萄牙、死亡之组、欧洲杯、德国\
    > > 2011-12-04：那些年、宣传、期末、男朋友、升旗\
    > > 2011-12-05：教室、老师、视帝、体育课、质量\
    > > 2011-12-06：乔尔、星期二、摄影、经济、音乐\
    > > 2011-12-07：陈超、星巴克、优秀、童鞋、投票\
    > > 2011-12-08：曼联、曼城、欧联杯、皇马、冻死\
    > > 2011-12-09：保罗、月全食、交易、火箭、黄蜂\
    > > 2011-12-10：变身、罗伊、穿越、皇马、巴萨\
    > > 2011-12-11：皇马、巴萨、卡卡、梅西、下半场\
    > > 2011-12-12：淘宝、阿内尔卡、双十二、申花、老师\
    > > 2011-12-13：南京、南京大屠杀、勿忘国耻、默哀、警报\
    > > 2011-12-14：流星雨、许愿、愿望、情人节、几颗\
    > > 2011-12-15：快船、保罗、巴萨、昨晚、龙门飞甲
    > 
    >   看来， 12 月 14 日果然有流星雨发生。
    > 
    >   注意，由于我们仅仅对比了相邻两天的状态，因而产生了个别实际上是由工作日/休息日的区别造成的"热词"，比如"教室"、"老师"、"星期二"等。把这样的词当作热词可能并不太妥。结合上周同日的数据，或者干脆直接与之前整个一周的数据来对比，或许可以部分地解决这一问题。
    > 
    >   事实上，有了上述工具，我们可以任意比较两段不同文本中的用词特点。更有趣的是，人人网状态的大多数发布者都填写了性别和年龄的个人信息，我们为何不把状态重新分成男性和女性两组，或者 80 后和 90 后两组，挖掘出不同属性的人都爱说什么？要知道，在过去，这样的问题需要进行大规模语言统计调查才能回答！然而，在互联网海量用户生成内容的支持下，我们可以轻而易举地挖掘出答案来。
    > 
    >   我真的做了这个工作（基于另一段日期内的数据）。男性爱说的词有：
    > 
    > > 兄弟、篮球、男篮、米兰、曼联、足球、蛋疼、皇马、比赛、国足、超级杯、球迷、中国、老婆、政府、航母、踢球、赛季、股市、砸蛋、牛逼、铁道部、媳妇、国际、美国、连败、魔兽、斯内德、红十字、经济、腐败、程序、郭美美、英雄、民主、鸟巢、米兰德比、官员、内涵、历史、训练、评级、金融、体育、记者、事故、程序员、媒体、投资、事件、社会、项目、伊布、主义、决赛、操蛋、纳尼、领导、喝酒、民族、新闻、言论、和谐、农民、体制、城管⋯⋯
    > 
    >   下面则是女性爱说的词：
    > 
    > > 一起玩、蛋糕、加好友、老公、呜呜、姐姐、嘻嘻、老虎、讨厌、妈妈、呜呜呜、啦啦啦、便宜、减肥、男朋友、老娘、逛街、无限、帅哥、礼物、互相、奶茶、委屈、各种、高跟鞋、指甲、城市猎人、闺蜜、巧克力、第二、爸爸、宠物、箱子、吼吼、大黄蜂、狮子、胃疼、玫瑰、包包、裙子、游戏、遇见、嘿嘿、灰常、眼睛、各位、妈咪、化妆、玫瑰花、蓝精灵、幸福、陪我玩、任务、怨念、舍不得、害怕、狗狗、眼泪、温暖、面膜、收藏、李民浩、神经、土豆、零食、痘痘、戒指、巨蟹、晒黑⋯⋯
    > 
    >   下面是 90 后用户爱用的词：
    > 
    > > 加好友、作业、各种、乖乖、蛋糕、来访、卧槽、通知书、麻将、聚会、补课、欢乐、刷屏、录取、无限、互相、速度、一起玩、啦啦啦、晚安、求陪同、基友、美女、矮油、巨蟹、五月天、第二、唱歌、老虎、扣扣、啧啧、帅哥、哈哈哈、尼玛、便宜、苦逼、斯内普、写作业、劳资、孩纸、哎哟、炎亚纶、箱子、无聊、求来访、查分、上课、果断、处女、首映、屏蔽、混蛋、暑假、吓死、新东方、组队、下学期、陪我玩、打雷、妹纸、水瓶、射手、搞基、吐槽、同学聚会、出去玩、呜呜、白羊、表白、做作业、签名、姐姐、停机、伏地魔、对象、哈哈、主页、情侣、无压力、共同、摩羯、碎觉、肿么办⋯⋯
    > 
    >   下面则是 80 后用户爱用的词：
    > 
    > > 加班、培训、周末、工作、公司、各位、值班、砸蛋、上班、任务、公务员、工资、领导、包包、办公室、校内、郭美美、时尚、企业、股市、新号码、英国、常联系、实验室、论文、忙碌、项目、部门、祈福、邀请、招聘、顺利、朋友、红十字、男朋友、媒体、产品、标准、号码、存钱、牛仔裤、曼联、政府、简单、立秋、事故、伯明翰、博士、辞职、健康、销售、深圳、奶茶、搬家、实验、投资、节日快乐、坚持、规则、考验、生活、体制、客户、发工资、忽悠、提供、教育、处理、惠存、沟通、团购、缺乏、腐败、启程、红十字会、结婚、管理、环境、暴跌、服务、变形金刚、祝福、银行⋯⋯
    > 
    >   不仅如此，不少状态还带有地理位置信息，因而我们可以站在空间的维度对信息进行观察。这个地方的人都爱说些什么？爱说这个词的人都分布在哪里？借助这些包含地理位置的签到信息，我们也能挖掘出很多有意思的结果来。例如，对北京用户的签到信息进行抽词，然后对于每一个抽出来的词，筛选出所有包含该词的签到信息并按地理坐标的位置聚类，这样我们便能找出那些地理分布最集中的词。结果非常有趣："考试"一词集中分布在海淀众高校区，"天津"一词集中出现在北京南站，"逛街"一词则全都在西单附近扎堆。北京首都国际机场也是一个非常特别的地点，"北京"、"登机"、"终于"、"再见"等词在这里出现的密度极高。
    > 
    >   从全国范围来看，不同区域的人也有明显的用词区别。我们可以将全国地图划分成网格，统计出所有签到信息在各个小格内出现的频数，作为标准分布；然后对于每一个抽出来的词，统计出包含该词的签到信息在各个小格内出现的频数，并与标准分布进行对比（可以采用余弦距离等公式），从而找出那些分布最反常的词。程序运行后发现，这样的词还真不少。一些明显具有南北差异的词，分布就会与整个背景相差甚远。例如，在节假日的时候，"滑雪"一词主要在北方出现，"登山"一词则主要在南方出现。地方特色也是造成词语分布差异的一大原因，例如"三里屯"一词几乎只在北京出现，"热干面"一词集中出现在武汉地区，"地铁"一词明显只有个别城市有所涉及。这种由当地人的用词特征反映出来的真实的地方特色，很可能是许多旅游爱好者梦寐以求的信息。另外，方言也会导致用词分布差异，例如"咋这么"主要分布在北方地区，"搞不懂"主要分布在南方城市，"伐"则非常集中地出现在上海地区。当数据规模足够大时，或许我们能通过计算的方法，自动对中国的方言区进行划分。
    > 
    >   其实，不仅仅是发布时间、用户年龄、用户性别、地理位置这四个维度，我们还可以对浏览器、用户职业、用户活跃度、用户行为偏好等各种各样的维度进行分析，甚至可以综合考虑以上维度，在某个特定范围内挖掘热点事件，或者根据语言习惯去寻找出某个特定的人群。或许这听上去太过理想化，不过我坚信，有了合适的算法，这些想法终究会被一一实现。
    

## unsupervise Chinese Word Segmentation

- [Self-supervised Chinese Word Segmentation](http://papersdb.cs.ualberta.ca/~papersdb/uploaded_files/391/paper_peng01selfsupervised.pdf)

- [Contextual Dependencies in Unsupervised Word Segmentation](https://cocosci.berkeley.edu/tom/papers/wordseg1.pdf)


- [A Simple and Effective Unsupervised Word Segmentation Approach](https://pdfs.semanticscholar.org/9a55/a63ce8fc4d9723db92f2f27bcd900e93d1da.pdf)

- [Empirical Study of Unsupervised Chinese Word Segmentation Methods
for SMT on Large-scale Corpora](https://pdfs.semanticscholar.org/b253/a5bf513a2d3205242fddcb506ed9cf276e42.pdf)


## ESA

- [jason2506/esapp: An unsupervised Chinese word segmentation tool.](https://github.com/jason2506/esapp)

- [A New Unsupervised Approach to
Word Segmentation](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00058)

## 未知詞擷取作法

- [未知詞擷取作法](http://ckipsvr.iis.sinica.edu.tw/uwe.htm)

    > 馬偉雲
    > 
    > 中研院資訊科學所詞庫小組
    > 
    > 傳統的中文斷詞系統碰到最大的瓶頸就是未知詞(辭典中沒有的詞)的擷取，統計上一篇文章當中 約有3%~5%的詞彙是未知詞，新聞類的文章更是遠高而此。而某些類型未知詞的詞構非常複雜，也不 一定具有強烈的統計特性。因此未知詞的擷取問題一直是中文語言處理上一個重要且困難的研究課題。
    > 
    > 前人的研究主要是針對特定類型的詞彙(如人名、地名、組織名)以詞構分析或是language model去 擷取他們。獲致不錯的成果。不過仍有許多未知詞的類型更加的複雜且多變，例如像"泛藍"、"泛綠"、"拉法葉"、"矽島"、"非典(SARS)"...等等。 這些詞彙往往扮演了文章中的關鍵角色必須加以擷取辨認。 傳統的作法是利用語料庫(corpus)的統計訊息(such as co-occurrence、mutual information、dice、t-socre...)想要一網打盡這些 林林總總的未知詞(通常侷限在2字詞或3字詞)。其基本假設是認為那些統計訊息值較高的pattern，被認為有較強的統計特性， 有較大的可能是一個未知詞。這樣子的假設雖然合理，不過在實際成效上卻仍然不盡人意，主要原因我們認為有 下面幾點:
    > 
    > 1.統計的材料以語料庫為主
    > 
    > 前人作法統計上大都以語料庫統計為主，文章本身的統計為輔甚或根本沒有利用。而我們知道一個未知詞常常 是同篇文章中重要的主題，出現次數往往很頻繁，若是以語料庫統計為主的作法很容易稀釋它本來在文章中的 強烈統計特性。因此，我們認為應以文章本身的統計為主，而以語料庫統計為輔，(一種作法是當某pattern在文章中統計特性或 出現次數已經很強烈，則無必要再參考語料庫，當某pattern在文章中統計特性不強，再參考語料庫看看有沒有扳回的機會) 這樣以文章統計為主的設計對於新聞這種隨日期而發展事件的文章顯得更加的適合:假設今天早報第一次報導"非典(SARS)"的消息，之前 從未有這樣的報導出現過，也沒有"非典"這個詞彙，但在這篇報導"非典(SARS)"的消息的文章當中，"非典"則出現相當多次。 因此善用文章本身的統計訊息是本作法的一個重要精神。在展示系統當中，我們只利用輸入的文章作為唯一的統計材料。
    > 
    > 2.統計特性強的pattern不一定是詞彙，而是片語或是不合理的pattern
    > 
    > 這樣的問題前人已經發現，一種作法是利用stop word list(如"的"，"是"...)去過濾那些包含stop word的pattern，這樣的作法合理但 仍嫌不足。事實上，我們要處理的根本問題其實應該是這樣子的：以一個單字(初步斷詞後)來說，它究竟是未知詞的詞素還是獨用的詞彙呢？ 過濾那些包含stop word的pattern的作法，就是認為這些stop word必定是獨用的詞彙而永遠不可能是未知詞的詞素，所以將包含stop word的pattern過濾掉。 不過單靠這樣的作法，對那些未列入stop word list的單字就無法判斷，因為在不同的上下文中兩者都有可能， 本作法利用單字(初步斷詞後)的上下文判斷此單字究竟是未知詞的詞素還是獨用的詞彙[2]。若是某一個pattern就算具有強烈的統計特性，但是其所有的組成元件 都是判定為獨用詞彙的話，我們仍然不擷取這樣的pattern。
    > 
    > 3.統計特性低的未知詞不容易被擷取出來
    > 
    > 前人對於此問題通常只能針對特定類型的未知詞，分析其詞構或其language model來解決這樣的問題， 而無法提出一般性的解法，針對所有統計特性低的未知詞一網打盡。針對這樣的缺點，本作法除了針對特定類型未知詞作專屬的詞構分析與 language model之外，更嘗試將所有種類的未知詞的詞構以context free grammar表示出來。這個context free grammar制訂的最主要原則 在於：任何一個擷取出來的未知詞在此context free grammar下，其詞構當中至少有一個組成元件為判斷為未知詞詞素的單字。 利用這個context free grammar搭配一個簡單的bottom-up merging algorithm，能夠解決大部分統計特性低的未知詞擷取問題[4]。
    > 
    > 目前我們的作法依序可分為以下幾個步驟：
    > 
    > 1.  初步斷詞
    > 2.  未知詞偵測
    > 3.  中國人名擷取
    > 4.  歐美譯名擷取
    > 5.  複合詞擷取
    > 6.  bottom-up merging algorithm
    > 7.  重新斷詞
    > 
    > 經過初步斷詞(我們使用連續三詞的長詞優先演算法[1])後，絕大多數的未知詞會被斷成較小的單位，即此未知詞的詞素，我們希望在接下來的步驟中， 將這些詞素重新組合成未知詞。我們觀察到99%的未知詞其詞構當中至少會有一個單字的詞素。因此我們希望利用未知詞偵測這個步驟去判定初步斷詞 後的單字哪些是詞素，哪些是獨用詞彙[2](在展示系統當中，判定為詞素的單字會標記一個問號)，之後的擷取步驟再根據這樣的資訊，把處理的焦點放在這些判定為詞素的單字身上，看看是否能和其相鄰的token來合併成未知詞。 我們針對一些特定類型的未知詞，如：中國人名，歐美譯名，複合詞等作了詞構分析及簡單的language model[3]，剩餘的 其它未知詞則交給bottom-up merging algorithm做最後的擷取動作[4]。目前為止，我們所擷取出來的pattern稱為"未知詞候選者"，參考這些"未知詞候 選者"搭配原始的辭典再做一次斷詞可得到最後的結果。重新斷詞的目的是在於：有時某一個未知詞可能在文章某處並沒有擷取出來，但是在文章的 另一處被擷取出來，所以重新斷詞可以使兩處都能有正確的斷詞結果。另外，未知詞候選者彼此間有時會有covering或overlapping這種衝突的現象，重新斷詞 能夠幫助我們在這些有衝突的未知詞候選者之中挑選出一個正確答案作為最後確定的未知詞。
    > 
    > 在展示系統當中，您可以輸入一篇文章(最簡單的方法是copy一篇新聞)，系統就會做未知詞擷取以及包含未知詞的斷詞標記動作。 最後會把結果秀出，秀出的結果不但包含了未知詞列表以及包含未知詞的斷詞標記結果，還有程式運作的過程，包含演算法中每一個步驟所相對應的結果。
    > 
    > 除了此web版本的展示系統之外，詞庫小組也已開發完成了windows平台下的執行程式與API(VC與BCB環境)以及linux平台下的執行程式。
    > 
    > 相關文章
    > 
    > [1] Chen, K.J. & S.H. Liu, "Word Identification for Mandarin Chinese Sentences," Proceedings of COLING 1992, pages 101-107
    > 
    > [2] Chen, K.J. & Ming-Hong Bai, "Unknown Word Detection for Chinese by a Corpus-based Learning Method," International Journal of Computational linguistics and Chinese Language Processing, 1998, Vol.3, #1, pages 27-44 [[PS](http://ckipsvr.iis.sinica.edu.tw/papers/Unknown%20Word%20Detection%20for%20Chinese%20by%20a%20Corpus-based%20Learning%20Method.ps)]
    > 
    > [3] Chen, K.J. & Wei-Yun Ma, "Unknown Word Extraction for Chinese Documents," Proceedings of COLING 2002, pages 169-175 [[PDF](http://ckipsvr.iis.sinica.edu.tw/papers/Unknown%20Word%20Extraction%20for%20Chinese%20Documents.pdf)] [[PS](http://ckipsvr.iis.sinica.edu.tw/papers/Unknown%20Word%20Extraction%20for%20Chinese%20Documents.ps)]
    > 
    > [4] Ma Wei-Yun & K.J. Chen, "A bottom-up Merging Algorithm for Chinese Unknown Word Extraction," Proceedings of ACL workshop on Chinese Language Processing 2003, pages 31-38 [[PDF](http://ckipsvr.iis.sinica.edu.tw/papers/A%20Bottom-up%20Merging%20Algorithm%20for%20Chinese%20Unknown%20Word%20Extraction.pdf)] [[PS](http://ckipsvr.iis.sinica.edu.tw/papers/A%20Bottom-up%20Merging%20Algorithm%20for%20Chinese%20Unknown%20Word%20Extraction.ps)]
    > 




# 工具評比

## 中文分词器分词效果评估对比
- [ysc/cws_evaluation: Java开源项目cws_evaluation：中文分词器分词效果评估对比](https://github.com/ysc/cws_evaluation)


## 竹間智能科技台北 Emotibot Taipei

- [竹間智能科技台北 Emotibot Taipei - 貼文](https://www.facebook.com/EmotibotTaipei/posts/1896621840559761:0)

- [有哪些比较好的中文分词方案？ - 竹间智能 Emotibot的回答 - 知乎](https://www.zhihu.com/question/19578687/answer/190569700)

    > 
    > 中文分词是中文文本处理的一个基础步骤，也是中文人机自然语言交互的基础模块。不同于英文的是，中文句子中没有词的界限，因此在进行中文自然语言处理时，通常需要先进行分词，分词效果将直接影响词性、句法树等模块的效果。当然分词只是一个工具，场景不同，要求也不同。
    > 
    > 在人机自然语言交互中，成熟的中文分词算法能够达到更好的自然语言处理效果，帮助计算机理解复杂的中文语言。竹间智能在构建中文自然语言对话系统时，结合语言学不断优化，训练出了一套具有较好分词效果的算法模型，为机器更好地理解中文自然语言奠定了基础。
    > 
    > 在此，对于 **中文分词方案、当前分词器存在的问题，以及中文分词需要考虑的因素及相关资源** ， ***竹间智能 自然语言与深度学习小组*** 做了些整理和总结，希望能为大家提供一些参考。
    > 
    > 中文分词根据实现原理和特点，主要分为以下2个类别：
    > 
    > **1、基于词典分词算法**
    > 
    > 也称字符串匹配分词算法。该算法是按照一定的策略将待匹配的字符串和一个已建立好的"充分大的"词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。常见的基于词典的分词算法分为以下几种：**正向最大匹配法、逆向最大匹配法**和**双向匹配分词法**等。
    > 
    > 基于词典的分词算法是应用最广泛、分词速度最快的。很长一段时间内研究者都在对基于字符串匹配方法进行优化，比如最大长度设定、字符串存储和查找方式以及对于词表的组织结构，比如采用TRIE索引树、哈希索引等。
    > 
    > **2、基于统计的机器学习算法**
    > 
    > 这类目前常用的是算法是**HMM、CRF、SVM、深度学习**等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。
    > 
    > Nianwen Xue在其论文《Combining Classifiers for Chinese Word Segmentation》中首次提出对每个字符进行标注，通过机器学习算法训练分类器进行分词，在论文《Chinese word segmentation as character tagging》中较为详细地阐述了基于字标注的分词法。
    > 
    > 常见的分词器都是使用**机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。**
    > 
    > 随着深度学习的兴起，也出现了**基于神经网络的分词器**，例如有人员尝试使用双向LSTM+CRF实现分词器，**其本质上是序列标注**，所以有通用性，命名实体识别等都可以使用该模型，据报道其分词器字符准确率可高达97.5%。算法框架的思路与论文《Neural Architectures for Named Entity Recognition》类似，利用该框架可以实现中文分词，如下图所示：
    > 
    > ![](https://pic2.zhimg.com/v2-aad7ef8156b33c51efeb0f7f4b6f614d_b.jpg)
    > 
    > ![](https://pic2.zhimg.com/80/v2-aad7ef8156b33c51efeb0f7f4b6f614d_hd.jpg)
    > 
    > 首先对语料进行字符嵌入，将得到的特征输入给双向LSTM，然后加一个CRF就得到标注结果。
    > 
    > **分词器当前存在问题：**
    > 
    > 目前中文分词难点主要有三个：
    > 
    > **1、分词标准**：比如人名，在哈工大的标准中姓和名是分开的，但在Hanlp中是合在一起的。这需要根据不同的需求制定不同的分词标准。
    > 
    > **2、歧义**：对同一个待切分字符串存在多个分词结果。
    > 
    > 歧义又分为组合型歧义、交集型歧义和真歧义三种类型。
    > 
    > 1) 组合型歧义：分词是有不同的粒度的，指某个词条中的一部分也可以切分为一个独立的词条。比如"中华人民共和国"，粗粒度的分词就是"中华人民共和国"，细粒度的分词可能是"中华/人民/共和国"
    > 
    > 2) 交集型歧义：在"郑州天和服装厂"中，"天和"是厂名，是一个专有词，"和服"也是一个词，它们共用了"和"字。
    > 
    > 3) 真歧义：本身的语法和语义都没有问题, 即便采用人工切分也会产生同样的歧义，只有通过上下文的语义环境才能给出正确的切分结果。例如：对于句子"美国会通过对台售武法案"，既可以切分成"美国/会/通过对台售武法案"，又可以切分成"美/国会/通过对台售武法案"。
    > 
    > **一般在搜索引擎中**，构建索引时和查询时会使用不同的分词算法。**常用的方案是，在索引的时候使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。**
    > 
    > **3、新词**：也称未被词典收录的词，该问题的解决依赖于人们对分词技术和汉语语言结构的进一步认识。
    > 
    > 另外，我们收集了如下**部分分词工具**，供参考：
    > 
    > 中科院计算所NLPIR [http://ictclas.nlpir.org/nlpir/](https://link.zhihu.com/?target=http%3A//ictclas.nlpir.org/nlpir/)
    > 
    > ansj分词器 [https://github.com/NLPchina/ansj_seg](https://link.zhihu.com/?target=https%3A//github.com/NLPchina/ansj_seg)
    > 
    > 哈工大的LTP [https://github.com/HIT-SCIR/ltp](https://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ltp)
    > 
    > 清华大学THULAC [https://github.com/thunlp/THULAC](https://link.zhihu.com/?target=https%3A//github.com/thunlp/THULAC)
    > 
    > 斯坦福分词器 [https://nlp.stanford.edu/software/segmenter.shtml](https://link.zhihu.com/?target=https%3A//nlp.stanford.edu/software/segmenter.shtml)
    > 
    > Hanlp分词器 [https://github.com/hankcs/HanLP](https://link.zhihu.com/?target=https%3A//github.com/hankcs/HanLP)
    > 
    > 结巴分词 [https://github.com/yanyiwu/cppjieba](https://link.zhihu.com/?target=https%3A//github.com/yanyiwu/cppjieba)
    > 
    > KCWS分词器(字嵌入+Bi-LSTM+CRF) [https://github.com/koth/kcws](https://link.zhihu.com/?target=https%3A//github.com/koth/kcws)
    > 
    > ZPar [https://github.com/frcchang/zpar/releases](https://link.zhihu.com/?target=https%3A//github.com/frcchang/zpar/releases)
    > 
    > IKAnalyzer [https://github.com/wks/ik-analyzer](https://link.zhihu.com/?target=https%3A//github.com/wks/ik-analyzer)
    > 
    > **以及部分分词器的简单说明：**
    > 
    > **哈工大的分词器**：主页上给过调用接口，每秒请求的次数有限制。
    > 
    > **清华大学THULAC**：目前已经有Java、Python和C++版本，并且代码开源。
    > 
    > **斯坦福分词器**：作为众多斯坦福自然语言处理中的一个包，目前最新版本3.7.0， Java实现的CRF算法。可以直接使用训练好的模型，也提供训练模型接口。
    > 
    > **Hanlp分词**：求解的是最短路径。优点：开源、有人维护、可以解答。原始模型用的训练语料是人民日报的语料，当然如果你有足够的语料也可以自己训练。
    > 
    > **结巴分词工具**：基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。
    > 
    > **字嵌入+Bi-LSTM+CRF分词器**：本质上是序列标注，这个分词器用人民日报的80万语料，据说按照字符正确率评估标准能达到97.5%的准确率，各位感兴趣可以去看看。
    > 
    > **ZPar分词器**：新加坡科技设计大学开发的中文分词器，包括分词、词性标注和Parser，支持多语言，据说效果是公开的分词器中最好的，C++语言编写。
    > 
    > **关于速度：**
    > 
    > 由于分词是基础组件，其性能也是关键的考量因素。通常，分词速度跟系统的软硬件环境有相关外，还与**词典的结构设计和算法复杂度**相关。比如我们之前跑过字嵌入+Bi-LSTM+CRF分词器，其速度相对较慢。另外，开源项目 [https://github.com/ysc/cws_evaluation](https://link.zhihu.com/?target=https%3A//github.com/ysc/cws_evaluation) 曾对多款分词器速度和效果进行过对比，可供大家参考。
    > 
    > **最后附上公开的分词数据集**
    > 
    > 测试数据集
    > 
    > 1、SIGHAN Bakeoff 2005 MSR,560KB
    > 
    > [http://sighan.cs.uchicago.edu/bakeoff2005/](https://link.zhihu.com/?target=http%3A//sighan.cs.uchicago.edu/bakeoff2005/)
    > 
    > 2、SIGHAN Bakeoff 2005 PKU, 510KB
    > 
    > [http://sighan.cs.uchicago.edu/bakeoff2005/](https://link.zhihu.com/?target=http%3A//sighan.cs.uchicago.edu/bakeoff2005/)
    > 
    > 3、人民日报 2014, 65MB
    > 
    > [https://pan.baidu.com/s/1hq3KKXe](https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1hq3KKXe)
    > 
    > *本回答来自 竹间智能 自然语言与深度学习小组 。*

## 中文處理工具簡介

- [中文處理工具簡介](https://g0v.hackpad.tw/ep/pad/static/aco0Hxp4IEz)

    > **1\. 中研院CKIP parser**
    > 
    > <http://ckipsvr.iis.sinica.edu.tw/>
    > 
    > [http://parser.iis.sinica.edu.tw/](http://l.facebook.com/l.php?u=http%3A%2F%2Fparser.iis.sinica.edu.tw%2F&h=_AQHnxBKI&enc=AZOdi5uvlh1UqhimwSHhZRY_VwVSMmTBiAY6G4QkuU5_wjd7Nm_ppkd0-lPk7cFUEOXJfRf3aEd_CSDdbaIeIqugJ5qz_fM78Qh_y71mZ3BDlyGrr_D4ira1FZm4ZH25a24Qc65tQaLo_-l8cqLS-xXR&s=1) 
    > 
    > [‪](https://www.facebook.com/hashtag/%E7%B9%81%E9%AB%94?source=feed_text)[#](https://g0v.hackpad.tw/ep/search/search?q=%23g0v)繁體‬ [‪](https://www.facebook.com/hashtag/%E6%96%B7%E8%A9%9E?source=feed_text)#斷詞‬ [‪](https://www.facebook.com/hashtag/%E8%A9%9E%E6%80%A7%E6%A8%99%E8%A8%98?source=feed_text)#詞性標記‬ [‪](https://www.facebook.com/hashtag/%E5%8F%A5%E5%9E%8B%E7%B5%90%E6%A7%8B?source=feed_text)#句型結構‬ [‪#](https://www.facebook.com/hashtag/%E4%BF%AE%E9%A3%BE%E9%97%9C%E4%BF%82?source=feed_text)修飾關係‬
    > 
    > 1\. 有點慢，準確率最高
    > 
    > 2\. 可透過web service呼叫（詞性較粗）或爬網頁（詞性較細）。
    > 
    > 3\. 可細分四十多種詞性，如名詞可細分為地方名詞、普通名詞，專有名詞等。
    > 
    > -   中研院的 CKIP parser 是比較建議使用在台灣語言環境中。但是很多時候分詞結果與辭典辭條的結果是不符合的，主要是因為在建立這個工具時，是依照專業家標記後的詞彙進行決定[詞彙詞性](http://rocling.iis.sinica.edu.tw/CKIP/tr/9305_2013%20revision.pdf)。但這個工具也年久失修...
    > -   ~~我申請帳號一直沒給認證信，工具下載下來也沒動靜，不知那邊出了問題~~
    > -   能用了，不過速度有點慢
    > -   現在繁體中文分詞器可以做到95%正確率，詞性標記也有90%，其他功能就比較低了。中文的詞性是很複雜的，又可以『轉品』，有的時候詞庫沒有涵蓋到的例子，也parser很難正確標記出來。
    > 
    > ---
    > 
    > **3\. mmseg 斷詞**
    > 
    > [http://technology.chtsai.org/mmseg/](http://l.facebook.com/l.php?u=http%3A%2F%2Ftechnology.chtsai.org%2Fmmseg%2F&h=hAQHVlEyn&enc=AZOMAL0e0_quhxtbQ7IxLeJy2uIBe9lj5Z5euHBQJGjKsUGZpcsqx0u2KoVTRp18YTuCItyW8UvdaWF-0RrPTkwFZOYBLtDpNMF-jHVvNWKinmSiiHSiwhD00vGVuLiIpiMLn1P43xkLhiDk3Ju0ZyKi&s=1) 
    > 
    > #繁體 #斷詞 [‪#](https://www.facebook.com/hashtag/%E5%BF%AB?source=feed_text)快‬
    > 
    > 可下載單機版，可自己訓練繁體模型，可使用自訂字典
    > 
    > ---
    > 
    > **12\. 國教院分詞系統**
    > 
    > 中研院 CKIP 的衍生系統，據國教院的同仁說，新近詞的收量較大，跑起來也稍快些。
    > 
    > http://120.127.233.228/Segmentor/
    > 
    > 另外還附有一個語料索引系統：http://120.127.233.228/Concordancer/
    > 


## Python 套件比較：繁體中文斷字

- [Python 套件比較：繁體中文斷字 – Pei Lee – Medium](https://medium.com/@peilee_98185/python-%E5%A5%97%E4%BB%B6%E6%AF%94%E8%BC%83-%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87%E6%96%B7%E5%AD%97-4e7a452138f8)

    > #### 結論
    > 
    > 整體看起來，雖然 Jieba 在一些小地方斷得沒有其他的好，但應該算是效果最符合預期的一個了，而且它好安裝又跑得快，單純做斷詞的話應該會是我的首選。
    > 
    > 第二個大概會選擇 HanLP，斷詞的結果也還算漂亮，雖然不好安裝，但執行速度算是跟 Jieba 相差不多。第三個會是 SnowNLP，雖然覺得它斷詞不夠漂亮，但因為它有情緒分析以及文章分類的功能，覺得往後應該還是會有不得不用它的時候。
    > 
    > 最後一個才是 Stanford NLP，斷得不漂亮，只要碰到沒聽過的詞，似乎傾向把它們拆成一個一個字，而且又難安裝，執行速度也慢，功能也沒有前兩個多......所以中文自然語言處理可能不會用到它，不過它英文的做出來的某些效果其實還不錯，所以英文的話也許還是有考慮的機會。


# 斷詞工具

## 中研院中文斷詞系統 CKIP

- [中文斷詞系統](http://ckipsvr.iis.sinica.edu.tw/)

### amigcamel/python-ckip: A Python API for CKIP Chinese Parser

- [amigcamel/python-ckip: A Python API for CKIP Chinese Parser](https://github.com/amigcamel/python-ckip)

    > (支援Python2 / Python3)
    > 想要使用CKIP Chinese Parser 進行小規模的斷詞測試
    > 毋需登入即可使用
    > 
    > 基本上它就是幫你方送HTTP request
    > 然後把結果撈出來

## 結巴中文分詞/斷詞

- [fxsjy/jieba: 结巴中文分词](https://github.com/fxsjy/jieba)

### 關於結巴(Jieba)斷詞的幾個問題

- [關於結巴(Jieba)斷詞的幾個問題 – 働いたら負け](https://blog.ldkrsi.men/%E9%97%9C%E6%96%BC%E7%B5%90%E5%B7%B4%E6%96%B7%E8%A9%9E%E7%9A%84%E5%B9%BE%E5%80%8B%E5%95%8F%E9%A1%8C/)

    > #### 切割句子的學術根據?
    > 
    > **re.compile("([\u4E00-\u9FD5a-zA-Z0-9+#&._]+)", re.U)**\
    > 在Jieba中採用上面這段程式碼來切割句子，看起來就是選取非文字、數字和主要符號的地方當成斷點，但給人感覺設定得很隨意，這些符號感覺包含的太少了，如%(百分比)-(減法)看起來也是不應該成為斷點的符號，還有+#&其相對應的全形符號是否也該納入考量？、(頓號)在學術上是該斷句的點嗎？
    > 
    > #### 漢字區間太小
    > 
    > 算是承上的問題。漢字(CJK)不含標點符號在Unicode裡總共佔了六個區域([wiki](https://zh.wikipedia.org/wiki/Unicode%E5%AD%97%E7%AC%A6%E5%B9%B3%E9%9D%A2%E6%98%A0%E5%B0%84))：中日韓統一表意文字、中日韓統一表意文字擴展A區、中日韓統一表意文字擴展B區、中日韓統一表意文字擴展C區、中日韓統一表意文字擴展D區、中日韓統一表意文字擴展E區、中日韓相容表意文字、中日韓相容表意文字補充區。Jieba只把中日韓統一表意文字(4E00-9FD5)當成是漢字，不在那個區間的漢字一律會被當成類似標點符號的存在，變成斷句點。雖說常用字都在(4E00-9FD5)區間裡，而且罕見字常常單獨成詞，不過人名裡罕見字出現的頻率還算不低(如藝人李㼈)，個人認為至少還要包含中日韓統一表意文字擴展A區(因B~E區多數是古代字，C區之後新細明體也無法顯示)才算符合正常使用需求。
    > 
    > #### 中日韓相容表意文字問題
    > 
    > 這個問題其實普遍存在在中文處理中，Jieba當然也沒有去處理。在[中日韓相容表意文字區間](https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%97%A5%E9%9F%93%E7%9B%B8%E5%AE%B9%E8%A1%A8%E6%84%8F%E6%96%87%E5%AD%97)中(F900-FAFF)躲了非常多日韓漢字及Big5時代的無用字，他們的外觀和和常用字區(4E00-9FD5)所對應的文字幾乎無異，打字時有一定的機率會選錯字，OCR也常常會去使用該區域的字。個人認為在進行所有的中文處理操做時第一步一定要將他們進行取代。
    > 
    > #### EmitProbMatrix機率總合大於1
    > 
    > 路徑為**jieba/finalseg/prob_emit.py**的這個[檔案](https://github.com/fxsjy/jieba/blob/master/jieba/finalseg/prob_emit.py)，裡面的數值意義為P(Observed[i]|Status[j])，Status為BEMS的其中一個，Observed為漢字。所以把對數值換算回機率表示(0~1)時，其總合應為1。但實際去加總的結果BEMS每個的合都大於1(介於1.15~1.36之間)，看起來不像誤差。我在生成台灣繁體版的機率表時有確認過，以python浮點數計算的精確度是不可能造成這麼大的誤差的。
    > 
    > #### 詞性標記
    > 
    > 要作嚴謹詞性研究的人千萬不要用結巴，從他的演算法很明顯可以看出，他一個詞只能有一個詞性，如：機車只能是名詞或形容詞的其中一個，不能在A句子中當名詞，B句子中當形容詞。[中研院的斷詞系統](http://ckipsvr.iis.sinica.edu.tw/)有提供多詞性，請去用這個。

### 模型的数据是如何生成的？

- [模型的数据是如何生成的？ · Issue #7 · fxsjy/jieba](https://github.com/fxsjy/jieba/issues/7)

    > @feriely ， 来源主要有两个，一个是网上能下载到的1998人民日报的切分语料还有一个msr的切分语料。另一个是我自己收集的一些txt小说，用ictclas把他们切分（可能有一定误差）。 然后用python脚本统计词频。
    > 
    > 要统计的主要有三个概率表：1)位置转换概率，即B（开头）,M（中间),E(结尾),S(独立成词）四种状态的转移概率；2）位置到单字的发射概率，比如P("和"|M)表示一个词的中间出现”和"这个字的概率；3) 词语以某种状态开头的概率，其实只有两种，要么是B，要么是S。
    > 
    > ---
    > 
    > 比如finalseg/prob_trans.py这个文件：
    > 
    > {'B': {'E': 0.8518218565181658, 'M': 0.14817814348183422},
    > 'E': {'B': 0.5544853051164425, 'S': 0.44551469488355755},
    > 'M': {'E': 0.7164487459986911, 'M': 0.2835512540013088},
    > 'S': {'B': 0.48617017333894563, 'S': 0.5138298266610544}}
    > 
    > P(E|B) = 0.851, P(M|B) = 0.149，说明当我们处于一个词的开头时，下一个字是结尾的概率要远高于下一个字是中间字的概率，符合我们的直觉，因为二个字的词比多个字的词更常见。
    > 
    > ---
    > 
    > BMES法是从BE改进的，M$又用到了BB1B2MES 6元法。 state越多效果越好。
    > 但我感觉state还是不够。只用几个state是把大量样本信息模糊化了, 反过来用viterbi估计分词，是去马赛克，效果不会多好。为什么不用更多的states呢？自然理解的话，O：sentence。 States：word_cut。 这样state数是2*汉字个数。
    > 
    > ---
    > 
    > [@whille](https://github.com/whille) , 状态多一些使得分词更准确这一点我也赞同。其实，在jieba分词的词性标注子模块posseg中，就是将BMES四种状态和20集中词性做笛卡尔集得到所有的状态，最后的效果也的确比finalseg要好，尤其是人名识别方面，但是速度就严重下降了。<https://github.com/fxsjy/jieba/blob/master/jieba/posseg/prob_start.py>
    > 

### 新詞發現(HMM)

- [很赞怎么就拆不开呢 · Issue #512 · fxsjy/jieba](https://github.com/fxsjy/jieba/issues/512)

    > 因为jieba默认会对词典里不存在的词做识别，根据单字成词能力猜测。如果你要禁用，传入HMM=False即可
    > 
    > ---
    > 
    > 我关闭了HMM还是有case无法拆分。
    > 
    > ```
    >     s = '北京华宇腾飞工贸有限公司'
    >     for ss in jieba.cut(s, HMM=False):
    >           print ss,
    > 
    >    # 输出结果
    >    北京华 宇 腾飞 工贸 有限公司
    > 
    > ```
    > 
    > 确认过, 字典中已经没有北京华
    > 
    > ---
    > 
    > @baiziyuandyufei 添加华宇是可以的。
    > HMM关闭以后，不就不会识别新词了吗？为什么还是会把"北京"和"华" 放在一起呢？

### 二次分詞

- [Rokid/better_jieba: 用结巴(Jieba)轻松实现细粒度分词](https://github.com/Rokid/better_jieba)


### 結巴分詞詞性對照表

- [好玩的分詞——python jieba分詞模組的基本用法 - 掃文資訊](https://tw.saowen.com/a/4145f47a12884f749ce11a4b15ac990cab092c260232ab09ee0d76ce48158ebd)

    > 
    > 附：結巴分詞詞性對照表（按詞性英文首字母排序）
    > =======================
    > 
    > ### 形容詞(1個一類，4個二類)
    > 
    > a 形容詞
    > 
    > ad 副形詞
    > 
    > an 名形詞
    > 
    > ag 形容詞性語素
    > 
    > al 形容詞性慣用語
    > 
    > ### 區別詞(1個一類，2個二類)
    > 
    > b 區別詞
    > 
    > bl 區別詞性慣用語
    > 
    > ### 連詞(1個一類，1個二類)
    > 
    > c 連詞
    > 
    > cc 並列連詞
    > 
    > ### 副詞(1個一類)
    > 
    > d 副詞
    > 
    > ### 嘆詞(1個一類)
    > 
    > e 嘆詞
    > 
    > ### 方位詞(1個一類)
    > 
    > f 方位詞
    > 
    > ### 字首(1個一類)
    > 
    > h 字首
    > 
    > ### 字尾(1個一類)
    > 
    > k 字尾
    > 
    > ### 數詞(1個一類，1個二類)
    > 
    > m 數詞
    > 
    > mq 數量詞
    > 
    > ### 名詞 (1個一類，7個二類，5個三類)
    > 
    > 名詞分為以下子類：
    > 
    > n 名詞
    > 
    > nr 人名
    > 
    > nr1 漢語姓氏
    > 
    > nr2 漢語名字
    > 
    > nrj 日語人名
    > 
    > nrf 音譯人名
    > 
    > ns 地名
    > 
    > nsf 音譯地名
    > 
    > nt 機構團體名
    > 
    > nz 其它專名
    > 
    > nl 名詞性慣用語
    > 
    > ng 名詞性語素
    > 
    > ### 擬聲詞(1個一類)
    > 
    > o 擬聲詞
    > 
    > ### 介詞(1個一類，2個二類)
    > 
    > p 介詞
    > 
    > pba 介詞"把"
    > 
    > pbei 介詞"被"
    > 
    > ### 量詞(1個一類，2個二類)
    > 
    > q 量詞
    > 
    > qv 動量詞
    > 
    > qt 時量詞
    > 
    > ### 代詞(1個一類，4個二類，6個三類)
    > 
    > r 代詞
    > 
    > rr 人稱代詞
    > 
    > rz 指示代詞
    > 
    > rzt 時間指示代詞
    > 
    > rzs 處所指示代詞
    > 
    > rzv 謂詞性指示代詞
    > 
    > ry 疑問代詞
    > 
    > ryt 時間疑問代詞
    > 
    > rys 處所疑問代詞
    > 
    > ryv 謂詞性疑問代詞
    > 
    > rg 代詞性語素
    > 
    > ### 處所詞(1個一類)
    > 
    > s 處所詞
    > 
    > ### 時間詞(1個一類，1個二類)
    > 
    > t 時間詞
    > 
    > tg 時間詞性語素
    > 
    > ### 助詞(1個一類，15個二類)
    > 
    > u 助詞
    > 
    > uzhe 著
    > 
    > ule 了 嘍
    > 
    > uguo 過
    > 
    > ude1 的 底
    > 
    > ude2 地
    > 
    > ude3 得
    > 
    > usuo 所
    > 
    > udeng 等 等等 云云
    > 
    > uyy 一樣 一般 似的 般
    > 
    > udh 的話
    > 
    > uls 來講 來說 而言 說來
    > 
    > uzhi 之
    > 
    > ulian 連 （"連小學生都會"）
    > 
    > ### 動詞(1個一類，9個二類)
    > 
    > v 動詞
    > 
    > vd 副動詞
    > 
    > vn 名動詞
    > 
    > vshi 動詞"是"
    > 
    > vyou 動詞"有"
    > 
    > vf 趨向動詞
    > 
    > vx 形式動詞
    > 
    > vi 不及物動詞（內動詞）
    > 
    > vl 動詞性慣用語
    > 
    > vg 動詞性語素
    > 
    > ### 標點符號(1個一類，16個二類)
    > 
    > w 標點符號
    > 
    > wkz 左括號，全形：（ 〔 ［ ｛ 《 【 〖 〈 半形：( [ { <
    > 
    > wky 右括號，全形：） 〕 ］ ｝ 》 】 〗 〉 半形： ) ] { >
    > 
    > wyz 左引號，全形：" ' 『
    > 
    > wyy 右引號，全形：" ' 』
    > 
    > wj 句號，全形：。
    > 
    > ww 問號，全形：？ 半形：?
    > 
    > wt 歎號，全形：！ 半形：!
    > 
    > wd 逗號，全形：， 半形：,
    > 
    > wf 分號，全形：； 半形： ;
    > 
    > wn 頓號，全形：、
    > 
    > wm 冒號，全形：： 半形： :
    > 
    > ws 省略號，全形：...... ...
    > 
    > wp 破折號，全形：------ －－ ------－ 半形：--- ----
    > 
    > wb 百分號千分號，全形：％ ‰ 半形：%
    > 
    > wh 單位符號，全形：￥ ＄ ￡ ° ℃ 半形：$
    > 
    > ### 字串(1個一類，2個二類)
    > 
    > x 字串
    > 
    > xx 非語素字
    > 
    > xu 網址URL
    > 
    > ### 語氣詞(1個一類)
    > 
    > y 語氣詞(delete yg)
    > 
    > ### 狀態詞(1個一類)
    > 
    > z 狀態詞
    > 

### jieba分词支持关键词带空格和特殊字符

- [jieba分词支持关键词带空格和特殊字符 - CSDN博客](https://blog.csdn.net/wangpei1949/article/details/57077007)

    >  > 在默认情况下，当关键词中带有空格或特殊字符，如World Economic Forum，用jieba分词，会将该词分成三个词：World，Economic，Forum。
    > 
    > ```py
    > #默认jieba：
    > import jieba
    > jieba.load_userdict("dict/userDict")
    > str1="People's Republic of China is the only legitimate 　government in China."
    > str2="World Economic Forum"
    > str3="Edu Trust认证"
    > for w in jieba.cut(str2):
    >     print w+" ",
    > #People  '  s     Republic     of     China     is     the     only     legitimate     government     in     China  .
    > #World     Economic     Forum
    > #Edu     Trust  认证
    > ```
    > 
    >  > 中英文中存在大量这样的专有名词。这种方式有时并不符合我们的需求。我们可以通过改jieba包**init**.py中几个正则表达式来解决这个问题。用户词典中词词性用@@分隔。\
    >  >\
    >  > 　　1\. 搜索\
    >  > 　　　```
    >  > 　　　re_han_default = re.compile("([\u4E00-\u9FD5a-zA-Z0-9+#&._]+)", re.U)
    >  > 　　　```
    >  > 　　　改成\
    >  > 　　　```
    >  > 　　　re_han_default = re.compile("(.+)", re.U)
    >  > 　　　```
    >  >\
    >  > 　　2\. 搜索\
    >  > 　　　```
    >  > 　　　re_userdict = re.compile('^(.+?)( [0-9]+)?( [a-z]+)?＄', re.U)
    >  > 　　　```
    >  > 　　　改成\
    >  > 　　　```
    >  > 　　　re_userdict = re.compile('^(.+?)(\u0040\u0040[0-9]+)?(\u0040\u0040[a-z]+)?$', re.U)
    >  > 　　　```
    >  >\
    >  > 　　3\. 搜索\
    >  > 　　　```
    >  > 　　　word, freq = line.split(' ')[:2]
    >  > 　　　```
    >  > 　　　改成\
    >  > 　　　```
    >  > 　　　word, freq = line.split('\u0040\u0040')[:2]
    >  > 　　　```
    >  >\
    >  > 　　4.补充：若用全模式继续改。\
    >  > 　　　搜索\
    >  > 　　　```
    >  > 　　　re_han_cut_all = re.compile("([\u4E00-\u9FD5]+)", re.U)
    >  > 　　　```
    >  > 　　　改成\
    >  > 　　　```
    >  > 　　　re_han_cut_all = re.compile("(.+)", re.U)
    >  > 　　　```
    > 
    > ```
    > #修改后测试
    > userDict：
    > 中华人民共和国@@n
    > People's Republic of China@@n
    > World Economic Forum@@n
    > 世界经济论坛@@n
    > 结果：
    > People's Republic of China     is     the     only     legitimate     government     in     China  .
    > World Economic Forum
    > Edu Trust认证
    > ```
    > 
    > [支持英文专有名词的jieba 　**init**.py](http://download.csdn.net/detail/wangpei1949/9759794)
    > 
    > ---
    > 
    > 软件重启一下就可以了，感谢博主





## HuhuSeg

- [Colearo/HuhuSeg: Simple Chinese segmentator, keywords extractor and other examples](https://github.com/Colearo/HuhuSeg)

    > 在TF-IDF实现的关键词提取之外，这里还实现了基于TextRank[5]的提取算法，不依赖于庞大的IDF模型，而是试图在文本中词语的共现关系图里找到被Rank最高的词语。当然，除此之外，这里的代码实现的关键词提取还使用了一个小trick，在通过TextRank提取完关键词之后，会再次扫描文本，找到top关键词中是否有邻接词可以组成短语


## Google Natural Language API

- [Machine Learning(二)：Natural Language API 介紹與實作 | GCP專門家](https://blog.gcp.expert/machine-learning-natural-language-api/)

    > ### **快速測試**
    > 
    > 首先，我們先連到 Natural Language API 的[連結](https://cloud.google.com/natural-language/?hl=zh-tw)。一如 Google 的風格，網頁非常的平易近人，馬上就給了個 API 快速測試的按鈕。廢話不多說，我們就趕緊來試試。
    > 
    > ![螢幕快照 2017-11-16 下午11.53.48.png](https://noootown.files.wordpress.com/2017/11/e89ea2e5b995e5bfabe785a7-2017-11-16-e4b88be58d8811-53-48.png?w=663)

- [[译] 使用Natural Language API分析文本的实体与情感 - 学习笔记 - SegmentFault 思否](https://segmentfault.com/a/1190000014216330)




## sentencepiece

- [google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation.](https://github.com/google/sentencepiece)

    > ### What is SentencePiece?
    > 
    > SentencePiece is a re-implementation of **sub-word units**, an effective way to alleviate the open vocabulary problems in neural machine translation. SentencePiece supports two segmentation algorithms, **byte-pair-encoding (BPE)** [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)] and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)]. Here are the high level differences from other implementations.





